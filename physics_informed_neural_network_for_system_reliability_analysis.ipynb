{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import os \n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PINN neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(number_hidden_layers = 2, num_neurons_per_layer = 50):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(1))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(num_neurons_per_layer, activation=tf.keras.activations.get('tanh'), \n",
    "                                    kernel_initializer='glorot_normal'))\n",
    "    model.add(tf.keras.layers.Dense(num_neurons_per_layer, activation=tf.keras.activations.get('tanh'), \n",
    "                                    kernel_initializer='glorot_normal'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(3, activation=tf.keras.activations.get('softmax')))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# A basic PINN Tensorflow class for solving a continuous-time in-homogeneous Markov chains\n",
    "class PINN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, X_u, Y_u, X_r, model):\n",
    "        \n",
    "        ######################################################################################\n",
    "        # Normalization constants\n",
    "        self.mu_x, self.sigma_x = X_r.mean(0), X_r.std(0)\n",
    "\n",
    "        # Normalize inputs\n",
    "        X_u = (X_u - self.mu_x)/self.sigma_x\n",
    "        X_r = (X_r - self.mu_x)/self.sigma_x\n",
    "        self.N_u = X_u.shape[0]\n",
    "        self.N_r = X_r.shape[0]\n",
    "\n",
    "        # Store data                \n",
    "        self.X_u = X_u\n",
    "        self.Y_u = Y_u\n",
    "        self.X_r = X_r\n",
    "\n",
    "        self.Xu_tf = tf.convert_to_tensor(X_u, dtype=tf.float32)\n",
    "        self.Yu_tf = tf.convert_to_tensor(Y_u, dtype=tf.float32)\n",
    "        self.Xr_tf = tf.convert_to_tensor(X_r, dtype=tf.float32)\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.iter = 0\n",
    "        \n",
    "    def get_r(self):\n",
    "        with tf.GradientTape(persistent=True) as g:\n",
    "            g.watch(self.Xr_tf)\n",
    "            \n",
    "            u = self.model(self.Xr_tf)\n",
    "            \n",
    "            u1 = u[:,0]\n",
    "            u2 = u[:,1]\n",
    "            u3 = u[:,2]\n",
    "            \n",
    "        u_x_1 = g.jacobian(u1, self.Xr_tf)[0]/self.sigma_x\n",
    "        u_x_2 = g.jacobian(u2, self.Xr_tf)[0]/self.sigma_x\n",
    "        u_x_3 = g.jacobian(u3, self.Xr_tf)[0]/self.sigma_x\n",
    "        \n",
    "        #calculate residuals\n",
    "        residual_1 = u_x_1-(-1.286e-4*u1)\n",
    "        residual_2 = u_x_2-(5.6e-5*u1-1.006e-4*u2)\n",
    "        residual_3 = u_x_3-(7.26e-5*u1+1.006e-4*u2)\n",
    "        \n",
    "        #total residual\n",
    "        residual = tf.reduce_mean(tf.square(residual_1)) + tf.reduce_mean(tf.square(residual_2)) + \\\n",
    "        tf.reduce_mean(tf.square(residual_3))\n",
    "        \n",
    "        loss_1 = tf.reduce_mean(tf.square(residual_1))\n",
    "        loss_2 = tf.reduce_mean(tf.square(residual_2))\n",
    "        loss_3 = tf.reduce_mean(tf.square(residual_3))\n",
    "        \n",
    "        del g\n",
    "        \n",
    "        return loss_1, loss_2, loss_3\n",
    "    \n",
    "    @tf.function\n",
    "    def loss_fn(self, weight = 1):\n",
    "        u_pred = self.model(self.Xu_tf)     \n",
    "\n",
    "        # Evaluate loss\n",
    "        loss_u = tf.reduce_mean(tf.square(self.Yu_tf[:, 0] - u_pred[:, 0]))\n",
    "        loss_1, loss_2, loss_3 = self.get_r()\n",
    "        \n",
    "        return [loss_u, loss_1, loss_2, loss_3]\n",
    "    \n",
    "    \n",
    "    def get_grad(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_fn()\n",
    "        \n",
    "        g = tape.gradient(loss, self.model.trainable_variables)\n",
    "\n",
    "        return loss, g\n",
    "    \n",
    "    def get_grad_by_PCG(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_fn()\n",
    "            \n",
    "            assert type(loss) is list\n",
    "            loss = tf.stack(loss)\n",
    "            tf.random.shuffle(loss)\n",
    "\n",
    "            grads_task = tf.vectorized_map(lambda x: tf.concat([tf.reshape(grad, [-1,]) \n",
    "                                for grad in tape.gradient(x, self.model.trainable_variables)\n",
    "                                if grad is not None], axis=0), loss)\n",
    "        \n",
    "        num_tasks = len(loss)\n",
    "\n",
    "        # Compute gradient projections.\n",
    "        def proj_grad(grad_task):\n",
    "            for k in range(num_tasks):\n",
    "                inner_product = tf.reduce_sum(grad_task*grads_task[k])\n",
    "                proj_direction = inner_product / tf.reduce_sum(grads_task[k]*grads_task[k])\n",
    "                grad_task = grad_task - tf.minimum(proj_direction, 0.) * grads_task[k]\n",
    "            return grad_task\n",
    "\n",
    "        proj_grads_flatten = tf.vectorized_map(proj_grad, grads_task)\n",
    "\n",
    "        # Unpack flattened projected gradients back to their original shapes.\n",
    "        proj_grads = []\n",
    "        for j in range(num_tasks):\n",
    "            start_idx = 0\n",
    "            for idx, var in enumerate(self.model.trainable_variables):\n",
    "                grad_shape = var.get_shape()\n",
    "                flatten_dim = np.prod([grad_shape.dims[i].value for i in range(len(grad_shape.dims))])\n",
    "                proj_grad = proj_grads_flatten[j][start_idx:start_idx+flatten_dim]\n",
    "                proj_grad = tf.reshape(proj_grad, grad_shape)\n",
    "                if len(proj_grads) < len(self.model.trainable_variables):\n",
    "                    proj_grads.append(proj_grad)\n",
    "                else:\n",
    "                    proj_grads[idx] += proj_grad               \n",
    "                start_idx += flatten_dim\n",
    "\n",
    "        grads_and_vars = list(zip(proj_grads, self.model.trainable_variables))\n",
    "        \n",
    "        del tape\n",
    "        return loss, proj_grads\n",
    "    \n",
    "    def callback(self):\n",
    "        if self.iter % 5 == 0:\n",
    "            print('Iteration {:05d}: loss = {}'.format(self.iter, ','.join(map(str, self.current_loss))))\n",
    "        self.iter += 1\n",
    "    \n",
    "    def train(self, N, optimizer, method):\n",
    "        \"\"\"This method performs a gradient descent type optimization.\"\"\"\n",
    "        \n",
    "        @tf.function\n",
    "        def train_step():\n",
    "            if method == 'original':\n",
    "                loss, grad_theta = self.get_grad()\n",
    "                \n",
    "            if method == 'PCG_gradient':\n",
    "                loss, grad_theta = self.get_grad_by_PCG()\n",
    "            \n",
    "            # Perform gradient descent step\n",
    "            optimizer.apply_gradients(zip(grad_theta, self.model.trainable_variables))\n",
    "            return loss\n",
    "        \n",
    "        for i in range(N):\n",
    "            \n",
    "            loss = train_step()\n",
    "            self.current_loss = tf.convert_to_tensor(loss).numpy()\n",
    "            self.callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# Number of training data\n",
    "N_u = 1                          # Boundary condition data on u(x)  \n",
    "N_r = 1000                        # Number of collocation points for minimizing the PDE residual\n",
    "lb  = np.array([0.0])            # Left boundary of the domain\n",
    "ub  = np.array([60000.0])        # Right boundary of the domain\n",
    "\n",
    "# Generate training data\n",
    "x_u = np.array([[0]])  ##TZ\n",
    "y_u = np.array([[1,0,0]])   ##TZ                    # Solution at boundary points (dimension N_u x 1)\n",
    "x_r = np.linspace(lb, ub, N_r)     # Location of collocation points (dimension N_r x 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# Test data for validating the model predictions\n",
    "n_star = 5000+1\n",
    "x_star = np.linspace(lb, ub, n_star) #N_star = x_star.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN_solver = PINN(x_u, y_u, x_r, init_model())\n",
    "initial_weights = PINN_solver.model.get_weights()\n",
    "#initial_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 00000: loss = 0.66330856,2.0703956e-09,2.1764046e-10,3.311472e-09\n",
      "Iteration 00005: loss = 0.4044222,2.0667874e-09,1.8826952e-10,3.4245322e-09\n",
      "Iteration 00010: loss = 0.17123163,2.6809184e-09,3.3949543e-10,3.557772e-09\n",
      "Iteration 00015: loss = 0.05464825,3.6558965e-09,6.4399047e-10,3.6765484e-09\n"
     ]
    }
   ],
   "source": [
    "optim = tf.keras.optimizers.Adam()\n",
    "PINN_solver.train(N=20, optimizer=optim, method = 'original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN_solver = PINN(x_u, y_u, x_r, init_model())\n",
    "PINN_solver.model.set_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PINN_solver.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 00000: loss = 0.66330856,2.0703956e-09,2.1764046e-10,3.311472e-09\n",
      "Iteration 00005: loss = 0.40704948,2.0526476e-09,1.7052787e-10,3.327413e-09\n",
      "Iteration 00010: loss = 0.19135009,2.5280582e-09,2.5054064e-10,3.2718956e-09\n"
     ]
    }
   ],
   "source": [
    "optim = tf.keras.optimizers.Adam()\n",
    "PINN_solver.train(N=20, optimizer=optim, method = 'PCG_gradient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.6)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "027e948c137d6acde9006b592cf7cd1a70a2e52a69a7472c7b58352c83d453a1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
