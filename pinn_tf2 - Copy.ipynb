{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import os \n",
    "import seaborn as sns\n",
    "import random\n",
    "import PCGrad_tf2 as PCGrad\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PINN neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(number_hidden_layers = 2, num_neurons_per_layer = 50):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(1))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(num_neurons_per_layer, activation=tf.keras.activations.get('tanh'), \n",
    "                                    kernel_initializer='glorot_normal'))\n",
    "    model.add(tf.keras.layers.Dense(num_neurons_per_layer, activation=tf.keras.activations.get('tanh'), \n",
    "                                    kernel_initializer='glorot_normal'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(3, activation=tf.keras.activations.get('softmax')))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# A basic PINN Tensorflow class for solving a continuous-time in-homogeneous Markov chains\n",
    "class PINN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, X_u, Y_u, X_r, model):\n",
    "        \n",
    "        ######################################################################################\n",
    "        # Normalization constants\n",
    "        self.mu_x, self.sigma_x = X_r.mean(0), X_r.std(0)\n",
    "\n",
    "        # Normalize inputs\n",
    "        X_u = (X_u - self.mu_x)/self.sigma_x\n",
    "        X_r = (X_r - self.mu_x)/self.sigma_x\n",
    "        self.N_u = X_u.shape[0]\n",
    "        self.N_r = X_r.shape[0]\n",
    "\n",
    "        # Store data                \n",
    "        self.X_u = X_u\n",
    "        self.Y_u = Y_u\n",
    "        self.X_r = X_r\n",
    "\n",
    "        self.Xu_tf = tf.convert_to_tensor(X_u, dtype=tf.float32)\n",
    "        self.Yu_tf = tf.convert_to_tensor(Y_u, dtype=tf.float32)\n",
    "        self.Xr_tf = tf.convert_to_tensor(X_r, dtype=tf.float32)\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.iter = 0\n",
    "        \n",
    "    def get_r(self):\n",
    "        with tf.GradientTape(persistent=True) as g:\n",
    "            g.watch(self.Xr_tf)\n",
    "            \n",
    "            u = self.model(self.Xr_tf)\n",
    "            \n",
    "            u1 = u[:,0]\n",
    "            u2 = u[:,1]\n",
    "            u3 = u[:,2]\n",
    "            \n",
    "        u_x_1 = g.jacobian(u1, self.Xr_tf)[0]/self.sigma_x\n",
    "        u_x_2 = g.jacobian(u2, self.Xr_tf)[0]/self.sigma_x\n",
    "        u_x_3 = g.jacobian(u3, self.Xr_tf)[0]/self.sigma_x\n",
    "        \n",
    "        #calculate residuals\n",
    "        residual_1 = u_x_1-(-1.286e-4*u1)\n",
    "        residual_2 = u_x_2-(5.6e-5*u1-1.006e-4*u2)\n",
    "        residual_3 = u_x_3-(7.26e-5*u1+1.006e-4*u2)\n",
    "        \n",
    "        #total residual\n",
    "        residual = tf.reduce_mean(tf.square(residual_1)) + tf.reduce_mean(tf.square(residual_2)) + \\\n",
    "        tf.reduce_mean(tf.square(residual_3))\n",
    "        \n",
    "        loss_1 = tf.reduce_mean(tf.square(residual_1))\n",
    "        loss_2 = tf.reduce_mean(tf.square(residual_2))\n",
    "        loss_3 = tf.reduce_mean(tf.square(residual_3))\n",
    "        \n",
    "        return loss_1, loss_2, loss_3\n",
    "    \n",
    "    def loss_fn(self, weight = 1):\n",
    "        u_pred = self.model(self.Xu_tf)     \n",
    "\n",
    "        # Evaluate loss\n",
    "        loss_u = tf.reduce_mean(tf.square(self.Yu_tf[:, 0] - u_pred[:, 0]))\n",
    "        loss_1, loss_2, loss_3 = self.get_r()\n",
    "        \n",
    "        return [loss_u, loss_1 + loss_2 + loss_3]\n",
    "    \n",
    "    def get_grad(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # This tape is for derivatives with\n",
    "            # respect to trainable variables\n",
    "            #tape.watch(self.model.trainable_variables)\n",
    "            loss = self.loss_fn()\n",
    "            \n",
    "        g = tape.gradient(loss, self.model.trainable_variables)\n",
    "        del tape\n",
    "        \n",
    "        return loss, g\n",
    "    \n",
    "    def callback(self):\n",
    "        if self.iter % 50 == 0:\n",
    "            print('It {:05d}: loss = {}'.format(self.iter, ','.join(map(str, self.current_loss))))\n",
    "        self.iter += 1\n",
    "    \n",
    "    def train(self, N, optimizer):\n",
    "        \"\"\"This method performs a gradient descent type optimization.\"\"\"\n",
    "        \n",
    "        @tf.function\n",
    "        def train_step():\n",
    "            loss, grad_theta = self.get_grad()\n",
    "            print (loss, grad_theta)\n",
    "            \n",
    "            # Perform gradient descent step\n",
    "            optimizer.apply_gradients(zip(grad_theta, self.model.trainable_variables))\n",
    "            return loss\n",
    "        \n",
    "        for i in range(N):\n",
    "            \n",
    "            loss = train_step()\n",
    "            \n",
    "            self.current_loss = loss\n",
    "            self.callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# Number of training data\n",
    "N_u = 1                          # Boundary condition data on u(x)  \n",
    "N_r = 500                     # Number of collocation points for minimizing the PDE residual\n",
    "lb  = np.array([0.0])            # Left boundary of the domain\n",
    "ub  = np.array([60000.0])        # Right boundary of the domain\n",
    "\n",
    "# Generate training data\n",
    "x_u = np.array([[0]])  ##TZ\n",
    "y_u = np.array([[1,0,0]])   ##TZ                    # Solution at boundary points (dimension N_u x 1)\n",
    "x_r = np.linspace(lb, ub, N_r)     # Location of collocation points (dimension N_r x 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# Test data for validating the model predictions\n",
    "n_star = 5000+1\n",
    "x_star = np.linspace(lb, ub, n_star) #N_star = x_star.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    }
   ],
   "source": [
    "# Compute per-task gradients.\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    model = init_model()\n",
    "\n",
    "    PINN_solver = PINN(x_u, y_u, x_r, model)\n",
    "\n",
    "    loss, grad_theta = PINN_solver.get_grad()\n",
    "    \n",
    "    assert type(loss) is list\n",
    "    num_tasks = len(loss)\n",
    "    loss = tf.stack(loss)\n",
    "    tf.random.shuffle(loss)\n",
    "    \n",
    "    grads_task = tf.vectorized_map(lambda x: tf.concat([tf.reshape(grad, [-1,]) \n",
    "                        for grad in tape.gradient(x, PINN_solver.model.trainable_variables)\n",
    "                        if grad is not None], axis=0), loss)\n",
    "    \n",
    "    # Compute gradient projections.\n",
    "    def proj_grad(grad_task):\n",
    "        for k in range(num_tasks):\n",
    "            inner_product = tf.reduce_sum(grad_task*grads_task[k])\n",
    "            proj_direction = inner_product / tf.reduce_sum(grads_task[k]*grads_task[k])\n",
    "            grad_task = grad_task - tf.minimum(proj_direction, 0.) * grads_task[k]\n",
    "        return grad_task\n",
    "\n",
    "    proj_grads_flatten = tf.vectorized_map(proj_grad, grads_task)\n",
    "\n",
    "    # Unpack flattened projected gradients back to their original shapes.\n",
    "    proj_grads = []\n",
    "    for j in range(num_tasks):\n",
    "        start_idx = 0\n",
    "        for idx, var in enumerate(PINN_solver.model.trainable_variables):\n",
    "            grad_shape = var.get_shape()\n",
    "            flatten_dim = np.prod([grad_shape.dims[i].value for i in range(len(grad_shape.dims))])\n",
    "            proj_grad = proj_grads_flatten[j][start_idx:start_idx+flatten_dim]\n",
    "            proj_grad = tf.reshape(proj_grad, grad_shape)\n",
    "            if len(proj_grads) < len(PINN_solver.model.trainable_variables):\n",
    "                proj_grads.append(proj_grad)\n",
    "            else:\n",
    "                proj_grads[idx] += proj_grad               \n",
    "            start_idx += flatten_dim\n",
    "\n",
    "    grads_and_vars = list(zip(proj_grads, PINN_solver.model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(1, 50), dtype=float32, numpy=\n",
       "  array([[ 0.06028628,  0.07499516,  0.21136032,  0.1017174 ,  0.10001646,\n",
       "           0.2180185 ,  0.15797585, -0.07003554, -0.00389981,  0.01137009,\n",
       "          -0.02241446, -0.09091282,  0.05556903,  0.10871845, -0.1239192 ,\n",
       "           0.01105038,  0.03507635, -0.19280973, -0.05704638,  0.0355982 ,\n",
       "           0.19094542, -0.22936013,  0.05067914, -0.11530754, -0.06230346,\n",
       "           0.02353636,  0.10330629, -0.02671034,  0.11104937, -0.17702961,\n",
       "           0.01933922, -0.04847102,  0.13104624,  0.05552543, -0.08177055,\n",
       "           0.0620217 , -0.00392139,  0.06948322, -0.04809517,  0.11449086,\n",
       "          -0.02711431,  0.10394347, -0.08001107, -0.02420769,  0.09115085,\n",
       "           0.01744652,  0.04111838,  0.05131472, -0.01880261,  0.01556524]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Variable 'dense_9/kernel:0' shape=(1, 50) dtype=float32, numpy=\n",
       "  array([[ 0.18140835,  0.05257547,  0.02610554,  0.00804662, -0.10092922,\n",
       "          -0.00568269, -0.11936078, -0.15548316,  0.02723309,  0.13006973,\n",
       "           0.27861303, -0.00535698, -0.06083969, -0.14861341,  0.1135246 ,\n",
       "          -0.19656196, -0.27391422, -0.1501329 , -0.16094452,  0.02686519,\n",
       "           0.05838255,  0.2212764 , -0.13672349, -0.08281358, -0.3390872 ,\n",
       "          -0.1511225 ,  0.11383723,  0.12171739, -0.03756246,  0.19285697,\n",
       "           0.44000477,  0.06927773,  0.00698908, -0.3499433 , -0.2515237 ,\n",
       "           0.37854722,  0.23465386, -0.04498102,  0.18777792, -0.16872996,\n",
       "          -0.0422953 , -0.04068026, -0.12935066, -0.20354348,  0.18806544,\n",
       "           0.38030162,  0.04112205, -0.21014641, -0.03086594,  0.3952109 ]],\n",
       "        dtype=float32)>),\n",
       " (<tf.Tensor: shape=(50,), dtype=float32, numpy=\n",
       "  array([-0.0032432 , -0.02669468, -0.05337528, -0.08075193, -0.10444561,\n",
       "         -0.04786786,  0.00088099, -0.07097092,  0.02473166, -0.01889683,\n",
       "          0.02851626,  0.02021062, -0.02641631, -0.02222029,  0.09268381,\n",
       "          0.06193147,  0.03045335,  0.06812163, -0.02568858,  0.01808152,\n",
       "         -0.07289058,  0.16821826, -0.0242204 ,  0.03161636,  0.07014698,\n",
       "         -0.07216993, -0.00512091,  0.00805485, -0.01040356,  0.07677138,\n",
       "         -0.00790629,  0.03127725, -0.04914855,  0.002669  ,  0.07466856,\n",
       "         -0.09179902,  0.00371755, -0.03853477,  0.03875445, -0.07589547,\n",
       "          0.03558636, -0.06218323,  0.0092613 ,  0.02621534, -0.00244379,\n",
       "         -0.0301744 ,  0.01700524,  0.01022431,  0.02426296,  0.02245681],\n",
       "        dtype=float32)>,\n",
       "  <tf.Variable 'dense_9/bias:0' shape=(50,) dtype=float32, numpy=\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        dtype=float32)>),\n",
       " (<tf.Tensor: shape=(50, 50), dtype=float32, numpy=\n",
       "  array([[ 0.01004069,  0.0012332 ,  0.00549442, ...,  0.02789136,\n",
       "           0.00297262, -0.03040367],\n",
       "         [ 0.00299786,  0.00036941,  0.00163966, ...,  0.00832678,\n",
       "           0.00088699, -0.009077  ],\n",
       "         [ 0.00149167,  0.00018386,  0.00081583, ...,  0.0041432 ,\n",
       "           0.00044133, -0.0045165 ],\n",
       "         ...,\n",
       "         [-0.01150637, -0.00141151, -0.00629761, ..., -0.03196387,\n",
       "          -0.00340731,  0.03484274],\n",
       "         [-0.0017632 , -0.00021732, -0.00096434, ..., -0.00489739,\n",
       "          -0.00052167,  0.00533864],\n",
       "         [ 0.01959156,  0.00237639,  0.01074079, ...,  0.05444118,\n",
       "           0.00581364, -0.05934036]], dtype=float32)>,\n",
       "  <tf.Variable 'dense_10/kernel:0' shape=(50, 50) dtype=float32, numpy=\n",
       "  array([[ 0.00126202,  0.00411532, -0.04468515, ..., -0.30272514,\n",
       "          -0.10435452, -0.10355286],\n",
       "         [ 0.01661035,  0.08189215, -0.07162736, ...,  0.08760697,\n",
       "          -0.09806865, -0.06132863],\n",
       "         [-0.04999831, -0.2051802 ,  0.0381561 , ...,  0.03589086,\n",
       "           0.23461884,  0.0316097 ],\n",
       "         ...,\n",
       "         [-0.08516819,  0.1413647 , -0.07970397, ...,  0.00172674,\n",
       "          -0.29658085, -0.0571258 ],\n",
       "         [-0.26597413,  0.02550414,  0.08865622, ...,  0.00229604,\n",
       "           0.04672238, -0.1644337 ],\n",
       "         [ 0.27428317,  0.15356915, -0.14715913, ...,  0.01918871,\n",
       "          -0.00379388,  0.11216293]], dtype=float32)>),\n",
       " (<tf.Tensor: shape=(50,), dtype=float32, numpy=\n",
       "  array([-0.04893754, -0.0701394 ,  0.01793029, -0.02384438, -0.0593347 ,\n",
       "          0.00344305, -0.09310313,  0.0092873 , -0.04708264,  0.00258038,\n",
       "          0.00767836,  0.03849253, -0.04728588, -0.0133369 ,  0.01605723,\n",
       "          0.04491314, -0.0038179 ,  0.04337058,  0.02355165,  0.00447612,\n",
       "          0.05124035, -0.03389154, -0.01416006, -0.01407291,  0.05471915,\n",
       "         -0.00738451, -0.06545985,  0.08741388,  0.01269515, -0.03828286,\n",
       "          0.08075864, -0.06302072,  0.04769866, -0.01127619, -0.05452577,\n",
       "         -0.07401049,  0.08040518,  0.08902889,  0.07345374, -0.04452194,\n",
       "         -0.00765496, -0.09378841,  0.00156917,  0.10833653,  0.00201414,\n",
       "          0.04816664, -0.04670803, -0.0645988 ,  0.0152204 ,  0.102626  ],\n",
       "        dtype=float32)>,\n",
       "  <tf.Variable 'dense_10/bias:0' shape=(50,) dtype=float32, numpy=\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        dtype=float32)>),\n",
       " (<tf.Tensor: shape=(50, 3), dtype=float32, numpy=\n",
       "  array([[-0.02577974,  0.01287656,  0.01290318],\n",
       "         [ 0.03949212, -0.01966953, -0.01982259],\n",
       "         [-0.09418005,  0.04694539,  0.04723466],\n",
       "         [ 0.00134605, -0.00065631, -0.00068974],\n",
       "         [-0.07954155,  0.0396607 ,  0.03988086],\n",
       "         [-0.08711099,  0.043438  ,  0.043673  ],\n",
       "         [ 0.01337054, -0.00666796, -0.00670258],\n",
       "         [-0.12759343,  0.06360105,  0.06399239],\n",
       "         [-0.07906623,  0.03941985,  0.03964639],\n",
       "         [-0.0841906 ,  0.04197237,  0.04221823],\n",
       "         [ 0.02311614, -0.01151914, -0.011597  ],\n",
       "         [ 0.12783255, -0.06370952, -0.06412304],\n",
       "         [ 0.02729543, -0.01358922, -0.01370622],\n",
       "         [ 0.05964884, -0.02976942, -0.02987942],\n",
       "         [-0.05059628,  0.0252264 ,  0.02536988],\n",
       "         [-0.0183445 ,  0.00915179,  0.00919271],\n",
       "         [-0.16789223,  0.08364946,  0.08424281],\n",
       "         [ 0.08251262, -0.04115538, -0.04135725],\n",
       "         [-0.04772398,  0.02380102,  0.02392297],\n",
       "         [-0.03612957,  0.01803714,  0.01809244],\n",
       "         [-0.02844103,  0.01415017,  0.01429086],\n",
       "         [ 0.04067153, -0.0202705 , -0.02040104],\n",
       "         [-0.00546655,  0.00271359,  0.00275297],\n",
       "         [-0.11928897,  0.0594603 ,  0.05982868],\n",
       "         [-0.06788066,  0.03383716,  0.03404351],\n",
       "         [ 0.11709835, -0.05836646, -0.05873191],\n",
       "         [-0.03937383,  0.01963558,  0.01973826],\n",
       "         [-0.09000873,  0.04486769,  0.04514105],\n",
       "         [ 0.11917568, -0.05938118, -0.0597945 ],\n",
       "         [-0.12325791,  0.06143752,  0.06182041],\n",
       "         [-0.04734451,  0.02360716,  0.02373735],\n",
       "         [ 0.07793289, -0.03883648, -0.03909643],\n",
       "         [ 0.01283118, -0.0063906 , -0.00644058],\n",
       "         [ 0.05002168, -0.0249297 , -0.02509198],\n",
       "         [ 0.10642492, -0.05305047, -0.05337446],\n",
       "         [-0.06327163,  0.03154521,  0.03172643],\n",
       "         [ 0.03235742, -0.0161316 , -0.01622582],\n",
       "         [-0.04375581,  0.02181537,  0.02194044],\n",
       "         [ 0.00854881, -0.00427005, -0.00427875],\n",
       "         [-0.02206464,  0.01101513,  0.01104951],\n",
       "         [-0.05845281,  0.02915054,  0.02930227],\n",
       "         [ 0.01151344, -0.00572691, -0.00578653],\n",
       "         [ 0.1897556 , -0.09448512, -0.09527051],\n",
       "         [ 0.07275058, -0.03628273, -0.03646785],\n",
       "         [-0.10892014,  0.05429027,  0.05462988],\n",
       "         [ 0.02239816, -0.01117516, -0.011223  ],\n",
       "         [ 0.06614271, -0.03296145, -0.03318126],\n",
       "         [-0.15302914,  0.07624997,  0.0767792 ],\n",
       "         [ 0.09070883, -0.04521011, -0.04549873],\n",
       "         [-0.09656315,  0.04814035,  0.04842282]], dtype=float32)>,\n",
       "  <tf.Variable 'dense_11/kernel:0' shape=(50, 3) dtype=float32, numpy=\n",
       "  array([[-0.01599696, -0.24740854, -0.01619047],\n",
       "         [ 0.02756175, -0.26364553,  0.28867725],\n",
       "         [ 0.00631285,  0.05733243, -0.18448548],\n",
       "         [-0.14645489, -0.223336  ,  0.0763889 ],\n",
       "         [ 0.07769036, -0.2003793 ,  0.17308745],\n",
       "         [ 0.26281568,  0.20758632, -0.08626336],\n",
       "         [ 0.21228877, -0.26914817, -0.05982146],\n",
       "         [ 0.06807801,  0.18359312,  0.30012676],\n",
       "         [ 0.09925792, -0.1475036 ,  0.0428293 ],\n",
       "         [ 0.20709494,  0.18652996,  0.04243544],\n",
       "         [ 0.19330212,  0.20225444,  0.02357391],\n",
       "         [ 0.03671396,  0.3287504 ,  0.317032  ],\n",
       "         [-0.02267152, -0.2722662 , -0.1817444 ],\n",
       "         [ 0.20992664,  0.06003013, -0.25801456],\n",
       "         [-0.04761806,  0.0883339 ,  0.28941205],\n",
       "         [ 0.171635  ,  0.31390676, -0.26256555],\n",
       "         [-0.15079033, -0.03500143,  0.2148417 ],\n",
       "         [-0.19736563, -0.00360584, -0.31719434],\n",
       "         [-0.04830602,  0.07513946,  0.01143965],\n",
       "         [ 0.13608882,  0.09398198, -0.24466962],\n",
       "         [-0.10546486,  0.1329639 , -0.13214396],\n",
       "         [-0.20137899, -0.30131435,  0.16774407],\n",
       "         [-0.21519239, -0.22041105,  0.17325094],\n",
       "         [ 0.3003265 ,  0.16731396,  0.08164421],\n",
       "         [-0.03599319,  0.29672453,  0.30384508],\n",
       "         [-0.17281537, -0.19858655, -0.11616865],\n",
       "         [ 0.13528526, -0.16752411,  0.19167158],\n",
       "         [-0.17127043,  0.32401833,  0.0623543 ],\n",
       "         [ 0.03670216,  0.07981279, -0.0665935 ],\n",
       "         [ 0.01523706, -0.12639964,  0.2784473 ],\n",
       "         [-0.25547883,  0.13040304, -0.2672067 ],\n",
       "         [-0.02889493, -0.31499666,  0.1182521 ],\n",
       "         [-0.3289196 , -0.0445188 ,  0.04864174],\n",
       "         [ 0.03609756, -0.04388708, -0.11228766],\n",
       "         [ 0.01481438, -0.28211343, -0.02282369],\n",
       "         [ 0.09811872, -0.27230334,  0.05134818],\n",
       "         [-0.11028819,  0.28109607, -0.05119014],\n",
       "         [-0.33311665,  0.16363713,  0.08240506],\n",
       "         [-0.14159153,  0.25890067,  0.20675513],\n",
       "         [-0.13474956, -0.30093804,  0.13834122],\n",
       "         [ 0.13622007,  0.03335854, -0.22919552],\n",
       "         [ 0.30466512, -0.14388676,  0.25864634],\n",
       "         [-0.06899026,  0.08208516,  0.22828642],\n",
       "         [-0.3135864 ,  0.29196104,  0.04499507],\n",
       "         [-0.02256861, -0.01337028, -0.02890688],\n",
       "         [ 0.04470241,  0.23309764, -0.19760877],\n",
       "         [ 0.09484401, -0.12269536,  0.15863535],\n",
       "         [ 0.21213618, -0.30704752, -0.15889183],\n",
       "         [-0.11189337, -0.0607087 , -0.23791197],\n",
       "         [-0.27068573,  0.30542174, -0.06316942]], dtype=float32)>),\n",
       " (<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.1792546 ,  0.21283524, -0.03358063], dtype=float32)>,\n",
       "  <tf.Variable 'dense_11/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_and_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optim = tf.keras.optimizers.Adam()\n",
    "optim = PCGrad(tf.keras.optimizers.Adam())\n",
    "#PINN_solver.train(N=40000, optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.6)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "027e948c137d6acde9006b592cf7cd1a70a2e52a69a7472c7b58352c83d453a1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
