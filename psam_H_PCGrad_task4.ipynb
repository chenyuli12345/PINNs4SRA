{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import os \n",
    "import seaborn as sns\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.training import optimizer\n",
    "from PCGrad import PCGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PINN neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# A basic PINN Tensorflow class for solving a continuous-time in-homogeneous Markov chains\n",
    "class HSMC:\n",
    "    # Initialize the class\n",
    "    def __init__(self, X_u, Y_u, X_r, layers_U, lam = 1.0, num_tasks=4):\n",
    "        \n",
    "        ######################################################################################\n",
    "        # Normalization constants\n",
    "        self.mu_x, self.sigma_x = X_r.mean(0), X_r.std(0)\n",
    "\n",
    "        # Normalize inputs\n",
    "        X_u = (X_u - self.mu_x)/self.sigma_x\n",
    "        X_r = (X_r - self.mu_x)/self.sigma_x\n",
    "        self.N_u = X_u.shape[0]\n",
    "        self.N_r = X_r.shape[0]\n",
    "\n",
    "        # Store data                \n",
    "        self.X_u = X_u\n",
    "        self.Y_u = Y_u\n",
    "        self.X_r = X_r\n",
    "\n",
    "        # Regularization parameters\n",
    "        self.lam = lam\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        ######################################################################################\n",
    "        # Neural network structures\n",
    "        self.layers_U = layers_U\n",
    "        #self.layers_S = layers_S\n",
    "        # Initialize network weights and biases\n",
    "        self.weights_U, self.biases_U = self.initialize_NN(layers_U)\n",
    "        \n",
    "        ######################################################################################\n",
    "        # Define Tensorflow session\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        # Define placeholders and computational graph\n",
    "        self.Xu_tf = tf.placeholder(tf.float32, shape=(None, self.X_u.shape[1]))\n",
    "        self.Yu_tf = tf.placeholder(tf.float32, shape=(None, self.Y_u.shape[1]))\n",
    "        self.Xr_tf = tf.placeholder(tf.float32, shape=(None, self.X_r.shape[1]))\n",
    "        \n",
    "        # Evaluate prediction\n",
    "        self.u_pred = self.net_u(self.Xu_tf)\n",
    "        self.r_pred = self.get_r(self.Xr_tf)\n",
    "        self.loss_1, self.loss_2, self.loss_3 = self.get_r(self.Xr_tf)        \n",
    "\n",
    "        # Evaluate loss\n",
    "        self.loss_u = tf.reduce_mean(tf.square(self.Yu_tf - self.u_pred))\n",
    "        self.loss_r = self.r_pred\n",
    "        \n",
    "         # Combine Loss Equal-Weighted\n",
    "        #self.loss = self.loss_u + self.lam *self.loss_r\n",
    "        \n",
    "        #PCGrad optimizer\n",
    "        self.optimizer = PCGrad(tf.train.AdamOptimizer()) # wrap your favorite optimizer\n",
    "        #self.loss = [self.loss_u, self.loss_r]# a list of per-task losses\n",
    "        self.loss = [self.loss_u, self.loss_1, self.loss_2, self.loss_3]# a list of per-task losses\n",
    "\n",
    "        assert len(self.loss) == self.num_tasks\n",
    "        \n",
    "        self.train_op = self.optimizer.minimize(self.loss,var_list=tf.nest.flatten([self.weights_U, self.biases_U]))\n",
    "        \n",
    "        \n",
    "        # Logger\n",
    "        self.loss_u_log = []\n",
    "        self.loss_r_log = []\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    \n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):      \n",
    "        # Xavier initialization\n",
    "        def xavier_init(size):\n",
    "            in_dim = size[0]\n",
    "            out_dim = size[1]\n",
    "            xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "            return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev, dtype=tf.float32)   \n",
    "        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "    \n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H, layers, weights, biases):\n",
    "        num_layers = len(layers)\n",
    "\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            \n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        H = tf.nn.softmax(tf.add(tf.matmul(H, W), b)) ##TZ  - use softmax in the last layer\n",
    "        return H\n",
    "    \n",
    "    # Forward pass for u\n",
    "    def net_u(self, x):\n",
    "        u = self.forward_pass(x,\n",
    "                              self.layers_U,\n",
    "                              self.weights_U,\n",
    "                              self.biases_U)\n",
    "        return u\n",
    "    \n",
    "    # Forward pass for f\n",
    "    def get_r(self, x):\n",
    "        #prediction for collocation points\n",
    "        u = self.net_u(x)\n",
    "        u1 = u[:,0:1]\n",
    "        u2 = u[:,1:2]\n",
    "        u3 = u[:,2:3]\n",
    "        \n",
    "        #derivatives of state probability\n",
    "        u_x_1 = tf.gradients(u1, x)[0]/self.sigma_x\n",
    "        u_x_2 = tf.gradients(u2, x)[0]/self.sigma_x\n",
    "        u_x_3 = tf.gradients(u3, x)[0]/self.sigma_x\n",
    "        \n",
    "        \n",
    "        #time stamp  \n",
    "        t_id = x*self.sigma_x + self.mu_x\n",
    "        \n",
    "        #calculate residuals\n",
    "        residual_1 = u_x_1-(-1.286e-4*u1)\n",
    "        residual_2 = u_x_2-(5.6e-5*u1-1.006e-4*u2)\n",
    "        residual_3 = u_x_3-(7.26e-5*u1+1.006e-4*u2)\n",
    "        \n",
    "        \n",
    "        #total residual\n",
    "        residual = tf.reduce_mean(tf.square(residual_1))+tf.reduce_mean(tf.square(residual_2))+\\\n",
    "        tf.reduce_mean(tf.square(residual_3))\n",
    "        \n",
    "        loss_1 = tf.reduce_mean(tf.square(residual_1))\n",
    "        loss_2 = tf.reduce_mean(tf.square(residual_2))\n",
    "        loss_3 = tf.reduce_mean(tf.square(residual_3))\n",
    "        \n",
    "        return loss_1, loss_2, loss_3\n",
    "\n",
    "    # Callback to print the loss at every optimization step\n",
    "    def callback(self, loss_u, loss_r):\n",
    "        print('Loss_u: %.3e, Loss_r: %.3e' % \n",
    "                      (loss_u, loss_r))\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss using Adam\n",
    "    def train(self, nIter = 10000): \n",
    "        \n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.Xu_tf: self.X_u, self.Yu_tf: self.Y_u,\n",
    "                   self.Xr_tf: self.X_r}\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        \n",
    "        for it in tqdm(range(nIter)):                 \n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "            \n",
    "            # Print\n",
    "            if it % 1000 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_u_value = self.sess.run(self.loss_u, tf_dict)\n",
    "                loss_r_value = self.sess.run(self.loss_r, tf_dict)\n",
    "                self.loss_u_log.append(loss_u_value)\n",
    "                self.loss_r_log.append(loss_r_value)\n",
    "                #print('It: %d, Loss: %.3e, Loss_u: %.3e, Loss_r: %.3e, Time: %.2f' % \n",
    "                #      (it, loss_value, loss_u_value, loss_r_value, elapsed))\n",
    "                print(loss_value)\n",
    "                start_time = timeit.default_timer()\n",
    "            \n",
    "    # Trains the model by minimizing the MSE loss using L-BFGS\n",
    "    def fine_tune(self):\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.Xu_tf: self.X_u, self.Yu_tf: self.Y_u,\n",
    "                   self.Xr_tf: self.X_r}\n",
    "\n",
    "        # Call SciPy's L-BFGS otpimizer\n",
    "        self.optimizer.minimize(self.sess, \n",
    "                                feed_dict = tf_dict,         \n",
    "                                fetches = [self.loss_u, self.loss_r], \n",
    "                                loss_callback = self.callback)\n",
    "        \n",
    "    # Evaluates predictions at collocation points           \n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_x)/self.sigma_x\n",
    "        tf_dict = {self.Xu_tf: X_star}       \n",
    "        u_star = self.sess.run(self.u_pred, tf_dict) \n",
    "        return u_star\n",
    "    \n",
    "    # Evaluates predictions at test points           \n",
    "    def predict_r(self, X_star):     \n",
    "        X_star = (X_star - self.mu_x)/self.sigma_x\n",
    "        tf_dict = {self.Xr_tf: X_star}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict) \n",
    "        return r_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# Number of training data\n",
    "N_u = 1                        # Boundary condition data on u(x)  \n",
    "N_r = 5000                     # Number of collocation points for minimizing the PDE residual\n",
    "lb  = np.array([0.0])         # Left boundary of the domain\n",
    "ub  = np.array([60000.0])          # Right boundary of the domain\n",
    "\n",
    "# Generate training data\n",
    "x_u = np.array([[0]])  ##TZ\n",
    "y_u = np.array([[1,0,0]])   ##TZ                    # Solution at boundary points (dimension N_u x 1)\n",
    "x_r = np.linspace(lb, ub, N_r)     # Location of collocation points (dimension N_r x 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# Test data for validating the model predictions\n",
    "n_star = 5000+1\n",
    "x_star = np.linspace(lb, ub, n_star) #N_star = x_star.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping: no known devices.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4020/3852838028.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlayers_S\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#  Define PINN model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHSMC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_u\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_u\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_U\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_tasks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4020/1685619620.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, X_u, Y_u, X_r, layers_U, lam, num_tasks)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_tasks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_U\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases_U\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \"\"\"\n\u001b[1;32m--> 407\u001b[1;33m     grads_and_vars = self.compute_gradients(\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgate_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgate_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[1;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[0;32m    498\u001b[0m                        \u001b[1;34m\"Optimizer.GATE_OP, Optimizer.GATE_GRAPH.  Not %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m                        gate_gradients)\n\u001b[1;32m--> 500\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_valid_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgrad_loss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_valid_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrad_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36m_assert_valid_dtypes\u001b[1;34m(self, tensors)\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[0mvalid_dtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_valid_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 894\u001b[1;33m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    895\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_dtypes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         raise ValueError(\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "######################################################################################\n",
    "# Fully-connected neural net architecture (dimensions of each layer)\n",
    "n_layers = 1\n",
    "layers_U = np.array([1,50,50,3])\n",
    "layers_S = np.array([1,50,50,3])\n",
    "#  Define PINN model\n",
    "model = HSMC(x_u, y_u, x_r, layers_U, lam=1, num_tasks=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# Load stored model\n",
    "# model.saver.restore(model.sess, \"../SavedModels/elliptic1d.ckpt\")\n",
    "# Train model using full batch gradient descent\n",
    "n_iter = 30000\n",
    "model.train(n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# Compute prediction using the trained model\n",
    "u_pred = model.predict_u(x_star)\n",
    "u_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=x_star[:,0:1].flatten(), y=u_pred[:,0:1].flatten(), color = 'red', linewidth = 2, label = \"Prediction-1\")\n",
    "sns.lineplot(x=x_star[:,0:1].flatten(), y=u_pred[:,1:2].flatten(), color = 'blue', linewidth = 2, label = \"Prediction-2\")\n",
    "sns.lineplot(x=x_star[:,0:1].flatten(), y=u_pred[:,2:3].flatten(), color = 'yellow', linewidth = 2, label = \"Prediction-3\")\n",
    "\n",
    "plt.plot(x_u[:,0:1], y_u[:,0:1], 'kx', linewidth = 2, label = \"Boundary data\")\n",
    "\n",
    "#plt.legend(frameon=False, loc='center right')\n",
    "plt.legend(loc='best',bbox_to_anchor=(1,1))\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$u(x)$')\n",
    "plt.title('System Reliability Assessment Based on the Proposed Method', fontsize=18)\n",
    "#plt.yscale(\"log\")\n",
    "#plt.xlim([0,500])\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subplot (for each state)\n",
    "# Plotting\n",
    "ax = plt.figure(1,figsize=(15,10))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(x_star[:,0:1], u_pred[:,0:1], 'r--', linewidth = 2, label = \"Prediction-1\")\n",
    "plt.legend(loc='best',bbox_to_anchor=(1,0.5))\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$P1(t)$')\n",
    "#plt.xlim([0,500])\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(x_star[:,0:1], u_pred[:,1:2], 'b--', linewidth = 2, label = \"Prediction-2\")\n",
    "plt.legend(loc='best',bbox_to_anchor=(1,0.5))\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('P2(t)')\n",
    "#plt.xlim([0,500])\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(x_star[:,0:1], u_pred[:,2:3], 'y--', linewidth = 2, label = \"Prediction-3\")\n",
    "plt.legend(loc='best',bbox_to_anchor=(1,0.5))\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$P3(t)$')\n",
    "#plt.xlim([0,500])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "## output results as csv file\n",
    "mc_output  = np.column_stack((np.array(x_star), u_pred))\n",
    "fileName = 'mspm_pidl_' + str(N_r) + '_' + str(n_iter) + '_' + str(n_layers) + '.csv'\n",
    "np.savetxt(fileName, mc_output, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## used time\n",
    "end_time = time.time() \n",
    "used_time = end_time - start_time\n",
    "print(\"--- %s seconds ---\" % (used_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot loss function every 50 iterations\n",
    "fig = plt.figure(2)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(model.loss_u_log, label = '$loss-u$')\n",
    "ax.plot(model.loss_r_log, label = '$loss-r$')\n",
    "ax.set_yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subplot (for each state)\n",
    "# Plotting\n",
    "ax = plt.figure(1,figsize=(10,5))\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model.loss_u_log, 'r--', linewidth = 2, label = \"$loss-u$\")\n",
    "plt.legend(loc='best',bbox_to_anchor=(1,0.5))\n",
    "plt.title('Boundary loss')\n",
    "plt.xlabel('$Epoch$')\n",
    "plt.ylabel('$Loss$')\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model.loss_r_log, 'r--', linewidth = 2, label = \"$loss-r$\")\n",
    "plt.legend(loc='best',bbox_to_anchor=(1,0.5))\n",
    "plt.title('PDE loss')\n",
    "plt.xlabel('$Epoch$')\n",
    "plt.ylabel('$Loss$')\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "027e948c137d6acde9006b592cf7cd1a70a2e52a69a7472c7b58352c83d453a1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
