{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data type\n",
    "DTYPE='float32'\n",
    "tf.keras.backend.set_floatx(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PINN neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(number_hidden_layers = 2, num_neurons_per_layer = 50):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(1))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(num_neurons_per_layer, activation=tf.keras.activations.get('tanh'), \n",
    "                                    kernel_initializer='glorot_normal'))\n",
    "    model.add(tf.keras.layers.Dense(num_neurons_per_layer, activation=tf.keras.activations.get('tanh'), \n",
    "                                    kernel_initializer='glorot_normal'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(12, activation=tf.keras.activations.get('softmax')))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# A basic PINN Tensorflow class for solving a continuous-time in-homogeneous Markov chains\n",
    "class PINN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, X_u, Y_u, X_r, model):\n",
    "        \n",
    "        ######################################################################################\n",
    "        # Normalization constants\n",
    "        self.mu_x, self.sigma_x = X_r.mean(0), X_r.std(0)\n",
    "\n",
    "        # Normalize inputs\n",
    "        X_u = (X_u - self.mu_x)/self.sigma_x\n",
    "        X_r = (X_r - self.mu_x)/self.sigma_x\n",
    "\n",
    "        # Store data                \n",
    "        self.X_u = X_u\n",
    "        self.Y_u = Y_u\n",
    "        self.X_r = X_r\n",
    "\n",
    "        self.Xu_tf = tf.convert_to_tensor(X_u, dtype=tf.float32)\n",
    "        self.Yu_tf = tf.convert_to_tensor(Y_u, dtype=tf.float32)\n",
    "        self.Xr_tf = tf.convert_to_tensor(X_r, dtype=tf.float32)\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.iter = 0\n",
    "        self.loss_log = []\n",
    "        self.weights_log = []\n",
    "        self.gradients_log = []\n",
    "        \n",
    "    def get_r(self):\n",
    "        with tf.GradientTape(persistent=True) as g:\n",
    "            g.watch(self.Xr_tf)\n",
    "            \n",
    "            u = self.model(self.Xr_tf)\n",
    "            \n",
    "            u1 = u[:,0:1];u2 = u[:,1:2];u3 = u[:,2:3];\n",
    "            u4 = u[:,3:4];u5 = u[:,4:5];u6 = u[:,5:6];\n",
    "            u7 = u[:,6:7];u8 = u[:,7:8];u9 = u[:,8:9];\n",
    "            u10 = u[:,9:10];u11 = u[:,10:11];u12 = u[:,11:12]\n",
    "            \n",
    "        #derivatives of state probability\n",
    "        u_x_1 = g.gradient(u1, self.Xr_tf)/self.sigma_x\n",
    "        u_x_2 = g.gradient(u2, self.Xr_tf)/self.sigma_x\n",
    "        u_x_3 = g.gradient(u3, self.Xr_tf)/self.sigma_x\n",
    "        u_x_4 = g.gradient(u4, self.Xr_tf)/self.sigma_x\n",
    "        u_x_5 = g.gradient(u5, self.Xr_tf)/self.sigma_x\n",
    "        u_x_6 = g.gradient(u6, self.Xr_tf)/self.sigma_x\n",
    "        u_x_7 = g.gradient(u7, self.Xr_tf)/self.sigma_x\n",
    "        u_x_8 = g.gradient(u8, self.Xr_tf)/self.sigma_x\n",
    "        u_x_9 = g.gradient(u9, self.Xr_tf)/self.sigma_x\n",
    "        u_x_10 = g.gradient(u10, self.Xr_tf)/self.sigma_x\n",
    "        u_x_11 = g.gradient(u11, self.Xr_tf)/self.sigma_x\n",
    "        u_x_12 = g.gradient(u12, self.Xr_tf)/self.sigma_x\n",
    "        \n",
    "        \n",
    "        #-----------------------------------------------\n",
    "        # specify transition rate\n",
    "        #for the element 1\n",
    "        Lambda2_1_1=7; Mu1_2_1=100;\n",
    "        #for the element 2\n",
    "        Lambda2_1_2=10; Mu1_2_2=80;\n",
    "        #for the element 3\n",
    "        Lambda3_2_3=10; Lambda3_1_3=0; Lambda2_1_3=7;\n",
    "        Mu1_3_3=0; Mu1_2_3=120; Mu2_3_3=110\n",
    "        #-----------------------------------------------\n",
    "        # calcualte ode terms\n",
    "        ode_1 = -(Lambda2_1_1+Lambda2_1_2+Lambda3_2_3)*u1+Mu1_2_1*u2+Mu1_2_2*u3+Mu2_3_3*u4;\n",
    "        ode_2 = Lambda2_1_1*u1-(Mu1_2_1+Lambda2_1_2+Lambda3_2_3)*u2+Mu1_2_2*u5+Mu2_3_3*u6;\n",
    "        ode_3 = Lambda2_1_2*u1-(Mu1_2_2+Lambda2_1_1+Lambda3_2_3)*u3+Mu1_2_1*u5+Mu2_3_3*u7;\n",
    "        ode_4 = Lambda3_2_3*u1-(Mu2_3_3+Lambda2_1_1+Lambda2_1_2+Lambda2_1_3)*u4+Mu1_2_1*u6+Mu1_2_2*u7+Mu1_2_3*u8;\n",
    "        ode_5 = Lambda2_1_2*u2+Lambda2_1_1*u3-(Mu1_2_2+Mu1_2_1+Lambda3_2_3)*u5+Mu2_3_3*u9;\n",
    "        ode_6 = Lambda3_2_3*u2+Lambda2_1_1*u4-(Mu2_3_3+Mu1_2_1+Lambda2_1_2+Lambda2_1_3)*u6+Mu1_2_2*u9+Mu1_2_3*u10;\n",
    "        ode_7 = Lambda3_2_3*u3+Lambda2_1_2*u4-(Mu2_3_3+Mu1_2_2+Lambda2_1_1+Lambda2_1_3)*u7+Mu1_2_1*u9+Mu2_3_3*u11;\n",
    "        ode_8 = Lambda2_1_3*u4-(Mu1_2_3+Lambda2_1_1+Lambda2_1_2)*u8+Mu1_2_1*u10+Mu1_2_2*u11;\n",
    "        ode_9 = Lambda3_2_3*u5+Lambda2_1_2*u6+Lambda2_1_1*u7-(Mu2_3_3+Mu1_2_2+Mu1_2_1+Lambda2_1_3)*u9+Mu1_2_3*u12;\n",
    "        ode_10 = Lambda2_1_3*u6+Lambda2_1_1*u8-(Mu1_2_3+Mu1_2_1+Lambda2_1_2)*u10+Mu1_2_2*u12;\n",
    "        ode_11 = Lambda2_1_3*u7+Lambda2_1_2*u8-(Mu1_2_3+Mu1_2_2+Lambda2_1_1)*u11+Mu1_2_1*u12;\n",
    "        ode_12 = Lambda2_1_3*u9+Lambda2_1_2*u10+Lambda2_1_1*u11-(Mu1_2_3+Mu1_2_2+Mu1_2_1)*u12\n",
    "        #-----------------------------------------------\n",
    "        \n",
    "        # calculate residuals\n",
    "        residual_1 = u_x_1-ode_1; residual_2 = u_x_2-ode_2; residual_3 = u_x_3-ode_3\n",
    "        residual_4 = u_x_4-ode_4; residual_5 = u_x_5-ode_5; residual_6 = u_x_6-ode_6;\n",
    "        residual_7 = u_x_7-ode_7; residual_8 = u_x_8-ode_8; residual_9 = u_x_9-ode_9;\n",
    "        residual_10 = u_x_10-ode_10; residual_11 = u_x_11-ode_11; residual_12 = u_x_12-ode_12\n",
    "        \n",
    "        #total residual\n",
    "        residual = tf.reduce_mean(tf.square(residual_1))+tf.reduce_mean(tf.square(residual_2))+\\\n",
    "        tf.reduce_mean(tf.square(residual_3))+tf.reduce_mean(tf.square(residual_4))+\\\n",
    "        tf.reduce_mean(tf.square(residual_5))+tf.reduce_mean(tf.square(residual_6))+\\\n",
    "        tf.reduce_mean(tf.square(residual_7))+tf.reduce_mean(tf.square(residual_8))+\\\n",
    "        tf.reduce_mean(tf.square(residual_9))+tf.reduce_mean(tf.square(residual_10))+\\\n",
    "        tf.reduce_mean(tf.square(residual_11))+tf.reduce_mean(tf.square(residual_12))\n",
    "        \n",
    "        loss_1 = tf.reduce_mean(tf.square(residual_1)); loss_2 = tf.reduce_mean(tf.square(residual_2));\n",
    "        loss_3 = tf.reduce_mean(tf.square(residual_3)); loss_4 = tf.reduce_mean(tf.square(residual_4)); \n",
    "        loss_5 = tf.reduce_mean(tf.square(residual_5)); loss_6 = tf.reduce_mean(tf.square(residual_6));\n",
    "        loss_7 = tf.reduce_mean(tf.square(residual_7)); loss_8 = tf.reduce_mean(tf.square(residual_8));\n",
    "        loss_9 = tf.reduce_mean(tf.square(residual_9)); loss_10 = tf.reduce_mean(tf.square(residual_10));\n",
    "        loss_11 = tf.reduce_mean(tf.square(residual_11)); loss_12 = tf.reduce_mean(tf.square(residual_12))\n",
    "        \n",
    "        del g\n",
    "        \n",
    "        return loss_1, loss_2, loss_3, loss_4, loss_5, loss_6, loss_7, loss_8, loss_9, loss_10, loss_11, loss_12\n",
    "    \n",
    "\n",
    "    def loss_fn(self, weight = 1):\n",
    "        u_pred = self.model(self.Xu_tf)\n",
    "\n",
    "        loss_u = tf.reduce_mean(tf.square(self.Yu_tf - u_pred))\n",
    "        loss_1, loss_2, loss_3, loss_4, loss_5, loss_6, loss_7, loss_8, loss_9, loss_10, loss_11, loss_12 = self.get_r()\n",
    "        \n",
    "        return [loss_u, loss_1 + loss_2 + loss_3 + loss_4 +loss_5+loss_6+loss_7+loss_8+loss_9+loss_10+loss_11+loss_12]\n",
    "    \n",
    "    \n",
    "    def get_grad(self):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(self.model.trainable_variables)\n",
    "            loss = self.loss_fn()\n",
    "        \n",
    "        g = tape.gradient(loss, self.model.trainable_variables)\n",
    "        \n",
    "        loss_u, loss_r = loss[0], loss[1]\n",
    "        g_u = tape.gradient(loss_u, self.model.trainable_variables)\n",
    "        g_r = tape.gradient(loss_r, self.model.trainable_variables)\n",
    "        \n",
    "        return loss, g, g_u, g_r\n",
    "    \n",
    "    def get_grad_by_PCG(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_fn()\n",
    "            \n",
    "            assert type(loss) is list\n",
    "            loss = tf.stack(loss)\n",
    "            tf.random.shuffle(loss)\n",
    "\n",
    "            grads_task = tf.vectorized_map(lambda x: tf.concat([tf.reshape(grad, [-1,]) \n",
    "                                for grad in tape.gradient(x, self.model.trainable_variables)\n",
    "                                if grad is not None], axis=0), loss)\n",
    "        \n",
    "        num_tasks = len(loss)\n",
    "\n",
    "        # Compute gradient projections.\n",
    "        def proj_grad(grad_task):\n",
    "            for k in range(num_tasks):\n",
    "                inner_product = tf.reduce_sum(grad_task*grads_task[k])\n",
    "                proj_direction = inner_product / tf.reduce_sum(grads_task[k]*grads_task[k])\n",
    "                grad_task = grad_task - tf.minimum(proj_direction, 0.) * grads_task[k]\n",
    "            return grad_task\n",
    "\n",
    "        proj_grads_flatten = tf.vectorized_map(proj_grad, grads_task)\n",
    "\n",
    "        # Unpack flattened projected gradients back to their original shapes.\n",
    "        proj_grads = []\n",
    "        for j in range(num_tasks):\n",
    "            start_idx = 0\n",
    "            for idx, var in enumerate(self.model.trainable_variables):\n",
    "                grad_shape = var.get_shape()\n",
    "                flatten_dim = np.prod([grad_shape.dims[i].value for i in range(len(grad_shape.dims))])\n",
    "                proj_grad = proj_grads_flatten[j][start_idx:start_idx+flatten_dim]\n",
    "                proj_grad = tf.reshape(proj_grad, grad_shape)\n",
    "                if len(proj_grads) < len(self.model.trainable_variables):\n",
    "                    proj_grads.append(proj_grad)\n",
    "                else:\n",
    "                    proj_grads[idx] += proj_grad               \n",
    "                start_idx += flatten_dim\n",
    "\n",
    "        grads_and_vars = list(zip(proj_grads, self.model.trainable_variables))\n",
    "        \n",
    "        del tape\n",
    "        return loss, proj_grads, None, None\n",
    "    \n",
    "    def callback(self):\n",
    "        if self.iter % 5 == 0:\n",
    "            print('Iteration {:05d}: loss = {}'.format(self.iter, ','.join(map(str, self.current_loss))))\n",
    "        self.iter += 1\n",
    "    \n",
    "    def train(self, N, optimizer, method):\n",
    "        \"\"\"This method performs a gradient descent type optimization.\"\"\"\n",
    "        \n",
    "        @tf.function\n",
    "        def train_step():\n",
    "            if method == 'original':\n",
    "                loss, grad_theta, g_u, g_r = self.get_grad()\n",
    "                \n",
    "            if method == 'PCG_gradient':\n",
    "                loss, grad_theta, g_u, g_r = self.get_grad_by_PCG()\n",
    "            \n",
    "            # Perform gradient descent step\n",
    "            optimizer.apply_gradients(zip(grad_theta, self.model.trainable_variables))\n",
    "            \n",
    "            return loss, grad_theta, g_u, g_r\n",
    "        \n",
    "        for i in range(N):\n",
    "            \n",
    "            loss, grad_theta, g_u, g_r = train_step()\n",
    "            \n",
    "            self.current_loss = tf.convert_to_tensor(loss).numpy()\n",
    "            self.loss_log.append(self.current_loss)\n",
    "            self.callback()\n",
    "            \n",
    "            if i%10000 == 0:\n",
    "                self.weights_log.append(PINN_solver.model.get_weights())\n",
    "                self.gradients_log.append([grad_theta, g_u, g_r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# Number of training data\n",
    "N_u = 1                       # Boundary condition data on u(x)  \n",
    "N_r = 500                     # Number of collocation points for minimizing the PDE residual\n",
    "lb  = np.array([0.0])         # Left boundary of the domain\n",
    "ub  = np.array([0.2])         # Right boundary of the domain\n",
    "\n",
    "# Generate training data\n",
    "x_u = np.array([[0]])  ##TZ\n",
    "y_u = np.array([[1,0,0,0,0,0,0,0,0,0,0,0]])   # Solution at boundary points (dimension N_u x 1)\n",
    "x_r = np.linspace(lb, ub, N_r)     # Location of collocation points (dimension N_r x 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 40000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PINN without PCGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN_solver = PINN(x_u, y_u, x_r, init_model())\n",
    "initial_weights = PINN_solver.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 00000: loss = 0.08280759,1988.7423\n",
      "Iteration 00005: loss = 0.08258636,1754.5768\n",
      "Iteration 00010: loss = 0.083634295,1622.6858\n",
      "Iteration 00015: loss = 0.085566424,1515.6445\n",
      "Iteration 00020: loss = 0.0874824,1395.9902\n",
      "Iteration 00025: loss = 0.08887321,1274.0547\n",
      "Iteration 00030: loss = 0.08972776,1180.1196\n",
      "Iteration 00035: loss = 0.09021867,1112.925\n",
      "Iteration 00040: loss = 0.09050992,1049.0425\n",
      "Iteration 00045: loss = 0.09073472,980.5673\n",
      "Iteration 00050: loss = 0.09097772,915.75635\n",
      "Iteration 00055: loss = 0.09113699,855.6688\n",
      "Iteration 00060: loss = 0.09097588,790.42395\n",
      "Iteration 00065: loss = 0.090209015,716.2355\n",
      "Iteration 00070: loss = 0.088424705,627.90393\n",
      "Iteration 00075: loss = 0.084477104,514.2722\n",
      "Iteration 00080: loss = 0.07523332,365.57\n",
      "Iteration 00085: loss = 0.055081908,194.42862\n",
      "Iteration 00090: loss = 0.027529182,79.035736\n",
      "Iteration 00095: loss = 0.012343578,58.09111\n",
      "Iteration 00100: loss = 0.009091509,48.657715\n",
      "Iteration 00105: loss = 0.0103438,31.635973\n",
      "Iteration 00110: loss = 0.012245785,29.201921\n",
      "Iteration 00115: loss = 0.012122247,24.481007\n",
      "Iteration 00120: loss = 0.01078702,19.359192\n",
      "Iteration 00125: loss = 0.010041734,17.661144\n",
      "Iteration 00130: loss = 0.010370825,15.400805\n",
      "Iteration 00135: loss = 0.0111673465,13.367813\n",
      "Iteration 00140: loss = 0.011485595,11.751491\n",
      "Iteration 00145: loss = 0.011131015,10.178841\n",
      "Iteration 00150: loss = 0.010702456,9.009358\n",
      "Iteration 00155: loss = 0.010602052,8.071249\n",
      "Iteration 00160: loss = 0.010683805,7.296179\n",
      "Iteration 00165: loss = 0.010653091,6.6912584\n",
      "Iteration 00170: loss = 0.010449021,6.1799507\n",
      "Iteration 00175: loss = 0.010203644,5.7464533\n",
      "Iteration 00180: loss = 0.010020542,5.369919\n",
      "Iteration 00185: loss = 0.009894153,5.032415\n",
      "Iteration 00190: loss = 0.009765741,4.734226\n",
      "Iteration 00195: loss = 0.009606027,4.4680047\n",
      "Iteration 00200: loss = 0.009442389,4.230569\n",
      "Iteration 00205: loss = 0.009311606,4.0179048\n",
      "Iteration 00210: loss = 0.00921262,3.8259366\n",
      "Iteration 00215: loss = 0.009120572,3.6519694\n",
      "Iteration 00220: loss = 0.009025868,3.4930553\n",
      "Iteration 00225: loss = 0.008938539,3.347179\n",
      "Iteration 00230: loss = 0.008865844,3.212624\n",
      "Iteration 00235: loss = 0.008802944,3.0880418\n",
      "Iteration 00240: loss = 0.008742652,2.9723177\n",
      "Iteration 00245: loss = 0.008683896,2.864469\n",
      "Iteration 00250: loss = 0.008629621,2.7636762\n",
      "Iteration 00255: loss = 0.008580726,2.6692245\n",
      "Iteration 00260: loss = 0.008534997,2.5805118\n",
      "Iteration 00265: loss = 0.008490694,2.497008\n",
      "Iteration 00270: loss = 0.0084483465,2.418252\n",
      "Iteration 00275: loss = 0.008408829,2.3438382\n",
      "Iteration 00280: loss = 0.008371769,2.2734041\n",
      "Iteration 00285: loss = 0.00833632,2.2066307\n",
      "Iteration 00290: loss = 0.008302295,2.1432328\n",
      "Iteration 00295: loss = 0.00826993,2.0829494\n",
      "Iteration 00300: loss = 0.008239212,2.0255494\n",
      "Iteration 00305: loss = 0.0082097715,1.9708216\n",
      "Iteration 00310: loss = 0.008181404,1.9185746\n",
      "Iteration 00315: loss = 0.008154144,1.8686364\n",
      "Iteration 00320: loss = 0.008127983,1.8208485\n",
      "Iteration 00325: loss = 0.008102761,1.7750663\n",
      "Iteration 00330: loss = 0.008078412,1.731159\n",
      "Iteration 00335: loss = 0.008054862,1.6890062\n",
      "Iteration 00340: loss = 0.008032113,1.6484962\n",
      "Iteration 00345: loss = 0.008010115,1.6095278\n",
      "Iteration 00350: loss = 0.007988797,1.572007\n",
      "Iteration 00355: loss = 0.00796812,1.5358461\n",
      "Iteration 00360: loss = 0.007948073,1.5009688\n",
      "Iteration 00365: loss = 0.007928613,1.4673009\n",
      "Iteration 00370: loss = 0.007909714,1.4347742\n",
      "Iteration 00375: loss = 0.007891343,1.4033241\n",
      "Iteration 00380: loss = 0.007873473,1.3728931\n",
      "Iteration 00385: loss = 0.007856097,1.3434308\n",
      "Iteration 00390: loss = 0.007839178,1.3148824\n",
      "Iteration 00395: loss = 0.007822722,1.2872038\n",
      "Iteration 00400: loss = 0.007806666,1.2603517\n",
      "Iteration 00405: loss = 0.00779102,1.2342836\n",
      "Iteration 00410: loss = 0.0077757756,1.2089658\n",
      "Iteration 00415: loss = 0.007760896,1.1843593\n",
      "Iteration 00420: loss = 0.0077463873,1.1604364\n",
      "Iteration 00425: loss = 0.007732222,1.1371613\n",
      "Iteration 00430: loss = 0.0077183973,1.1145095\n",
      "Iteration 00435: loss = 0.0077048894,1.092453\n",
      "Iteration 00440: loss = 0.007691693,1.0709679\n",
      "Iteration 00445: loss = 0.0076787914,1.0500283\n",
      "Iteration 00450: loss = 0.0076661925,1.0296161\n",
      "Iteration 00455: loss = 0.0076538683,1.0097076\n",
      "Iteration 00460: loss = 0.007641807,0.9902856\n",
      "Iteration 00465: loss = 0.0076300134,0.9713309\n",
      "Iteration 00470: loss = 0.0076184715,0.9528284\n",
      "Iteration 00475: loss = 0.007607166,0.93476\n",
      "Iteration 00480: loss = 0.0075960997,0.917112\n",
      "Iteration 00485: loss = 0.0075852596,0.8998691\n",
      "Iteration 00490: loss = 0.007574634,0.8830202\n",
      "Iteration 00495: loss = 0.007564226,0.8665511\n",
      "Iteration 00500: loss = 0.0075540054,0.8504497\n",
      "Iteration 00505: loss = 0.007543989,0.8347068\n",
      "Iteration 00510: loss = 0.00753417,0.819309\n",
      "Iteration 00515: loss = 0.007524529,0.80424654\n",
      "Iteration 00520: loss = 0.007515052,0.78951234\n",
      "Iteration 00525: loss = 0.0075057573,0.7750964\n",
      "Iteration 00530: loss = 0.007496607,0.76098716\n",
      "Iteration 00535: loss = 0.0074876375,0.74718004\n",
      "Iteration 00540: loss = 0.007478809,0.73366594\n",
      "Iteration 00545: loss = 0.007470123,0.7204365\n",
      "Iteration 00550: loss = 0.0074615735,0.707486\n",
      "Iteration 00555: loss = 0.007453172,0.69480485\n",
      "Iteration 00560: loss = 0.0074448865,0.6823922\n",
      "Iteration 00565: loss = 0.0074367314,0.6702335\n",
      "Iteration 00570: loss = 0.0074286927,0.65832996\n",
      "Iteration 00575: loss = 0.007420773,0.64667165\n",
      "Iteration 00580: loss = 0.0074129626,0.63525486\n",
      "Iteration 00585: loss = 0.007405255,0.6240731\n",
      "Iteration 00590: loss = 0.007397654,0.61311996\n",
      "Iteration 00595: loss = 0.0073901415,0.60239285\n",
      "Iteration 00600: loss = 0.0073827393,0.5918853\n",
      "Iteration 00605: loss = 0.0073754103,0.58159125\n",
      "Iteration 00610: loss = 0.0073681804,0.57150954\n",
      "Iteration 00615: loss = 0.007361025,0.56163216\n",
      "Iteration 00620: loss = 0.007353964,0.55195594\n",
      "Iteration 00625: loss = 0.007346967,0.54247844\n",
      "Iteration 00630: loss = 0.007340049,0.53319186\n",
      "Iteration 00635: loss = 0.0073331944,0.52409446\n",
      "Iteration 00640: loss = 0.0073264074,0.5151815\n",
      "Iteration 00645: loss = 0.0073196944,0.5064489\n",
      "Iteration 00650: loss = 0.0073130447,0.49789464\n",
      "Iteration 00655: loss = 0.0073064403,0.48951283\n",
      "Iteration 00660: loss = 0.0072999005,0.48130056\n",
      "Iteration 00665: loss = 0.007293423,0.47325495\n",
      "Iteration 00670: loss = 0.007287001,0.46537066\n",
      "Iteration 00675: loss = 0.0072806217,0.4576457\n",
      "Iteration 00680: loss = 0.007274303,0.4500767\n",
      "Iteration 00685: loss = 0.0072680204,0.4426604\n",
      "Iteration 00690: loss = 0.00726179,0.43539512\n",
      "Iteration 00695: loss = 0.0072556078,0.4282758\n",
      "Iteration 00700: loss = 0.007249462,0.42129722\n",
      "Iteration 00705: loss = 0.0072433646,0.4144623\n",
      "Iteration 00710: loss = 0.0072373184,0.407763\n",
      "Iteration 00715: loss = 0.0072312946,0.40119937\n",
      "Iteration 00720: loss = 0.007225326,0.39476705\n",
      "Iteration 00725: loss = 0.007219381,0.38846394\n",
      "Iteration 00730: loss = 0.007213497,0.38228852\n",
      "Iteration 00735: loss = 0.0072076316,0.3762356\n",
      "Iteration 00740: loss = 0.007201791,0.37030548\n",
      "Iteration 00745: loss = 0.0071960096,0.36449385\n",
      "Iteration 00750: loss = 0.0071902503,0.35879794\n",
      "Iteration 00755: loss = 0.0071845427,0.35321772\n",
      "Iteration 00760: loss = 0.007178858,0.34774953\n",
      "Iteration 00765: loss = 0.0071732122,0.34239018\n",
      "Iteration 00770: loss = 0.007167598,0.3371391\n",
      "Iteration 00775: loss = 0.007162012,0.33199328\n",
      "Iteration 00780: loss = 0.007156474,0.32695046\n",
      "Iteration 00785: loss = 0.00715097,0.32201022\n",
      "Iteration 00790: loss = 0.007145489,0.31716818\n",
      "Iteration 00795: loss = 0.0071400437,0.3124234\n",
      "Iteration 00800: loss = 0.0071346406,0.3077737\n",
      "Iteration 00805: loss = 0.0071292617,0.30321828\n",
      "Iteration 00810: loss = 0.007123925,0.29875368\n",
      "Iteration 00815: loss = 0.0071186065,0.29437986\n",
      "Iteration 00820: loss = 0.0071133305,0.29009348\n",
      "Iteration 00825: loss = 0.007108096,0.28589356\n",
      "Iteration 00830: loss = 0.0071029053,0.28177807\n",
      "Iteration 00835: loss = 0.0070977365,0.27774554\n",
      "Iteration 00840: loss = 0.0070926026,0.27379334\n",
      "Iteration 00845: loss = 0.0070874994,0.26992226\n",
      "Iteration 00850: loss = 0.0070824367,0.2661281\n",
      "Iteration 00855: loss = 0.0070774094,0.26241064\n",
      "Iteration 00860: loss = 0.007072421,0.2587679\n",
      "Iteration 00865: loss = 0.007067477,0.2551988\n",
      "Iteration 00870: loss = 0.0070625655,0.25170177\n",
      "Iteration 00875: loss = 0.007057684,0.24827443\n",
      "Iteration 00880: loss = 0.0070528486,0.24491678\n",
      "Iteration 00885: loss = 0.00704804,0.241626\n",
      "Iteration 00890: loss = 0.007043276,0.23840165\n",
      "Iteration 00895: loss = 0.0070385416,0.23524185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 00900: loss = 0.0070338566,0.2321458\n",
      "Iteration 00905: loss = 0.007029207,0.2291116\n",
      "Iteration 00910: loss = 0.0070246006,0.22613786\n",
      "Iteration 00915: loss = 0.0070200334,0.22322476\n",
      "Iteration 00920: loss = 0.0070155077,0.22036868\n",
      "Iteration 00925: loss = 0.007011013,0.21756995\n",
      "Iteration 00930: loss = 0.0070065595,0.21482655\n",
      "Iteration 00935: loss = 0.0070021567,0.21213819\n",
      "Iteration 00940: loss = 0.0069977897,0.20950234\n",
      "Iteration 00945: loss = 0.006993459,0.20691907\n",
      "Iteration 00950: loss = 0.006989177,0.20438632\n",
      "Iteration 00955: loss = 0.0069849286,0.20190361\n",
      "Iteration 00960: loss = 0.006980714,0.19946964\n",
      "Iteration 00965: loss = 0.006976562,0.19708295\n",
      "Iteration 00970: loss = 0.006972441,0.19474319\n",
      "Iteration 00975: loss = 0.0069683655,0.19244888\n",
      "Iteration 00980: loss = 0.006964321,0.19019827\n",
      "Iteration 00985: loss = 0.006960314,0.18799137\n",
      "Iteration 00990: loss = 0.0069563636,0.18582669\n",
      "Iteration 00995: loss = 0.0069524446,0.1837036\n",
      "Iteration 01000: loss = 0.006948577,0.18162042\n",
      "Iteration 01005: loss = 0.006944744,0.17957692\n",
      "Iteration 01010: loss = 0.006940952,0.1775714\n",
      "Iteration 01015: loss = 0.0069372063,0.17560391\n",
      "Iteration 01020: loss = 0.0069334917,0.17367293\n",
      "Iteration 01025: loss = 0.006929833,0.17177749\n",
      "Iteration 01030: loss = 0.0069262045,0.16991761\n",
      "Iteration 01035: loss = 0.00692262,0.16809133\n",
      "Iteration 01040: loss = 0.006919081,0.16629815\n",
      "Iteration 01045: loss = 0.006915573,0.16453734\n",
      "Iteration 01050: loss = 0.0069121104,0.16280875\n",
      "Iteration 01055: loss = 0.0069086924,0.16111012\n",
      "Iteration 01060: loss = 0.0069053136,0.15944234\n",
      "Iteration 01065: loss = 0.0069019627,0.1578037\n",
      "Iteration 01070: loss = 0.006898672,0.1561935\n",
      "Iteration 01075: loss = 0.0068953973,0.15461127\n",
      "Iteration 01080: loss = 0.006892176,0.15305656\n",
      "Iteration 01085: loss = 0.0068889954,0.15152836\n",
      "Iteration 01090: loss = 0.0068858503,0.150026\n",
      "Iteration 01095: loss = 0.0068827383,0.14854886\n",
      "Iteration 01100: loss = 0.00687967,0.14709668\n",
      "Iteration 01105: loss = 0.0068766363,0.14566845\n",
      "Iteration 01110: loss = 0.0068736393,0.14426354\n",
      "Iteration 01115: loss = 0.00687068,0.14288177\n",
      "Iteration 01120: loss = 0.0068677613,0.14152224\n",
      "Iteration 01125: loss = 0.0068648695,0.1401848\n",
      "Iteration 01130: loss = 0.006862012,0.13886833\n",
      "Iteration 01135: loss = 0.0068591963,0.13757291\n",
      "Iteration 01140: loss = 0.006856413,0.13629794\n",
      "Iteration 01145: loss = 0.006853676,0.13504249\n",
      "Iteration 01150: loss = 0.0068509616,0.13380675\n",
      "Iteration 01155: loss = 0.006848278,0.13258944\n",
      "Iteration 01160: loss = 0.0068456307,0.13139084\n",
      "Iteration 01165: loss = 0.0068430086,0.13021031\n",
      "Iteration 01170: loss = 0.006840428,0.12904742\n",
      "Iteration 01175: loss = 0.0068378835,0.12790176\n",
      "Iteration 01180: loss = 0.0068353596,0.12677279\n",
      "Iteration 01185: loss = 0.0068328814,0.12566037\n",
      "Iteration 01190: loss = 0.0068304203,0.12456403\n",
      "Iteration 01195: loss = 0.006827991,0.123483375\n",
      "Iteration 01200: loss = 0.0068256017,0.12241817\n",
      "Iteration 01205: loss = 0.0068232287,0.12136784\n",
      "Iteration 01210: loss = 0.0068209,0.12033234\n",
      "Iteration 01215: loss = 0.0068185744,0.119311064\n",
      "Iteration 01220: loss = 0.0068162954,0.1183041\n",
      "Iteration 01225: loss = 0.006814051,0.117310785\n",
      "Iteration 01230: loss = 0.0068118176,0.116330974\n",
      "Iteration 01235: loss = 0.0068096183,0.11536448\n",
      "Iteration 01240: loss = 0.0068074428,0.11441078\n",
      "Iteration 01245: loss = 0.0068052933,0.11346983\n",
      "Iteration 01250: loss = 0.0068031684,0.11254117\n",
      "Iteration 01255: loss = 0.00680107,0.111624695\n",
      "Iteration 01260: loss = 0.0067989905,0.11072017\n",
      "Iteration 01265: loss = 0.0067969337,0.1098273\n",
      "Iteration 01270: loss = 0.006794917,0.10894592\n",
      "Iteration 01275: loss = 0.0067929085,0.108075775\n",
      "Iteration 01280: loss = 0.0067909397,0.107216544\n",
      "Iteration 01285: loss = 0.0067889746,0.10636827\n",
      "Iteration 01290: loss = 0.006787041,0.10553048\n",
      "Iteration 01295: loss = 0.0067851185,0.10470314\n",
      "Iteration 01300: loss = 0.0067832335,0.10388602\n",
      "Iteration 01305: loss = 0.0067813606,0.10307889\n",
      "Iteration 01310: loss = 0.006779507,0.10228151\n",
      "Iteration 01315: loss = 0.0067776707,0.101493746\n",
      "Iteration 01320: loss = 0.006775866,0.10071558\n",
      "Iteration 01325: loss = 0.0067740716,0.0999467\n",
      "Iteration 01330: loss = 0.0067722904,0.09918687\n",
      "Iteration 01335: loss = 0.0067705475,0.09843613\n",
      "Iteration 01340: loss = 0.006768815,0.09769417\n",
      "Iteration 01345: loss = 0.006767104,0.096961066\n",
      "Iteration 01350: loss = 0.0067653935,0.09623629\n",
      "Iteration 01355: loss = 0.0067637116,0.09551993\n",
      "Iteration 01360: loss = 0.0067620375,0.09481185\n",
      "Iteration 01365: loss = 0.0067603993,0.09411189\n",
      "Iteration 01370: loss = 0.006758772,0.09341976\n",
      "Iteration 01375: loss = 0.0067571527,0.0927356\n",
      "Iteration 01380: loss = 0.0067555453,0.092059195\n",
      "Iteration 01385: loss = 0.006753961,0.09139033\n",
      "Iteration 01390: loss = 0.006752398,0.090728916\n",
      "Iteration 01395: loss = 0.0067508444,0.09007479\n",
      "Iteration 01400: loss = 0.0067493054,0.089427985\n",
      "Iteration 01405: loss = 0.006747786,0.0887884\n",
      "Iteration 01410: loss = 0.006746271,0.08815557\n",
      "Iteration 01415: loss = 0.006744779,0.08752981\n",
      "Iteration 01420: loss = 0.0067433002,0.08691086\n",
      "Iteration 01425: loss = 0.006741829,0.08629845\n",
      "Iteration 01430: loss = 0.0067403684,0.08569273\n",
      "Iteration 01435: loss = 0.0067389235,0.0850936\n",
      "Iteration 01440: loss = 0.006737501,0.08450062\n",
      "Iteration 01445: loss = 0.0067360797,0.083914146\n",
      "Iteration 01450: loss = 0.0067346725,0.083333775\n",
      "Iteration 01455: loss = 0.0067332853,0.08275961\n",
      "Iteration 01460: loss = 0.00673191,0.082191385\n",
      "Iteration 01465: loss = 0.006730543,0.08162918\n",
      "Iteration 01470: loss = 0.006729185,0.081072666\n",
      "Iteration 01475: loss = 0.006727828,0.080522075\n",
      "Iteration 01480: loss = 0.0067265104,0.07997709\n",
      "Iteration 01485: loss = 0.006725187,0.07943779\n",
      "Iteration 01490: loss = 0.006723869,0.07890391\n",
      "Iteration 01495: loss = 0.0067225676,0.07837556\n",
      "Iteration 01500: loss = 0.006721282,0.07785238\n",
      "Iteration 01505: loss = 0.0067199953,0.077334754\n",
      "Iteration 01510: loss = 0.0067187226,0.07682231\n",
      "Iteration 01515: loss = 0.0067174635,0.07631486\n",
      "Iteration 01520: loss = 0.006716208,0.075812556\n",
      "Iteration 01525: loss = 0.0067149694,0.07531533\n",
      "Iteration 01530: loss = 0.0067137373,0.07482287\n",
      "Iteration 01535: loss = 0.0067125154,0.07433544\n",
      "Iteration 01540: loss = 0.0067113,0.07385278\n",
      "Iteration 01545: loss = 0.006710099,0.07337482\n",
      "Iteration 01550: loss = 0.006708903,0.07290151\n",
      "Iteration 01555: loss = 0.0067077167,0.07243301\n",
      "Iteration 01560: loss = 0.006706536,0.07196883\n",
      "Iteration 01565: loss = 0.0067053647,0.071509205\n",
      "Iteration 01570: loss = 0.0067042075,0.07105413\n",
      "Iteration 01575: loss = 0.006703052,0.070603274\n",
      "Iteration 01580: loss = 0.0067019085,0.07015689\n",
      "Iteration 01585: loss = 0.006700765,0.06971457\n",
      "Iteration 01590: loss = 0.0066996366,0.069276646\n",
      "Iteration 01595: loss = 0.006698529,0.06884276\n",
      "Iteration 01600: loss = 0.0066974107,0.06841304\n",
      "Iteration 01605: loss = 0.006696301,0.06798729\n",
      "Iteration 01610: loss = 0.006695204,0.06756558\n",
      "Iteration 01615: loss = 0.006694115,0.06714786\n",
      "Iteration 01620: loss = 0.006693024,0.06673399\n",
      "Iteration 01625: loss = 0.0066919434,0.066323906\n",
      "Iteration 01630: loss = 0.006690884,0.06591768\n",
      "Iteration 01635: loss = 0.0066898186,0.06551507\n",
      "Iteration 01640: loss = 0.006688763,0.065116316\n",
      "Iteration 01645: loss = 0.006687719,0.06472116\n",
      "Iteration 01650: loss = 0.0066866726,0.064329624\n",
      "Iteration 01655: loss = 0.0066856393,0.06394172\n",
      "Iteration 01660: loss = 0.0066846125,0.06355719\n",
      "Iteration 01665: loss = 0.006683592,0.06317615\n",
      "Iteration 01670: loss = 0.006682575,0.06279857\n",
      "Iteration 01675: loss = 0.0066815647,0.06242443\n",
      "Iteration 01680: loss = 0.0066805617,0.062053546\n",
      "Iteration 01685: loss = 0.006679565,0.061686017\n",
      "Iteration 01690: loss = 0.006678576,0.061321795\n",
      "Iteration 01695: loss = 0.006677588,0.060960785\n",
      "Iteration 01700: loss = 0.0066766143,0.06060289\n",
      "Iteration 01705: loss = 0.0066756275,0.06024815\n",
      "Iteration 01710: loss = 0.006674677,0.0598966\n",
      "Iteration 01715: loss = 0.0066737155,0.05954807\n",
      "Iteration 01720: loss = 0.00667276,0.05920261\n",
      "Iteration 01725: loss = 0.006671816,0.058860123\n",
      "Iteration 01730: loss = 0.006670859,0.058520555\n",
      "Iteration 01735: loss = 0.006669926,0.058183953\n",
      "Iteration 01740: loss = 0.006668981,0.057850253\n",
      "Iteration 01745: loss = 0.0066680647,0.057519376\n",
      "Iteration 01750: loss = 0.0066671404,0.057191376\n",
      "Iteration 01755: loss = 0.006666228,0.056866184\n",
      "Iteration 01760: loss = 0.0066653085,0.056543685\n",
      "Iteration 01765: loss = 0.006664403,0.056223862\n",
      "Iteration 01770: loss = 0.0066635204,0.05590692\n",
      "Iteration 01775: loss = 0.0066626235,0.055592455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 01780: loss = 0.00666172,0.055280745\n",
      "Iteration 01785: loss = 0.006660832,0.0549715\n",
      "Iteration 01790: loss = 0.0066599525,0.054664936\n",
      "Iteration 01795: loss = 0.006659076,0.054360982\n",
      "Iteration 01800: loss = 0.006658202,0.054059416\n",
      "Iteration 01805: loss = 0.0066573345,0.053760424\n",
      "Iteration 01810: loss = 0.006656473,0.053463794\n",
      "Iteration 01815: loss = 0.0066556227,0.05316966\n",
      "Iteration 01820: loss = 0.0066547696,0.052877862\n",
      "Iteration 01825: loss = 0.006653925,0.05258853\n",
      "Iteration 01830: loss = 0.0066530644,0.05230145\n",
      "Iteration 01835: loss = 0.00665224,0.05201672\n",
      "Iteration 01840: loss = 0.006651407,0.05173439\n",
      "Iteration 01845: loss = 0.0066505834,0.051454194\n",
      "Iteration 01850: loss = 0.006649746,0.05117632\n",
      "Iteration 01855: loss = 0.0066489354,0.050900586\n",
      "Iteration 01860: loss = 0.0066481195,0.05062709\n",
      "Iteration 01865: loss = 0.0066473014,0.05035575\n",
      "Iteration 01870: loss = 0.006646503,0.050086573\n",
      "Iteration 01875: loss = 0.006645687,0.0498195\n",
      "Iteration 01880: loss = 0.006644892,0.049554557\n",
      "Iteration 01885: loss = 0.0066441013,0.049291596\n",
      "Iteration 01890: loss = 0.0066433153,0.04903079\n",
      "Iteration 01895: loss = 0.0066425125,0.04877198\n",
      "Iteration 01900: loss = 0.006641747,0.048515208\n",
      "Iteration 01905: loss = 0.006640962,0.048260376\n",
      "Iteration 01910: loss = 0.0066401954,0.048007477\n",
      "Iteration 01915: loss = 0.006639423,0.04775652\n",
      "Iteration 01920: loss = 0.006638643,0.04750759\n",
      "Iteration 01925: loss = 0.0066378918,0.04726047\n",
      "Iteration 01930: loss = 0.0066371295,0.047015272\n",
      "Iteration 01935: loss = 0.0066363695,0.04677191\n",
      "Iteration 01940: loss = 0.006635633,0.04653047\n",
      "Iteration 01945: loss = 0.0066348943,0.046290793\n",
      "Iteration 01950: loss = 0.006634128,0.04605282\n",
      "Iteration 01955: loss = 0.006633399,0.045816712\n",
      "Iteration 01960: loss = 0.006632664,0.045582376\n",
      "Iteration 01965: loss = 0.0066319294,0.045349766\n",
      "Iteration 01970: loss = 0.006631195,0.04511892\n",
      "Iteration 01975: loss = 0.0066304766,0.04488978\n",
      "Iteration 01980: loss = 0.006629763,0.044662327\n",
      "Iteration 01985: loss = 0.006629038,0.044436473\n",
      "Iteration 01990: loss = 0.0066283247,0.044212334\n",
      "Iteration 01995: loss = 0.0066276076,0.043989815\n",
      "Iteration 02000: loss = 0.006626911,0.04376888\n",
      "Iteration 02005: loss = 0.006626208,0.043549653\n",
      "Iteration 02010: loss = 0.006625499,0.043331996\n",
      "Iteration 02015: loss = 0.0066248053,0.043115903\n",
      "Iteration 02020: loss = 0.0066241114,0.042901307\n",
      "Iteration 02025: loss = 0.0066234213,0.042688243\n",
      "Iteration 02030: loss = 0.0066227377,0.042476762\n",
      "Iteration 02035: loss = 0.006622055,0.04226677\n",
      "Iteration 02040: loss = 0.0066213664,0.042058267\n",
      "Iteration 02045: loss = 0.0066206944,0.041851226\n",
      "Iteration 02050: loss = 0.006620023,0.04164568\n",
      "Iteration 02055: loss = 0.0066193533,0.041441567\n",
      "Iteration 02060: loss = 0.0066186804,0.041238926\n",
      "Iteration 02065: loss = 0.0066180225,0.041037627\n",
      "Iteration 02070: loss = 0.0066173547,0.040837765\n",
      "Iteration 02075: loss = 0.0066167046,0.040639304\n",
      "Iteration 02080: loss = 0.00661605,0.04044217\n",
      "Iteration 02085: loss = 0.006615395,0.04024654\n",
      "Iteration 02090: loss = 0.0066147377,0.040052168\n",
      "Iteration 02095: loss = 0.00661411,0.03985911\n",
      "Iteration 02100: loss = 0.0066134655,0.039667398\n",
      "Iteration 02105: loss = 0.0066128294,0.039476972\n",
      "Iteration 02110: loss = 0.0066121803,0.039287865\n",
      "Iteration 02115: loss = 0.006611556,0.039100118\n",
      "Iteration 02120: loss = 0.0066109165,0.038913563\n",
      "Iteration 02125: loss = 0.0066102915,0.03872823\n",
      "Iteration 02130: loss = 0.0066096596,0.038544264\n",
      "Iteration 02135: loss = 0.006609037,0.03836137\n",
      "Iteration 02140: loss = 0.0066084205,0.038179833\n",
      "Iteration 02145: loss = 0.006607806,0.0379995\n",
      "Iteration 02150: loss = 0.0066071935,0.0378203\n",
      "Iteration 02155: loss = 0.0066065877,0.037642337\n",
      "Iteration 02160: loss = 0.0066059767,0.037465524\n",
      "Iteration 02165: loss = 0.006605373,0.0372899\n",
      "Iteration 02170: loss = 0.0066047735,0.037115473\n",
      "Iteration 02175: loss = 0.0066041667,0.0369421\n",
      "Iteration 02180: loss = 0.0066035725,0.036769904\n",
      "Iteration 02185: loss = 0.006602975,0.03659882\n",
      "Iteration 02190: loss = 0.006602393,0.036428876\n",
      "Iteration 02195: loss = 0.0066017904,0.036260027\n",
      "Iteration 02200: loss = 0.006601209,0.036092322\n",
      "Iteration 02205: loss = 0.0066006277,0.035925668\n",
      "Iteration 02210: loss = 0.0066000517,0.03576003\n",
      "Iteration 02215: loss = 0.0065994724,0.03559541\n",
      "Iteration 02220: loss = 0.006598892,0.035431985\n",
      "Iteration 02225: loss = 0.006598309,0.035269555\n",
      "Iteration 02230: loss = 0.006597741,0.035108116\n",
      "Iteration 02235: loss = 0.0065971743,0.03494773\n",
      "Iteration 02240: loss = 0.0065966067,0.034788318\n",
      "Iteration 02245: loss = 0.0065960404,0.03462998\n",
      "Iteration 02250: loss = 0.0065954826,0.03447261\n",
      "Iteration 02255: loss = 0.006594924,0.034316175\n",
      "Iteration 02260: loss = 0.006594365,0.034160756\n",
      "Iteration 02265: loss = 0.0065938174,0.034006365\n",
      "Iteration 02270: loss = 0.0065932632,0.033852834\n",
      "Iteration 02275: loss = 0.006592717,0.033700295\n",
      "Iteration 02280: loss = 0.0065921643,0.033548743\n",
      "Iteration 02285: loss = 0.0065916185,0.033398088\n",
      "Iteration 02290: loss = 0.0065910723,0.033248346\n",
      "Iteration 02295: loss = 0.006590525,0.033099566\n",
      "Iteration 02300: loss = 0.0065899924,0.03295163\n",
      "Iteration 02305: loss = 0.00658947,0.032804642\n",
      "Iteration 02310: loss = 0.0065889307,0.032658454\n",
      "Iteration 02315: loss = 0.0065884027,0.03251333\n",
      "Iteration 02320: loss = 0.006587865,0.032368973\n",
      "Iteration 02325: loss = 0.0065873377,0.032225512\n",
      "Iteration 02330: loss = 0.0065868087,0.03208289\n",
      "Iteration 02335: loss = 0.0065862914,0.031941164\n",
      "Iteration 02340: loss = 0.0065857675,0.031800266\n",
      "Iteration 02345: loss = 0.0065852497,0.031660203\n",
      "Iteration 02350: loss = 0.006584723,0.031521015\n",
      "Iteration 02355: loss = 0.0065842173,0.031382605\n",
      "Iteration 02360: loss = 0.0065837125,0.031245036\n",
      "Iteration 02365: loss = 0.0065831966,0.031108305\n",
      "Iteration 02370: loss = 0.0065826923,0.030972334\n",
      "Iteration 02375: loss = 0.006582187,0.030837186\n",
      "Iteration 02380: loss = 0.006581679,0.030702839\n",
      "Iteration 02385: loss = 0.0065811854,0.030569226\n",
      "Iteration 02390: loss = 0.006580673,0.030436419\n",
      "Iteration 02395: loss = 0.006580176,0.030304408\n",
      "Iteration 02400: loss = 0.006579689,0.030173184\n",
      "Iteration 02405: loss = 0.0065791854,0.030042637\n",
      "Iteration 02410: loss = 0.006578693,0.029912937\n",
      "Iteration 02415: loss = 0.0065782126,0.02978392\n",
      "Iteration 02420: loss = 0.006577714,0.029655645\n",
      "Iteration 02425: loss = 0.006577239,0.029528137\n",
      "Iteration 02430: loss = 0.0065767444,0.02940131\n",
      "Iteration 02435: loss = 0.006576268,0.029275224\n",
      "Iteration 02440: loss = 0.006575773,0.02914989\n",
      "Iteration 02445: loss = 0.0065753055,0.02902524\n",
      "Iteration 02450: loss = 0.0065748175,0.028901275\n",
      "Iteration 02455: loss = 0.006574353,0.028778045\n",
      "Iteration 02460: loss = 0.0065738675,0.028655536\n",
      "Iteration 02465: loss = 0.0065734037,0.028533697\n",
      "Iteration 02470: loss = 0.0065729297,0.02841252\n",
      "Iteration 02475: loss = 0.0065724603,0.028292028\n",
      "Iteration 02480: loss = 0.0065720025,0.028172225\n",
      "Iteration 02485: loss = 0.0065715387,0.028053055\n",
      "Iteration 02490: loss = 0.006571073,0.027934538\n",
      "Iteration 02495: loss = 0.0065706167,0.027816711\n",
      "Iteration 02500: loss = 0.0065701385,0.027699538\n",
      "Iteration 02505: loss = 0.006569702,0.027582996\n",
      "Iteration 02510: loss = 0.0065692235,0.0274671\n",
      "Iteration 02515: loss = 0.0065687704,0.027351875\n",
      "Iteration 02520: loss = 0.00656833,0.027237097\n",
      "Iteration 02525: loss = 0.006567865,0.027123164\n",
      "Iteration 02530: loss = 0.0065674237,0.027009828\n",
      "Iteration 02535: loss = 0.0065669715,0.026897\n",
      "Iteration 02540: loss = 0.0065665278,0.02678487\n",
      "Iteration 02545: loss = 0.0065660826,0.026673313\n",
      "Iteration 02550: loss = 0.006565636,0.026562355\n",
      "Iteration 02555: loss = 0.006565199,0.026452022\n",
      "Iteration 02560: loss = 0.00656475,0.026342247\n",
      "Iteration 02565: loss = 0.006564314,0.026233101\n",
      "Iteration 02570: loss = 0.0065638777,0.026124455\n",
      "Iteration 02575: loss = 0.006563455,0.026016539\n",
      "Iteration 02580: loss = 0.006563008,0.025909085\n",
      "Iteration 02585: loss = 0.006562576,0.025802154\n",
      "Iteration 02590: loss = 0.0065621487,0.025695872\n",
      "Iteration 02595: loss = 0.0065617138,0.025590152\n",
      "Iteration 02600: loss = 0.0065612844,0.025484964\n",
      "Iteration 02605: loss = 0.0065608625,0.02538039\n",
      "Iteration 02610: loss = 0.0065604295,0.02527626\n",
      "Iteration 02615: loss = 0.0065600076,0.025172751\n",
      "Iteration 02620: loss = 0.006559598,0.025069796\n",
      "Iteration 02625: loss = 0.0065591694,0.02496739\n",
      "Iteration 02630: loss = 0.006558751,0.024865445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 02635: loss = 0.0065583284,0.024764106\n",
      "Iteration 02640: loss = 0.0065579135,0.0246632\n",
      "Iteration 02645: loss = 0.006557506,0.024562892\n",
      "Iteration 02650: loss = 0.00655709,0.024463069\n",
      "Iteration 02655: loss = 0.006556677,0.02436374\n",
      "Iteration 02660: loss = 0.0065562613,0.024264963\n",
      "Iteration 02665: loss = 0.0065558474,0.02416663\n",
      "Iteration 02670: loss = 0.0065554474,0.024068926\n",
      "Iteration 02675: loss = 0.0065550376,0.023971627\n",
      "Iteration 02680: loss = 0.006554629,0.023874843\n",
      "Iteration 02685: loss = 0.006554236,0.023778578\n",
      "Iteration 02690: loss = 0.006553817,0.023682773\n",
      "Iteration 02695: loss = 0.006553412,0.023587437\n",
      "Iteration 02700: loss = 0.0065530245,0.02349264\n",
      "Iteration 02705: loss = 0.006552608,0.023398325\n",
      "Iteration 02710: loss = 0.0065522236,0.023304442\n",
      "Iteration 02715: loss = 0.006551822,0.023210987\n",
      "Iteration 02720: loss = 0.006551431,0.023118071\n",
      "Iteration 02725: loss = 0.0065510334,0.023025589\n",
      "Iteration 02730: loss = 0.006550642,0.022933653\n",
      "Iteration 02735: loss = 0.006550252,0.022842059\n",
      "Iteration 02740: loss = 0.006549867,0.022750938\n",
      "Iteration 02745: loss = 0.0065494576,0.02266039\n",
      "Iteration 02750: loss = 0.006549077,0.02257015\n",
      "Iteration 02755: loss = 0.0065486864,0.0224804\n",
      "Iteration 02760: loss = 0.0065482943,0.02239112\n",
      "Iteration 02765: loss = 0.00654792,0.022302272\n",
      "Iteration 02770: loss = 0.0065475316,0.02221389\n",
      "Iteration 02775: loss = 0.0065471474,0.022125896\n",
      "Iteration 02780: loss = 0.0065467698,0.022038382\n",
      "Iteration 02785: loss = 0.006546386,0.021951227\n",
      "Iteration 02790: loss = 0.0065460145,0.021864545\n",
      "Iteration 02795: loss = 0.006545631,0.021778276\n",
      "Iteration 02800: loss = 0.0065452573,0.021692438\n",
      "Iteration 02805: loss = 0.0065448866,0.021606972\n",
      "Iteration 02810: loss = 0.006544499,0.021521984\n",
      "Iteration 02815: loss = 0.0065441374,0.021437375\n",
      "Iteration 02820: loss = 0.0065437504,0.021353174\n",
      "Iteration 02825: loss = 0.0065433946,0.021269377\n",
      "Iteration 02830: loss = 0.0065430175,0.021185976\n",
      "Iteration 02835: loss = 0.0065426547,0.021102993\n",
      "Iteration 02840: loss = 0.006542281,0.021020409\n",
      "Iteration 02845: loss = 0.0065419185,0.020938233\n",
      "Iteration 02850: loss = 0.0065415483,0.020856446\n",
      "Iteration 02855: loss = 0.006541183,0.020775022\n",
      "Iteration 02860: loss = 0.0065408237,0.020694003\n",
      "Iteration 02865: loss = 0.0065404517,0.020613384\n",
      "Iteration 02870: loss = 0.006540094,0.020533163\n",
      "Iteration 02875: loss = 0.0065397397,0.020453308\n",
      "Iteration 02880: loss = 0.006539365,0.020373786\n",
      "Iteration 02885: loss = 0.006539006,0.020294728\n",
      "Iteration 02890: loss = 0.006538657,0.020215943\n",
      "Iteration 02895: loss = 0.006538293,0.020137582\n",
      "Iteration 02900: loss = 0.006537944,0.020059582\n",
      "Iteration 02905: loss = 0.006537584,0.019981932\n",
      "Iteration 02910: loss = 0.006537233,0.019904684\n",
      "Iteration 02915: loss = 0.006536883,0.019827811\n",
      "Iteration 02920: loss = 0.006536542,0.01975126\n",
      "Iteration 02925: loss = 0.0065361722,0.019675065\n",
      "Iteration 02930: loss = 0.0065358244,0.019599237\n",
      "Iteration 02935: loss = 0.0065354854,0.019523798\n",
      "Iteration 02940: loss = 0.0065351245,0.019448705\n",
      "Iteration 02945: loss = 0.0065347734,0.019373866\n",
      "Iteration 02950: loss = 0.006534446,0.019299425\n",
      "Iteration 02955: loss = 0.006534089,0.019225378\n",
      "Iteration 02960: loss = 0.0065337433,0.019151602\n",
      "Iteration 02965: loss = 0.006533394,0.019078214\n",
      "Iteration 02970: loss = 0.006533058,0.019005151\n",
      "Iteration 02975: loss = 0.006532712,0.018932417\n",
      "Iteration 02980: loss = 0.006532365,0.018860035\n",
      "Iteration 02985: loss = 0.0065320334,0.018788\n",
      "Iteration 02990: loss = 0.0065316833,0.018716205\n",
      "Iteration 02995: loss = 0.0065313526,0.01864482\n",
      "Iteration 03000: loss = 0.00653102,0.018573761\n",
      "Iteration 03005: loss = 0.006530681,0.018502973\n",
      "Iteration 03010: loss = 0.006530342,0.018432552\n",
      "Iteration 03015: loss = 0.006530003,0.018362459\n",
      "Iteration 03020: loss = 0.0065296744,0.018292692\n",
      "Iteration 03025: loss = 0.0065293373,0.018223181\n",
      "Iteration 03030: loss = 0.0065290197,0.018154042\n",
      "Iteration 03035: loss = 0.006528675,0.018085135\n",
      "Iteration 03040: loss = 0.006528344,0.018016633\n",
      "Iteration 03045: loss = 0.00652802,0.017948398\n",
      "Iteration 03050: loss = 0.0065276735,0.01788045\n",
      "Iteration 03055: loss = 0.0065273545,0.017812863\n",
      "Iteration 03060: loss = 0.006527027,0.017745536\n",
      "Iteration 03065: loss = 0.0065267035,0.017678533\n",
      "Iteration 03070: loss = 0.006526373,0.017611818\n",
      "Iteration 03075: loss = 0.0065260455,0.01754538\n",
      "Iteration 03080: loss = 0.006525707,0.017479297\n",
      "Iteration 03085: loss = 0.0065253954,0.01741344\n",
      "Iteration 03090: loss = 0.006525068,0.017347952\n",
      "Iteration 03095: loss = 0.006524741,0.017282672\n",
      "Iteration 03100: loss = 0.006524423,0.017217727\n",
      "Iteration 03105: loss = 0.006524104,0.017153036\n",
      "Iteration 03110: loss = 0.0065237754,0.017088678\n",
      "Iteration 03115: loss = 0.0065234643,0.017024584\n",
      "Iteration 03120: loss = 0.006523147,0.01696077\n",
      "Iteration 03125: loss = 0.006522827,0.016897228\n",
      "Iteration 03130: loss = 0.0065225256,0.01683401\n",
      "Iteration 03135: loss = 0.006522203,0.016771048\n",
      "Iteration 03140: loss = 0.0065218764,0.016708309\n",
      "Iteration 03145: loss = 0.006521559,0.016645938\n",
      "Iteration 03150: loss = 0.00652124,0.016583802\n",
      "Iteration 03155: loss = 0.006520923,0.016521888\n",
      "Iteration 03160: loss = 0.006520619,0.016460309\n",
      "Iteration 03165: loss = 0.0065202913,0.016398977\n",
      "Iteration 03170: loss = 0.006519996,0.016337851\n",
      "Iteration 03175: loss = 0.0065196804,0.016277136\n",
      "Iteration 03180: loss = 0.006519373,0.016216554\n",
      "Iteration 03185: loss = 0.0065190606,0.016156273\n",
      "Iteration 03190: loss = 0.006518744,0.016096266\n",
      "Iteration 03195: loss = 0.00651844,0.01603651\n",
      "Iteration 03200: loss = 0.006518133,0.015977032\n",
      "Iteration 03205: loss = 0.0065178336,0.015917793\n",
      "Iteration 03210: loss = 0.0065175216,0.015858782\n",
      "Iteration 03215: loss = 0.0065172166,0.015800087\n",
      "Iteration 03220: loss = 0.0065169185,0.015741603\n",
      "Iteration 03225: loss = 0.006516605,0.015683368\n",
      "Iteration 03230: loss = 0.006516298,0.015625434\n",
      "Iteration 03235: loss = 0.006516002,0.015567702\n",
      "Iteration 03240: loss = 0.006515689,0.015510221\n",
      "Iteration 03245: loss = 0.00651539,0.015453027\n",
      "Iteration 03250: loss = 0.0065150824,0.015396008\n",
      "Iteration 03255: loss = 0.0065147853,0.0153392665\n",
      "Iteration 03260: loss = 0.006514476,0.0152827725\n",
      "Iteration 03265: loss = 0.0065141753,0.015226518\n",
      "Iteration 03270: loss = 0.006513897,0.015170523\n",
      "Iteration 03275: loss = 0.0065135923,0.015114698\n",
      "Iteration 03280: loss = 0.006513297,0.015059174\n",
      "Iteration 03285: loss = 0.0065129935,0.015003835\n",
      "Iteration 03290: loss = 0.0065126917,0.014948764\n",
      "Iteration 03295: loss = 0.0065123863,0.014893945\n",
      "Iteration 03300: loss = 0.0065120975,0.014839353\n",
      "Iteration 03305: loss = 0.0065118135,0.014784966\n",
      "Iteration 03310: loss = 0.0065115057,0.014730785\n",
      "Iteration 03315: loss = 0.006511222,0.014676854\n",
      "Iteration 03320: loss = 0.0065109297,0.014623174\n",
      "Iteration 03325: loss = 0.006510628,0.014569713\n",
      "Iteration 03330: loss = 0.0065103327,0.01451649\n",
      "Iteration 03335: loss = 0.0065100403,0.014463436\n",
      "Iteration 03340: loss = 0.006509771,0.014410682\n",
      "Iteration 03345: loss = 0.0065094773,0.014358081\n",
      "Iteration 03350: loss = 0.0065091713,0.014305769\n",
      "Iteration 03355: loss = 0.006508874,0.014253624\n",
      "Iteration 03360: loss = 0.006508587,0.014201692\n",
      "Iteration 03365: loss = 0.0065083154,0.014150017\n",
      "Iteration 03370: loss = 0.0065080356,0.01409851\n",
      "Iteration 03375: loss = 0.0065077394,0.01404721\n",
      "Iteration 03380: loss = 0.006507427,0.013996187\n",
      "Iteration 03385: loss = 0.0065071434,0.013945349\n",
      "Iteration 03390: loss = 0.006506887,0.013894701\n",
      "Iteration 03395: loss = 0.0065066046,0.013844265\n",
      "Iteration 03400: loss = 0.0065062963,0.013794074\n",
      "Iteration 03405: loss = 0.0065059983,0.013744066\n",
      "Iteration 03410: loss = 0.0065057315,0.01369428\n",
      "Iteration 03415: loss = 0.006505461,0.01364464\n",
      "Iteration 03420: loss = 0.006505171,0.013595272\n",
      "Iteration 03425: loss = 0.0065048765,0.013546093\n",
      "Iteration 03430: loss = 0.0065046027,0.013497134\n",
      "Iteration 03435: loss = 0.0065043396,0.013448322\n",
      "Iteration 03440: loss = 0.0065040397,0.013399772\n",
      "Iteration 03445: loss = 0.0065037464,0.0133513715\n",
      "Iteration 03450: loss = 0.006503478,0.01330317\n",
      "Iteration 03455: loss = 0.006503221,0.01325518\n",
      "Iteration 03460: loss = 0.0065029226,0.013207415\n",
      "Iteration 03465: loss = 0.0065026362,0.013159833\n",
      "Iteration 03470: loss = 0.0065023787,0.013112425\n",
      "Iteration 03475: loss = 0.006502107,0.013065173\n",
      "Iteration 03480: loss = 0.0065018064,0.013018224\n",
      "Iteration 03485: loss = 0.006501526,0.012971442\n",
      "Iteration 03490: loss = 0.0065012774,0.012924752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 03495: loss = 0.0065009785,0.012878331\n",
      "Iteration 03500: loss = 0.0065006916,0.012832095\n",
      "Iteration 03505: loss = 0.0065004397,0.012785993\n",
      "Iteration 03510: loss = 0.0065001585,0.01274013\n",
      "Iteration 03515: loss = 0.0064998777,0.0126944585\n",
      "Iteration 03520: loss = 0.0064996234,0.012648938\n",
      "Iteration 03525: loss = 0.006499341,0.012603619\n",
      "Iteration 03530: loss = 0.0064990544,0.0125585\n",
      "Iteration 03535: loss = 0.0064987973,0.012513566\n",
      "Iteration 03540: loss = 0.0064985272,0.012468783\n",
      "Iteration 03545: loss = 0.0064982497,0.012424216\n",
      "Iteration 03550: loss = 0.0064979936,0.012379797\n",
      "Iteration 03555: loss = 0.0064977254,0.01233558\n",
      "Iteration 03560: loss = 0.0064974427,0.012291532\n",
      "Iteration 03565: loss = 0.0064971913,0.0122476565\n",
      "Iteration 03570: loss = 0.00649691,0.012203926\n",
      "Iteration 03575: loss = 0.0064966455,0.012160439\n",
      "Iteration 03580: loss = 0.0064963843,0.012117072\n",
      "Iteration 03585: loss = 0.006496094,0.012073888\n",
      "Iteration 03590: loss = 0.0064958446,0.012030868\n",
      "Iteration 03595: loss = 0.00649556,0.011988075\n",
      "Iteration 03600: loss = 0.0064952974,0.0119454255\n",
      "Iteration 03605: loss = 0.006495044,0.011902934\n",
      "Iteration 03610: loss = 0.0064947624,0.011860637\n",
      "Iteration 03615: loss = 0.0064945254,0.011818487\n",
      "Iteration 03620: loss = 0.0064942427,0.011776517\n",
      "Iteration 03625: loss = 0.006493984,0.011734717\n",
      "Iteration 03630: loss = 0.006493715,0.011693047\n",
      "Iteration 03635: loss = 0.006493459,0.011651569\n",
      "Iteration 03640: loss = 0.0064931978,0.011610281\n",
      "Iteration 03645: loss = 0.0064929333,0.011569141\n",
      "Iteration 03650: loss = 0.0064926767,0.011528169\n",
      "Iteration 03655: loss = 0.0064923964,0.01148736\n",
      "Iteration 03660: loss = 0.0064921565,0.011446676\n",
      "Iteration 03665: loss = 0.0064918734,0.011406202\n",
      "Iteration 03670: loss = 0.0064916406,0.011365846\n",
      "Iteration 03675: loss = 0.0064913607,0.011325675\n",
      "Iteration 03680: loss = 0.0064911186,0.011285637\n",
      "Iteration 03685: loss = 0.0064908485,0.011245822\n",
      "Iteration 03690: loss = 0.006490596,0.011206096\n",
      "Iteration 03695: loss = 0.0064903325,0.011166572\n",
      "Iteration 03700: loss = 0.0064900704,0.011127214\n",
      "Iteration 03705: loss = 0.006489824,0.011087983\n",
      "Iteration 03710: loss = 0.0064895605,0.011048865\n",
      "Iteration 03715: loss = 0.006489292,0.011009983\n",
      "Iteration 03720: loss = 0.006489053,0.01097119\n",
      "Iteration 03725: loss = 0.006488785,0.010932587\n",
      "Iteration 03730: loss = 0.006488551,0.01089411\n",
      "Iteration 03735: loss = 0.0064882785,0.010855807\n",
      "Iteration 03740: loss = 0.006488027,0.010817636\n",
      "Iteration 03745: loss = 0.0064877667,0.010779638\n",
      "Iteration 03750: loss = 0.006487517,0.010741781\n",
      "Iteration 03755: loss = 0.0064872648,0.0107040405\n",
      "Iteration 03760: loss = 0.0064870096,0.01066648\n",
      "Iteration 03765: loss = 0.006486755,0.010629056\n",
      "Iteration 03770: loss = 0.006486501,0.010591786\n",
      "Iteration 03775: loss = 0.0064862557,0.010554687\n",
      "Iteration 03780: loss = 0.0064859954,0.010517699\n",
      "Iteration 03785: loss = 0.006485745,0.010480865\n",
      "Iteration 03790: loss = 0.0064854976,0.010444169\n",
      "Iteration 03795: loss = 0.006485248,0.0104076\n",
      "Iteration 03800: loss = 0.00648501,0.010371252\n",
      "Iteration 03805: loss = 0.006484753,0.010334969\n",
      "Iteration 03810: loss = 0.0064845136,0.010298829\n",
      "Iteration 03815: loss = 0.006484246,0.010262875\n",
      "Iteration 03820: loss = 0.006484009,0.010227018\n",
      "Iteration 03825: loss = 0.006483763,0.010191311\n",
      "Iteration 03830: loss = 0.0064835027,0.010155764\n",
      "Iteration 03835: loss = 0.006483263,0.010120343\n",
      "Iteration 03840: loss = 0.006483019,0.010085059\n",
      "Iteration 03845: loss = 0.006482776,0.010049895\n",
      "Iteration 03850: loss = 0.006482525,0.010014886\n",
      "Iteration 03855: loss = 0.00648227,0.009980011\n",
      "Iteration 03860: loss = 0.0064820317,0.009945266\n",
      "Iteration 03865: loss = 0.0064817853,0.009910654\n",
      "Iteration 03870: loss = 0.0064815334,0.00987622\n",
      "Iteration 03875: loss = 0.0064812903,0.009841861\n",
      "Iteration 03880: loss = 0.006481061,0.00980763\n",
      "Iteration 03885: loss = 0.0064807967,0.009773598\n",
      "Iteration 03890: loss = 0.006480549,0.009739655\n",
      "Iteration 03895: loss = 0.0064803194,0.00970583\n",
      "Iteration 03900: loss = 0.0064800656,0.00967216\n",
      "Iteration 03905: loss = 0.0064798314,0.0096386\n",
      "Iteration 03910: loss = 0.0064795893,0.009605179\n",
      "Iteration 03915: loss = 0.0064793564,0.009571914\n",
      "Iteration 03920: loss = 0.006479092,0.009538763\n",
      "Iteration 03925: loss = 0.0064788735,0.009505687\n",
      "Iteration 03930: loss = 0.006478635,0.009472799\n",
      "Iteration 03935: loss = 0.0064783827,0.009439974\n",
      "Iteration 03940: loss = 0.0064781476,0.00940736\n",
      "Iteration 03945: loss = 0.0064779017,0.009374812\n",
      "Iteration 03950: loss = 0.006477678,0.009342415\n",
      "Iteration 03955: loss = 0.0064774216,0.009310175\n",
      "Iteration 03960: loss = 0.006477186,0.009278037\n",
      "Iteration 03965: loss = 0.0064769387,0.009245971\n",
      "Iteration 03970: loss = 0.006476721,0.009214018\n",
      "Iteration 03975: loss = 0.0064764656,0.009182292\n",
      "Iteration 03980: loss = 0.006476246,0.009150592\n",
      "Iteration 03985: loss = 0.006475992,0.009119085\n",
      "Iteration 03990: loss = 0.0064757788,0.009087645\n",
      "Iteration 03995: loss = 0.006475532,0.009056388\n",
      "Iteration 04000: loss = 0.006475294,0.009025168\n",
      "Iteration 04005: loss = 0.006475041,0.008994113\n",
      "Iteration 04010: loss = 0.006474824,0.00896317\n",
      "Iteration 04015: loss = 0.006474585,0.008932361\n",
      "Iteration 04020: loss = 0.0064743645,0.008901644\n",
      "Iteration 04025: loss = 0.0064741164,0.008871054\n",
      "Iteration 04030: loss = 0.0064738803,0.008840636\n",
      "Iteration 04035: loss = 0.0064736526,0.008810251\n",
      "Iteration 04040: loss = 0.0064734207,0.008780008\n",
      "Iteration 04045: loss = 0.0064731953,0.008749873\n",
      "Iteration 04050: loss = 0.006472956,0.008719856\n",
      "Iteration 04055: loss = 0.0064727203,0.008689978\n",
      "Iteration 04060: loss = 0.006472487,0.008660193\n",
      "Iteration 04065: loss = 0.006472251,0.008630558\n",
      "Iteration 04070: loss = 0.006472031,0.008600943\n",
      "Iteration 04075: loss = 0.006471813,0.008571526\n",
      "Iteration 04080: loss = 0.006471575,0.008542186\n",
      "Iteration 04085: loss = 0.006471341,0.008512989\n",
      "Iteration 04090: loss = 0.006471099,0.008483859\n",
      "Iteration 04095: loss = 0.006470883,0.008454881\n",
      "Iteration 04100: loss = 0.0064706416,0.008426002\n",
      "Iteration 04105: loss = 0.006470416,0.0083972365\n",
      "Iteration 04110: loss = 0.006470188,0.008368553\n",
      "Iteration 04115: loss = 0.0064699706,0.008339984\n",
      "Iteration 04120: loss = 0.0064697396,0.008311555\n",
      "Iteration 04125: loss = 0.0064695175,0.008283204\n",
      "Iteration 04130: loss = 0.0064692846,0.008254974\n",
      "Iteration 04135: loss = 0.0064690565,0.008226832\n",
      "Iteration 04140: loss = 0.006468825,0.0081988145\n",
      "Iteration 04145: loss = 0.0064685973,0.008170908\n",
      "Iteration 04150: loss = 0.006468367,0.008143099\n",
      "Iteration 04155: loss = 0.006468139,0.008115403\n",
      "Iteration 04160: loss = 0.006467901,0.008087784\n",
      "Iteration 04165: loss = 0.0064676893,0.008060271\n",
      "Iteration 04170: loss = 0.0064674695,0.008032899\n",
      "Iteration 04175: loss = 0.0064672525,0.008005608\n",
      "Iteration 04180: loss = 0.0064670234,0.007978458\n",
      "Iteration 04185: loss = 0.0064668017,0.007951356\n",
      "Iteration 04190: loss = 0.006466575,0.007924361\n",
      "Iteration 04195: loss = 0.0064663556,0.007897485\n",
      "Iteration 04200: loss = 0.006466133,0.007870706\n",
      "Iteration 04205: loss = 0.006465901,0.007844023\n",
      "Iteration 04210: loss = 0.006465689,0.007817466\n",
      "Iteration 04215: loss = 0.006465466,0.007790995\n",
      "Iteration 04220: loss = 0.0064652427,0.007764629\n",
      "Iteration 04225: loss = 0.006465021,0.0077383528\n",
      "Iteration 04230: loss = 0.0064648013,0.0077121593\n",
      "Iteration 04235: loss = 0.0064645875,0.007686066\n",
      "Iteration 04240: loss = 0.0064643635,0.007660113\n",
      "Iteration 04245: loss = 0.006464141,0.007634232\n",
      "Iteration 04250: loss = 0.006463922,0.0076084267\n",
      "Iteration 04255: loss = 0.0064637032,0.00758275\n",
      "Iteration 04260: loss = 0.0064634737,0.007557163\n",
      "Iteration 04265: loss = 0.0064632595,0.007531658\n",
      "Iteration 04270: loss = 0.0064630453,0.0075062346\n",
      "Iteration 04275: loss = 0.0064628324,0.007480937\n",
      "Iteration 04280: loss = 0.0064626136,0.007455715\n",
      "Iteration 04285: loss = 0.0064623803,0.00743064\n",
      "Iteration 04290: loss = 0.0064621703,0.0074056266\n",
      "Iteration 04295: loss = 0.0064619575,0.00738068\n",
      "Iteration 04300: loss = 0.0064617577,0.0073558073\n",
      "Iteration 04305: loss = 0.0064615295,0.0073310817\n",
      "Iteration 04310: loss = 0.0064613135,0.007306438\n",
      "Iteration 04315: loss = 0.0064611062,0.0072818594\n",
      "Iteration 04320: loss = 0.006460879,0.007257401\n",
      "Iteration 04325: loss = 0.0064606606,0.0072330753\n",
      "Iteration 04330: loss = 0.0064604487,0.007208788\n",
      "Iteration 04335: loss = 0.0064602313,0.007184554\n",
      "Iteration 04340: loss = 0.006460026,0.0071604755\n",
      "Iteration 04345: loss = 0.006459803,0.00713646\n",
      "Iteration 04350: loss = 0.0064596036,0.007112531\n",
      "Iteration 04355: loss = 0.006459382,0.007088677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 04360: loss = 0.0064591733,0.0070649497\n",
      "Iteration 04365: loss = 0.006458953,0.007041275\n",
      "Iteration 04370: loss = 0.006458737,0.0070177047\n",
      "Iteration 04375: loss = 0.00645854,0.006994226\n",
      "Iteration 04380: loss = 0.0064583346,0.006970822\n",
      "Iteration 04385: loss = 0.0064581255,0.006947517\n",
      "Iteration 04390: loss = 0.006457914,0.0069242856\n",
      "Iteration 04395: loss = 0.006457698,0.006901169\n",
      "Iteration 04400: loss = 0.0064574797,0.0068781315\n",
      "Iteration 04405: loss = 0.006457265,0.006855176\n",
      "Iteration 04410: loss = 0.0064570713,0.0068322625\n",
      "Iteration 04415: loss = 0.0064568613,0.0068094917\n",
      "Iteration 04420: loss = 0.006456649,0.006786797\n",
      "Iteration 04425: loss = 0.006456431,0.006764181\n",
      "Iteration 04430: loss = 0.006456221,0.0067416197\n",
      "Iteration 04435: loss = 0.0064560124,0.006719144\n",
      "Iteration 04440: loss = 0.0064558177,0.006696796\n",
      "Iteration 04445: loss = 0.006455589,0.0066744983\n",
      "Iteration 04450: loss = 0.006455396,0.0066523235\n",
      "Iteration 04455: loss = 0.006455196,0.0066301534\n",
      "Iteration 04460: loss = 0.0064549856,0.006608108\n",
      "Iteration 04465: loss = 0.0064547765,0.0065861517\n",
      "Iteration 04470: loss = 0.006454568,0.006564277\n",
      "Iteration 04475: loss = 0.0064543467,0.006542472\n",
      "Iteration 04480: loss = 0.0064541698,0.006520723\n",
      "Iteration 04485: loss = 0.0064539644,0.006499098\n",
      "Iteration 04490: loss = 0.006453738,0.006477542\n",
      "Iteration 04495: loss = 0.006453538,0.0064560645\n",
      "Iteration 04500: loss = 0.0064533446,0.006434638\n",
      "Iteration 04505: loss = 0.0064531355,0.0064133294\n",
      "Iteration 04510: loss = 0.006452922,0.0063920766\n",
      "Iteration 04515: loss = 0.006452734,0.0063708797\n",
      "Iteration 04520: loss = 0.006452536,0.006349831\n",
      "Iteration 04525: loss = 0.0064523183,0.0063288235\n",
      "Iteration 04530: loss = 0.0064521153,0.006307895\n",
      "Iteration 04535: loss = 0.0064519257,0.006287024\n",
      "Iteration 04540: loss = 0.006451709,0.0062662447\n",
      "Iteration 04545: loss = 0.006451495,0.0062455675\n",
      "Iteration 04550: loss = 0.0064513073,0.0062249033\n",
      "Iteration 04555: loss = 0.0064511183,0.0062043485\n",
      "Iteration 04560: loss = 0.0064509013,0.0061838767\n",
      "Iteration 04565: loss = 0.006450724,0.0061634565\n",
      "Iteration 04570: loss = 0.0064505227,0.0061431006\n",
      "Iteration 04575: loss = 0.006450306,0.006122883\n",
      "Iteration 04580: loss = 0.0064501143,0.006102695\n",
      "Iteration 04585: loss = 0.006449934,0.006082592\n",
      "Iteration 04590: loss = 0.0064497143,0.0060625607\n",
      "Iteration 04595: loss = 0.0064495336,0.0060425894\n",
      "Iteration 04600: loss = 0.006449332,0.0060227253\n",
      "Iteration 04605: loss = 0.0064491145,0.0060028965\n",
      "Iteration 04610: loss = 0.0064489315,0.005983157\n",
      "Iteration 04615: loss = 0.006448719,0.0059635118\n",
      "Iteration 04620: loss = 0.0064485115,0.0059438925\n",
      "Iteration 04625: loss = 0.0064483415,0.0059243473\n",
      "Iteration 04630: loss = 0.006448126,0.005904906\n",
      "Iteration 04635: loss = 0.006447956,0.005885489\n",
      "Iteration 04640: loss = 0.006447749,0.005866183\n",
      "Iteration 04645: loss = 0.006447526,0.005846971\n",
      "Iteration 04650: loss = 0.0064473595,0.005827753\n",
      "Iteration 04655: loss = 0.006447149,0.0058086873\n",
      "Iteration 04660: loss = 0.0064469785,0.005789599\n",
      "Iteration 04665: loss = 0.006446767,0.0057706633\n",
      "Iteration 04670: loss = 0.0064465427,0.0057517784\n",
      "Iteration 04675: loss = 0.0064463946,0.0057329237\n",
      "Iteration 04680: loss = 0.006446173,0.0057141827\n",
      "Iteration 04685: loss = 0.0064460128,0.0056954566\n",
      "Iteration 04690: loss = 0.0064457934,0.0056768632\n",
      "Iteration 04695: loss = 0.00644562,0.005658271\n",
      "Iteration 04700: loss = 0.006445398,0.0056398166\n",
      "Iteration 04705: loss = 0.006445221,0.0056213774\n",
      "Iteration 04710: loss = 0.006445075,0.005602973\n",
      "Iteration 04715: loss = 0.0064448095,0.0055847173\n",
      "Iteration 04720: loss = 0.006444661,0.005566466\n",
      "Iteration 04725: loss = 0.006444428,0.0055483794\n",
      "Iteration 04730: loss = 0.0064443075,0.005530189\n",
      "Iteration 04735: loss = 0.006444044,0.005512233\n",
      "Iteration 04740: loss = 0.006443905,0.0054942267\n",
      "Iteration 04745: loss = 0.006443665,0.005476394\n",
      "Iteration 04750: loss = 0.0064435587,0.005458462\n",
      "Iteration 04755: loss = 0.00644326,0.0054408507\n",
      "Iteration 04760: loss = 0.0064431992,0.0054230355\n",
      "Iteration 04765: loss = 0.0064428393,0.00540559\n",
      "Iteration 04770: loss = 0.00644286,0.0053878725\n",
      "Iteration 04775: loss = 0.006442399,0.0053708935\n",
      "Iteration 04780: loss = 0.0064426083,0.0053536356\n",
      "Iteration 04785: loss = 0.006441787,0.005338741\n",
      "Iteration 04790: loss = 0.0064427946,0.005328214\n",
      "Iteration 04795: loss = 0.0064399703,0.005361742\n",
      "Iteration 04800: loss = 0.00644639,0.0056768255\n",
      "Iteration 04805: loss = 0.006427986,0.008461392\n",
      "Iteration 04810: loss = 0.006475379,0.025694571\n",
      "Iteration 04815: loss = 0.0064139203,0.018870637\n",
      "Iteration 04820: loss = 0.006417641,0.015222127\n",
      "Iteration 04825: loss = 0.006439242,0.0052424986\n",
      "Iteration 04830: loss = 0.0064525553,0.007843524\n",
      "Iteration 04835: loss = 0.006451973,0.007595483\n",
      "Iteration 04840: loss = 0.0064433333,0.005314766\n",
      "Iteration 04845: loss = 0.006435633,0.0055247154\n",
      "Iteration 04850: loss = 0.006434412,0.005696797\n",
      "Iteration 04855: loss = 0.006438453,0.0051370244\n",
      "Iteration 04860: loss = 0.006442109,0.0052112774\n",
      "Iteration 04865: loss = 0.0064416328,0.00516944\n",
      "Iteration 04870: loss = 0.006438663,0.0050617135\n",
      "Iteration 04875: loss = 0.0064373235,0.0050929873\n",
      "Iteration 04880: loss = 0.0064386614,0.00502489\n",
      "Iteration 04885: loss = 0.0064396127,0.005026576\n",
      "Iteration 04890: loss = 0.0064385026,0.0049925046\n",
      "Iteration 04895: loss = 0.006437598,0.00498437\n",
      "Iteration 04900: loss = 0.0064381664,0.004960661\n",
      "Iteration 04905: loss = 0.0064382274,0.004946615\n",
      "Iteration 04910: loss = 0.0064374297,0.0049304105\n",
      "Iteration 04915: loss = 0.0064374753,0.0049132146\n",
      "Iteration 04920: loss = 0.006437566,0.004898223\n",
      "Iteration 04925: loss = 0.006436909,0.0048831217\n",
      "Iteration 04930: loss = 0.006437167,0.0048667\n",
      "Iteration 04935: loss = 0.006436664,0.004851177\n",
      "Iteration 04940: loss = 0.0064367354,0.0048353355\n",
      "Iteration 04945: loss = 0.0064362925,0.004820386\n",
      "Iteration 04950: loss = 0.0064365324,0.004805507\n",
      "Iteration 04955: loss = 0.006435495,0.004796267\n",
      "Iteration 04960: loss = 0.006437296,0.004808122\n",
      "Iteration 04965: loss = 0.006432187,0.0049963095\n",
      "Iteration 04970: loss = 0.0064456263,0.006603102\n",
      "Iteration 04975: loss = 0.006406668,0.020227596\n",
      "Iteration 04980: loss = 0.0064809886,0.045357335\n",
      "Iteration 04985: loss = 0.0064501953,0.009288687\n",
      "Iteration 04990: loss = 0.0064239656,0.007085002\n",
      "Iteration 04995: loss = 0.006416392,0.0116099855\n",
      "Iteration 05000: loss = 0.0064213243,0.008369496\n",
      "Iteration 05005: loss = 0.0064307204,0.0049658697\n",
      "Iteration 05010: loss = 0.0064382567,0.0049318857\n",
      "Iteration 05015: loss = 0.0064408244,0.005519939\n",
      "Iteration 05020: loss = 0.0064384914,0.0050195153\n",
      "Iteration 05025: loss = 0.0064340536,0.004590487\n",
      "Iteration 05030: loss = 0.0064310026,0.004716319\n",
      "Iteration 05035: loss = 0.0064310227,0.0046798033\n",
      "Iteration 05040: loss = 0.00643307,0.004547079\n",
      "Iteration 05045: loss = 0.006434442,0.0045685135\n",
      "Iteration 05050: loss = 0.006433835,0.004534208\n",
      "Iteration 05055: loss = 0.006432308,0.0045077717\n",
      "Iteration 05060: loss = 0.006431808,0.0045017907\n",
      "Iteration 05065: loss = 0.006432474,0.0044755824\n",
      "Iteration 05070: loss = 0.0064326953,0.0044656997\n",
      "Iteration 05075: loss = 0.0064319684,0.004447642\n",
      "Iteration 05080: loss = 0.00643163,0.004435083\n",
      "Iteration 05085: loss = 0.0064317933,0.0044197147\n",
      "Iteration 05090: loss = 0.0064316564,0.004405925\n",
      "Iteration 05095: loss = 0.0064312457,0.0043924144\n",
      "Iteration 05100: loss = 0.006431222,0.0043782373\n",
      "Iteration 05105: loss = 0.0064310767,0.0043645455\n",
      "Iteration 05110: loss = 0.0064307693,0.0043511954\n",
      "Iteration 05115: loss = 0.0064307996,0.0043374207\n",
      "Iteration 05120: loss = 0.006430423,0.00432407\n",
      "Iteration 05125: loss = 0.0064304913,0.0043103923\n",
      "Iteration 05130: loss = 0.006430024,0.004297451\n",
      "Iteration 05135: loss = 0.006430282,0.0042844545\n",
      "Iteration 05140: loss = 0.0064293146,0.004276523\n",
      "Iteration 05145: loss = 0.006430944,0.004287489\n",
      "Iteration 05150: loss = 0.0064263376,0.0044534444\n",
      "Iteration 05155: loss = 0.0064382455,0.005834908\n",
      "Iteration 05160: loss = 0.0064034187,0.018004864\n",
      "Iteration 05165: loss = 0.0064777634,0.055235837\n",
      "Iteration 05170: loss = 0.006435612,0.0052494616\n",
      "Iteration 05175: loss = 0.0064109727,0.011414284\n",
      "Iteration 05180: loss = 0.006408174,0.013906423\n",
      "Iteration 05185: loss = 0.006416301,0.0077072596\n",
      "Iteration 05190: loss = 0.006426539,0.0042417594\n",
      "Iteration 05195: loss = 0.006433533,0.0047625913\n",
      "Iteration 05200: loss = 0.006435195,0.0053022094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 05205: loss = 0.006432433,0.0045931623\n",
      "Iteration 05210: loss = 0.0064280084,0.004098366\n",
      "Iteration 05215: loss = 0.0064249337,0.0042435033\n",
      "Iteration 05220: loss = 0.006424744,0.0042325365\n",
      "Iteration 05225: loss = 0.0064266417,0.004066769\n",
      "Iteration 05230: loss = 0.0064282627,0.0040801764\n",
      "Iteration 05235: loss = 0.006428041,0.0040648496\n",
      "Iteration 05240: loss = 0.00642662,0.00402185\n",
      "Iteration 05245: loss = 0.006425774,0.004023641\n",
      "Iteration 05250: loss = 0.0064261667,0.0039981916\n",
      "Iteration 05255: loss = 0.006426653,0.00398872\n",
      "Iteration 05260: loss = 0.006426234,0.0039728554\n",
      "Iteration 05265: loss = 0.0064256233,0.00396231\n",
      "Iteration 05270: loss = 0.006425669,0.0039482196\n",
      "Iteration 05275: loss = 0.006425798,0.0039367275\n",
      "Iteration 05280: loss = 0.0064253714,0.00392389\n",
      "Iteration 05285: loss = 0.006425133,0.003912219\n",
      "Iteration 05290: loss = 0.006425234,0.0038999876\n",
      "Iteration 05295: loss = 0.006424896,0.003887855\n",
      "Iteration 05300: loss = 0.0064247637,0.0038758381\n",
      "Iteration 05305: loss = 0.006424673,0.0038639146\n",
      "Iteration 05310: loss = 0.0064244405,0.0038521127\n",
      "Iteration 05315: loss = 0.0064243437,0.0038402458\n",
      "Iteration 05320: loss = 0.0064241397,0.0038285027\n",
      "Iteration 05325: loss = 0.0064240303,0.0038167168\n",
      "Iteration 05330: loss = 0.00642377,0.0038052052\n",
      "Iteration 05335: loss = 0.0064238436,0.0037938417\n",
      "Iteration 05340: loss = 0.0064230966,0.0037864312\n",
      "Iteration 05345: loss = 0.006424571,0.0038021295\n",
      "Iteration 05350: loss = 0.0064195073,0.0040804\n",
      "Iteration 05355: loss = 0.0064355303,0.007332087\n",
      "Iteration 05360: loss = 0.006382819,0.04239424\n",
      "Iteration 05365: loss = 0.006463419,0.047548044\n",
      "Iteration 05370: loss = 0.0064579877,0.03744493\n",
      "Iteration 05375: loss = 0.0064430335,0.015550475\n",
      "Iteration 05380: loss = 0.006431429,0.0060482584\n",
      "Iteration 05385: loss = 0.0064238515,0.0037453058\n",
      "Iteration 05390: loss = 0.0064191245,0.0039852024\n",
      "Iteration 05395: loss = 0.0064164675,0.0045994404\n",
      "Iteration 05400: loss = 0.0064156204,0.004811242\n",
      "Iteration 05405: loss = 0.0064161527,0.004556397\n",
      "Iteration 05410: loss = 0.0064175427,0.0041024433\n",
      "Iteration 05415: loss = 0.0064193173,0.0037447424\n",
      "Iteration 05420: loss = 0.0064210207,0.0036141658\n",
      "Iteration 05425: loss = 0.006422136,0.0036327073\n",
      "Iteration 05430: loss = 0.006422399,0.0036526576\n",
      "Iteration 05435: loss = 0.0064219073,0.0036181894\n",
      "Iteration 05440: loss = 0.006421013,0.0035742912\n",
      "Iteration 05445: loss = 0.0064201783,0.0035612653\n",
      "Iteration 05450: loss = 0.006419758,0.003557297\n",
      "Iteration 05455: loss = 0.0064197765,0.0035418514\n",
      "Iteration 05460: loss = 0.0064200354,0.0035268592\n",
      "Iteration 05465: loss = 0.006420126,0.0035178924\n",
      "Iteration 05470: loss = 0.0064198878,0.0035064186\n",
      "Iteration 05475: loss = 0.0064194775,0.0034953617\n",
      "Iteration 05480: loss = 0.006419236,0.0034855187\n",
      "Iteration 05485: loss = 0.0064192084,0.003474242\n",
      "Iteration 05490: loss = 0.0064192065,0.003463982\n",
      "Iteration 05495: loss = 0.0064189923,0.0034533367\n",
      "Iteration 05500: loss = 0.0064187334,0.0034431908\n",
      "Iteration 05505: loss = 0.006418641,0.003432666\n",
      "Iteration 05510: loss = 0.0064185467,0.003422366\n",
      "Iteration 05515: loss = 0.006418355,0.003412082\n",
      "Iteration 05520: loss = 0.0064181727,0.003401887\n",
      "Iteration 05525: loss = 0.0064180735,0.003391609\n",
      "Iteration 05530: loss = 0.0064179036,0.0033814397\n",
      "Iteration 05535: loss = 0.0064177606,0.0033712883\n",
      "Iteration 05540: loss = 0.0064176326,0.003361171\n",
      "Iteration 05545: loss = 0.0064174472,0.003351131\n",
      "Iteration 05550: loss = 0.0064173476,0.0033410462\n",
      "Iteration 05555: loss = 0.006417129,0.003331145\n",
      "Iteration 05560: loss = 0.006417072,0.003321047\n",
      "Iteration 05565: loss = 0.006416783,0.0033113586\n",
      "Iteration 05570: loss = 0.006416908,0.0033018019\n",
      "Iteration 05575: loss = 0.0064161792,0.003296352\n",
      "Iteration 05580: loss = 0.0064174496,0.0033088778\n",
      "Iteration 05585: loss = 0.0064134826,0.0034985882\n",
      "Iteration 05590: loss = 0.0064249784,0.0054409653\n",
      "Iteration 05595: loss = 0.0063878545,0.026130447\n",
      "Iteration 05600: loss = 0.0064659226,0.08146232\n",
      "Iteration 05605: loss = 0.0064355913,0.016586568\n",
      "Iteration 05610: loss = 0.0064131045,0.0033758623\n",
      "Iteration 05615: loss = 0.00640381,0.008065452\n",
      "Iteration 05620: loss = 0.0064020385,0.010023231\n",
      "Iteration 05625: loss = 0.006403808,0.008287275\n",
      "Iteration 05630: loss = 0.0064072036,0.005560092\n",
      "Iteration 05635: loss = 0.006411208,0.003719469\n",
      "Iteration 05640: loss = 0.0064147464,0.0031762053\n",
      "Iteration 05645: loss = 0.0064169797,0.0033490872\n",
      "Iteration 05650: loss = 0.0064176326,0.0034948073\n",
      "Iteration 05655: loss = 0.006416906,0.0033624535\n",
      "Iteration 05660: loss = 0.0064153536,0.0031764877\n",
      "Iteration 05665: loss = 0.0064137853,0.003134224\n",
      "Iteration 05670: loss = 0.006412933,0.0031610033\n",
      "Iteration 05675: loss = 0.0064129867,0.0031384174\n",
      "Iteration 05680: loss = 0.006413529,0.0031030276\n",
      "Iteration 05685: loss = 0.0064139306,0.0030972222\n",
      "Iteration 05690: loss = 0.006413854,0.003089657\n",
      "Iteration 05695: loss = 0.0064133625,0.0030744062\n",
      "Iteration 05700: loss = 0.006412919,0.0030674054\n",
      "Iteration 05705: loss = 0.006412843,0.0030573476\n",
      "Iteration 05710: loss = 0.006412947,0.0030474248\n",
      "Iteration 05715: loss = 0.00641285,0.0030387577\n",
      "Iteration 05720: loss = 0.0064125494,0.003029419\n",
      "Iteration 05725: loss = 0.0064123743,0.0030207718\n",
      "Iteration 05730: loss = 0.0064123296,0.003011558\n",
      "Iteration 05735: loss = 0.0064122025,0.0030027316\n",
      "Iteration 05740: loss = 0.0064120037,0.0029939162\n",
      "Iteration 05745: loss = 0.006411874,0.002985079\n",
      "Iteration 05750: loss = 0.006411763,0.0029763065\n",
      "Iteration 05755: loss = 0.0064115953,0.002967591\n",
      "Iteration 05760: loss = 0.0064114532,0.0029588412\n",
      "Iteration 05765: loss = 0.006411366,0.0029501491\n",
      "Iteration 05770: loss = 0.0064111613,0.0029415584\n",
      "Iteration 05775: loss = 0.006411096,0.0029328172\n",
      "Iteration 05780: loss = 0.006410886,0.0029242546\n",
      "Iteration 05785: loss = 0.0064108223,0.0029156024\n",
      "Iteration 05790: loss = 0.006410563,0.0029072787\n",
      "Iteration 05795: loss = 0.0064106383,0.0028990838\n",
      "Iteration 05800: loss = 0.006410031,0.0028946127\n",
      "Iteration 05805: loss = 0.0064111203,0.002908439\n",
      "Iteration 05810: loss = 0.0064075426,0.0031016604\n",
      "Iteration 05815: loss = 0.006417889,0.005059404\n",
      "Iteration 05820: loss = 0.0063843713,0.026157953\n",
      "Iteration 05825: loss = 0.006457273,0.09167766\n",
      "Iteration 05830: loss = 0.006425841,0.014649741\n",
      "Iteration 05835: loss = 0.006405147,0.003473057\n",
      "Iteration 05840: loss = 0.0063976063,0.009315928\n",
      "Iteration 05845: loss = 0.0063966042,0.010904968\n",
      "Iteration 05850: loss = 0.006398428,0.008561251\n",
      "Iteration 05855: loss = 0.0064016324,0.005439967\n",
      "Iteration 05860: loss = 0.0064053647,0.0034112989\n",
      "Iteration 05865: loss = 0.006408529,0.0027945866\n",
      "Iteration 05870: loss = 0.00641048,0.0029563773\n",
      "Iteration 05875: loss = 0.0064112055,0.0031325058\n",
      "Iteration 05880: loss = 0.006410713,0.0030214826\n",
      "Iteration 05885: loss = 0.006409386,0.0028192466\n",
      "Iteration 05890: loss = 0.0064079263,0.0027521723\n",
      "Iteration 05895: loss = 0.006407034,0.0027794286\n",
      "Iteration 05900: loss = 0.0064069214,0.002769468\n",
      "Iteration 05905: loss = 0.0064073727,0.0027314306\n",
      "Iteration 05910: loss = 0.00640778,0.0027225765\n",
      "Iteration 05915: loss = 0.0064077764,0.00271834\n",
      "Iteration 05920: loss = 0.006407388,0.0027045226\n",
      "Iteration 05925: loss = 0.0064069736,0.002697373\n",
      "Iteration 05930: loss = 0.006406803,0.002690068\n",
      "Iteration 05935: loss = 0.0064068823,0.00268059\n",
      "Iteration 05940: loss = 0.006406857,0.0026734774\n",
      "Iteration 05945: loss = 0.0064066015,0.0026651635\n",
      "Iteration 05950: loss = 0.0064063673,0.002657916\n",
      "Iteration 05955: loss = 0.0064063114,0.002649835\n",
      "Iteration 05960: loss = 0.0064062644,0.0026423049\n",
      "Iteration 05965: loss = 0.0064060413,0.002634587\n",
      "Iteration 05970: loss = 0.0064058923,0.0026270533\n",
      "Iteration 05975: loss = 0.006405832,0.002619401\n",
      "Iteration 05980: loss = 0.006405665,0.002611855\n",
      "Iteration 05985: loss = 0.006405531,0.0026043477\n",
      "Iteration 05990: loss = 0.0064054006,0.002596811\n",
      "Iteration 05995: loss = 0.0064052925,0.0025893399\n",
      "Iteration 06000: loss = 0.0064051044,0.0025819724\n",
      "Iteration 06005: loss = 0.0064050816,0.0025744615\n",
      "Iteration 06010: loss = 0.0064048036,0.0025674803\n",
      "Iteration 06015: loss = 0.0064049126,0.0025606267\n",
      "Iteration 06020: loss = 0.006404323,0.0025567517\n",
      "Iteration 06025: loss = 0.006405147,0.002563107\n",
      "Iteration 06030: loss = 0.006402819,0.0026482935\n",
      "Iteration 06035: loss = 0.00640842,0.0033158534\n",
      "Iteration 06040: loss = 0.0063919746,0.009265937\n",
      "Iteration 06045: loss = 0.006436571,0.0518575\n",
      "Iteration 06050: loss = 0.0063774004,0.042585462\n",
      "Iteration 06055: loss = 0.006380147,0.033551052\n",
      "Iteration 06060: loss = 0.0063955807,0.0060380436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 06065: loss = 0.0064076963,0.0034049358\n",
      "Iteration 06070: loss = 0.006412742,0.007248851\n",
      "Iteration 06075: loss = 0.0064119003,0.006668743\n",
      "Iteration 06080: loss = 0.006407898,0.0037321397\n",
      "Iteration 06085: loss = 0.0064033107,0.0024678293\n",
      "Iteration 06090: loss = 0.006400212,0.0028896504\n",
      "Iteration 06095: loss = 0.006399727,0.0030179583\n",
      "Iteration 06100: loss = 0.0064012357,0.002581546\n",
      "Iteration 06105: loss = 0.006403057,0.0024512128\n",
      "Iteration 06110: loss = 0.006403791,0.0025414333\n",
      "Iteration 06115: loss = 0.006403117,0.0024649166\n",
      "Iteration 06120: loss = 0.0064019267,0.0024207896\n",
      "Iteration 06125: loss = 0.006401352,0.0024384474\n",
      "Iteration 06130: loss = 0.0064016604,0.002407509\n",
      "Iteration 06135: loss = 0.006402107,0.0024023612\n",
      "Iteration 06140: loss = 0.006401922,0.0023937426\n",
      "Iteration 06145: loss = 0.0064013847,0.0023846284\n",
      "Iteration 06150: loss = 0.006401246,0.0023782514\n",
      "Iteration 06155: loss = 0.00640136,0.0023700846\n",
      "Iteration 06160: loss = 0.006401228,0.0023634564\n",
      "Iteration 06165: loss = 0.0064009703,0.002356695\n",
      "Iteration 06170: loss = 0.006400896,0.0023498079\n",
      "Iteration 06175: loss = 0.006400798,0.0023432008\n",
      "Iteration 06180: loss = 0.0064006303,0.002336518\n",
      "Iteration 06185: loss = 0.0064005093,0.002329924\n",
      "Iteration 06190: loss = 0.006400442,0.002323326\n",
      "Iteration 06195: loss = 0.006400237,0.002316812\n",
      "Iteration 06200: loss = 0.0064001847,0.0023101494\n",
      "Iteration 06205: loss = 0.006400006,0.0023036308\n",
      "Iteration 06210: loss = 0.006399924,0.0022971078\n",
      "Iteration 06215: loss = 0.006399743,0.0022907325\n",
      "Iteration 06220: loss = 0.006399747,0.0022844211\n",
      "Iteration 06225: loss = 0.006399343,0.002279942\n",
      "Iteration 06230: loss = 0.0063998583,0.0022831385\n",
      "Iteration 06235: loss = 0.0063980226,0.0023583092\n",
      "Iteration 06240: loss = 0.0064030006,0.0031069098\n",
      "Iteration 06245: loss = 0.0063865986,0.011336\n",
      "Iteration 06250: loss = 0.006434131,0.07718789\n",
      "Iteration 06255: loss = 0.0063823196,0.023195513\n",
      "Iteration 06260: loss = 0.0063760746,0.042226028\n",
      "Iteration 06265: loss = 0.0063817822,0.0243312\n",
      "Iteration 06270: loss = 0.0063898824,0.009358872\n",
      "Iteration 06275: loss = 0.006395574,0.0031936981\n",
      "Iteration 06280: loss = 0.0063990224,0.0022582547\n",
      "Iteration 06285: loss = 0.006401394,0.003023832\n",
      "Iteration 06290: loss = 0.006402155,0.0035359785\n",
      "Iteration 06295: loss = 0.006401406,0.003285991\n",
      "Iteration 06300: loss = 0.0064001847,0.0026757247\n",
      "Iteration 06305: loss = 0.0063987076,0.002252277\n",
      "Iteration 06310: loss = 0.00639722,0.002190597\n",
      "Iteration 06315: loss = 0.0063963663,0.0022650661\n",
      "Iteration 06320: loss = 0.0063962974,0.002253095\n",
      "Iteration 06325: loss = 0.006396707,0.0021812539\n",
      "Iteration 06330: loss = 0.006397164,0.0021558495\n",
      "Iteration 06335: loss = 0.006397389,0.002163529\n",
      "Iteration 06340: loss = 0.0063971677,0.0021510532\n",
      "Iteration 06345: loss = 0.0063967127,0.002136912\n",
      "Iteration 06350: loss = 0.006396402,0.0021341848\n",
      "Iteration 06355: loss = 0.006396314,0.002126974\n",
      "Iteration 06360: loss = 0.0063963826,0.0021192334\n",
      "Iteration 06365: loss = 0.00639634,0.0021141567\n",
      "Iteration 06370: loss = 0.0063961316,0.0021074738\n",
      "Iteration 06375: loss = 0.00639593,0.0021020097\n",
      "Iteration 06380: loss = 0.006395848,0.0020958583\n",
      "Iteration 06385: loss = 0.006395787,0.0020900634\n",
      "Iteration 06390: loss = 0.0063956617,0.0020841416\n",
      "Iteration 06395: loss = 0.0063954615,0.0020784917\n",
      "Iteration 06400: loss = 0.0063953944,0.0020725979\n",
      "Iteration 06405: loss = 0.006395323,0.0020668786\n",
      "Iteration 06410: loss = 0.0063951476,0.002061151\n",
      "Iteration 06415: loss = 0.0063950256,0.0020554175\n",
      "Iteration 06420: loss = 0.0063949344,0.002049697\n",
      "Iteration 06425: loss = 0.0063948017,0.0020440095\n",
      "Iteration 06430: loss = 0.006394677,0.002038376\n",
      "Iteration 06435: loss = 0.006394575,0.0020326953\n",
      "Iteration 06440: loss = 0.0063944315,0.002027111\n",
      "Iteration 06445: loss = 0.006394358,0.0020214904\n",
      "Iteration 06450: loss = 0.0063941446,0.00201627\n",
      "Iteration 06455: loss = 0.006394236,0.002011526\n",
      "Iteration 06460: loss = 0.0063936617,0.0020127092\n",
      "Iteration 06465: loss = 0.0063946745,0.0020502394\n",
      "Iteration 06470: loss = 0.0063913804,0.0024347384\n",
      "Iteration 06475: loss = 0.00640107,0.0063357083\n",
      "Iteration 06480: loss = 0.006370717,0.045634422\n",
      "Iteration 06485: loss = 0.006423375,0.09015325\n",
      "Iteration 06490: loss = 0.0064115324,0.037549924\n",
      "Iteration 06495: loss = 0.0063978117,0.005382753\n",
      "Iteration 06500: loss = 0.0063914903,0.0024593957\n",
      "Iteration 06505: loss = 0.006388108,0.0056150137\n",
      "Iteration 06510: loss = 0.00638655,0.0068583167\n",
      "Iteration 06515: loss = 0.006387401,0.0057292758\n",
      "Iteration 06520: loss = 0.006388807,0.0037838495\n",
      "Iteration 06525: loss = 0.0063904524,0.0023754267\n",
      "Iteration 06530: loss = 0.006392433,0.0019363707\n",
      "Iteration 06535: loss = 0.0063935295,0.0020698216\n",
      "Iteration 06540: loss = 0.0063936785,0.0021830022\n",
      "Iteration 06545: loss = 0.0063932925,0.0020773937\n",
      "Iteration 06550: loss = 0.0063924002,0.0019375171\n",
      "Iteration 06555: loss = 0.006391585,0.0019159298\n",
      "Iteration 06560: loss = 0.006391222,0.0019378874\n",
      "Iteration 06565: loss = 0.0063912496,0.0019174356\n",
      "Iteration 06570: loss = 0.006391482,0.0018937681\n",
      "Iteration 06575: loss = 0.006391641,0.0018936007\n",
      "Iteration 06580: loss = 0.0063914624,0.0018873402\n",
      "Iteration 06585: loss = 0.00639117,0.0018782181\n",
      "Iteration 06590: loss = 0.0063909455,0.0018748349\n",
      "Iteration 06595: loss = 0.0063908733,0.0018685395\n",
      "Iteration 06600: loss = 0.0063908664,0.0018630702\n",
      "Iteration 06605: loss = 0.006390779,0.0018581831\n",
      "Iteration 06610: loss = 0.0063905804,0.0018528926\n",
      "Iteration 06615: loss = 0.0063904584,0.00184801\n",
      "Iteration 06620: loss = 0.006390383,0.0018427933\n",
      "Iteration 06625: loss = 0.006390288,0.0018378305\n",
      "Iteration 06630: loss = 0.0063901544,0.0018328265\n",
      "Iteration 06635: loss = 0.006390033,0.001827822\n",
      "Iteration 06640: loss = 0.006389935,0.0018228446\n",
      "Iteration 06645: loss = 0.0063898168,0.001817893\n",
      "Iteration 06650: loss = 0.006389693,0.0018129614\n",
      "Iteration 06655: loss = 0.006389594,0.0018080479\n",
      "Iteration 06660: loss = 0.006389486,0.0018031078\n",
      "Iteration 06665: loss = 0.0063893497,0.0017982379\n",
      "Iteration 06670: loss = 0.006389268,0.0017933433\n",
      "Iteration 06675: loss = 0.0063891374,0.0017884764\n",
      "Iteration 06680: loss = 0.006389024,0.0017836092\n",
      "Iteration 06685: loss = 0.006388925,0.0017787325\n",
      "Iteration 06690: loss = 0.0063887867,0.0017739302\n",
      "Iteration 06695: loss = 0.0063887085,0.0017690668\n",
      "Iteration 06700: loss = 0.00638858,0.0017642877\n",
      "Iteration 06705: loss = 0.0063884794,0.001759473\n",
      "Iteration 06710: loss = 0.00638836,0.0017546946\n",
      "Iteration 06715: loss = 0.0063882307,0.0017499769\n",
      "Iteration 06720: loss = 0.0063882284,0.0017457729\n",
      "Iteration 06725: loss = 0.0063876384,0.0017566378\n",
      "Iteration 06730: loss = 0.0063897404,0.002089821\n",
      "Iteration 06735: loss = 0.0063786968,0.0107272575\n",
      "Iteration 06740: loss = 0.006426627,0.1714222\n",
      "Iteration 06745: loss = 0.0063883103,0.002795042\n",
      "Iteration 06750: loss = 0.00639006,0.003662689\n",
      "Iteration 06755: loss = 0.006393518,0.0139959\n",
      "Iteration 06760: loss = 0.0063984743,0.02337496\n",
      "Iteration 06765: loss = 0.0063984212,0.019624121\n",
      "Iteration 06770: loss = 0.0063929786,0.009060917\n",
      "Iteration 06775: loss = 0.006388999,0.002617862\n",
      "Iteration 06780: loss = 0.0063863266,0.0018373503\n",
      "Iteration 06785: loss = 0.0063844547,0.0027513755\n",
      "Iteration 06790: loss = 0.006384541,0.002935786\n",
      "Iteration 06795: loss = 0.0063850675,0.0024535172\n",
      "Iteration 06800: loss = 0.0063853166,0.0019653453\n",
      "Iteration 06805: loss = 0.0063858903,0.0017281643\n",
      "Iteration 06810: loss = 0.0063862833,0.0016732903\n",
      "Iteration 06815: loss = 0.0063863355,0.0016810177\n",
      "Iteration 06820: loss = 0.0063864402,0.0016889162\n",
      "Iteration 06825: loss = 0.006386338,0.0016857107\n",
      "Iteration 06830: loss = 0.0063861213,0.0016755273\n",
      "Iteration 06835: loss = 0.006385999,0.0016642937\n",
      "Iteration 06840: loss = 0.0063858125,0.001654605\n",
      "Iteration 06845: loss = 0.006385658,0.0016467338\n",
      "Iteration 06850: loss = 0.006385543,0.00164059\n",
      "Iteration 06855: loss = 0.006385403,0.0016354104\n",
      "Iteration 06860: loss = 0.0063852626,0.0016305496\n",
      "Iteration 06865: loss = 0.006385157,0.0016260147\n",
      "Iteration 06870: loss = 0.006385043,0.0016216303\n",
      "Iteration 06875: loss = 0.006384929,0.0016173258\n",
      "Iteration 06880: loss = 0.0063848277,0.0016130263\n",
      "Iteration 06885: loss = 0.0063847075,0.0016087692\n",
      "Iteration 06890: loss = 0.0063845855,0.0016045398\n",
      "Iteration 06895: loss = 0.0063844803,0.0016003523\n",
      "Iteration 06900: loss = 0.0063843816,0.0015961614\n",
      "Iteration 06905: loss = 0.0063842726,0.0015919877\n",
      "Iteration 06910: loss = 0.006384164,0.0015878374\n",
      "Iteration 06915: loss = 0.006384067,0.0015836817\n",
      "Iteration 06920: loss = 0.0063839555,0.0015795266\n",
      "Iteration 06925: loss = 0.006383858,0.001575415\n",
      "Iteration 06930: loss = 0.006383753,0.001571289\n",
      "Iteration 06935: loss = 0.0063836374,0.0015671798\n",
      "Iteration 06940: loss = 0.006383544,0.0015630904\n",
      "Iteration 06945: loss = 0.006383434,0.0015589937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 06950: loss = 0.006383334,0.0015549334\n",
      "Iteration 06955: loss = 0.0063832398,0.001550836\n",
      "Iteration 06960: loss = 0.006383126,0.0015467817\n",
      "Iteration 06965: loss = 0.006383035,0.0015427268\n",
      "Iteration 06970: loss = 0.006382931,0.0015386919\n",
      "Iteration 06975: loss = 0.0063828235,0.0015346555\n",
      "Iteration 06980: loss = 0.00638272,0.0015306385\n",
      "Iteration 06985: loss = 0.0063826195,0.0015266404\n",
      "Iteration 06990: loss = 0.0063825217,0.001522639\n",
      "Iteration 06995: loss = 0.0063824155,0.001518636\n",
      "Iteration 07000: loss = 0.0063822954,0.0015146695\n",
      "Iteration 07005: loss = 0.0063822116,0.001510706\n",
      "Iteration 07010: loss = 0.0063821035,0.0015067309\n",
      "Iteration 07015: loss = 0.0063819974,0.0015027954\n",
      "Iteration 07020: loss = 0.0063818977,0.001498855\n",
      "Iteration 07025: loss = 0.0063817897,0.0014948964\n",
      "Iteration 07030: loss = 0.006381707,0.0014910023\n",
      "Iteration 07035: loss = 0.0063815974,0.0014870743\n",
      "Iteration 07040: loss = 0.006381474,0.0014831775\n",
      "Iteration 07045: loss = 0.006381396,0.00147929\n",
      "Iteration 07050: loss = 0.0063812807,0.0014754173\n",
      "Iteration 07055: loss = 0.006381191,0.0014715345\n",
      "Iteration 07060: loss = 0.0063810837,0.0014676816\n",
      "Iteration 07065: loss = 0.0063809794,0.0014638086\n",
      "Iteration 07070: loss = 0.006380869,0.0014599683\n",
      "Iteration 07075: loss = 0.006380766,0.0014561348\n",
      "Iteration 07080: loss = 0.0063806754,0.0014523122\n",
      "Iteration 07085: loss = 0.006380551,0.0014485221\n",
      "Iteration 07090: loss = 0.006380487,0.0014447137\n",
      "Iteration 07095: loss = 0.0063803364,0.0014411651\n",
      "Iteration 07100: loss = 0.0063803755,0.0014389318\n",
      "Iteration 07105: loss = 0.0063798265,0.0014534437\n",
      "Iteration 07110: loss = 0.006381203,0.0016571622\n",
      "Iteration 07115: loss = 0.0063758194,0.004476121\n",
      "Iteration 07120: loss = 0.006395402,0.04334243\n",
      "Iteration 07125: loss = 0.006355828,0.15629716\n",
      "Iteration 07130: loss = 0.0063643954,0.0627774\n",
      "Iteration 07135: loss = 0.006370854,0.021204727\n",
      "Iteration 07140: loss = 0.006374737,0.008575016\n",
      "Iteration 07145: loss = 0.006377302,0.004340985\n",
      "Iteration 07150: loss = 0.0063773,0.002663636\n",
      "Iteration 07155: loss = 0.0063781682,0.0019030795\n",
      "Iteration 07160: loss = 0.0063782367,0.0015538033\n",
      "Iteration 07165: loss = 0.0063786847,0.0014167198\n",
      "Iteration 07170: loss = 0.0063792337,0.0013928444\n",
      "Iteration 07175: loss = 0.006379194,0.0014219077\n",
      "Iteration 07180: loss = 0.0063792877,0.0014616847\n",
      "Iteration 07185: loss = 0.0063791643,0.00148826\n",
      "Iteration 07190: loss = 0.0063790954,0.001486077\n",
      "Iteration 07195: loss = 0.0063789547,0.0014600749\n",
      "Iteration 07200: loss = 0.0063787457,0.001423931\n",
      "Iteration 07205: loss = 0.006378507,0.0013895825\n",
      "Iteration 07210: loss = 0.006378195,0.0013676754\n",
      "Iteration 07215: loss = 0.0063779983,0.0013595211\n",
      "Iteration 07220: loss = 0.0063777785,0.0013585573\n",
      "Iteration 07225: loss = 0.0063776956,0.0013565621\n",
      "Iteration 07230: loss = 0.0063775964,0.0013514566\n",
      "Iteration 07235: loss = 0.0063775685,0.001346064\n",
      "Iteration 07240: loss = 0.0063775275,0.0013422868\n",
      "Iteration 07245: loss = 0.006377443,0.0013392264\n",
      "Iteration 07250: loss = 0.0063773505,0.0013356846\n",
      "Iteration 07255: loss = 0.006377217,0.0013319876\n",
      "Iteration 07260: loss = 0.0063771084,0.001328669\n",
      "Iteration 07265: loss = 0.006376991,0.0013253419\n",
      "Iteration 07270: loss = 0.006376916,0.001321831\n",
      "Iteration 07275: loss = 0.0063768276,0.0013184813\n",
      "Iteration 07280: loss = 0.006376742,0.0013150696\n",
      "Iteration 07285: loss = 0.006376618,0.001311741\n",
      "Iteration 07290: loss = 0.006376535,0.0013083911\n",
      "Iteration 07295: loss = 0.0063764225,0.0013050316\n",
      "Iteration 07300: loss = 0.006376341,0.001301703\n",
      "Iteration 07305: loss = 0.0063762427,0.0012983597\n",
      "Iteration 07310: loss = 0.0063761366,0.0012950534\n",
      "Iteration 07315: loss = 0.006376048,0.0012917409\n",
      "Iteration 07320: loss = 0.0063759587,0.0012884459\n",
      "Iteration 07325: loss = 0.0063758544,0.0012851218\n",
      "Iteration 07330: loss = 0.006375752,0.0012818398\n",
      "Iteration 07335: loss = 0.0063756616,0.0012785719\n",
      "Iteration 07340: loss = 0.006375573,0.0012752628\n",
      "Iteration 07345: loss = 0.0063754674,0.0012720313\n",
      "Iteration 07350: loss = 0.006375378,0.0012687695\n",
      "Iteration 07355: loss = 0.0063752704,0.0012655316\n",
      "Iteration 07360: loss = 0.006375182,0.0012622783\n",
      "Iteration 07365: loss = 0.006375076,0.001259034\n",
      "Iteration 07370: loss = 0.006374994,0.001255817\n",
      "Iteration 07375: loss = 0.0063748895,0.00125267\n",
      "Iteration 07380: loss = 0.006374831,0.001249623\n",
      "Iteration 07385: loss = 0.006374616,0.0012482522\n",
      "Iteration 07390: loss = 0.006374871,0.0012594374\n",
      "Iteration 07395: loss = 0.006373657,0.0014148194\n",
      "Iteration 07400: loss = 0.0063774064,0.0033811047\n",
      "Iteration 07405: loss = 0.0063636177,0.030277127\n",
      "Iteration 07410: loss = 0.006398801,0.16113313\n",
      "Iteration 07415: loss = 0.0063814237,0.026389064\n",
      "Iteration 07420: loss = 0.0063754064,0.0022701952\n",
      "Iteration 07425: loss = 0.0063731926,0.0014990515\n",
      "Iteration 07430: loss = 0.0063731354,0.0023277292\n",
      "Iteration 07435: loss = 0.006372022,0.0026613933\n",
      "Iteration 07440: loss = 0.006371952,0.0026740103\n",
      "Iteration 07445: loss = 0.0063716057,0.0025524008\n",
      "Iteration 07450: loss = 0.0063718352,0.002366053\n",
      "Iteration 07455: loss = 0.006372077,0.00214798\n",
      "Iteration 07460: loss = 0.0063720457,0.001906724\n",
      "Iteration 07465: loss = 0.006372144,0.0016664925\n",
      "Iteration 07470: loss = 0.006372249,0.0014578728\n",
      "Iteration 07475: loss = 0.0063724783,0.0013052183\n",
      "Iteration 07480: loss = 0.0063726776,0.0012184444\n",
      "Iteration 07485: loss = 0.0063728224,0.0011897977\n",
      "Iteration 07490: loss = 0.0063728415,0.0011912091\n",
      "Iteration 07495: loss = 0.0063728373,0.001196963\n",
      "Iteration 07500: loss = 0.00637275,0.0011935906\n",
      "Iteration 07505: loss = 0.0063725836,0.001183008\n",
      "Iteration 07510: loss = 0.0063724057,0.0011748562\n",
      "Iteration 07515: loss = 0.006372251,0.0011720791\n",
      "Iteration 07520: loss = 0.0063721077,0.001170176\n",
      "Iteration 07525: loss = 0.0063720476,0.0011665408\n",
      "Iteration 07530: loss = 0.0063719903,0.0011628277\n",
      "Iteration 07535: loss = 0.006371934,0.001160028\n",
      "Iteration 07540: loss = 0.006371844,0.001157214\n",
      "Iteration 07545: loss = 0.0063717254,0.00115415\n",
      "Iteration 07550: loss = 0.0063716304,0.00115129\n",
      "Iteration 07555: loss = 0.006371534,0.0011484958\n",
      "Iteration 07560: loss = 0.00637145,0.0011455216\n",
      "Iteration 07565: loss = 0.0063713863,0.0011427103\n",
      "Iteration 07570: loss = 0.006371265,0.0011398197\n",
      "Iteration 07575: loss = 0.00637118,0.0011369818\n",
      "Iteration 07580: loss = 0.006371097,0.0011341337\n",
      "Iteration 07585: loss = 0.006371008,0.0011313056\n",
      "Iteration 07590: loss = 0.00637091,0.0011284726\n",
      "Iteration 07595: loss = 0.006370815,0.0011256726\n",
      "Iteration 07600: loss = 0.006370721,0.001122874\n",
      "Iteration 07605: loss = 0.0063706376,0.0011200504\n",
      "Iteration 07610: loss = 0.0063705523,0.0011172463\n",
      "Iteration 07615: loss = 0.006370448,0.001114538\n",
      "Iteration 07620: loss = 0.0063703894,0.0011117338\n",
      "Iteration 07625: loss = 0.0063702706,0.0011088757\n",
      "Iteration 07630: loss = 0.0063701714,0.0011061353\n",
      "Iteration 07635: loss = 0.0063701053,0.0011033469\n",
      "Iteration 07640: loss = 0.0063700043,0.0011006056\n",
      "Iteration 07645: loss = 0.006369934,0.0010979252\n",
      "Iteration 07650: loss = 0.006369769,0.0010959527\n",
      "Iteration 07655: loss = 0.006369853,0.0010968528\n",
      "Iteration 07660: loss = 0.0063693435,0.0011182907\n",
      "Iteration 07665: loss = 0.0063703787,0.0012989807\n",
      "Iteration 07670: loss = 0.006366962,0.0030089132\n",
      "Iteration 07675: loss = 0.006377436,0.020281857\n",
      "Iteration 07680: loss = 0.0063517154,0.121112555\n",
      "Iteration 07685: loss = 0.0063693114,0.0014377469\n",
      "Iteration 07690: loss = 0.0063760043,0.02413776\n",
      "Iteration 07695: loss = 0.006377114,0.02538074\n",
      "Iteration 07700: loss = 0.0063753203,0.014600203\n",
      "Iteration 07705: loss = 0.006372346,0.005661588\n",
      "Iteration 07710: loss = 0.0063700494,0.0016498691\n",
      "Iteration 07715: loss = 0.0063681756,0.0011306826\n",
      "Iteration 07720: loss = 0.0063674524,0.001780953\n",
      "Iteration 07725: loss = 0.0063671675,0.0020624711\n",
      "Iteration 07730: loss = 0.0063672983,0.0017016723\n",
      "Iteration 07735: loss = 0.0063676797,0.0012204198\n",
      "Iteration 07740: loss = 0.006368222,0.0010504203\n",
      "Iteration 07745: loss = 0.006368533,0.00111639\n",
      "Iteration 07750: loss = 0.0063685537,0.0011347885\n",
      "Iteration 07755: loss = 0.0063682324,0.0010689005\n",
      "Iteration 07760: loss = 0.0063678436,0.0010407148\n",
      "Iteration 07765: loss = 0.0063675824,0.0010532123\n",
      "Iteration 07770: loss = 0.0063675474,0.0010444728\n",
      "Iteration 07775: loss = 0.0063676327,0.0010325313\n",
      "Iteration 07780: loss = 0.006367628,0.0010334442\n",
      "Iteration 07785: loss = 0.006367494,0.0010287785\n",
      "Iteration 07790: loss = 0.0063673244,0.0010252612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 07795: loss = 0.0063672015,0.0010234214\n",
      "Iteration 07800: loss = 0.0063671856,0.0010199853\n",
      "Iteration 07805: loss = 0.0063671046,0.0010179275\n",
      "Iteration 07810: loss = 0.00636702,0.001015001\n",
      "Iteration 07815: loss = 0.00636687,0.0010127759\n",
      "Iteration 07820: loss = 0.006366854,0.0010100324\n",
      "Iteration 07825: loss = 0.0063667465,0.0010076077\n",
      "Iteration 07830: loss = 0.0063666566,0.0010051002\n",
      "Iteration 07835: loss = 0.006366575,0.0010026458\n",
      "Iteration 07840: loss = 0.0063664746,0.0010001841\n",
      "Iteration 07845: loss = 0.006366404,0.0009977152\n",
      "Iteration 07850: loss = 0.0063663037,0.0009952874\n",
      "Iteration 07855: loss = 0.0063662506,0.0009929457\n",
      "Iteration 07860: loss = 0.0063661262,0.0009907719\n",
      "Iteration 07865: loss = 0.006366091,0.0009883015\n",
      "Iteration 07870: loss = 0.0063659344,0.0009861558\n",
      "Iteration 07875: loss = 0.006365958,0.0009843191\n",
      "Iteration 07880: loss = 0.0063656946,0.0009854159\n",
      "Iteration 07885: loss = 0.0063659498,0.0009996223\n",
      "Iteration 07890: loss = 0.006365061,0.0011040241\n",
      "Iteration 07895: loss = 0.006367142,0.0019039904\n",
      "Iteration 07900: loss = 0.006360742,0.009131228\n",
      "Iteration 07905: loss = 0.006378913,0.06438693\n",
      "Iteration 07910: loss = 0.006354198,0.065104194\n",
      "Iteration 07915: loss = 0.006355457,0.04417194\n",
      "Iteration 07920: loss = 0.0063619004,0.006435367\n",
      "Iteration 07925: loss = 0.0063668895,0.001564506\n",
      "Iteration 07930: loss = 0.006368415,0.0060263854\n",
      "Iteration 07935: loss = 0.006368575,0.0067059477\n",
      "Iteration 07940: loss = 0.0063670943,0.0038682995\n",
      "Iteration 07945: loss = 0.0063658315,0.0014141512\n",
      "Iteration 07950: loss = 0.0063644573,0.0010019675\n",
      "Iteration 07955: loss = 0.0063636457,0.0014474135\n",
      "Iteration 07960: loss = 0.006363468,0.0014047447\n",
      "Iteration 07965: loss = 0.0063639022,0.0010373968\n",
      "Iteration 07970: loss = 0.0063644387,0.0009529272\n",
      "Iteration 07975: loss = 0.0063646766,0.0010292245\n",
      "Iteration 07980: loss = 0.0063644354,0.0009772955\n",
      "Iteration 07985: loss = 0.0063639753,0.0009349353\n",
      "Iteration 07990: loss = 0.0063637295,0.0009527119\n",
      "Iteration 07995: loss = 0.0063637234,0.0009370199\n",
      "Iteration 08000: loss = 0.006363856,0.00092943636\n",
      "Iteration 08005: loss = 0.0063638096,0.0009310241\n",
      "Iteration 08010: loss = 0.0063636084,0.0009233264\n",
      "Iteration 08015: loss = 0.006363465,0.0009235466\n",
      "Iteration 08020: loss = 0.0063634347,0.0009189202\n",
      "Iteration 08025: loss = 0.006363418,0.0009175482\n",
      "Iteration 08030: loss = 0.006363271,0.0009144812\n",
      "Iteration 08035: loss = 0.0063631614,0.0009128534\n",
      "Iteration 08040: loss = 0.0063631427,0.0009101824\n",
      "Iteration 08045: loss = 0.006363027,0.0009079729\n",
      "Iteration 08050: loss = 0.006362922,0.0009058487\n",
      "Iteration 08055: loss = 0.0063629057,0.00090358115\n",
      "Iteration 08060: loss = 0.0063627777,0.0009013632\n",
      "Iteration 08065: loss = 0.006362704,0.00089917966\n",
      "Iteration 08070: loss = 0.006362632,0.00089700834\n",
      "Iteration 08075: loss = 0.006362528,0.00089499285\n",
      "Iteration 08080: loss = 0.0063624983,0.00089312217\n",
      "Iteration 08085: loss = 0.006362321,0.00089225755\n",
      "Iteration 08090: loss = 0.0063624284,0.0008946049\n",
      "Iteration 08095: loss = 0.0063619423,0.00091493444\n",
      "Iteration 08100: loss = 0.0063627902,0.001041116\n",
      "Iteration 08105: loss = 0.0063604154,0.001966919\n",
      "Iteration 08110: loss = 0.0063666925,0.0095563885\n",
      "Iteration 08115: loss = 0.006350148,0.06427435\n",
      "Iteration 08120: loss = 0.0063722017,0.058455624\n",
      "Iteration 08125: loss = 0.006369783,0.037438132\n",
      "Iteration 08130: loss = 0.006363994,0.003151707\n",
      "Iteration 08135: loss = 0.0063598803,0.003649195\n",
      "Iteration 08140: loss = 0.0063582286,0.008535359\n",
      "Iteration 08145: loss = 0.0063583762,0.0060701165\n",
      "Iteration 08150: loss = 0.006359969,0.0018970489\n",
      "Iteration 08155: loss = 0.006361699,0.000915464\n",
      "Iteration 08160: loss = 0.006362572,0.0016627852\n",
      "Iteration 08165: loss = 0.0063623483,0.0015521949\n",
      "Iteration 08170: loss = 0.006361358,0.00094494515\n",
      "Iteration 08175: loss = 0.006360591,0.00091217714\n",
      "Iteration 08180: loss = 0.006360292,0.0010150094\n",
      "Iteration 08185: loss = 0.0063605197,0.0008840125\n",
      "Iteration 08190: loss = 0.0063608685,0.00086367107\n",
      "Iteration 08195: loss = 0.0063608433,0.0008821983\n",
      "Iteration 08200: loss = 0.0063605853,0.00084633694\n",
      "Iteration 08205: loss = 0.006360309,0.0008534625\n",
      "Iteration 08210: loss = 0.0063602724,0.00084510987\n",
      "Iteration 08215: loss = 0.0063603627,0.00084137544\n",
      "Iteration 08220: loss = 0.0063602584,0.00083901174\n",
      "Iteration 08225: loss = 0.00636008,0.0008359929\n",
      "Iteration 08230: loss = 0.006360018,0.0008337626\n",
      "Iteration 08235: loss = 0.0063599865,0.0008316218\n",
      "Iteration 08240: loss = 0.006359909,0.0008293281\n",
      "Iteration 08245: loss = 0.0063597783,0.0008275962\n",
      "Iteration 08250: loss = 0.006359756,0.00082536816\n",
      "Iteration 08255: loss = 0.0063596577,0.0008233613\n",
      "Iteration 08260: loss = 0.0063595683,0.0008214276\n",
      "Iteration 08265: loss = 0.006359497,0.00081949204\n",
      "Iteration 08270: loss = 0.0063594114,0.00081753195\n",
      "Iteration 08275: loss = 0.0063593476,0.000815595\n",
      "Iteration 08280: loss = 0.006359258,0.00081364997\n",
      "Iteration 08285: loss = 0.006359194,0.000811694\n",
      "Iteration 08290: loss = 0.0063591152,0.0008097796\n",
      "Iteration 08295: loss = 0.0063590254,0.00080786814\n",
      "Iteration 08300: loss = 0.0063589546,0.0008059478\n",
      "Iteration 08305: loss = 0.0063588726,0.0008041769\n",
      "Iteration 08310: loss = 0.006358829,0.00080294744\n",
      "Iteration 08315: loss = 0.006358572,0.00080912793\n",
      "Iteration 08320: loss = 0.0063591585,0.00090462086\n",
      "Iteration 08325: loss = 0.0063566626,0.0023519117\n",
      "Iteration 08330: loss = 0.0063663335,0.025547836\n",
      "Iteration 08335: loss = 0.006340493,0.20265557\n",
      "Iteration 08340: loss = 0.0063514677,0.0284057\n",
      "Iteration 08345: loss = 0.0063548754,0.0049213674\n",
      "Iteration 08350: loss = 0.0063572787,0.0024041065\n",
      "Iteration 08355: loss = 0.0063578,0.0022543552\n",
      "Iteration 08360: loss = 0.0063564456,0.0023536475\n",
      "Iteration 08365: loss = 0.0063567422,0.0022944608\n",
      "Iteration 08370: loss = 0.006356411,0.0020485255\n",
      "Iteration 08375: loss = 0.006357013,0.0017187061\n",
      "Iteration 08380: loss = 0.006356994,0.0014131316\n",
      "Iteration 08385: loss = 0.0063568074,0.0011703605\n",
      "Iteration 08390: loss = 0.006356999,0.0009996446\n",
      "Iteration 08395: loss = 0.006357003,0.0008884751\n",
      "Iteration 08400: loss = 0.006357195,0.0008228466\n",
      "Iteration 08405: loss = 0.0063571776,0.00078833976\n",
      "Iteration 08410: loss = 0.006357147,0.00077222125\n",
      "Iteration 08415: loss = 0.00635717,0.0007671951\n",
      "Iteration 08420: loss = 0.0063571404,0.00076707196\n",
      "Iteration 08425: loss = 0.0063571255,0.00076795317\n",
      "Iteration 08430: loss = 0.0063570593,0.0007675339\n",
      "Iteration 08435: loss = 0.006356964,0.0007653847\n",
      "Iteration 08440: loss = 0.0063568745,0.000761839\n",
      "Iteration 08445: loss = 0.00635678,0.00075810275\n",
      "Iteration 08450: loss = 0.0063566803,0.00075511436\n",
      "Iteration 08455: loss = 0.006356569,0.0007532476\n",
      "Iteration 08460: loss = 0.0063564912,0.0007517017\n",
      "Iteration 08465: loss = 0.0063564214,0.00075001083\n",
      "Iteration 08470: loss = 0.0063563525,0.00074817735\n",
      "Iteration 08475: loss = 0.0063562915,0.00074635504\n",
      "Iteration 08480: loss = 0.006356228,0.00074463186\n",
      "Iteration 08485: loss = 0.006356161,0.00074295286\n",
      "Iteration 08490: loss = 0.0063560703,0.0007411962\n",
      "Iteration 08495: loss = 0.0063559976,0.0007394942\n",
      "Iteration 08500: loss = 0.0063559324,0.0007378032\n",
      "Iteration 08505: loss = 0.006355856,0.00073607994\n",
      "Iteration 08510: loss = 0.006355773,0.00073440623\n",
      "Iteration 08515: loss = 0.0063557294,0.0007327148\n",
      "Iteration 08520: loss = 0.006355639,0.0007310094\n",
      "Iteration 08525: loss = 0.00635557,0.0007293439\n",
      "Iteration 08530: loss = 0.0063555073,0.0007276346\n",
      "Iteration 08535: loss = 0.0063554402,0.00072596443\n",
      "Iteration 08540: loss = 0.0063553522,0.0007243006\n",
      "Iteration 08545: loss = 0.0063552894,0.0007226281\n",
      "Iteration 08550: loss = 0.0063551986,0.00072095724\n",
      "Iteration 08555: loss = 0.006355127,0.0007192878\n",
      "Iteration 08560: loss = 0.006355077,0.0007176211\n",
      "Iteration 08565: loss = 0.006354997,0.0007159817\n",
      "Iteration 08570: loss = 0.0063549043,0.0007143596\n",
      "Iteration 08575: loss = 0.006354854,0.0007126629\n",
      "Iteration 08580: loss = 0.006354786,0.00071100483\n",
      "Iteration 08585: loss = 0.00635469,0.0007093804\n",
      "Iteration 08590: loss = 0.006354641,0.0007077259\n",
      "Iteration 08595: loss = 0.0063545518,0.0007062464\n",
      "Iteration 08600: loss = 0.0063545466,0.00070523156\n",
      "Iteration 08605: loss = 0.006354334,0.0007064599\n",
      "Iteration 08610: loss = 0.006354533,0.000717285\n",
      "Iteration 08615: loss = 0.006353811,0.0007909161\n",
      "Iteration 08620: loss = 0.0063554044,0.001320017\n",
      "Iteration 08625: loss = 0.0063507366,0.005897697\n",
      "Iteration 08630: loss = 0.0063641914,0.0435761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 08635: loss = 0.0063414783,0.10361358\n",
      "Iteration 08640: loss = 0.0063479412,0.021986501\n",
      "Iteration 08645: loss = 0.0063547124,0.0013050288\n",
      "Iteration 08650: loss = 0.006358516,0.010697648\n",
      "Iteration 08655: loss = 0.0063584917,0.011588657\n",
      "Iteration 08660: loss = 0.0063566244,0.0057420437\n",
      "Iteration 08665: loss = 0.0063545094,0.0013708792\n",
      "Iteration 08670: loss = 0.006353346,0.0008163904\n",
      "Iteration 08675: loss = 0.006352329,0.001620996\n",
      "Iteration 08680: loss = 0.006352106,0.0015383787\n",
      "Iteration 08685: loss = 0.006352678,0.00087874994\n",
      "Iteration 08690: loss = 0.006353313,0.0006879597\n",
      "Iteration 08695: loss = 0.0063536535,0.000822763\n",
      "Iteration 08700: loss = 0.0063534766,0.0007676471\n",
      "Iteration 08705: loss = 0.006352979,0.0006743008\n",
      "Iteration 08710: loss = 0.00635264,0.0006997031\n",
      "Iteration 08715: loss = 0.0063526276,0.0006901404\n",
      "Iteration 08720: loss = 0.006352809,0.0006689704\n",
      "Iteration 08725: loss = 0.006352797,0.0006765444\n",
      "Iteration 08730: loss = 0.006352632,0.00066700584\n",
      "Iteration 08735: loss = 0.006352482,0.0006666579\n",
      "Iteration 08740: loss = 0.006352425,0.0006640473\n",
      "Iteration 08745: loss = 0.0063524228,0.0006618865\n",
      "Iteration 08750: loss = 0.006352372,0.0006601699\n",
      "Iteration 08755: loss = 0.0063522286,0.0006585191\n",
      "Iteration 08760: loss = 0.006352162,0.0006568438\n",
      "Iteration 08765: loss = 0.0063521527,0.0006554924\n",
      "Iteration 08770: loss = 0.0063520423,0.0006538107\n",
      "Iteration 08775: loss = 0.0063520004,0.0006523144\n",
      "Iteration 08780: loss = 0.006351926,0.0006508434\n",
      "Iteration 08785: loss = 0.0063518374,0.0006493442\n",
      "Iteration 08790: loss = 0.0063517615,0.00064787234\n",
      "Iteration 08795: loss = 0.0063517336,0.0006465471\n",
      "Iteration 08800: loss = 0.006351618,0.00064544386\n",
      "Iteration 08805: loss = 0.006351628,0.0006444721\n",
      "Iteration 08810: loss = 0.006351422,0.00064506626\n",
      "Iteration 08815: loss = 0.0063515943,0.0006512853\n",
      "Iteration 08820: loss = 0.0063510514,0.00068816316\n",
      "Iteration 08825: loss = 0.0063520833,0.00091795775\n",
      "Iteration 08830: loss = 0.0063491077,0.0026812735\n",
      "Iteration 08835: loss = 0.006357333,0.017240012\n",
      "Iteration 08840: loss = 0.0063382722,0.09541298\n",
      "Iteration 08845: loss = 0.006353891,0.010490393\n",
      "Iteration 08850: loss = 0.0063588577,0.037335765\n",
      "Iteration 08855: loss = 0.0063568954,0.01670872\n",
      "Iteration 08860: loss = 0.006352329,0.0017256811\n",
      "Iteration 08865: loss = 0.0063494723,0.0018832847\n",
      "Iteration 08870: loss = 0.0063483682,0.0044868225\n",
      "Iteration 08875: loss = 0.00634867,0.0033065402\n",
      "Iteration 08880: loss = 0.006349849,0.0010803925\n",
      "Iteration 08885: loss = 0.006350923,0.00069355796\n",
      "Iteration 08890: loss = 0.006351343,0.0011280833\n",
      "Iteration 08895: loss = 0.0063510085,0.0009289773\n",
      "Iteration 08900: loss = 0.0063504004,0.0006249667\n",
      "Iteration 08905: loss = 0.006349864,0.0006963924\n",
      "Iteration 08910: loss = 0.0063497103,0.00069341465\n",
      "Iteration 08915: loss = 0.006349998,0.00061468343\n",
      "Iteration 08920: loss = 0.006350214,0.0006377379\n",
      "Iteration 08925: loss = 0.006350085,0.00062127935\n",
      "Iteration 08930: loss = 0.0063497485,0.0006130671\n",
      "Iteration 08935: loss = 0.0063496404,0.00061534415\n",
      "Iteration 08940: loss = 0.006349758,0.0006075392\n",
      "Iteration 08945: loss = 0.0063497336,0.0006080269\n",
      "Iteration 08950: loss = 0.0063495655,0.00060466945\n",
      "Iteration 08955: loss = 0.006349471,0.00060411723\n",
      "Iteration 08960: loss = 0.0063494705,0.0006019956\n",
      "Iteration 08965: loss = 0.0063494258,0.00060063455\n",
      "Iteration 08970: loss = 0.0063493014,0.0005994921\n",
      "Iteration 08975: loss = 0.006349268,0.0005979141\n",
      "Iteration 08980: loss = 0.0063491818,0.000596542\n",
      "Iteration 08985: loss = 0.0063491124,0.00059523614\n",
      "Iteration 08990: loss = 0.00634908,0.0005938532\n",
      "Iteration 08995: loss = 0.0063490006,0.0005924981\n",
      "Iteration 09000: loss = 0.0063488907,0.0005913487\n",
      "Iteration 09005: loss = 0.0063488954,0.00059047685\n",
      "Iteration 09010: loss = 0.006348726,0.00059083744\n",
      "Iteration 09015: loss = 0.0063488916,0.00059524586\n",
      "Iteration 09020: loss = 0.006348384,0.0006252052\n",
      "Iteration 09025: loss = 0.0063492996,0.000810897\n",
      "Iteration 09030: loss = 0.0063466188,0.0022438858\n",
      "Iteration 09035: loss = 0.0063540866,0.014468874\n",
      "Iteration 09040: loss = 0.0063358247,0.090088435\n",
      "Iteration 09045: loss = 0.006352631,0.020676792\n",
      "Iteration 09050: loss = 0.006356234,0.042116303\n",
      "Iteration 09055: loss = 0.0063542,0.015397394\n",
      "Iteration 09060: loss = 0.0063495734,0.001234326\n",
      "Iteration 09065: loss = 0.0063463715,0.0021445167\n",
      "Iteration 09070: loss = 0.006345483,0.0046704626\n",
      "Iteration 09075: loss = 0.006346105,0.0035482438\n",
      "Iteration 09080: loss = 0.006347079,0.0012547342\n",
      "Iteration 09085: loss = 0.006347859,0.0005823832\n",
      "Iteration 09090: loss = 0.0063485187,0.0009836035\n",
      "Iteration 09095: loss = 0.0063485247,0.00096182793\n",
      "Iteration 09100: loss = 0.0063479636,0.00062380807\n",
      "Iteration 09105: loss = 0.0063472684,0.00059561874\n",
      "Iteration 09110: loss = 0.00634699,0.0006552696\n",
      "Iteration 09115: loss = 0.006347224,0.0005820763\n",
      "Iteration 09120: loss = 0.006347494,0.0005701169\n",
      "Iteration 09125: loss = 0.006347461,0.0005805754\n",
      "Iteration 09130: loss = 0.0063471985,0.0005602556\n",
      "Iteration 09135: loss = 0.0063470197,0.000564846\n",
      "Iteration 09140: loss = 0.006347016,0.0005587579\n",
      "Iteration 09145: loss = 0.006347056,0.00055762305\n",
      "Iteration 09150: loss = 0.00634699,0.0005555852\n",
      "Iteration 09155: loss = 0.0063468474,0.0005544631\n",
      "Iteration 09160: loss = 0.0063468236,0.0005526186\n",
      "Iteration 09165: loss = 0.0063468046,0.00055171724\n",
      "Iteration 09170: loss = 0.0063466704,0.0005501603\n",
      "Iteration 09175: loss = 0.0063466183,0.0005489407\n",
      "Iteration 09180: loss = 0.006346584,0.0005476849\n",
      "Iteration 09185: loss = 0.006346526,0.00054647337\n",
      "Iteration 09190: loss = 0.0063464437,0.00054529484\n",
      "Iteration 09195: loss = 0.0063463743,0.0005440861\n",
      "Iteration 09200: loss = 0.006346336,0.00054290524\n",
      "Iteration 09205: loss = 0.0063462313,0.00054208207\n",
      "Iteration 09210: loss = 0.0063462555,0.00054150756\n",
      "Iteration 09215: loss = 0.0063460623,0.00054267264\n",
      "Iteration 09220: loss = 0.006346241,0.0005502837\n",
      "Iteration 09225: loss = 0.0063456367,0.0005977887\n",
      "Iteration 09230: loss = 0.006346846,0.0008955902\n",
      "Iteration 09235: loss = 0.006343449,0.003175654\n",
      "Iteration 09240: loss = 0.006352875,0.021668265\n",
      "Iteration 09245: loss = 0.0063326564,0.10318436\n",
      "Iteration 09250: loss = 0.0063458434,0.0019878512\n",
      "Iteration 09255: loss = 0.0063527166,0.028982552\n",
      "Iteration 09260: loss = 0.0063524493,0.01950074\n",
      "Iteration 09265: loss = 0.006348023,0.0037901348\n",
      "Iteration 09270: loss = 0.006344669,0.0007414991\n",
      "Iteration 09275: loss = 0.006343162,0.0032099618\n",
      "Iteration 09280: loss = 0.006343628,0.0032038167\n",
      "Iteration 09285: loss = 0.006344447,0.001254637\n",
      "Iteration 09290: loss = 0.006345226,0.0005301194\n",
      "Iteration 09295: loss = 0.0063458364,0.00090908137\n",
      "Iteration 09300: loss = 0.006345812,0.0008477885\n",
      "Iteration 09305: loss = 0.006345244,0.0005390948\n",
      "Iteration 09310: loss = 0.0063445573,0.0005783633\n",
      "Iteration 09315: loss = 0.0063444288,0.000594347\n",
      "Iteration 09320: loss = 0.006344736,0.0005182511\n",
      "Iteration 09325: loss = 0.006344929,0.0005375578\n",
      "Iteration 09330: loss = 0.0063448087,0.00052440644\n",
      "Iteration 09335: loss = 0.00634456,0.0005159774\n",
      "Iteration 09340: loss = 0.0063444586,0.0005187867\n",
      "Iteration 09345: loss = 0.0063445023,0.000511474\n",
      "Iteration 09350: loss = 0.0063444986,0.00051282113\n",
      "Iteration 09355: loss = 0.006344348,0.0005091883\n",
      "Iteration 09360: loss = 0.006344261,0.00050885684\n",
      "Iteration 09365: loss = 0.0063442606,0.0005069999\n",
      "Iteration 09370: loss = 0.006344208,0.0005058428\n",
      "Iteration 09375: loss = 0.006344125,0.0005047397\n",
      "Iteration 09380: loss = 0.0063440707,0.00050357805\n",
      "Iteration 09385: loss = 0.006344003,0.0005025086\n",
      "Iteration 09390: loss = 0.006343949,0.0005014159\n",
      "Iteration 09395: loss = 0.006343885,0.00050032657\n",
      "Iteration 09400: loss = 0.0063438527,0.00049930747\n",
      "Iteration 09405: loss = 0.0063437293,0.00049867015\n",
      "Iteration 09410: loss = 0.0063437596,0.00049850735\n",
      "Iteration 09415: loss = 0.006343571,0.000500566\n",
      "Iteration 09420: loss = 0.0063437927,0.00051133416\n",
      "Iteration 09425: loss = 0.006343076,0.0005778809\n",
      "Iteration 09430: loss = 0.006344588,0.0010232693\n",
      "Iteration 09435: loss = 0.006340275,0.0045982497\n",
      "Iteration 09440: loss = 0.0063523394,0.033396192\n",
      "Iteration 09445: loss = 0.0063303974,0.10863809\n",
      "Iteration 09450: loss = 0.006339461,0.006626759\n",
      "Iteration 09455: loss = 0.006347845,0.00925959\n",
      "Iteration 09460: loss = 0.0063499846,0.018047534\n",
      "Iteration 09465: loss = 0.0063472227,0.01000938\n",
      "Iteration 09470: loss = 0.0063443347,0.0019666716\n",
      "Iteration 09475: loss = 0.006342143,0.00065259193\n",
      "Iteration 09480: loss = 0.0063414257,0.0020374914\n",
      "Iteration 09485: loss = 0.0063413833,0.001939306\n",
      "Iteration 09490: loss = 0.006342016,0.000819604\n",
      "Iteration 09495: loss = 0.0063429154,0.00049884024\n",
      "Iteration 09500: loss = 0.0063433093,0.00073276635\n",
      "Iteration 09505: loss = 0.0063430704,0.00063830643\n",
      "Iteration 09510: loss = 0.006342456,0.0004791874\n",
      "Iteration 09515: loss = 0.00634208,0.0005282685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 09520: loss = 0.0063420944,0.0005107187\n",
      "Iteration 09525: loss = 0.006342349,0.0004753\n",
      "Iteration 09530: loss = 0.006342461,0.0004906366\n",
      "Iteration 09535: loss = 0.0063422904,0.00047447503\n",
      "Iteration 09540: loss = 0.006342076,0.00047628034\n",
      "Iteration 09545: loss = 0.0063420166,0.00047275698\n",
      "Iteration 09550: loss = 0.0063420585,0.0004710656\n",
      "Iteration 09555: loss = 0.006341987,0.00046946292\n",
      "Iteration 09560: loss = 0.0063418713,0.00046844548\n",
      "Iteration 09565: loss = 0.0063418318,0.0004668787\n",
      "Iteration 09570: loss = 0.0063418294,0.00046617602\n",
      "Iteration 09575: loss = 0.0063417344,0.00046479504\n",
      "Iteration 09580: loss = 0.00634169,0.00046385964\n",
      "Iteration 09585: loss = 0.006341633,0.0004627917\n",
      "Iteration 09590: loss = 0.0063415747,0.0004618057\n",
      "Iteration 09595: loss = 0.006341496,0.0004608533\n",
      "Iteration 09600: loss = 0.006341485,0.00045993537\n",
      "Iteration 09605: loss = 0.0063413675,0.0004592767\n",
      "Iteration 09610: loss = 0.0063413796,0.00045868833\n",
      "Iteration 09615: loss = 0.0063411915,0.00045928563\n",
      "Iteration 09620: loss = 0.0063413642,0.0004636374\n",
      "Iteration 09625: loss = 0.0063408655,0.0004922206\n",
      "Iteration 09630: loss = 0.0063418415,0.0006684493\n",
      "Iteration 09635: loss = 0.0063390234,0.0020091413\n",
      "Iteration 09640: loss = 0.006346907,0.013358532\n",
      "Iteration 09645: loss = 0.0063276086,0.085746944\n",
      "Iteration 09650: loss = 0.006344845,0.025902255\n",
      "Iteration 09655: loss = 0.0063474663,0.042682596\n",
      "Iteration 09660: loss = 0.0063454206,0.013203312\n",
      "Iteration 09665: loss = 0.0063415724,0.00065550156\n",
      "Iteration 09670: loss = 0.0063392282,0.002743811\n",
      "Iteration 09675: loss = 0.006338317,0.0049732113\n",
      "Iteration 09680: loss = 0.006338909,0.0032842632\n",
      "Iteration 09685: loss = 0.006339781,0.00095310714\n",
      "Iteration 09690: loss = 0.006340539,0.00049144914\n",
      "Iteration 09695: loss = 0.00634118,0.0009287797\n",
      "Iteration 09700: loss = 0.0063410285,0.000821974\n",
      "Iteration 09705: loss = 0.006340399,0.00047749223\n",
      "Iteration 09710: loss = 0.00633974,0.00048390077\n",
      "Iteration 09715: loss = 0.0063395984,0.00053415744\n",
      "Iteration 09720: loss = 0.0063398923,0.00045144733\n",
      "Iteration 09725: loss = 0.006340187,0.00044950497\n",
      "Iteration 09730: loss = 0.006340176,0.00045666855\n",
      "Iteration 09735: loss = 0.0063399295,0.00043602457\n",
      "Iteration 09740: loss = 0.00633973,0.0004426586\n",
      "Iteration 09745: loss = 0.0063397586,0.00043523381\n",
      "Iteration 09750: loss = 0.0063397908,0.00043509546\n",
      "Iteration 09755: loss = 0.006339705,0.0004329958\n",
      "Iteration 09760: loss = 0.006339558,0.00043231476\n",
      "Iteration 09765: loss = 0.0063395384,0.00043068454\n",
      "Iteration 09770: loss = 0.0063395216,0.0004300606\n",
      "Iteration 09775: loss = 0.0063394257,0.00042876427\n",
      "Iteration 09780: loss = 0.0063393614,0.00042791906\n",
      "Iteration 09785: loss = 0.006339327,0.00042695616\n",
      "Iteration 09790: loss = 0.0063392757,0.00042606838\n",
      "Iteration 09795: loss = 0.006339203,0.0004251801\n",
      "Iteration 09800: loss = 0.006339163,0.0004242579\n",
      "Iteration 09805: loss = 0.0063391454,0.00042338046\n",
      "Iteration 09810: loss = 0.00633903,0.00042284495\n",
      "Iteration 09815: loss = 0.006339054,0.0004225521\n",
      "Iteration 09820: loss = 0.0063388534,0.0004238794\n",
      "Iteration 09825: loss = 0.0063390783,0.0004309013\n",
      "Iteration 09830: loss = 0.0063384473,0.00047340314\n",
      "Iteration 09835: loss = 0.006339785,0.00073868956\n",
      "Iteration 09840: loss = 0.006336074,0.0027691745\n",
      "Iteration 09845: loss = 0.0063464125,0.019448394\n",
      "Iteration 09850: loss = 0.0063239634,0.10036466\n",
      "Iteration 09855: loss = 0.006337512,0.0044430234\n",
      "Iteration 09860: loss = 0.006343678,0.03242635\n",
      "Iteration 09865: loss = 0.0063434914,0.018711405\n",
      "Iteration 09870: loss = 0.006339613,0.0029454639\n",
      "Iteration 09875: loss = 0.0063370466,0.0008142103\n",
      "Iteration 09880: loss = 0.006335853,0.0033764476\n",
      "Iteration 09885: loss = 0.0063363262,0.00315656\n",
      "Iteration 09890: loss = 0.006337282,0.0011462043\n",
      "Iteration 09895: loss = 0.0063384697,0.00041427603\n",
      "Iteration 09900: loss = 0.0063393563,0.0007902257\n",
      "Iteration 09905: loss = 0.006339213,0.00074847526\n",
      "Iteration 09910: loss = 0.0063383873,0.0004352418\n",
      "Iteration 09915: loss = 0.0063376236,0.0004539276\n",
      "Iteration 09920: loss = 0.006337477,0.00048700703\n",
      "Iteration 09925: loss = 0.006337742,0.00040915952\n",
      "Iteration 09930: loss = 0.006338011,0.0004212033\n",
      "Iteration 09935: loss = 0.006337878,0.00041537627\n",
      "Iteration 09940: loss = 0.0063376073,0.0004027434\n",
      "Iteration 09945: loss = 0.0063374373,0.00040785637\n",
      "Iteration 09950: loss = 0.0063375174,0.0004001531\n",
      "Iteration 09955: loss = 0.006337539,0.00040159936\n",
      "Iteration 09960: loss = 0.0063374112,0.00039852026\n",
      "Iteration 09965: loss = 0.0063372925,0.00039871974\n",
      "Iteration 09970: loss = 0.006337293,0.00039688902\n",
      "Iteration 09975: loss = 0.0063372836,0.0003963846\n",
      "Iteration 09980: loss = 0.006337177,0.0003954206\n",
      "Iteration 09985: loss = 0.006337134,0.000394438\n",
      "Iteration 09990: loss = 0.0063370974,0.0003936985\n",
      "Iteration 09995: loss = 0.0063370224,0.00039282365\n",
      "Iteration 10000: loss = 0.006336957,0.0003920279\n",
      "Iteration 10005: loss = 0.0063369754,0.00039130327\n",
      "Iteration 10010: loss = 0.0063368473,0.00039090146\n",
      "Iteration 10015: loss = 0.0063368785,0.0003904577\n",
      "Iteration 10020: loss = 0.006336657,0.00039135027\n",
      "Iteration 10025: loss = 0.0063368734,0.0003959167\n",
      "Iteration 10030: loss = 0.0063362457,0.00042501008\n",
      "Iteration 10035: loss = 0.00633755,0.00060939137\n",
      "Iteration 10040: loss = 0.006334079,0.0020170552\n",
      "Iteration 10045: loss = 0.00634381,0.0138652595\n",
      "Iteration 10050: loss = 0.0063205697,0.08713087\n",
      "Iteration 10055: loss = 0.0063369977,0.022358548\n",
      "Iteration 10060: loss = 0.006340965,0.041578744\n",
      "Iteration 10065: loss = 0.006339399,0.013729943\n",
      "Iteration 10070: loss = 0.0063347253,0.0007179486\n",
      "Iteration 10075: loss = 0.006331902,0.0024313815\n",
      "Iteration 10080: loss = 0.0063311122,0.004741193\n",
      "Iteration 10085: loss = 0.006332386,0.003180495\n",
      "Iteration 10090: loss = 0.006334152,0.0008919008\n",
      "Iteration 10095: loss = 0.006335929,0.00042579888\n",
      "Iteration 10100: loss = 0.0063371644,0.0008616757\n",
      "Iteration 10105: loss = 0.006337089,0.0007456633\n",
      "Iteration 10110: loss = 0.006336247,0.00040675333\n",
      "Iteration 10115: loss = 0.0063354373,0.00042436385\n",
      "Iteration 10120: loss = 0.0063353055,0.0004671995\n",
      "Iteration 10125: loss = 0.0063356757,0.0003840691\n",
      "Iteration 10130: loss = 0.0063360366,0.00038701494\n",
      "Iteration 10135: loss = 0.00633597,0.0003908907\n",
      "Iteration 10140: loss = 0.0063355896,0.00037163915\n",
      "Iteration 10145: loss = 0.006335242,0.0003793041\n",
      "Iteration 10150: loss = 0.006335287,0.0003709423\n",
      "Iteration 10155: loss = 0.0063353996,0.00037207582\n",
      "Iteration 10160: loss = 0.006335271,0.0003691446\n",
      "Iteration 10165: loss = 0.006335141,0.00036906346\n",
      "Iteration 10170: loss = 0.006335147,0.0003672868\n",
      "Iteration 10175: loss = 0.006335178,0.00036682983\n",
      "Iteration 10180: loss = 0.006335079,0.0003657537\n",
      "Iteration 10185: loss = 0.006334994,0.000365136\n",
      "Iteration 10190: loss = 0.006334988,0.0003643793\n",
      "Iteration 10195: loss = 0.0063348557,0.00036364945\n",
      "Iteration 10200: loss = 0.006334944,0.0003627758\n",
      "Iteration 10205: loss = 0.0063348482,0.00036205133\n",
      "Iteration 10210: loss = 0.00633475,0.00036131163\n",
      "Iteration 10215: loss = 0.006334675,0.00036060647\n",
      "Iteration 10220: loss = 0.0063346815,0.00035983627\n",
      "Iteration 10225: loss = 0.0063347407,0.00035922189\n",
      "Iteration 10230: loss = 0.0063344594,0.00035962655\n",
      "Iteration 10235: loss = 0.0063346624,0.00036324718\n",
      "Iteration 10240: loss = 0.0063339896,0.00039217912\n",
      "Iteration 10245: loss = 0.006335786,0.00061784714\n",
      "Iteration 10250: loss = 0.006330272,0.0027729985\n",
      "Iteration 10255: loss = 0.006347299,0.024543963\n",
      "Iteration 10260: loss = 0.0063123717,0.13127023\n",
      "Iteration 10265: loss = 0.0063148397,0.0022745205\n",
      "Iteration 10270: loss = 0.0063226004,0.013060577\n",
      "Iteration 10275: loss = 0.006329555,0.020035157\n",
      "Iteration 10280: loss = 0.0063322983,0.014714356\n",
      "Iteration 10285: loss = 0.006333813,0.0073391777\n",
      "Iteration 10290: loss = 0.0063338648,0.002431816\n",
      "Iteration 10295: loss = 0.0063331383,0.0005207412\n",
      "Iteration 10300: loss = 0.0063320585,0.00046868395\n",
      "Iteration 10305: loss = 0.006331987,0.00088983943\n",
      "Iteration 10310: loss = 0.0063326336,0.0009718281\n",
      "Iteration 10315: loss = 0.0063337404,0.0006852662\n",
      "Iteration 10320: loss = 0.0063349004,0.00040925207\n",
      "Iteration 10325: loss = 0.0063357963,0.0003562566\n",
      "Iteration 10330: loss = 0.00633592,0.00040420186\n",
      "Iteration 10335: loss = 0.0063351314,0.0003932208\n",
      "Iteration 10340: loss = 0.0063339546,0.00035083777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10345: loss = 0.0063329046,0.00034800396\n",
      "Iteration 10350: loss = 0.006332485,0.0003552081\n",
      "Iteration 10355: loss = 0.006332629,0.0003462747\n",
      "Iteration 10360: loss = 0.0063330843,0.00034294138\n",
      "Iteration 10365: loss = 0.0063333884,0.00034386557\n",
      "Iteration 10370: loss = 0.0063333143,0.00034050638\n",
      "Iteration 10375: loss = 0.0063331164,0.0003406119\n",
      "Iteration 10380: loss = 0.006333033,0.00033954883\n",
      "Iteration 10385: loss = 0.0063330117,0.00033863843\n",
      "Iteration 10390: loss = 0.006332954,0.00033805994\n",
      "Iteration 10395: loss = 0.006332909,0.00033714477\n",
      "Iteration 10400: loss = 0.006332837,0.00033660082\n",
      "Iteration 10405: loss = 0.0063328333,0.00033579426\n",
      "Iteration 10410: loss = 0.0063326955,0.00033521617\n",
      "Iteration 10415: loss = 0.006332751,0.00033448712\n",
      "Iteration 10420: loss = 0.0063326596,0.00033384818\n",
      "Iteration 10425: loss = 0.006332591,0.00033316703\n",
      "Iteration 10430: loss = 0.0063325544,0.00033249942\n",
      "Iteration 10435: loss = 0.0063325427,0.00033179446\n",
      "Iteration 10440: loss = 0.0063325013,0.00033113017\n",
      "Iteration 10445: loss = 0.0063323663,0.00033058494\n",
      "Iteration 10450: loss = 0.0063324985,0.00032996238\n",
      "Iteration 10455: loss = 0.0063321698,0.00033013223\n",
      "Iteration 10460: loss = 0.00633248,0.00033108733\n",
      "Iteration 10465: loss = 0.006331764,0.00033991004\n",
      "Iteration 10470: loss = 0.0063332804,0.00038562482\n",
      "Iteration 10475: loss = 0.0063291728,0.0007167817\n",
      "Iteration 10480: loss = 0.006340628,0.0034019419\n",
      "Iteration 10485: loss = 0.006307703,0.027393308\n",
      "Iteration 10490: loss = 0.0063648936,0.1083558\n",
      "Iteration 10495: loss = 0.00637521,0.003959596\n",
      "Iteration 10500: loss = 0.0063623856,0.016757814\n",
      "Iteration 10505: loss = 0.0063488707,0.019651046\n",
      "Iteration 10510: loss = 0.0063425177,0.008824262\n",
      "Iteration 10515: loss = 0.0063430555,0.0015229118\n",
      "Iteration 10520: loss = 0.006345089,0.0007613492\n",
      "Iteration 10525: loss = 0.0063453633,0.001944358\n",
      "Iteration 10530: loss = 0.0063431934,0.0017627042\n",
      "Iteration 10535: loss = 0.006340094,0.00073294435\n",
      "Iteration 10540: loss = 0.0063367044,0.00036138901\n",
      "Iteration 10545: loss = 0.006334307,0.0005412593\n",
      "Iteration 10550: loss = 0.006333266,0.00050020084\n",
      "Iteration 10555: loss = 0.0063333404,0.00033504516\n",
      "Iteration 10560: loss = 0.0063334308,0.0003471055\n",
      "Iteration 10565: loss = 0.006332826,0.00035462645\n",
      "Iteration 10570: loss = 0.0063318885,0.00031733466\n",
      "Iteration 10575: loss = 0.0063310633,0.00032655738\n",
      "Iteration 10580: loss = 0.006330809,0.00032098772\n",
      "Iteration 10585: loss = 0.0063308985,0.00031564332\n",
      "Iteration 10590: loss = 0.006330831,0.0003174283\n",
      "Iteration 10595: loss = 0.006330561,0.00031368822\n",
      "Iteration 10600: loss = 0.00633049,0.00031410513\n",
      "Iteration 10605: loss = 0.006330673,0.00031211547\n",
      "Iteration 10610: loss = 0.006330781,0.00031170872\n",
      "Iteration 10615: loss = 0.0063307006,0.0003108269\n",
      "Iteration 10620: loss = 0.0063307616,0.00031000096\n",
      "Iteration 10625: loss = 0.006330749,0.00030938687\n",
      "Iteration 10630: loss = 0.006330682,0.00030878192\n",
      "Iteration 10635: loss = 0.006330605,0.00030823104\n",
      "Iteration 10640: loss = 0.0063305236,0.0003076548\n",
      "Iteration 10645: loss = 0.006330462,0.0003070564\n",
      "Iteration 10650: loss = 0.0063303914,0.00030650134\n",
      "Iteration 10655: loss = 0.006330469,0.00030590803\n",
      "Iteration 10660: loss = 0.006330269,0.00030590393\n",
      "Iteration 10665: loss = 0.0063304245,0.0003063783\n",
      "Iteration 10670: loss = 0.0063299513,0.00031170988\n",
      "Iteration 10675: loss = 0.006330849,0.00033984738\n",
      "Iteration 10680: loss = 0.00632859,0.0005331141\n",
      "Iteration 10685: loss = 0.006334541,0.0020270832\n",
      "Iteration 10690: loss = 0.006317222,0.015510513\n",
      "Iteration 10695: loss = 0.0063583944,0.09251935\n",
      "Iteration 10700: loss = 0.0063368413,0.01318407\n",
      "Iteration 10705: loss = 0.006329043,0.039685708\n",
      "Iteration 10710: loss = 0.006326202,0.018921336\n",
      "Iteration 10715: loss = 0.006325457,0.0028093264\n",
      "Iteration 10720: loss = 0.0063278624,0.00058544206\n",
      "Iteration 10725: loss = 0.006329713,0.0028645182\n",
      "Iteration 10730: loss = 0.0063297977,0.0032215538\n",
      "Iteration 10735: loss = 0.006327973,0.0016067118\n",
      "Iteration 10740: loss = 0.006326141,0.0004203063\n",
      "Iteration 10745: loss = 0.0063249418,0.00044519836\n",
      "Iteration 10750: loss = 0.0063251215,0.0006605157\n",
      "Iteration 10755: loss = 0.006326601,0.00047122\n",
      "Iteration 10760: loss = 0.0063287434,0.00029789325\n",
      "Iteration 10765: loss = 0.00633032,0.00034539448\n",
      "Iteration 10770: loss = 0.006330587,0.0003403286\n",
      "Iteration 10775: loss = 0.0063300575,0.00029427372\n",
      "Iteration 10780: loss = 0.0063293763,0.00030744547\n",
      "Iteration 10785: loss = 0.006329162,0.0003001116\n",
      "Iteration 10790: loss = 0.006329172,0.0002926752\n",
      "Iteration 10795: loss = 0.0063291094,0.00029579765\n",
      "Iteration 10800: loss = 0.006328875,0.00029104805\n",
      "Iteration 10805: loss = 0.006328743,0.00029248866\n",
      "Iteration 10810: loss = 0.006329017,0.00028965058\n",
      "Iteration 10815: loss = 0.006329136,0.00028973934\n",
      "Iteration 10820: loss = 0.006328899,0.00028859198\n",
      "Iteration 10825: loss = 0.006328738,0.00028833532\n",
      "Iteration 10830: loss = 0.0063286987,0.00028758394\n",
      "Iteration 10835: loss = 0.006328743,0.00028698723\n",
      "Iteration 10840: loss = 0.0063287267,0.0002863758\n",
      "Iteration 10845: loss = 0.006328693,0.00028580285\n",
      "Iteration 10850: loss = 0.006328717,0.0002853682\n",
      "Iteration 10855: loss = 0.0063284487,0.00028524874\n",
      "Iteration 10860: loss = 0.0063285637,0.00028461363\n",
      "Iteration 10865: loss = 0.006328447,0.00028420473\n",
      "Iteration 10870: loss = 0.0063286386,0.00028430225\n",
      "Iteration 10875: loss = 0.0063280705,0.00028737422\n",
      "Iteration 10880: loss = 0.006328855,0.00029929512\n",
      "Iteration 10885: loss = 0.006327275,0.00036815178\n",
      "Iteration 10890: loss = 0.006331062,0.00081476936\n",
      "Iteration 10895: loss = 0.006320235,0.0044309357\n",
      "Iteration 10900: loss = 0.0063500744,0.03314419\n",
      "Iteration 10905: loss = 0.006304651,0.10315493\n",
      "Iteration 10910: loss = 0.006295619,0.0075131543\n",
      "Iteration 10915: loss = 0.0063083046,0.009326624\n",
      "Iteration 10920: loss = 0.0063247527,0.016361604\n",
      "Iteration 10925: loss = 0.0063354657,0.008358262\n",
      "Iteration 10930: loss = 0.006339952,0.0013939702\n",
      "Iteration 10935: loss = 0.0063396934,0.0007884179\n",
      "Iteration 10940: loss = 0.0063371514,0.0020340306\n",
      "Iteration 10945: loss = 0.006334757,0.0015845736\n",
      "Iteration 10950: loss = 0.006333416,0.0005026123\n",
      "Iteration 10955: loss = 0.00633177,0.00035366876\n",
      "Iteration 10960: loss = 0.0063295774,0.00053214247\n",
      "Iteration 10965: loss = 0.0063269963,0.00036936835\n",
      "Iteration 10970: loss = 0.006325063,0.00028833427\n",
      "Iteration 10975: loss = 0.006324616,0.0003391012\n",
      "Iteration 10980: loss = 0.0063256905,0.0002933439\n",
      "Iteration 10985: loss = 0.006327311,0.00027867127\n",
      "Iteration 10990: loss = 0.0063282386,0.0002855335\n",
      "Iteration 10995: loss = 0.0063280724,0.00027157392\n",
      "Iteration 11000: loss = 0.006327521,0.00027704038\n",
      "Iteration 11005: loss = 0.0063272617,0.00027046213\n",
      "Iteration 11010: loss = 0.006327052,0.00027244404\n",
      "Iteration 11015: loss = 0.0063269194,0.00026974338\n",
      "Iteration 11020: loss = 0.0063269897,0.00026988564\n",
      "Iteration 11025: loss = 0.0063272608,0.0002683333\n",
      "Iteration 11030: loss = 0.0063271746,0.0002679525\n",
      "Iteration 11035: loss = 0.0063268715,0.0002677795\n",
      "Iteration 11040: loss = 0.006326886,0.0002669554\n",
      "Iteration 11045: loss = 0.0063269027,0.00026637767\n",
      "Iteration 11050: loss = 0.006326905,0.00026586227\n",
      "Iteration 11055: loss = 0.0063268174,0.0002653675\n",
      "Iteration 11060: loss = 0.0063267467,0.00026489326\n",
      "Iteration 11065: loss = 0.00632673,0.00026439797\n",
      "Iteration 11070: loss = 0.006326666,0.00026392168\n",
      "Iteration 11075: loss = 0.006326636,0.00026343804\n",
      "Iteration 11080: loss = 0.0063267886,0.0002631388\n",
      "Iteration 11085: loss = 0.006326332,0.00026493426\n",
      "Iteration 11090: loss = 0.006326953,0.00027666523\n",
      "Iteration 11095: loss = 0.006325319,0.00038523303\n",
      "Iteration 11100: loss = 0.006330222,0.0014619642\n",
      "Iteration 11105: loss = 0.0063136625,0.013890103\n",
      "Iteration 11110: loss = 0.006359206,0.11052407\n",
      "Iteration 11115: loss = 0.006346609,0.011945564\n",
      "Iteration 11120: loss = 0.0063296384,0.037982613\n",
      "Iteration 11125: loss = 0.006317521,0.029111248\n",
      "Iteration 11130: loss = 0.006310048,0.015995579\n",
      "Iteration 11135: loss = 0.006309248,0.0073219915\n",
      "Iteration 11140: loss = 0.0063109417,0.002789523\n",
      "Iteration 11145: loss = 0.006313328,0.0008903265\n",
      "Iteration 11150: loss = 0.0063154534,0.00041851465\n",
      "Iteration 11155: loss = 0.006317997,0.0005213927\n",
      "Iteration 11160: loss = 0.0063197394,0.00068417727\n",
      "Iteration 11165: loss = 0.006320857,0.0006828284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11170: loss = 0.0063216225,0.0005317065\n",
      "Iteration 11175: loss = 0.006322184,0.00036222214\n",
      "Iteration 11180: loss = 0.0063225473,0.00027828786\n",
      "Iteration 11185: loss = 0.0063229334,0.0002758959\n",
      "Iteration 11190: loss = 0.006323555,0.00028685384\n",
      "Iteration 11195: loss = 0.0063242186,0.00027491766\n",
      "Iteration 11200: loss = 0.006324896,0.00025715167\n",
      "Iteration 11205: loss = 0.006325467,0.00025361273\n",
      "Iteration 11210: loss = 0.0063258805,0.0002563534\n",
      "Iteration 11215: loss = 0.0063259597,0.00025351846\n",
      "Iteration 11220: loss = 0.0063258014,0.00025125407\n",
      "Iteration 11225: loss = 0.00632564,0.00025156172\n",
      "Iteration 11230: loss = 0.0063254256,0.0002507435\n",
      "Iteration 11235: loss = 0.006325266,0.00025002938\n",
      "Iteration 11240: loss = 0.0063251383,0.00024984067\n",
      "Iteration 11245: loss = 0.006325131,0.0002491839\n",
      "Iteration 11250: loss = 0.0063251606,0.00024862893\n",
      "Iteration 11255: loss = 0.0063251723,0.0002481357\n",
      "Iteration 11260: loss = 0.0063251345,0.00024762872\n",
      "Iteration 11265: loss = 0.0063250926,0.00024720925\n",
      "Iteration 11270: loss = 0.0063249823,0.00024685235\n",
      "Iteration 11275: loss = 0.006325005,0.00024630484\n",
      "Iteration 11280: loss = 0.006324906,0.00024592932\n",
      "Iteration 11285: loss = 0.0063248803,0.00024541264\n",
      "Iteration 11290: loss = 0.0063248784,0.00024495064\n",
      "Iteration 11295: loss = 0.0063248593,0.00024445483\n",
      "Iteration 11300: loss = 0.006324791,0.00024403844\n",
      "Iteration 11305: loss = 0.00632472,0.00024362742\n",
      "Iteration 11310: loss = 0.0063246884,0.00024317624\n",
      "Iteration 11315: loss = 0.006324777,0.00024270154\n",
      "Iteration 11320: loss = 0.0063245203,0.00024280939\n",
      "Iteration 11325: loss = 0.0063246116,0.00024303814\n",
      "Iteration 11330: loss = 0.0063244533,0.00024608488\n",
      "Iteration 11335: loss = 0.006324917,0.00026283498\n",
      "Iteration 11340: loss = 0.006323404,0.0003764437\n",
      "Iteration 11345: loss = 0.006327231,0.00126088\n",
      "Iteration 11350: loss = 0.0063160625,0.0094330935\n",
      "Iteration 11355: loss = 0.0063463342,0.070310846\n",
      "Iteration 11360: loss = 0.006321641,0.052067623\n",
      "Iteration 11365: loss = 0.0063074604,0.046190854\n",
      "Iteration 11370: loss = 0.006303702,0.010601305\n",
      "Iteration 11375: loss = 0.0063064843,0.0007168444\n",
      "Iteration 11380: loss = 0.006315388,0.0027385452\n",
      "Iteration 11385: loss = 0.006324658,0.0045293476\n",
      "Iteration 11390: loss = 0.006331131,0.003478529\n",
      "Iteration 11395: loss = 0.0063334648,0.001465466\n",
      "Iteration 11400: loss = 0.0063323905,0.00038527956\n",
      "Iteration 11405: loss = 0.006328291,0.00039467914\n",
      "Iteration 11410: loss = 0.006323449,0.0005847609\n",
      "Iteration 11415: loss = 0.006320244,0.00045990068\n",
      "Iteration 11420: loss = 0.006319867,0.0002773908\n",
      "Iteration 11425: loss = 0.0063218386,0.00026501715\n",
      "Iteration 11430: loss = 0.0063244547,0.00028629956\n",
      "Iteration 11435: loss = 0.006325728,0.00024931054\n",
      "Iteration 11440: loss = 0.0063249078,0.00023731754\n",
      "Iteration 11445: loss = 0.006323229,0.00024491485\n",
      "Iteration 11450: loss = 0.0063224733,0.00023667316\n",
      "Iteration 11455: loss = 0.0063230735,0.00023491302\n",
      "Iteration 11460: loss = 0.006323987,0.00023338631\n",
      "Iteration 11465: loss = 0.0063239704,0.00023131202\n",
      "Iteration 11470: loss = 0.0063232197,0.00023204277\n",
      "Iteration 11475: loss = 0.0063230526,0.00023099537\n",
      "Iteration 11480: loss = 0.0063234456,0.00023020388\n",
      "Iteration 11485: loss = 0.006323539,0.00022947995\n",
      "Iteration 11490: loss = 0.0063231797,0.0002293651\n",
      "Iteration 11495: loss = 0.006323159,0.00022891082\n",
      "Iteration 11500: loss = 0.006323194,0.00022839059\n",
      "Iteration 11505: loss = 0.0063232984,0.00022786937\n",
      "Iteration 11510: loss = 0.006322969,0.00022773663\n",
      "Iteration 11515: loss = 0.006323122,0.00022707455\n",
      "Iteration 11520: loss = 0.0063231196,0.00022663892\n",
      "Iteration 11525: loss = 0.0063229185,0.00022638282\n",
      "Iteration 11530: loss = 0.0063230046,0.00022583455\n",
      "Iteration 11535: loss = 0.0063229124,0.00022548604\n",
      "Iteration 11540: loss = 0.0063229096,0.00022505113\n",
      "Iteration 11545: loss = 0.00632274,0.00022484639\n",
      "Iteration 11550: loss = 0.006323114,0.00022469033\n",
      "Iteration 11555: loss = 0.0063221063,0.00022866356\n",
      "Iteration 11560: loss = 0.006324386,0.00025050165\n",
      "Iteration 11565: loss = 0.006318126,0.00046837836\n",
      "Iteration 11570: loss = 0.00633637,0.0026237029\n",
      "Iteration 11575: loss = 0.0062815025,0.026789272\n",
      "Iteration 11580: loss = 0.0063702543,0.12813427\n",
      "Iteration 11585: loss = 0.0064175203,0.0151350545\n",
      "Iteration 11590: loss = 0.0063901357,0.014162969\n",
      "Iteration 11595: loss = 0.006341551,0.014327192\n",
      "Iteration 11600: loss = 0.006291825,0.011561344\n",
      "Iteration 11605: loss = 0.006275754,0.008528809\n",
      "Iteration 11610: loss = 0.0062965346,0.0032897452\n",
      "Iteration 11615: loss = 0.0063260323,0.00075836654\n",
      "Iteration 11620: loss = 0.0063389814,0.00061861164\n",
      "Iteration 11625: loss = 0.006333323,0.0005395022\n",
      "Iteration 11630: loss = 0.0063218283,0.0006214018\n",
      "Iteration 11635: loss = 0.0063165077,0.0006079653\n",
      "Iteration 11640: loss = 0.0063184802,0.00035479886\n",
      "Iteration 11645: loss = 0.0063225045,0.00021942487\n",
      "Iteration 11650: loss = 0.0063243653,0.00025042263\n",
      "Iteration 11655: loss = 0.0063237813,0.00026509922\n",
      "Iteration 11660: loss = 0.006322213,0.0002322835\n",
      "Iteration 11665: loss = 0.006321117,0.00021776857\n",
      "Iteration 11670: loss = 0.0063208747,0.00022497623\n",
      "Iteration 11675: loss = 0.0063211396,0.00022164323\n",
      "Iteration 11680: loss = 0.0063214465,0.0002155215\n",
      "Iteration 11685: loss = 0.006321722,0.00021618255\n",
      "Iteration 11690: loss = 0.0063219215,0.00021501008\n",
      "Iteration 11695: loss = 0.0063219904,0.00021385547\n",
      "Iteration 11700: loss = 0.0063218623,0.00021391391\n",
      "Iteration 11705: loss = 0.0063215825,0.00021321047\n",
      "Iteration 11710: loss = 0.006321373,0.00021325343\n",
      "Iteration 11715: loss = 0.006321375,0.00021261506\n",
      "Iteration 11720: loss = 0.0063213618,0.00021227193\n",
      "Iteration 11725: loss = 0.006321329,0.00021186065\n",
      "Iteration 11730: loss = 0.0063212947,0.00021148387\n",
      "Iteration 11735: loss = 0.0063213017,0.00021107387\n",
      "Iteration 11740: loss = 0.0063212807,0.00021068889\n",
      "Iteration 11745: loss = 0.0063212416,0.00021032372\n",
      "Iteration 11750: loss = 0.006321209,0.00020996899\n",
      "Iteration 11755: loss = 0.0063211764,0.00020959423\n",
      "Iteration 11760: loss = 0.0063211867,0.00020922799\n",
      "Iteration 11765: loss = 0.0063210484,0.00020921772\n",
      "Iteration 11770: loss = 0.006321168,0.00020902307\n",
      "Iteration 11775: loss = 0.0063208546,0.00020972955\n",
      "Iteration 11780: loss = 0.0063212104,0.00021121337\n",
      "Iteration 11785: loss = 0.0063204714,0.00022027321\n",
      "Iteration 11790: loss = 0.006321883,0.000262905\n",
      "Iteration 11795: loss = 0.0063184244,0.00055114314\n",
      "Iteration 11800: loss = 0.0063274675,0.0027326883\n",
      "Iteration 11805: loss = 0.0063020554,0.021323541\n",
      "Iteration 11810: loss = 0.006357308,0.09716709\n",
      "Iteration 11815: loss = 0.0063232947,0.00083640765\n",
      "Iteration 11820: loss = 0.00631176,0.025649033\n",
      "Iteration 11825: loss = 0.006311759,0.01952339\n",
      "Iteration 11830: loss = 0.0063154176,0.004622711\n",
      "Iteration 11835: loss = 0.0063204537,0.00021155286\n",
      "Iteration 11840: loss = 0.0063233715,0.0020337831\n",
      "Iteration 11845: loss = 0.006323639,0.0026914573\n",
      "Iteration 11850: loss = 0.006322149,0.0012263407\n",
      "Iteration 11855: loss = 0.0063203233,0.00022959273\n",
      "Iteration 11860: loss = 0.0063187014,0.00042711917\n",
      "Iteration 11865: loss = 0.0063185594,0.00054583966\n",
      "Iteration 11870: loss = 0.0063195713,0.0002743035\n",
      "Iteration 11875: loss = 0.006320713,0.0002187575\n",
      "Iteration 11880: loss = 0.0063210377,0.00027620522\n",
      "Iteration 11885: loss = 0.006320669,0.00021630585\n",
      "Iteration 11890: loss = 0.006320052,0.00021045505\n",
      "Iteration 11895: loss = 0.0063199014,0.0002178567\n",
      "Iteration 11900: loss = 0.0063202386,0.00020090726\n",
      "Iteration 11905: loss = 0.0063204304,0.00020682973\n",
      "Iteration 11910: loss = 0.006320204,0.00020051611\n",
      "Iteration 11915: loss = 0.006319985,0.00020213652\n",
      "Iteration 11920: loss = 0.0063200463,0.00019968739\n",
      "Iteration 11925: loss = 0.006320104,0.00019986302\n",
      "Iteration 11930: loss = 0.0063199964,0.00019882008\n",
      "Iteration 11935: loss = 0.0063198972,0.0001989498\n",
      "Iteration 11940: loss = 0.006319938,0.00019818709\n",
      "Iteration 11945: loss = 0.0063198903,0.0001978553\n",
      "Iteration 11950: loss = 0.0063198362,0.00019750284\n",
      "Iteration 11955: loss = 0.0063198064,0.0001971047\n",
      "Iteration 11960: loss = 0.006319795,0.00019680429\n",
      "Iteration 11965: loss = 0.006319687,0.0001966658\n",
      "Iteration 11970: loss = 0.006319756,0.00019644301\n",
      "Iteration 11975: loss = 0.0063196043,0.0001965941\n",
      "Iteration 11980: loss = 0.006319742,0.00019691477\n",
      "Iteration 11985: loss = 0.006319415,0.00019978825\n",
      "Iteration 11990: loss = 0.006319962,0.0002134193\n",
      "Iteration 11995: loss = 0.006318601,0.00030129665\n",
      "Iteration 12000: loss = 0.0063219783,0.0009321647\n",
      "Iteration 12005: loss = 0.006312246,0.006450104\n",
      "Iteration 12010: loss = 0.006339476,0.05007819\n",
      "Iteration 12015: loss = 0.0063018952,0.08387259\n",
      "Iteration 12020: loss = 0.0063070343,0.031331353\n",
      "Iteration 12025: loss = 0.006316906,0.00068497646\n",
      "Iteration 12030: loss = 0.0063217594,0.004522173\n",
      "Iteration 12035: loss = 0.006324388,0.008463771\n",
      "Iteration 12040: loss = 0.0063238665,0.005959099\n",
      "Iteration 12045: loss = 0.0063212328,0.001930436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12050: loss = 0.0063181934,0.00022893136\n",
      "Iteration 12055: loss = 0.006316586,0.000560839\n",
      "Iteration 12060: loss = 0.00631599,0.000918416\n",
      "Iteration 12065: loss = 0.0063166344,0.0005600252\n",
      "Iteration 12070: loss = 0.0063180323,0.00021098225\n",
      "Iteration 12075: loss = 0.006319147,0.00024887946\n",
      "Iteration 12080: loss = 0.0063193366,0.0002925136\n",
      "Iteration 12085: loss = 0.006318964,0.00021038223\n",
      "Iteration 12090: loss = 0.0063184393,0.0001969565\n",
      "Iteration 12095: loss = 0.006318307,0.00021203668\n",
      "Iteration 12100: loss = 0.0063186814,0.00019127817\n",
      "Iteration 12105: loss = 0.0063189957,0.00019215187\n",
      "Iteration 12110: loss = 0.0063189953,0.00019169698\n",
      "Iteration 12115: loss = 0.0063187345,0.0001882159\n",
      "Iteration 12120: loss = 0.006318603,0.0001896073\n",
      "Iteration 12125: loss = 0.0063186754,0.00018723469\n",
      "Iteration 12130: loss = 0.006318694,0.00018748759\n",
      "Iteration 12135: loss = 0.0063185687,0.00018661683\n",
      "Iteration 12140: loss = 0.0063184933,0.00018659224\n",
      "Iteration 12145: loss = 0.006318569,0.00018600863\n",
      "Iteration 12150: loss = 0.0063184965,0.00018562176\n",
      "Iteration 12155: loss = 0.0063184462,0.00018535162\n",
      "Iteration 12160: loss = 0.0063184146,0.00018500444\n",
      "Iteration 12165: loss = 0.006318381,0.00018468394\n",
      "Iteration 12170: loss = 0.006318357,0.00018434672\n",
      "Iteration 12175: loss = 0.0063183294,0.00018402658\n",
      "Iteration 12180: loss = 0.0063183005,0.00018371818\n",
      "Iteration 12185: loss = 0.006318238,0.00018342826\n",
      "Iteration 12190: loss = 0.0063182544,0.00018309563\n",
      "Iteration 12195: loss = 0.006318092,0.00018323553\n",
      "Iteration 12200: loss = 0.0063182632,0.00018407645\n",
      "Iteration 12205: loss = 0.006317857,0.00019164008\n",
      "Iteration 12210: loss = 0.0063188057,0.00024200502\n",
      "Iteration 12215: loss = 0.006315851,0.00068534043\n",
      "Iteration 12220: loss = 0.0063248402,0.0050539523\n",
      "Iteration 12225: loss = 0.006297692,0.04846892\n",
      "Iteration 12230: loss = 0.006337412,0.10396373\n",
      "Iteration 12235: loss = 0.0063399635,0.03821952\n",
      "Iteration 12240: loss = 0.0063391537,0.00370994\n",
      "Iteration 12245: loss = 0.006332163,0.0011332153\n",
      "Iteration 12250: loss = 0.006325826,0.004130823\n",
      "Iteration 12255: loss = 0.0063202516,0.0052293576\n",
      "Iteration 12260: loss = 0.006315604,0.0042289323\n",
      "Iteration 12265: loss = 0.0063126697,0.0024258448\n",
      "Iteration 12270: loss = 0.0063121556,0.0009568316\n",
      "Iteration 12275: loss = 0.006312677,0.00028297558\n",
      "Iteration 12280: loss = 0.006314181,0.00023756194\n",
      "Iteration 12285: loss = 0.0063162246,0.00036842938\n",
      "Iteration 12290: loss = 0.0063178656,0.0003667916\n",
      "Iteration 12295: loss = 0.00631874,0.0002529537\n",
      "Iteration 12300: loss = 0.0063188463,0.00018181014\n",
      "Iteration 12305: loss = 0.0063183103,0.00019068999\n",
      "Iteration 12310: loss = 0.006317505,0.00020156408\n",
      "Iteration 12315: loss = 0.00631703,0.00018515906\n",
      "Iteration 12320: loss = 0.0063169543,0.00017764272\n",
      "Iteration 12325: loss = 0.0063172025,0.00018135677\n",
      "Iteration 12330: loss = 0.006317431,0.00017809313\n",
      "Iteration 12335: loss = 0.006317463,0.00017576243\n",
      "Iteration 12340: loss = 0.0063172807,0.00017660776\n",
      "Iteration 12345: loss = 0.006317144,0.0001753762\n",
      "Iteration 12350: loss = 0.006317125,0.00017515842\n",
      "Iteration 12355: loss = 0.0063171946,0.00017467936\n",
      "Iteration 12360: loss = 0.0063172043,0.00017419354\n",
      "Iteration 12365: loss = 0.006317085,0.00017404323\n",
      "Iteration 12370: loss = 0.0063169315,0.00017380196\n",
      "Iteration 12375: loss = 0.0063170395,0.00017341944\n",
      "Iteration 12380: loss = 0.0063169985,0.00017308176\n",
      "Iteration 12385: loss = 0.006317018,0.00017272969\n",
      "Iteration 12390: loss = 0.006316844,0.00017256162\n",
      "Iteration 12395: loss = 0.0063168784,0.00017221179\n",
      "Iteration 12400: loss = 0.006316894,0.00017187902\n",
      "Iteration 12405: loss = 0.006316799,0.0001716341\n",
      "Iteration 12410: loss = 0.0063167955,0.00017133287\n",
      "Iteration 12415: loss = 0.006316731,0.00017105695\n",
      "Iteration 12420: loss = 0.0063167284,0.00017074647\n",
      "Iteration 12425: loss = 0.0063167703,0.00017050648\n",
      "Iteration 12430: loss = 0.006316379,0.00017121105\n",
      "Iteration 12435: loss = 0.0063171624,0.00017264209\n",
      "Iteration 12440: loss = 0.006315375,0.00018716272\n",
      "Iteration 12445: loss = 0.0063194074,0.00026643416\n",
      "Iteration 12450: loss = 0.0063088574,0.00093206787\n",
      "Iteration 12455: loss = 0.0063385274,0.0068339994\n",
      "Iteration 12460: loss = 0.0062609655,0.05554762\n",
      "Iteration 12465: loss = 0.0063166767,0.07174774\n",
      "Iteration 12470: loss = 0.00637274,0.0296599\n",
      "Iteration 12475: loss = 0.006361568,0.0032721967\n",
      "Iteration 12480: loss = 0.0063261664,0.003353618\n",
      "Iteration 12485: loss = 0.006295943,0.0063040466\n",
      "Iteration 12490: loss = 0.0062826346,0.0051047397\n",
      "Iteration 12495: loss = 0.006287018,0.0020413995\n",
      "Iteration 12500: loss = 0.006301418,0.000501854\n",
      "Iteration 12505: loss = 0.006316569,0.0006332994\n",
      "Iteration 12510: loss = 0.0063247825,0.00074517075\n",
      "Iteration 12515: loss = 0.006325833,0.00039660762\n",
      "Iteration 12520: loss = 0.0063226144,0.00023322515\n",
      "Iteration 12525: loss = 0.0063185454,0.00027964334\n",
      "Iteration 12530: loss = 0.006315768,0.0002207666\n",
      "Iteration 12535: loss = 0.0063148458,0.00016822157\n",
      "Iteration 12540: loss = 0.006314609,0.00019364257\n",
      "Iteration 12545: loss = 0.0063143247,0.00018004805\n",
      "Iteration 12550: loss = 0.0063142013,0.00017085244\n",
      "Iteration 12555: loss = 0.0063145775,0.00017244724\n",
      "Iteration 12560: loss = 0.0063152313,0.0001647968\n",
      "Iteration 12565: loss = 0.0063157505,0.00016597127\n",
      "Iteration 12570: loss = 0.006315785,0.00016343605\n",
      "Iteration 12575: loss = 0.006315661,0.00016421387\n",
      "Iteration 12580: loss = 0.006315784,0.00016273488\n",
      "Iteration 12585: loss = 0.0063158423,0.00016278695\n",
      "Iteration 12590: loss = 0.006315692,0.00016225665\n",
      "Iteration 12595: loss = 0.0063155987,0.00016213505\n",
      "Iteration 12600: loss = 0.0063156555,0.00016190019\n",
      "Iteration 12605: loss = 0.006315494,0.00016159256\n",
      "Iteration 12610: loss = 0.0063155405,0.00016124424\n",
      "Iteration 12615: loss = 0.006315429,0.00016104602\n",
      "Iteration 12620: loss = 0.006315422,0.00016072358\n",
      "Iteration 12625: loss = 0.006315363,0.00016048239\n",
      "Iteration 12630: loss = 0.006315324,0.00016025535\n",
      "Iteration 12635: loss = 0.006315296,0.00016000493\n",
      "Iteration 12640: loss = 0.00631523,0.00015977124\n",
      "Iteration 12645: loss = 0.0063152066,0.00015949199\n",
      "Iteration 12650: loss = 0.006315116,0.00015933695\n",
      "Iteration 12655: loss = 0.0063152295,0.00015951491\n",
      "Iteration 12660: loss = 0.0063148052,0.000164812\n",
      "Iteration 12665: loss = 0.0063160476,0.00021750486\n",
      "Iteration 12670: loss = 0.0063116015,0.0009042857\n",
      "Iteration 12675: loss = 0.006327978,0.01065712\n",
      "Iteration 12680: loss = 0.006275097,0.120139174\n",
      "Iteration 12685: loss = 0.006311215,0.013788376\n",
      "Iteration 12690: loss = 0.0063159633,0.033575863\n",
      "Iteration 12695: loss = 0.0063169706,0.026455691\n",
      "Iteration 12700: loss = 0.0063170623,0.017002702\n",
      "Iteration 12705: loss = 0.006320926,0.010337089\n",
      "Iteration 12710: loss = 0.0063233315,0.006259654\n",
      "Iteration 12715: loss = 0.0063239946,0.0038634327\n",
      "Iteration 12720: loss = 0.006323311,0.0024489157\n",
      "Iteration 12725: loss = 0.006322145,0.0015951508\n",
      "Iteration 12730: loss = 0.0063198544,0.0010585025\n",
      "Iteration 12735: loss = 0.006318178,0.00071754406\n",
      "Iteration 12740: loss = 0.006316857,0.0004950528\n",
      "Iteration 12745: loss = 0.0063158157,0.00034489293\n",
      "Iteration 12750: loss = 0.0063150763,0.0002489437\n",
      "Iteration 12755: loss = 0.0063147284,0.00019120998\n",
      "Iteration 12760: loss = 0.006314386,0.00016408725\n",
      "Iteration 12765: loss = 0.0063142,0.00015622652\n",
      "Iteration 12770: loss = 0.0063141324,0.00015777364\n",
      "Iteration 12775: loss = 0.0063141026,0.00016053041\n",
      "Iteration 12780: loss = 0.006314093,0.00016062526\n",
      "Iteration 12785: loss = 0.006314196,0.00015767061\n",
      "Iteration 12790: loss = 0.0063143,0.00015484048\n",
      "Iteration 12795: loss = 0.0063143764,0.00015383895\n",
      "Iteration 12800: loss = 0.0063144546,0.000153883\n",
      "Iteration 12805: loss = 0.006314431,0.00015364533\n",
      "Iteration 12810: loss = 0.0063143685,0.00015308562\n",
      "Iteration 12815: loss = 0.0063143037,0.00015277337\n",
      "Iteration 12820: loss = 0.0063142455,0.0001526321\n",
      "Iteration 12825: loss = 0.006314212,0.00015231514\n",
      "Iteration 12830: loss = 0.006314253,0.00015198538\n",
      "Iteration 12835: loss = 0.0063142166,0.00015173922\n",
      "Iteration 12840: loss = 0.00631417,0.00015149932\n",
      "Iteration 12845: loss = 0.0063141263,0.00015125226\n",
      "Iteration 12850: loss = 0.0063141063,0.00015100342\n",
      "Iteration 12855: loss = 0.0063140714,0.00015077906\n",
      "Iteration 12860: loss = 0.006314033,0.00015053968\n",
      "Iteration 12865: loss = 0.006313994,0.00015031884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12870: loss = 0.0063139624,0.00015007846\n",
      "Iteration 12875: loss = 0.0063139475,0.00014980699\n",
      "Iteration 12880: loss = 0.0063138665,0.0001496996\n",
      "Iteration 12885: loss = 0.0063139237,0.00014937793\n",
      "Iteration 12890: loss = 0.0063138697,0.00014909089\n",
      "Iteration 12895: loss = 0.006313804,0.00014893073\n",
      "Iteration 12900: loss = 0.006313766,0.0001486612\n",
      "Iteration 12905: loss = 0.006313743,0.00014842454\n",
      "Iteration 12910: loss = 0.0063137114,0.00014820475\n",
      "Iteration 12915: loss = 0.00631374,0.00014791847\n",
      "Iteration 12920: loss = 0.0063135796,0.00014794772\n",
      "Iteration 12925: loss = 0.006313693,0.00014788675\n",
      "Iteration 12930: loss = 0.0063134977,0.00014842102\n",
      "Iteration 12935: loss = 0.0063137882,0.00014998051\n",
      "Iteration 12940: loss = 0.006313123,0.00015986792\n",
      "Iteration 12945: loss = 0.0063143913,0.00021031052\n",
      "Iteration 12950: loss = 0.0063110343,0.00057866756\n",
      "Iteration 12955: loss = 0.0063200984,0.003509351\n",
      "Iteration 12960: loss = 0.006294612,0.028826097\n",
      "Iteration 12965: loss = 0.006342947,0.10199299\n",
      "Iteration 12970: loss = 0.00633108,0.0041962187\n",
      "Iteration 12975: loss = 0.006321573,0.01026398\n",
      "Iteration 12980: loss = 0.0063111912,0.017474145\n",
      "Iteration 12985: loss = 0.006305773,0.00969785\n",
      "Iteration 12990: loss = 0.0063048345,0.0021161402\n",
      "Iteration 12995: loss = 0.006306313,0.00024178367\n",
      "Iteration 13000: loss = 0.0063091205,0.0012358362\n",
      "Iteration 13005: loss = 0.0063120467,0.0015383482\n",
      "Iteration 13010: loss = 0.0063133533,0.0007222419\n",
      "Iteration 13015: loss = 0.0063137817,0.00016206675\n",
      "Iteration 13020: loss = 0.006313786,0.00026028726\n",
      "Iteration 13025: loss = 0.0063136793,0.0003333274\n",
      "Iteration 13030: loss = 0.0063136797,0.0001875716\n",
      "Iteration 13035: loss = 0.0063135573,0.00015007825\n",
      "Iteration 13040: loss = 0.0063129943,0.0001822889\n",
      "Iteration 13045: loss = 0.006312508,0.00015390055\n",
      "Iteration 13050: loss = 0.006312462,0.0001471008\n",
      "Iteration 13055: loss = 0.0063128467,0.00015205998\n",
      "Iteration 13060: loss = 0.006313329,0.00014234093\n",
      "Iteration 13065: loss = 0.00631325,0.00014548998\n",
      "Iteration 13070: loss = 0.0063126613,0.00014247965\n",
      "Iteration 13075: loss = 0.0063124783,0.00014345156\n",
      "Iteration 13080: loss = 0.0063128155,0.00014158463\n",
      "Iteration 13085: loss = 0.0063129514,0.00014152868\n",
      "Iteration 13090: loss = 0.0063126883,0.00014109522\n",
      "Iteration 13095: loss = 0.006312538,0.00014122535\n",
      "Iteration 13100: loss = 0.006312747,0.00014062095\n",
      "Iteration 13105: loss = 0.0063126325,0.00014038158\n",
      "Iteration 13110: loss = 0.006312538,0.00014025852\n",
      "Iteration 13115: loss = 0.006312531,0.0001399827\n",
      "Iteration 13120: loss = 0.006312641,0.0001396453\n",
      "Iteration 13125: loss = 0.0063124527,0.00013957889\n",
      "Iteration 13130: loss = 0.0063124546,0.00013931585\n",
      "Iteration 13135: loss = 0.0063124816,0.00013904086\n",
      "Iteration 13140: loss = 0.006312395,0.00013888285\n",
      "Iteration 13145: loss = 0.006312351,0.00013870011\n",
      "Iteration 13150: loss = 0.0063124304,0.00013838813\n",
      "Iteration 13155: loss = 0.006312158,0.00013848937\n",
      "Iteration 13160: loss = 0.006312628,0.00013806936\n",
      "Iteration 13165: loss = 0.0063116453,0.00014071778\n",
      "Iteration 13170: loss = 0.0063133594,0.00015599624\n",
      "Iteration 13175: loss = 0.006310126,0.00035998126\n",
      "Iteration 13180: loss = 0.0063153193,0.003173069\n",
      "Iteration 13185: loss = 0.0063150604,0.044381276\n",
      "Iteration 13190: loss = 0.0062676705,0.16753273\n",
      "Iteration 13195: loss = 0.00630709,0.07524192\n",
      "Iteration 13200: loss = 0.00632103,0.033992562\n",
      "Iteration 13205: loss = 0.006321521,0.019191688\n",
      "Iteration 13210: loss = 0.0063226596,0.012390255\n",
      "Iteration 13215: loss = 0.0063230186,0.008287112\n",
      "Iteration 13220: loss = 0.006322291,0.0054716566\n",
      "Iteration 13225: loss = 0.006321618,0.0035149348\n",
      "Iteration 13230: loss = 0.0063211475,0.0022169624\n",
      "Iteration 13235: loss = 0.0063197818,0.0013910002\n",
      "Iteration 13240: loss = 0.0063188574,0.00088729564\n",
      "Iteration 13245: loss = 0.006317981,0.0005837354\n",
      "Iteration 13250: loss = 0.0063167973,0.00040311692\n",
      "Iteration 13255: loss = 0.0063157864,0.00029267475\n",
      "Iteration 13260: loss = 0.006314851,0.00022661807\n",
      "Iteration 13265: loss = 0.0063138953,0.00018501011\n",
      "Iteration 13270: loss = 0.006313073,0.00015974708\n",
      "Iteration 13275: loss = 0.006312484,0.00014512049\n",
      "Iteration 13280: loss = 0.0063119554,0.00013799158\n",
      "Iteration 13285: loss = 0.006311687,0.00013494439\n",
      "Iteration 13290: loss = 0.006311562,0.00013422527\n",
      "Iteration 13295: loss = 0.0063115023,0.00013453451\n",
      "Iteration 13300: loss = 0.006311465,0.00013472002\n",
      "Iteration 13305: loss = 0.0063114553,0.00013446214\n",
      "Iteration 13310: loss = 0.006311406,0.00013379425\n",
      "Iteration 13315: loss = 0.0063114236,0.0001331499\n",
      "Iteration 13320: loss = 0.006311502,0.00013270321\n",
      "Iteration 13325: loss = 0.006311552,0.00013242931\n",
      "Iteration 13330: loss = 0.006311532,0.00013221506\n",
      "Iteration 13335: loss = 0.0063114073,0.0001320969\n",
      "Iteration 13340: loss = 0.00631134,0.00013190377\n",
      "Iteration 13345: loss = 0.006311333,0.00013167581\n",
      "Iteration 13350: loss = 0.0063114217,0.00013139275\n",
      "Iteration 13355: loss = 0.00631121,0.00013132376\n",
      "Iteration 13360: loss = 0.006311203,0.00013110618\n",
      "Iteration 13365: loss = 0.0063112807,0.00013080363\n",
      "Iteration 13370: loss = 0.0063112453,0.00013060242\n",
      "Iteration 13375: loss = 0.006311105,0.00013049995\n",
      "Iteration 13380: loss = 0.006311131,0.00013023677\n",
      "Iteration 13385: loss = 0.0063111247,0.0001300158\n",
      "Iteration 13390: loss = 0.006310955,0.00013002168\n",
      "Iteration 13395: loss = 0.006311281,0.0001295306\n",
      "Iteration 13400: loss = 0.0063106804,0.00012996228\n",
      "Iteration 13405: loss = 0.006311277,0.00012909388\n",
      "Iteration 13410: loss = 0.0063107223,0.00012935017\n",
      "Iteration 13415: loss = 0.00631113,0.0001287434\n",
      "Iteration 13420: loss = 0.006310469,0.00012941622\n",
      "Iteration 13425: loss = 0.0063117673,0.0001289871\n",
      "Iteration 13430: loss = 0.006308663,0.00013767392\n",
      "Iteration 13435: loss = 0.0063162385,0.00016913791\n",
      "Iteration 13440: loss = 0.006295594,0.0005039668\n",
      "Iteration 13445: loss = 0.0063539012,0.0030707682\n",
      "Iteration 13450: loss = 0.006235652,0.012428534\n",
      "Iteration 13455: loss = 0.0063178935,0.021465642\n",
      "Iteration 13460: loss = 0.006291388,0.015563506\n",
      "Iteration 13465: loss = 0.0063291774,0.0006048058\n",
      "Iteration 13470: loss = 0.0063443794,0.0061522685\n",
      "Iteration 13475: loss = 0.006318283,0.0021354456\n",
      "Iteration 13480: loss = 0.006314309,0.0006701214\n",
      "Iteration 13485: loss = 0.0063179936,0.0016881474\n",
      "Iteration 13490: loss = 0.00630308,0.0004953392\n",
      "Iteration 13495: loss = 0.0063036904,0.00023187902\n",
      "Iteration 13500: loss = 0.006309519,0.0005228445\n",
      "Iteration 13505: loss = 0.006305374,0.000508508\n",
      "Iteration 13510: loss = 0.0063139703,0.00036584766\n",
      "Iteration 13515: loss = 0.006310105,0.0002969919\n",
      "Iteration 13520: loss = 0.0063140113,0.00030282483\n",
      "Iteration 13525: loss = 0.0063072057,0.00047585796\n",
      "Iteration 13530: loss = 0.0063150637,0.0011808521\n",
      "Iteration 13535: loss = 0.0062982626,0.0044509447\n",
      "Iteration 13540: loss = 0.00633307,0.018945629\n",
      "Iteration 13545: loss = 0.0062788776,0.048196707\n",
      "Iteration 13550: loss = 0.006312605,0.005315121\n",
      "Iteration 13555: loss = 0.006322486,0.0145601835\n",
      "Iteration 13560: loss = 0.0063111796,0.0005514385\n",
      "Iteration 13565: loss = 0.006305905,0.006108514\n",
      "Iteration 13570: loss = 0.0063114935,0.00017781698\n",
      "Iteration 13575: loss = 0.0063146353,0.0022180728\n",
      "Iteration 13580: loss = 0.006309848,0.0002391249\n",
      "Iteration 13585: loss = 0.0063060042,0.0009028008\n",
      "Iteration 13590: loss = 0.006308911,0.00014920614\n",
      "Iteration 13595: loss = 0.006312124,0.00045010867\n",
      "Iteration 13600: loss = 0.006310288,0.00012949096\n",
      "Iteration 13605: loss = 0.00630914,0.00022423209\n",
      "Iteration 13610: loss = 0.0063104243,0.00017746218\n",
      "Iteration 13615: loss = 0.0063096564,0.0001231668\n",
      "Iteration 13620: loss = 0.0063092574,0.00014983382\n",
      "Iteration 13625: loss = 0.006310498,0.00014772377\n",
      "Iteration 13630: loss = 0.0063094883,0.00013240913\n",
      "Iteration 13635: loss = 0.006309787,0.00012362479\n",
      "Iteration 13640: loss = 0.006309657,0.000121558434\n",
      "Iteration 13645: loss = 0.0063096955,0.00012149659\n",
      "Iteration 13650: loss = 0.0063097677,0.00012238932\n",
      "Iteration 13655: loss = 0.0063093603,0.00012694775\n",
      "Iteration 13660: loss = 0.0063101538,0.00014743902\n",
      "Iteration 13665: loss = 0.0063082627,0.0002863282\n",
      "Iteration 13670: loss = 0.0063132034,0.0013383409\n",
      "Iteration 13675: loss = 0.006298678,0.010835273\n",
      "Iteration 13680: loss = 0.006336872,0.07404073\n",
      "Iteration 13685: loss = 0.006304772,0.03138551\n",
      "Iteration 13690: loss = 0.0062918426,0.04195688\n",
      "Iteration 13695: loss = 0.006288545,0.012683898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13700: loss = 0.0062930402,0.0008283678\n",
      "Iteration 13705: loss = 0.006304394,0.001616974\n",
      "Iteration 13710: loss = 0.006314369,0.0036242048\n",
      "Iteration 13715: loss = 0.00632035,0.0030238088\n",
      "Iteration 13720: loss = 0.0063206516,0.0012109748\n",
      "Iteration 13725: loss = 0.0063154846,0.00019471746\n",
      "Iteration 13730: loss = 0.006308444,0.00027418038\n",
      "Iteration 13735: loss = 0.0063041504,0.0004807844\n",
      "Iteration 13740: loss = 0.0063042995,0.00030508067\n",
      "Iteration 13745: loss = 0.0063078552,0.00012666786\n",
      "Iteration 13750: loss = 0.0063113817,0.0001567512\n",
      "Iteration 13755: loss = 0.006311839,0.00016836738\n",
      "Iteration 13760: loss = 0.006309837,0.00012243539\n",
      "Iteration 13765: loss = 0.0063080974,0.0001271881\n",
      "Iteration 13770: loss = 0.0063081696,0.00013030687\n",
      "Iteration 13775: loss = 0.006309377,0.00011764777\n",
      "Iteration 13780: loss = 0.0063097742,0.00012098141\n",
      "Iteration 13785: loss = 0.006309081,0.00011797117\n",
      "Iteration 13790: loss = 0.0063088317,0.00011818372\n",
      "Iteration 13795: loss = 0.0063090418,0.00011735075\n",
      "Iteration 13800: loss = 0.0063090883,0.000116913085\n",
      "Iteration 13805: loss = 0.0063090175,0.00011670345\n",
      "Iteration 13810: loss = 0.0063089374,0.00011657497\n",
      "Iteration 13815: loss = 0.0063088723,0.0001163171\n",
      "Iteration 13820: loss = 0.0063089267,0.000116065356\n",
      "Iteration 13825: loss = 0.0063089635,0.00011580999\n",
      "Iteration 13830: loss = 0.006308697,0.00011593351\n",
      "Iteration 13835: loss = 0.006309053,0.000115441246\n",
      "Iteration 13840: loss = 0.0063084476,0.000115959294\n",
      "Iteration 13845: loss = 0.0063092797,0.00011512587\n",
      "Iteration 13850: loss = 0.0063078008,0.00011718621\n",
      "Iteration 13855: loss = 0.0063107014,0.00011853301\n",
      "Iteration 13860: loss = 0.0063038855,0.00015190405\n",
      "Iteration 13865: loss = 0.0063213077,0.0003332019\n",
      "Iteration 13870: loss = 0.006273575,0.0019548708\n",
      "Iteration 13875: loss = 0.0063757193,0.006699663\n",
      "Iteration 13880: loss = 0.0063136555,0.00042652347\n",
      "Iteration 13885: loss = 0.006270274,0.0037581893\n",
      "Iteration 13890: loss = 0.0062971623,0.016332861\n",
      "Iteration 13895: loss = 0.006255509,0.073580585\n",
      "Iteration 13900: loss = 0.006304279,0.011228771\n",
      "Iteration 13905: loss = 0.0063165165,0.028939767\n",
      "Iteration 13910: loss = 0.0063123615,0.005690278\n",
      "Iteration 13915: loss = 0.0063043423,0.0008618017\n",
      "Iteration 13920: loss = 0.0063006827,0.0050855875\n",
      "Iteration 13925: loss = 0.006302199,0.0029122445\n",
      "Iteration 13930: loss = 0.006306929,0.00017889444\n",
      "Iteration 13935: loss = 0.006310861,0.0007526097\n",
      "Iteration 13940: loss = 0.0063112304,0.00085099804\n",
      "Iteration 13945: loss = 0.0063094455,0.00015240771\n",
      "Iteration 13950: loss = 0.0063079023,0.00026644164\n",
      "Iteration 13955: loss = 0.006308045,0.00026289502\n",
      "Iteration 13960: loss = 0.006309466,0.000113757735\n",
      "Iteration 13965: loss = 0.0063102334,0.00017678339\n",
      "Iteration 13970: loss = 0.006309452,0.00011761428\n",
      "Iteration 13975: loss = 0.006308569,0.00013162987\n",
      "Iteration 13980: loss = 0.006308552,0.000115658695\n",
      "Iteration 13985: loss = 0.0063087144,0.00011789776\n",
      "Iteration 13990: loss = 0.0063082525,0.000111832676\n",
      "Iteration 13995: loss = 0.006307769,0.00011458602\n",
      "Iteration 14000: loss = 0.0063078064,0.0001110172\n",
      "Iteration 14005: loss = 0.006307903,0.000112138114\n",
      "Iteration 14010: loss = 0.0063077,0.00011163691\n",
      "Iteration 14015: loss = 0.0063078697,0.00011033743\n",
      "Iteration 14020: loss = 0.006307959,0.00011036822\n",
      "Iteration 14025: loss = 0.0063078306,0.00011046996\n",
      "Iteration 14030: loss = 0.006307891,0.00011000071\n",
      "Iteration 14035: loss = 0.0063077677,0.00010978765\n",
      "Iteration 14040: loss = 0.0063077076,0.000109580695\n",
      "Iteration 14045: loss = 0.006307718,0.00010939385\n",
      "Iteration 14050: loss = 0.006307656,0.00010937428\n",
      "Iteration 14055: loss = 0.006307798,0.00010971623\n",
      "Iteration 14060: loss = 0.006307438,0.000113599395\n",
      "Iteration 14065: loss = 0.006308218,0.00014056386\n",
      "Iteration 14070: loss = 0.0063058403,0.00038469012\n",
      "Iteration 14075: loss = 0.0063131065,0.0028516725\n",
      "Iteration 14080: loss = 0.0062896716,0.03013542\n",
      "Iteration 14085: loss = 0.0063400636,0.12652178\n",
      "Iteration 14090: loss = 0.006325897,0.015892304\n",
      "Iteration 14095: loss = 0.0063186046,0.00062937324\n",
      "Iteration 14100: loss = 0.0063109524,0.0059567993\n",
      "Iteration 14105: loss = 0.0063075814,0.008141775\n",
      "Iteration 14110: loss = 0.006305712,0.007090434\n",
      "Iteration 14115: loss = 0.0063046957,0.0048368014\n",
      "Iteration 14120: loss = 0.0063048154,0.0026590386\n",
      "Iteration 14125: loss = 0.006305832,0.0011024576\n",
      "Iteration 14130: loss = 0.0063063405,0.00031242217\n",
      "Iteration 14135: loss = 0.0063074995,0.00010832798\n",
      "Iteration 14140: loss = 0.0063081314,0.00018592746\n",
      "Iteration 14145: loss = 0.0063081984,0.00026592007\n",
      "Iteration 14150: loss = 0.006307876,0.00023825515\n",
      "Iteration 14155: loss = 0.0063073696,0.00015659262\n",
      "Iteration 14160: loss = 0.006306808,0.00011067782\n",
      "Iteration 14165: loss = 0.006306699,0.000114202354\n",
      "Iteration 14170: loss = 0.0063068685,0.00012296633\n",
      "Iteration 14175: loss = 0.0063072764,0.00011394265\n",
      "Iteration 14180: loss = 0.0063076187,0.00010628962\n",
      "Iteration 14185: loss = 0.0063076676,0.00010780967\n",
      "Iteration 14190: loss = 0.006307386,0.00010819903\n",
      "Iteration 14195: loss = 0.006307065,0.00010610356\n",
      "Iteration 14200: loss = 0.0063068937,0.00010650429\n",
      "Iteration 14205: loss = 0.006307043,0.00010604576\n",
      "Iteration 14210: loss = 0.006307265,0.00010520727\n",
      "Iteration 14215: loss = 0.0063072126,0.000105289226\n",
      "Iteration 14220: loss = 0.0063069896,0.00010507392\n",
      "Iteration 14225: loss = 0.0063069067,0.00010507544\n",
      "Iteration 14230: loss = 0.006307075,0.00010462834\n",
      "Iteration 14235: loss = 0.006307022,0.000104507635\n",
      "Iteration 14240: loss = 0.0063069104,0.00010444235\n",
      "Iteration 14245: loss = 0.006306862,0.000104297724\n",
      "Iteration 14250: loss = 0.0063069,0.0001040707\n",
      "Iteration 14255: loss = 0.0063068797,0.00010393147\n",
      "Iteration 14260: loss = 0.0063067316,0.00010394574\n",
      "Iteration 14265: loss = 0.0063069216,0.00010367372\n",
      "Iteration 14270: loss = 0.006306635,0.00010377535\n",
      "Iteration 14275: loss = 0.0063067735,0.0001033392\n",
      "Iteration 14280: loss = 0.0063067763,0.00010316746\n",
      "Iteration 14285: loss = 0.00630657,0.000103220635\n",
      "Iteration 14290: loss = 0.0063066855,0.00010291327\n",
      "Iteration 14295: loss = 0.0063065607,0.00010285604\n",
      "Iteration 14300: loss = 0.006306801,0.000102776634\n",
      "Iteration 14305: loss = 0.006306051,0.00010505701\n",
      "Iteration 14310: loss = 0.006307686,0.00011358312\n",
      "Iteration 14315: loss = 0.006303528,0.00019721032\n",
      "Iteration 14320: loss = 0.006314969,0.0009251577\n",
      "Iteration 14325: loss = 0.0062803295,0.0087876795\n",
      "Iteration 14330: loss = 0.006373313,0.073904306\n",
      "Iteration 14335: loss = 0.006330079,0.045171507\n",
      "Iteration 14340: loss = 0.0062784064,0.038773376\n",
      "Iteration 14345: loss = 0.006285827,0.012035281\n",
      "Iteration 14350: loss = 0.0063168705,0.0016464731\n",
      "Iteration 14355: loss = 0.006345075,0.0021641129\n",
      "Iteration 14360: loss = 0.0063476637,0.0033570693\n",
      "Iteration 14365: loss = 0.0063293297,0.0024013931\n",
      "Iteration 14370: loss = 0.0063078576,0.0013905532\n",
      "Iteration 14375: loss = 0.006295316,0.00081385084\n",
      "Iteration 14380: loss = 0.006294059,0.00036680832\n",
      "Iteration 14385: loss = 0.006299835,0.00020967986\n",
      "Iteration 14390: loss = 0.006306127,0.00025077973\n",
      "Iteration 14395: loss = 0.0063097402,0.0002183746\n",
      "Iteration 14400: loss = 0.0063105007,0.00013515231\n",
      "Iteration 14405: loss = 0.006309247,0.00011663276\n",
      "Iteration 14410: loss = 0.0063072816,0.00012177944\n",
      "Iteration 14415: loss = 0.006305618,0.00010809678\n",
      "Iteration 14420: loss = 0.006304664,0.00010487629\n",
      "Iteration 14425: loss = 0.006304575,0.000108478314\n",
      "Iteration 14430: loss = 0.006305083,0.000102819075\n",
      "Iteration 14435: loss = 0.0063057053,0.00010091937\n",
      "Iteration 14440: loss = 0.0063060396,0.00010051481\n",
      "Iteration 14445: loss = 0.0063060895,9.9254234e-05\n",
      "Iteration 14450: loss = 0.0063060876,9.954131e-05\n",
      "Iteration 14455: loss = 0.0063061547,9.888371e-05\n",
      "Iteration 14460: loss = 0.006306158,9.8861754e-05\n",
      "Iteration 14465: loss = 0.00630601,9.86753e-05\n",
      "Iteration 14470: loss = 0.006305918,9.856755e-05\n",
      "Iteration 14475: loss = 0.0063058813,9.842064e-05\n",
      "Iteration 14480: loss = 0.0063058343,9.831165e-05\n",
      "Iteration 14485: loss = 0.0063057803,9.820495e-05\n",
      "Iteration 14490: loss = 0.0063057356,9.807596e-05\n",
      "Iteration 14495: loss = 0.0063057095,9.7943295e-05\n",
      "Iteration 14500: loss = 0.0063056056,9.793922e-05\n",
      "Iteration 14505: loss = 0.0063056964,9.787278e-05\n",
      "Iteration 14510: loss = 0.0063055432,9.8052464e-05\n",
      "Iteration 14515: loss = 0.006305698,9.802077e-05\n",
      "Iteration 14520: loss = 0.0063054115,9.934795e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14525: loss = 0.006305806,0.00010217099\n",
      "Iteration 14530: loss = 0.006304937,0.00011838888\n",
      "Iteration 14535: loss = 0.0063067656,0.00020183017\n",
      "Iteration 14540: loss = 0.006302148,0.00080268265\n",
      "Iteration 14545: loss = 0.0063147894,0.005674605\n",
      "Iteration 14550: loss = 0.006280182,0.044165056\n",
      "Iteration 14555: loss = 0.0063334876,0.076822825\n",
      "Iteration 14560: loss = 0.0063206065,0.022833468\n",
      "Iteration 14565: loss = 0.0063062157,0.00012883956\n",
      "Iteration 14570: loss = 0.0062965364,0.0072457306\n",
      "Iteration 14575: loss = 0.0062959287,0.009053028\n",
      "Iteration 14580: loss = 0.006298376,0.004287465\n",
      "Iteration 14585: loss = 0.0063023474,0.0005766029\n",
      "Iteration 14590: loss = 0.0063059423,0.00028208492\n",
      "Iteration 14595: loss = 0.0063077006,0.0009826405\n",
      "Iteration 14600: loss = 0.006307166,0.0007512507\n",
      "Iteration 14605: loss = 0.0063058953,0.00018340867\n",
      "Iteration 14610: loss = 0.006304419,0.00013832015\n",
      "Iteration 14615: loss = 0.0063038603,0.0002470454\n",
      "Iteration 14620: loss = 0.006304462,0.00014491336\n",
      "Iteration 14625: loss = 0.0063053207,9.887619e-05\n",
      "Iteration 14630: loss = 0.0063057207,0.00012894173\n",
      "Iteration 14635: loss = 0.0063054617,0.00010193618\n",
      "Iteration 14640: loss = 0.006304968,0.00010010644\n",
      "Iteration 14645: loss = 0.006304896,0.000102204154\n",
      "Iteration 14650: loss = 0.0063051707,9.468948e-05\n",
      "Iteration 14655: loss = 0.006305294,9.735066e-05\n",
      "Iteration 14660: loss = 0.0063051176,9.43742e-05\n",
      "Iteration 14665: loss = 0.0063050017,9.537253e-05\n",
      "Iteration 14670: loss = 0.0063050613,9.409648e-05\n",
      "Iteration 14675: loss = 0.0063051004,9.433541e-05\n",
      "Iteration 14680: loss = 0.006304998,9.386566e-05\n",
      "Iteration 14685: loss = 0.0063049686,9.381358e-05\n",
      "Iteration 14690: loss = 0.006304968,9.3566094e-05\n",
      "Iteration 14695: loss = 0.006304939,9.343671e-05\n",
      "Iteration 14700: loss = 0.0063048466,9.3486866e-05\n",
      "Iteration 14705: loss = 0.006304951,9.33682e-05\n",
      "Iteration 14710: loss = 0.006304817,9.336695e-05\n",
      "Iteration 14715: loss = 0.006304906,9.3122166e-05\n",
      "Iteration 14720: loss = 0.0063047144,9.338715e-05\n",
      "Iteration 14725: loss = 0.0063049193,9.430098e-05\n",
      "Iteration 14730: loss = 0.006304475,9.9514786e-05\n",
      "Iteration 14735: loss = 0.006305251,0.00012218778\n",
      "Iteration 14740: loss = 0.0063034273,0.00025919208\n",
      "Iteration 14745: loss = 0.006308085,0.0012222022\n",
      "Iteration 14750: loss = 0.006295104,0.009277223\n",
      "Iteration 14755: loss = 0.0063291383,0.06189294\n",
      "Iteration 14760: loss = 0.00629068,0.04173506\n",
      "Iteration 14765: loss = 0.0062901615,0.036333576\n",
      "Iteration 14770: loss = 0.006297556,0.0047320547\n",
      "Iteration 14775: loss = 0.0063040196,0.0009071772\n",
      "Iteration 14780: loss = 0.0063089635,0.0051775365\n",
      "Iteration 14785: loss = 0.006308656,0.0048100976\n",
      "Iteration 14790: loss = 0.006305885,0.0016526026\n",
      "Iteration 14795: loss = 0.006302964,0.000118139076\n",
      "Iteration 14800: loss = 0.0063009565,0.0004974512\n",
      "Iteration 14805: loss = 0.006301355,0.00071681465\n",
      "Iteration 14810: loss = 0.0063035493,0.00028294575\n",
      "Iteration 14815: loss = 0.006305736,9.521027e-05\n",
      "Iteration 14820: loss = 0.006306696,0.00020117694\n",
      "Iteration 14825: loss = 0.006305989,0.00015099451\n",
      "Iteration 14830: loss = 0.0063043684,9.118575e-05\n",
      "Iteration 14835: loss = 0.006303487,0.000120046\n",
      "Iteration 14840: loss = 0.0063037737,9.946634e-05\n",
      "Iteration 14845: loss = 0.006304542,9.369571e-05\n",
      "Iteration 14850: loss = 0.006304786,9.605788e-05\n",
      "Iteration 14855: loss = 0.006304472,9.013229e-05\n",
      "Iteration 14860: loss = 0.00630419,9.2794e-05\n",
      "Iteration 14865: loss = 0.0063042585,8.987448e-05\n",
      "Iteration 14870: loss = 0.0063043744,9.0712405e-05\n",
      "Iteration 14875: loss = 0.0063041816,8.9722824e-05\n",
      "Iteration 14880: loss = 0.006304176,8.986828e-05\n",
      "Iteration 14885: loss = 0.006304288,8.9400826e-05\n",
      "Iteration 14890: loss = 0.0063041802,8.917981e-05\n",
      "Iteration 14895: loss = 0.0063040555,8.9299705e-05\n",
      "Iteration 14900: loss = 0.006304223,8.9037814e-05\n",
      "Iteration 14905: loss = 0.0063040624,8.890823e-05\n",
      "Iteration 14910: loss = 0.0063039963,8.875037e-05\n",
      "Iteration 14915: loss = 0.0063040666,8.8614135e-05\n",
      "Iteration 14920: loss = 0.0063039865,8.863429e-05\n",
      "Iteration 14925: loss = 0.0063040718,8.8734625e-05\n",
      "Iteration 14930: loss = 0.0063038114,8.950503e-05\n",
      "Iteration 14935: loss = 0.006304173,9.184047e-05\n",
      "Iteration 14940: loss = 0.006303284,0.00010909594\n",
      "Iteration 14945: loss = 0.0063054976,0.00021852776\n",
      "Iteration 14950: loss = 0.006299269,0.0011389116\n",
      "Iteration 14955: loss = 0.00631747,0.009569674\n",
      "Iteration 14960: loss = 0.006269261,0.07304383\n",
      "Iteration 14965: loss = 0.006306827,0.03492627\n",
      "Iteration 14970: loss = 0.0063377228,0.041766744\n",
      "Iteration 14975: loss = 0.0063455408,0.014715414\n",
      "Iteration 14980: loss = 0.006334093,0.0022545583\n",
      "Iteration 14985: loss = 0.0063160867,0.00070476095\n",
      "Iteration 14990: loss = 0.0062974184,0.0019468262\n",
      "Iteration 14995: loss = 0.006285111,0.0024616562\n",
      "Iteration 15000: loss = 0.0062816404,0.0018573701\n",
      "Iteration 15005: loss = 0.006285341,0.0008849873\n",
      "Iteration 15010: loss = 0.00629443,0.00023899175\n",
      "Iteration 15015: loss = 0.00630419,0.00010685155\n",
      "Iteration 15020: loss = 0.0063107014,0.00020718825\n",
      "Iteration 15025: loss = 0.0063119843,0.00022183379\n",
      "Iteration 15030: loss = 0.006308731,0.00013219117\n",
      "Iteration 15035: loss = 0.006303767,8.685076e-05\n",
      "Iteration 15040: loss = 0.0063007637,0.00010652348\n",
      "Iteration 15045: loss = 0.006300921,0.00010766867\n",
      "Iteration 15050: loss = 0.006303003,8.936347e-05\n",
      "Iteration 15055: loss = 0.00630464,8.689495e-05\n",
      "Iteration 15060: loss = 0.006304732,8.920046e-05\n",
      "Iteration 15065: loss = 0.006303863,8.611183e-05\n",
      "Iteration 15070: loss = 0.0063031423,8.6840024e-05\n",
      "Iteration 15075: loss = 0.0063030007,8.71907e-05\n",
      "Iteration 15080: loss = 0.00630348,8.548137e-05\n",
      "Iteration 15085: loss = 0.0063038985,8.534719e-05\n",
      "Iteration 15090: loss = 0.0063035437,8.5042164e-05\n",
      "Iteration 15095: loss = 0.0063031153,8.548479e-05\n",
      "Iteration 15100: loss = 0.006303419,8.489078e-05\n",
      "Iteration 15105: loss = 0.0063036145,8.460172e-05\n",
      "Iteration 15110: loss = 0.006303165,8.4869724e-05\n",
      "Iteration 15115: loss = 0.006303342,8.453839e-05\n",
      "Iteration 15120: loss = 0.006303256,8.4454936e-05\n",
      "Iteration 15125: loss = 0.0063033397,8.421971e-05\n",
      "Iteration 15130: loss = 0.006303052,8.441185e-05\n",
      "Iteration 15135: loss = 0.006303419,8.4080886e-05\n",
      "Iteration 15140: loss = 0.0063030277,8.44099e-05\n",
      "Iteration 15145: loss = 0.0063029714,8.468549e-05\n",
      "Iteration 15150: loss = 0.0063039404,8.739375e-05\n",
      "Iteration 15155: loss = 0.00630006,0.00012521047\n",
      "Iteration 15160: loss = 0.0063137487,0.0004797495\n",
      "Iteration 15165: loss = 0.00626638,0.0047812914\n",
      "Iteration 15170: loss = 0.006362615,0.030081298\n",
      "Iteration 15175: loss = 0.0063970014,0.077528656\n",
      "Iteration 15180: loss = 0.006351881,0.0035041275\n",
      "Iteration 15185: loss = 0.006336546,0.021132668\n",
      "Iteration 15190: loss = 0.0063315425,0.012439824\n",
      "Iteration 15195: loss = 0.0063315723,0.0015713675\n",
      "Iteration 15200: loss = 0.006325927,0.0024981073\n",
      "Iteration 15205: loss = 0.00631501,0.0028195463\n",
      "Iteration 15210: loss = 0.006303117,0.0005333868\n",
      "Iteration 15215: loss = 0.006294931,0.00033944845\n",
      "Iteration 15220: loss = 0.0062923934,0.0007731083\n",
      "Iteration 15225: loss = 0.006295221,0.0002520021\n",
      "Iteration 15230: loss = 0.006299043,0.0001919393\n",
      "Iteration 15235: loss = 0.0063011055,0.00022549469\n",
      "Iteration 15240: loss = 0.0063012596,8.753872e-05\n",
      "Iteration 15245: loss = 0.00630136,0.00013293818\n",
      "Iteration 15250: loss = 0.0063025113,8.709864e-05\n",
      "Iteration 15255: loss = 0.0063035614,9.7589014e-05\n",
      "Iteration 15260: loss = 0.006303368,8.400216e-05\n",
      "Iteration 15265: loss = 0.0063028224,8.900161e-05\n",
      "Iteration 15270: loss = 0.0063030445,8.152314e-05\n",
      "Iteration 15275: loss = 0.006303201,8.410851e-05\n",
      "Iteration 15280: loss = 0.0063028242,8.141554e-05\n",
      "Iteration 15285: loss = 0.0063027125,8.165777e-05\n",
      "Iteration 15290: loss = 0.006302856,8.194131e-05\n",
      "Iteration 15295: loss = 0.0063025863,8.117774e-05\n",
      "Iteration 15300: loss = 0.0063025723,8.070274e-05\n",
      "Iteration 15305: loss = 0.0063025947,8.0827725e-05\n",
      "Iteration 15310: loss = 0.0063024,8.112397e-05\n",
      "Iteration 15315: loss = 0.0063025225,8.0931306e-05\n",
      "Iteration 15320: loss = 0.006302299,8.104187e-05\n",
      "Iteration 15325: loss = 0.006302517,8.15745e-05\n",
      "Iteration 15330: loss = 0.006302064,8.6136824e-05\n",
      "Iteration 15335: loss = 0.006302954,0.00010801798\n",
      "Iteration 15340: loss = 0.0063006408,0.00027128996\n",
      "Iteration 15345: loss = 0.006306879,0.0015967514\n",
      "Iteration 15350: loss = 0.006288555,0.013920873\n",
      "Iteration 15355: loss = 0.006335497,0.08617369\n",
      "Iteration 15360: loss = 0.0062973727,0.009502195\n",
      "Iteration 15365: loss = 0.0062906984,0.034584433\n",
      "Iteration 15370: loss = 0.006292829,0.019640643\n",
      "Iteration 15375: loss = 0.0062986608,0.004611202\n",
      "Iteration 15380: loss = 0.006304579,0.00012364943\n",
      "Iteration 15385: loss = 0.006307564,0.0011068408\n",
      "Iteration 15390: loss = 0.0063081994,0.002189404\n",
      "Iteration 15395: loss = 0.00630758,0.0016930129\n",
      "Iteration 15400: loss = 0.006305232,0.0005908972\n",
      "Iteration 15405: loss = 0.0063032354,8.600266e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15410: loss = 0.006301679,0.00020538906\n",
      "Iteration 15415: loss = 0.006301128,0.00029328614\n",
      "Iteration 15420: loss = 0.006301629,0.00015812174\n",
      "Iteration 15425: loss = 0.0063025127,7.8664845e-05\n",
      "Iteration 15430: loss = 0.006302958,0.000109775654\n",
      "Iteration 15435: loss = 0.0063028694,0.000102481004\n",
      "Iteration 15440: loss = 0.0063023698,7.8552934e-05\n",
      "Iteration 15445: loss = 0.006301925,8.629449e-05\n",
      "Iteration 15450: loss = 0.0063019576,8.286767e-05\n",
      "Iteration 15455: loss = 0.0063021667,7.828195e-05\n",
      "Iteration 15460: loss = 0.0063022505,8.046784e-05\n",
      "Iteration 15465: loss = 0.006302038,7.7818724e-05\n",
      "Iteration 15470: loss = 0.006301895,7.877016e-05\n",
      "Iteration 15475: loss = 0.006301938,7.767474e-05\n",
      "Iteration 15480: loss = 0.0063019954,7.774247e-05\n",
      "Iteration 15485: loss = 0.006301937,7.7362274e-05\n",
      "Iteration 15490: loss = 0.00630188,7.734858e-05\n",
      "Iteration 15495: loss = 0.006301846,7.716115e-05\n",
      "Iteration 15500: loss = 0.006301844,7.707813e-05\n",
      "Iteration 15505: loss = 0.006301802,7.693017e-05\n",
      "Iteration 15510: loss = 0.006301751,7.6891614e-05\n",
      "Iteration 15515: loss = 0.006301832,7.6821314e-05\n",
      "Iteration 15520: loss = 0.006301707,7.683684e-05\n",
      "Iteration 15525: loss = 0.0063017528,7.659847e-05\n",
      "Iteration 15530: loss = 0.0063016494,7.6479475e-05\n",
      "Iteration 15535: loss = 0.006301615,7.632845e-05\n",
      "Iteration 15540: loss = 0.006301612,7.619231e-05\n",
      "Iteration 15545: loss = 0.006301585,7.6091485e-05\n",
      "Iteration 15550: loss = 0.006301572,7.598208e-05\n",
      "Iteration 15555: loss = 0.0063014254,7.6251126e-05\n",
      "Iteration 15560: loss = 0.006301665,7.787184e-05\n",
      "Iteration 15565: loss = 0.0063010626,9.59009e-05\n",
      "Iteration 15570: loss = 0.0063029346,0.00028282264\n",
      "Iteration 15575: loss = 0.0062960745,0.0027130432\n",
      "Iteration 15580: loss = 0.0063208747,0.034834977\n",
      "Iteration 15585: loss = 0.0062686675,0.14936088\n",
      "Iteration 15590: loss = 0.0062762685,0.045030676\n",
      "Iteration 15595: loss = 0.0062840856,0.010475736\n",
      "Iteration 15600: loss = 0.006285832,0.0027744796\n",
      "Iteration 15605: loss = 0.006290467,0.00092137844\n",
      "Iteration 15610: loss = 0.006292908,0.00036811276\n",
      "Iteration 15615: loss = 0.006294759,0.00017373676\n",
      "Iteration 15620: loss = 0.006297142,0.00010855094\n",
      "Iteration 15625: loss = 0.0062992373,0.00010783602\n",
      "Iteration 15630: loss = 0.006300662,0.00013821275\n",
      "Iteration 15635: loss = 0.0063022054,0.00017557679\n",
      "Iteration 15640: loss = 0.006303012,0.00020267896\n",
      "Iteration 15645: loss = 0.0063035465,0.00020891687\n",
      "Iteration 15650: loss = 0.0063038156,0.00019164561\n",
      "Iteration 15655: loss = 0.006303446,0.00015810606\n",
      "Iteration 15660: loss = 0.006302941,0.00012085233\n",
      "Iteration 15665: loss = 0.006302307,9.264821e-05\n",
      "Iteration 15670: loss = 0.0063016415,7.7585195e-05\n",
      "Iteration 15675: loss = 0.0063011926,7.478618e-05\n",
      "Iteration 15680: loss = 0.0063009956,7.7323464e-05\n",
      "Iteration 15685: loss = 0.006301033,7.817469e-05\n",
      "Iteration 15690: loss = 0.0063012033,7.6340366e-05\n",
      "Iteration 15695: loss = 0.0063013323,7.4310374e-05\n",
      "Iteration 15700: loss = 0.0063014054,7.3677336e-05\n",
      "Iteration 15705: loss = 0.00630136,7.394149e-05\n",
      "Iteration 15710: loss = 0.0063012675,7.380638e-05\n",
      "Iteration 15715: loss = 0.006301208,7.341863e-05\n",
      "Iteration 15720: loss = 0.0063011493,7.3377574e-05\n",
      "Iteration 15725: loss = 0.006301151,7.323153e-05\n",
      "Iteration 15730: loss = 0.0063010952,7.308595e-05\n",
      "Iteration 15735: loss = 0.006301074,7.297687e-05\n",
      "Iteration 15740: loss = 0.006301077,7.284756e-05\n",
      "Iteration 15745: loss = 0.006301019,7.275729e-05\n",
      "Iteration 15750: loss = 0.0063009677,7.2673734e-05\n",
      "Iteration 15755: loss = 0.006300948,7.257202e-05\n",
      "Iteration 15760: loss = 0.006301007,7.242987e-05\n",
      "Iteration 15765: loss = 0.0063008033,7.247631e-05\n",
      "Iteration 15770: loss = 0.0063008647,7.2279436e-05\n",
      "Iteration 15775: loss = 0.006300846,7.2143484e-05\n",
      "Iteration 15780: loss = 0.0063008065,7.207229e-05\n",
      "Iteration 15785: loss = 0.006300835,7.1908056e-05\n",
      "Iteration 15790: loss = 0.006300693,7.19098e-05\n",
      "Iteration 15795: loss = 0.0063007507,7.173445e-05\n",
      "Iteration 15800: loss = 0.006300708,7.166275e-05\n",
      "Iteration 15805: loss = 0.00630053,7.190904e-05\n",
      "Iteration 15810: loss = 0.0063009155,7.1612274e-05\n",
      "Iteration 15815: loss = 0.006299987,7.3019735e-05\n",
      "Iteration 15820: loss = 0.006301978,7.329476e-05\n",
      "Iteration 15825: loss = 0.0062970207,9.2807546e-05\n",
      "Iteration 15830: loss = 0.0063106245,0.0002038233\n",
      "Iteration 15835: loss = 0.006269595,0.0014488635\n",
      "Iteration 15840: loss = 0.0063760546,0.007899293\n",
      "Iteration 15845: loss = 0.0062850504,0.0011081763\n",
      "Iteration 15850: loss = 0.0062722904,0.006955551\n",
      "Iteration 15855: loss = 0.006239205,0.027903266\n",
      "Iteration 15860: loss = 0.0063062552,0.057662595\n",
      "Iteration 15865: loss = 0.006282875,0.0007133601\n",
      "Iteration 15870: loss = 0.0062684366,0.016304154\n",
      "Iteration 15875: loss = 0.006277939,0.0055297813\n",
      "Iteration 15880: loss = 0.006294372,0.0010137465\n",
      "Iteration 15885: loss = 0.0063006277,0.003948091\n",
      "Iteration 15890: loss = 0.0062968656,0.00059491093\n",
      "Iteration 15895: loss = 0.0062922766,0.0007648833\n",
      "Iteration 15900: loss = 0.006293399,0.0007913458\n",
      "Iteration 15905: loss = 0.006298324,0.000102387\n",
      "Iteration 15910: loss = 0.006300939,0.00039350888\n",
      "Iteration 15915: loss = 0.0062994515,7.4419004e-05\n",
      "Iteration 15920: loss = 0.0062983376,0.00019056018\n",
      "Iteration 15925: loss = 0.006300105,7.074621e-05\n",
      "Iteration 15930: loss = 0.0063013732,0.00011614474\n",
      "Iteration 15935: loss = 0.0063005206,7.090929e-05\n",
      "Iteration 15940: loss = 0.0063002612,8.425336e-05\n",
      "Iteration 15945: loss = 0.0063010897,7.739585e-05\n",
      "Iteration 15950: loss = 0.00630065,6.900566e-05\n",
      "Iteration 15955: loss = 0.006300165,7.352135e-05\n",
      "Iteration 15960: loss = 0.0063004442,7.165603e-05\n",
      "Iteration 15965: loss = 0.006299963,6.957851e-05\n",
      "Iteration 15970: loss = 0.006299975,6.885815e-05\n",
      "Iteration 15975: loss = 0.006300013,6.896377e-05\n",
      "Iteration 15980: loss = 0.0062998333,6.9612375e-05\n",
      "Iteration 15985: loss = 0.0063002296,7.080506e-05\n",
      "Iteration 15990: loss = 0.006299686,7.692825e-05\n",
      "Iteration 15995: loss = 0.0063008573,0.000105817424\n",
      "Iteration 16000: loss = 0.006298035,0.0002902368\n",
      "Iteration 16005: loss = 0.006305022,0.0016100613\n",
      "Iteration 16010: loss = 0.0062854574,0.012660753\n",
      "Iteration 16015: loss = 0.006333439,0.07356551\n",
      "Iteration 16020: loss = 0.0062938263,0.015884468\n",
      "Iteration 16025: loss = 0.006287255,0.03462768\n",
      "Iteration 16030: loss = 0.00628981,0.011213574\n",
      "Iteration 16035: loss = 0.0062962533,0.00026770015\n",
      "Iteration 16040: loss = 0.0063014887,0.0020708996\n",
      "Iteration 16045: loss = 0.006302845,0.003836898\n",
      "Iteration 16050: loss = 0.006301412,0.0021861494\n",
      "Iteration 16055: loss = 0.006298981,0.0003247707\n",
      "Iteration 16060: loss = 0.006296433,0.00020002511\n",
      "Iteration 16065: loss = 0.0062962943,0.0005491047\n",
      "Iteration 16070: loss = 0.0062978044,0.00031588195\n",
      "Iteration 16075: loss = 0.006300086,6.946354e-05\n",
      "Iteration 16080: loss = 0.006301675,0.00014041003\n",
      "Iteration 16085: loss = 0.0063015614,0.00013140285\n",
      "Iteration 16090: loss = 0.0063004554,6.7123845e-05\n",
      "Iteration 16095: loss = 0.006299484,8.913563e-05\n",
      "Iteration 16100: loss = 0.006299388,7.62018e-05\n",
      "Iteration 16105: loss = 0.0062997714,6.89927e-05\n",
      "Iteration 16110: loss = 0.006299937,7.255546e-05\n",
      "Iteration 16115: loss = 0.0062997104,6.662468e-05\n",
      "Iteration 16120: loss = 0.0062996694,6.861822e-05\n",
      "Iteration 16125: loss = 0.0062998827,6.616014e-05\n",
      "Iteration 16130: loss = 0.0062997993,6.6695655e-05\n",
      "Iteration 16135: loss = 0.0062995832,6.619474e-05\n",
      "Iteration 16140: loss = 0.0062995255,6.622076e-05\n",
      "Iteration 16145: loss = 0.0062995777,6.588213e-05\n",
      "Iteration 16150: loss = 0.006299641,6.575437e-05\n",
      "Iteration 16155: loss = 0.0062995944,6.5644876e-05\n",
      "Iteration 16160: loss = 0.0062994882,6.5607645e-05\n",
      "Iteration 16165: loss = 0.0062994664,6.552602e-05\n",
      "Iteration 16170: loss = 0.006299337,6.570673e-05\n",
      "Iteration 16175: loss = 0.00629952,6.564507e-05\n",
      "Iteration 16180: loss = 0.006299317,6.594806e-05\n",
      "Iteration 16185: loss = 0.0062994896,6.662264e-05\n",
      "Iteration 16190: loss = 0.006299069,7.025454e-05\n",
      "Iteration 16195: loss = 0.0062999036,8.628468e-05\n",
      "Iteration 16200: loss = 0.0062978487,0.0001897102\n",
      "Iteration 16205: loss = 0.0063029043,0.0009175617\n",
      "Iteration 16210: loss = 0.006288916,0.0071437936\n",
      "Iteration 16215: loss = 0.006326528,0.052178957\n",
      "Iteration 16220: loss = 0.0062829615,0.057303518\n",
      "Iteration 16225: loss = 0.0062721875,0.03244417\n",
      "Iteration 16230: loss = 0.006277529,0.0025404342\n",
      "Iteration 16235: loss = 0.0062929946,0.002166259\n",
      "Iteration 16240: loss = 0.006310373,0.0058549694\n",
      "Iteration 16245: loss = 0.006318727,0.0048147375\n",
      "Iteration 16250: loss = 0.006316444,0.0017611824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16255: loss = 0.0063067847,0.00015819399\n",
      "Iteration 16260: loss = 0.00629515,0.0003277355\n",
      "Iteration 16265: loss = 0.0062898323,0.00065567624\n",
      "Iteration 16270: loss = 0.0062920894,0.0003692078\n",
      "Iteration 16275: loss = 0.0062984093,7.619997e-05\n",
      "Iteration 16280: loss = 0.006302914,0.00011848319\n",
      "Iteration 16285: loss = 0.0063025686,0.00014378969\n",
      "Iteration 16290: loss = 0.006299741,7.522515e-05\n",
      "Iteration 16295: loss = 0.0062980033,7.444437e-05\n",
      "Iteration 16300: loss = 0.0062982403,8.269302e-05\n",
      "Iteration 16305: loss = 0.0062990743,6.434969e-05\n",
      "Iteration 16310: loss = 0.00629961,6.775179e-05\n",
      "Iteration 16315: loss = 0.006299664,6.513402e-05\n",
      "Iteration 16320: loss = 0.006298997,6.397238e-05\n",
      "Iteration 16325: loss = 0.0062985686,6.507736e-05\n",
      "Iteration 16330: loss = 0.006299181,6.296725e-05\n",
      "Iteration 16335: loss = 0.0062993485,6.305039e-05\n",
      "Iteration 16340: loss = 0.0062987874,6.310976e-05\n",
      "Iteration 16345: loss = 0.00629894,6.2805186e-05\n",
      "Iteration 16350: loss = 0.0062990277,6.2660285e-05\n",
      "Iteration 16355: loss = 0.00629885,6.257642e-05\n",
      "Iteration 16360: loss = 0.006298886,6.2394116e-05\n",
      "Iteration 16365: loss = 0.00629887,6.228617e-05\n",
      "Iteration 16370: loss = 0.006298783,6.224868e-05\n",
      "Iteration 16375: loss = 0.006298777,6.214991e-05\n",
      "Iteration 16380: loss = 0.0062987194,6.207712e-05\n",
      "Iteration 16385: loss = 0.0062988563,6.185307e-05\n",
      "Iteration 16390: loss = 0.006298244,6.2615996e-05\n",
      "Iteration 16395: loss = 0.0063002333,6.367865e-05\n",
      "Iteration 16400: loss = 0.0062931315,0.00011181744\n",
      "Iteration 16405: loss = 0.006319914,0.00073058455\n",
      "Iteration 16410: loss = 0.006224289,0.009056622\n",
      "Iteration 16415: loss = 0.0063245646,0.0055210306\n",
      "Iteration 16420: loss = 0.006369818,0.027250059\n",
      "Iteration 16425: loss = 0.0063042063,0.06165534\n",
      "Iteration 16430: loss = 0.0063212686,0.00082671584\n",
      "Iteration 16435: loss = 0.0063224887,0.015733296\n",
      "Iteration 16440: loss = 0.0063006985,0.005807827\n",
      "Iteration 16445: loss = 0.006281162,0.00086659065\n",
      "Iteration 16450: loss = 0.006275453,0.0040722224\n",
      "Iteration 16455: loss = 0.0062837005,0.0010548215\n",
      "Iteration 16460: loss = 0.006295621,0.00047487702\n",
      "Iteration 16465: loss = 0.0063012526,0.0008758319\n",
      "Iteration 16470: loss = 0.0063003413,6.802171e-05\n",
      "Iteration 16475: loss = 0.006299013,0.00035409676\n",
      "Iteration 16480: loss = 0.006300565,0.00011589931\n",
      "Iteration 16485: loss = 0.0063022617,0.00014782062\n",
      "Iteration 16490: loss = 0.0063011213,8.689043e-05\n",
      "Iteration 16495: loss = 0.006298986,9.584981e-05\n",
      "Iteration 16500: loss = 0.006298955,6.58012e-05\n",
      "Iteration 16505: loss = 0.006299507,7.854603e-05\n",
      "Iteration 16510: loss = 0.00629857,6.0443388e-05\n",
      "Iteration 16515: loss = 0.0062981597,6.640857e-05\n",
      "Iteration 16520: loss = 0.0062986575,6.380416e-05\n",
      "Iteration 16525: loss = 0.006298266,6.0178012e-05\n",
      "Iteration 16530: loss = 0.006298173,6.0727532e-05\n",
      "Iteration 16535: loss = 0.0062984074,6.1121835e-05\n",
      "Iteration 16540: loss = 0.0062980764,6.131585e-05\n",
      "Iteration 16545: loss = 0.0062983762,6.0734328e-05\n",
      "Iteration 16550: loss = 0.00629803,6.132894e-05\n",
      "Iteration 16555: loss = 0.0062984154,6.2415565e-05\n",
      "Iteration 16560: loss = 0.0062976703,7.174206e-05\n",
      "Iteration 16565: loss = 0.0062991437,0.000113738075\n",
      "Iteration 16570: loss = 0.00629559,0.00039231198\n",
      "Iteration 16575: loss = 0.006304868,0.002488032\n",
      "Iteration 16580: loss = 0.0062789544,0.020109178\n",
      "Iteration 16585: loss = 0.006335681,0.08729125\n",
      "Iteration 16590: loss = 0.006301753,0.00014864563\n",
      "Iteration 16595: loss = 0.0062888935,0.020180555\n",
      "Iteration 16600: loss = 0.006287502,0.017327467\n",
      "Iteration 16605: loss = 0.006293818,0.0047114855\n",
      "Iteration 16610: loss = 0.0062993006,8.010332e-05\n",
      "Iteration 16615: loss = 0.0063025267,0.0013398645\n",
      "Iteration 16620: loss = 0.006303405,0.0021393767\n",
      "Iteration 16625: loss = 0.0063016764,0.0010468368\n",
      "Iteration 16630: loss = 0.0062988703,0.00011016043\n",
      "Iteration 16635: loss = 0.0062969644,0.00020181\n",
      "Iteration 16640: loss = 0.0062964405,0.00034643494\n",
      "Iteration 16645: loss = 0.0062971427,0.00013882061\n",
      "Iteration 16650: loss = 0.0062984317,6.5157226e-05\n",
      "Iteration 16655: loss = 0.0062988945,0.00011770788\n",
      "Iteration 16660: loss = 0.00629854,7.531103e-05\n",
      "Iteration 16665: loss = 0.0062978957,6.2992985e-05\n",
      "Iteration 16670: loss = 0.0062976587,7.3927265e-05\n",
      "Iteration 16675: loss = 0.0062980014,5.8105634e-05\n",
      "Iteration 16680: loss = 0.0062982664,6.3317326e-05\n",
      "Iteration 16685: loss = 0.0062981043,5.8568206e-05\n",
      "Iteration 16690: loss = 0.0062978268,5.954496e-05\n",
      "Iteration 16695: loss = 0.00629784,5.800755e-05\n",
      "Iteration 16700: loss = 0.006297978,5.8263664e-05\n",
      "Iteration 16705: loss = 0.006297882,5.7322548e-05\n",
      "Iteration 16710: loss = 0.0062977304,5.781216e-05\n",
      "Iteration 16715: loss = 0.0062977984,5.718593e-05\n",
      "Iteration 16720: loss = 0.0062977145,5.7104808e-05\n",
      "Iteration 16725: loss = 0.0062976237,5.7213263e-05\n",
      "Iteration 16730: loss = 0.006297693,5.7053294e-05\n",
      "Iteration 16735: loss = 0.006297639,5.68877e-05\n",
      "Iteration 16740: loss = 0.0062976144,5.6781108e-05\n",
      "Iteration 16745: loss = 0.0062976056,5.6723307e-05\n",
      "Iteration 16750: loss = 0.006297555,5.66502e-05\n",
      "Iteration 16755: loss = 0.0062975623,5.649181e-05\n",
      "Iteration 16760: loss = 0.0062975194,5.6410274e-05\n",
      "Iteration 16765: loss = 0.00629738,5.650444e-05\n",
      "Iteration 16770: loss = 0.0062975255,5.6994802e-05\n",
      "Iteration 16775: loss = 0.0062971897,6.231613e-05\n",
      "Iteration 16780: loss = 0.0062981136,0.000104449326\n",
      "Iteration 16785: loss = 0.0062950156,0.0005617463\n",
      "Iteration 16790: loss = 0.0063053784,0.006067901\n",
      "Iteration 16795: loss = 0.0062717614,0.06768657\n",
      "Iteration 16800: loss = 0.0063162525,0.06315499\n",
      "Iteration 16805: loss = 0.006318241,0.05669747\n",
      "Iteration 16810: loss = 0.0063136593,0.02858725\n",
      "Iteration 16815: loss = 0.0063089356,0.012890357\n",
      "Iteration 16820: loss = 0.0063061626,0.0055328305\n",
      "Iteration 16825: loss = 0.006302657,0.0021693865\n",
      "Iteration 16830: loss = 0.0062998994,0.0007102189\n",
      "Iteration 16835: loss = 0.0062985676,0.00016749912\n",
      "Iteration 16840: loss = 0.006296609,5.8370963e-05\n",
      "Iteration 16845: loss = 0.006296014,0.00011943372\n",
      "Iteration 16850: loss = 0.006295763,0.00020730756\n",
      "Iteration 16855: loss = 0.0062957318,0.00024856188\n",
      "Iteration 16860: loss = 0.006296364,0.00022450555\n",
      "Iteration 16865: loss = 0.006296989,0.00016180165\n",
      "Iteration 16870: loss = 0.0062976647,9.958388e-05\n",
      "Iteration 16875: loss = 0.006298195,6.314422e-05\n",
      "Iteration 16880: loss = 0.006298471,5.566574e-05\n",
      "Iteration 16885: loss = 0.006298344,6.122303e-05\n",
      "Iteration 16890: loss = 0.0062978934,6.257527e-05\n",
      "Iteration 16895: loss = 0.006297376,5.8715283e-05\n",
      "Iteration 16900: loss = 0.006297006,5.564798e-05\n",
      "Iteration 16905: loss = 0.006296962,5.56679e-05\n",
      "Iteration 16910: loss = 0.0062971525,5.5772114e-05\n",
      "Iteration 16915: loss = 0.0062974263,5.4666263e-05\n",
      "Iteration 16920: loss = 0.006297519,5.426978e-05\n",
      "Iteration 16925: loss = 0.006297384,5.441941e-05\n",
      "Iteration 16930: loss = 0.0062971017,5.439544e-05\n",
      "Iteration 16935: loss = 0.0062969537,5.4463657e-05\n",
      "Iteration 16940: loss = 0.0062971027,5.411479e-05\n",
      "Iteration 16945: loss = 0.0062971823,5.3930067e-05\n",
      "Iteration 16950: loss = 0.006297108,5.3897325e-05\n",
      "Iteration 16955: loss = 0.0062969867,5.3887463e-05\n",
      "Iteration 16960: loss = 0.006296959,5.3814307e-05\n",
      "Iteration 16965: loss = 0.0062969755,5.3679567e-05\n",
      "Iteration 16970: loss = 0.006296952,5.360813e-05\n",
      "Iteration 16975: loss = 0.0062968708,5.35749e-05\n",
      "Iteration 16980: loss = 0.006296858,5.348396e-05\n",
      "Iteration 16985: loss = 0.006296961,5.331856e-05\n",
      "Iteration 16990: loss = 0.0062966384,5.3605778e-05\n",
      "Iteration 16995: loss = 0.0062968857,5.3213444e-05\n",
      "Iteration 17000: loss = 0.006296776,5.3120253e-05\n",
      "Iteration 17005: loss = 0.006296608,5.3216114e-05\n",
      "Iteration 17010: loss = 0.0062967488,5.2944437e-05\n",
      "Iteration 17015: loss = 0.0062966966,5.2891843e-05\n",
      "Iteration 17020: loss = 0.0062965653,5.292214e-05\n",
      "Iteration 17025: loss = 0.006296752,5.266013e-05\n",
      "Iteration 17030: loss = 0.006296242,5.3375094e-05\n",
      "Iteration 17035: loss = 0.0062972964,5.3532836e-05\n",
      "Iteration 17040: loss = 0.0062947813,6.334688e-05\n",
      "Iteration 17045: loss = 0.0063011325,0.00011178809\n",
      "Iteration 17050: loss = 0.0062830863,0.0006148611\n",
      "Iteration 17055: loss = 0.0063375323,0.0053955014\n",
      "Iteration 17060: loss = 0.006202636,0.043571267\n",
      "Iteration 17065: loss = 0.006274861,0.078210555\n",
      "Iteration 17070: loss = 0.006278387,0.012024339\n",
      "Iteration 17075: loss = 0.0062338207,0.0065844236\n",
      "Iteration 17080: loss = 0.006228071,0.013121443\n",
      "Iteration 17085: loss = 0.0062697926,0.0069976687\n",
      "Iteration 17090: loss = 0.0063116383,0.0023164153\n",
      "Iteration 17095: loss = 0.006321484,0.0008767746\n",
      "Iteration 17100: loss = 0.0063066487,0.00079384807\n",
      "Iteration 17105: loss = 0.0062908777,0.0011124243\n",
      "Iteration 17110: loss = 0.006288942,0.00046286627\n",
      "Iteration 17115: loss = 0.0062953266,6.248786e-05\n",
      "Iteration 17120: loss = 0.0062999427,0.00023943488\n",
      "Iteration 17125: loss = 0.006300099,0.0001742133\n",
      "Iteration 17130: loss = 0.0062977783,5.2445437e-05\n",
      "Iteration 17135: loss = 0.0062956703,9.151036e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17140: loss = 0.0062949993,7.755931e-05\n",
      "Iteration 17145: loss = 0.0062954314,5.616194e-05\n",
      "Iteration 17150: loss = 0.006296472,6.15849e-05\n",
      "Iteration 17155: loss = 0.0062975246,5.208317e-05\n",
      "Iteration 17160: loss = 0.006297691,5.4586566e-05\n",
      "Iteration 17165: loss = 0.006296847,5.1259856e-05\n",
      "Iteration 17170: loss = 0.0062961676,5.2604548e-05\n",
      "Iteration 17175: loss = 0.0062960517,5.1847346e-05\n",
      "Iteration 17180: loss = 0.006296227,5.1900264e-05\n",
      "Iteration 17185: loss = 0.006296204,5.1271258e-05\n",
      "Iteration 17190: loss = 0.0062962645,5.1150968e-05\n",
      "Iteration 17195: loss = 0.0062964465,5.0906972e-05\n",
      "Iteration 17200: loss = 0.0062963576,5.0804185e-05\n",
      "Iteration 17205: loss = 0.006296335,5.0671413e-05\n",
      "Iteration 17210: loss = 0.0062963194,5.060703e-05\n",
      "Iteration 17215: loss = 0.0062962095,5.0593313e-05\n",
      "Iteration 17220: loss = 0.0062961853,5.0497343e-05\n",
      "Iteration 17225: loss = 0.0062961453,5.0426977e-05\n",
      "Iteration 17230: loss = 0.006296082,5.040015e-05\n",
      "Iteration 17235: loss = 0.0062961522,5.0408056e-05\n",
      "Iteration 17240: loss = 0.0062958733,5.1377967e-05\n",
      "Iteration 17245: loss = 0.0062964153,5.4954893e-05\n",
      "Iteration 17250: loss = 0.006294997,8.443503e-05\n",
      "Iteration 17255: loss = 0.006298838,0.0003209455\n",
      "Iteration 17260: loss = 0.006287001,0.0027413026\n",
      "Iteration 17265: loss = 0.0063247173,0.027926678\n",
      "Iteration 17270: loss = 0.006246406,0.11449924\n",
      "Iteration 17275: loss = 0.0062767067,0.0148923835\n",
      "Iteration 17280: loss = 0.0062975492,0.00047027244\n",
      "Iteration 17285: loss = 0.006305219,0.0059980806\n",
      "Iteration 17290: loss = 0.0063085333,0.00805724\n",
      "Iteration 17295: loss = 0.006306956,0.006633087\n",
      "Iteration 17300: loss = 0.006303552,0.0040530334\n",
      "Iteration 17305: loss = 0.006300934,0.0018073379\n",
      "Iteration 17310: loss = 0.0062978473,0.0004942629\n",
      "Iteration 17315: loss = 0.0062959194,6.326936e-05\n",
      "Iteration 17320: loss = 0.0062948368,0.00012747028\n",
      "Iteration 17325: loss = 0.006294335,0.00025963117\n",
      "Iteration 17330: loss = 0.00629466,0.00024229234\n",
      "Iteration 17335: loss = 0.006295365,0.00012751386\n",
      "Iteration 17340: loss = 0.0062960633,5.439687e-05\n",
      "Iteration 17345: loss = 0.006296618,5.7117548e-05\n",
      "Iteration 17350: loss = 0.0062967907,7.1695744e-05\n",
      "Iteration 17355: loss = 0.0062965243,5.9818703e-05\n",
      "Iteration 17360: loss = 0.006296174,4.892946e-05\n",
      "Iteration 17365: loss = 0.006295914,5.1540253e-05\n",
      "Iteration 17370: loss = 0.0062958305,5.1679835e-05\n",
      "Iteration 17375: loss = 0.0062959087,4.8874826e-05\n",
      "Iteration 17380: loss = 0.006296044,4.9339196e-05\n",
      "Iteration 17385: loss = 0.006295983,4.9044407e-05\n",
      "Iteration 17390: loss = 0.0062958584,4.839809e-05\n",
      "Iteration 17395: loss = 0.006295766,4.8724956e-05\n",
      "Iteration 17400: loss = 0.006295782,4.8267437e-05\n",
      "Iteration 17405: loss = 0.0062957783,4.823226e-05\n",
      "Iteration 17410: loss = 0.006295745,4.8134578e-05\n",
      "Iteration 17415: loss = 0.0062956996,4.804442e-05\n",
      "Iteration 17420: loss = 0.006295676,4.7981554e-05\n",
      "Iteration 17425: loss = 0.0062956433,4.787676e-05\n",
      "Iteration 17430: loss = 0.0062956344,4.7809663e-05\n",
      "Iteration 17435: loss = 0.006295552,4.7822075e-05\n",
      "Iteration 17440: loss = 0.006295603,4.7726833e-05\n",
      "Iteration 17445: loss = 0.006295498,4.775039e-05\n",
      "Iteration 17450: loss = 0.006295539,4.7579695e-05\n",
      "Iteration 17455: loss = 0.0062954775,4.7482485e-05\n",
      "Iteration 17460: loss = 0.006295452,4.7383077e-05\n",
      "Iteration 17465: loss = 0.00629543,4.732266e-05\n",
      "Iteration 17470: loss = 0.006295389,4.7255227e-05\n",
      "Iteration 17475: loss = 0.006295362,4.7191934e-05\n",
      "Iteration 17480: loss = 0.006295348,4.712148e-05\n",
      "Iteration 17485: loss = 0.0062953136,4.704443e-05\n",
      "Iteration 17490: loss = 0.0062952917,4.697906e-05\n",
      "Iteration 17495: loss = 0.006295258,4.690249e-05\n",
      "Iteration 17500: loss = 0.0062952503,4.6827554e-05\n",
      "Iteration 17505: loss = 0.006295232,4.6737645e-05\n",
      "Iteration 17510: loss = 0.0062952093,4.669646e-05\n",
      "Iteration 17515: loss = 0.0062951655,4.66528e-05\n",
      "Iteration 17520: loss = 0.00629518,4.663223e-05\n",
      "Iteration 17525: loss = 0.0062949643,4.8665275e-05\n",
      "Iteration 17530: loss = 0.0062958566,9.3956674e-05\n",
      "Iteration 17535: loss = 0.0062912167,0.0013574557\n",
      "Iteration 17540: loss = 0.006316123,0.03752841\n",
      "Iteration 17545: loss = 0.006252991,0.25564507\n",
      "Iteration 17550: loss = 0.0062656994,0.13366333\n",
      "Iteration 17555: loss = 0.006284466,0.05641485\n",
      "Iteration 17560: loss = 0.006299852,0.00610448\n",
      "Iteration 17565: loss = 0.006312667,0.0039331773\n",
      "Iteration 17570: loss = 0.0063120886,0.012951661\n",
      "Iteration 17575: loss = 0.006303053,0.006561503\n",
      "Iteration 17580: loss = 0.006295036,0.00026749415\n",
      "Iteration 17585: loss = 0.006289422,0.0011705711\n",
      "Iteration 17590: loss = 0.0062895403,0.0018683554\n",
      "Iteration 17595: loss = 0.006292595,0.0006855451\n",
      "Iteration 17600: loss = 0.0062955194,5.3162745e-05\n",
      "Iteration 17605: loss = 0.0062975218,0.00018724914\n",
      "Iteration 17610: loss = 0.0062981765,0.00026437157\n",
      "Iteration 17615: loss = 0.006297616,0.00015154977\n",
      "Iteration 17620: loss = 0.0062968377,5.9213555e-05\n",
      "Iteration 17625: loss = 0.006296182,4.837266e-05\n",
      "Iteration 17630: loss = 0.0062956293,6.1939914e-05\n",
      "Iteration 17635: loss = 0.0062955017,6.314276e-05\n",
      "Iteration 17640: loss = 0.006295457,5.5403732e-05\n",
      "Iteration 17645: loss = 0.006295459,4.897861e-05\n",
      "Iteration 17650: loss = 0.006295551,4.615994e-05\n",
      "Iteration 17655: loss = 0.0062955446,4.5848974e-05\n",
      "Iteration 17660: loss = 0.0062955464,4.6024783e-05\n",
      "Iteration 17665: loss = 0.006295499,4.5999528e-05\n",
      "Iteration 17670: loss = 0.006295481,4.587262e-05\n",
      "Iteration 17675: loss = 0.0062954314,4.5662935e-05\n",
      "Iteration 17680: loss = 0.0062953965,4.5475463e-05\n",
      "Iteration 17685: loss = 0.0062953434,4.5330984e-05\n",
      "Iteration 17690: loss = 0.0062953127,4.5224966e-05\n",
      "Iteration 17695: loss = 0.006295267,4.514007e-05\n",
      "Iteration 17700: loss = 0.006295211,4.506162e-05\n",
      "Iteration 17705: loss = 0.0062951646,4.498247e-05\n",
      "Iteration 17710: loss = 0.0062951297,4.49122e-05\n",
      "Iteration 17715: loss = 0.0062950957,4.4847886e-05\n",
      "Iteration 17720: loss = 0.006295049,4.4775436e-05\n",
      "Iteration 17725: loss = 0.006295013,4.4706758e-05\n",
      "Iteration 17730: loss = 0.0062949695,4.464095e-05\n",
      "Iteration 17735: loss = 0.0062949415,4.457035e-05\n",
      "Iteration 17740: loss = 0.0062949155,4.450626e-05\n",
      "Iteration 17745: loss = 0.006294876,4.4434688e-05\n",
      "Iteration 17750: loss = 0.0062948638,4.4352786e-05\n",
      "Iteration 17755: loss = 0.0062947664,4.435728e-05\n",
      "Iteration 17760: loss = 0.006294759,4.42557e-05\n",
      "Iteration 17765: loss = 0.0062947418,4.4182983e-05\n",
      "Iteration 17770: loss = 0.0062947217,4.4103297e-05\n",
      "Iteration 17775: loss = 0.0062946975,4.4040462e-05\n",
      "Iteration 17780: loss = 0.0062946565,4.3967346e-05\n",
      "Iteration 17785: loss = 0.0062946193,4.3911197e-05\n",
      "Iteration 17790: loss = 0.006294616,4.3827626e-05\n",
      "Iteration 17795: loss = 0.006294589,4.3766784e-05\n",
      "Iteration 17800: loss = 0.0062945336,4.3724795e-05\n",
      "Iteration 17805: loss = 0.0062944978,4.367214e-05\n",
      "Iteration 17810: loss = 0.0062944777,4.3598102e-05\n",
      "Iteration 17815: loss = 0.0062944745,4.3493885e-05\n",
      "Iteration 17820: loss = 0.0062944577,4.3445336e-05\n",
      "Iteration 17825: loss = 0.00629439,4.34205e-05\n",
      "Iteration 17830: loss = 0.006294358,4.3343487e-05\n",
      "Iteration 17835: loss = 0.0062943697,4.3256663e-05\n",
      "Iteration 17840: loss = 0.006294343,4.3183587e-05\n",
      "Iteration 17845: loss = 0.006294301,4.3143424e-05\n",
      "Iteration 17850: loss = 0.006294289,4.3063104e-05\n",
      "Iteration 17855: loss = 0.0062942416,4.3016335e-05\n",
      "Iteration 17860: loss = 0.0062942337,4.2941734e-05\n",
      "Iteration 17865: loss = 0.00629428,4.2842395e-05\n",
      "Iteration 17870: loss = 0.0062940638,4.2979103e-05\n",
      "Iteration 17875: loss = 0.006294271,4.265973e-05\n",
      "Iteration 17880: loss = 0.006294062,4.2786676e-05\n",
      "Iteration 17885: loss = 0.006294174,4.256837e-05\n",
      "Iteration 17890: loss = 0.006294072,4.2581156e-05\n",
      "Iteration 17895: loss = 0.00629406,4.250245e-05\n",
      "Iteration 17900: loss = 0.006294076,4.239768e-05\n",
      "Iteration 17905: loss = 0.006294009,4.2394095e-05\n",
      "Iteration 17910: loss = 0.0062941257,4.2210202e-05\n",
      "Iteration 17915: loss = 0.006293588,4.3017622e-05\n",
      "Iteration 17920: loss = 0.006295096,4.309942e-05\n",
      "Iteration 17925: loss = 0.0062903133,6.448554e-05\n",
      "Iteration 17930: loss = 0.0063067134,0.00025637198\n",
      "Iteration 17935: loss = 0.0062477537,0.0030820651\n",
      "Iteration 17940: loss = 0.006386951,0.01171395\n",
      "Iteration 17945: loss = 0.006331179,0.00781225\n",
      "Iteration 17950: loss = 0.0063410476,0.019201027\n",
      "Iteration 17955: loss = 0.006276663,0.03800209\n",
      "Iteration 17960: loss = 0.0063120746,0.0019538424\n",
      "Iteration 17965: loss = 0.0063247555,0.01170941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17970: loss = 0.00630662,0.0005523488\n",
      "Iteration 17975: loss = 0.006297991,0.0047016926\n",
      "Iteration 17980: loss = 0.0063058524,0.00022201927\n",
      "Iteration 17985: loss = 0.006309789,0.0018096096\n",
      "Iteration 17990: loss = 0.0063014296,0.000109670276\n",
      "Iteration 17995: loss = 0.0062958132,0.0007204809\n",
      "Iteration 18000: loss = 0.0062989625,9.0562804e-05\n",
      "Iteration 18005: loss = 0.006299091,0.0002389511\n",
      "Iteration 18010: loss = 0.006294796,0.00014843387\n",
      "Iteration 18015: loss = 0.0062954132,4.9379363e-05\n",
      "Iteration 18020: loss = 0.0062965765,0.00010750917\n",
      "Iteration 18025: loss = 0.006294141,8.164375e-05\n",
      "Iteration 18030: loss = 0.0062951464,4.4540946e-05\n",
      "Iteration 18035: loss = 0.006294886,4.3129636e-05\n",
      "Iteration 18040: loss = 0.0062939706,5.3664662e-05\n",
      "Iteration 18045: loss = 0.0062951185,6.546694e-05\n",
      "Iteration 18050: loss = 0.0062931697,9.37003e-05\n",
      "Iteration 18055: loss = 0.0062958947,0.00018149908\n",
      "Iteration 18060: loss = 0.0062906197,0.000576117\n",
      "Iteration 18065: loss = 0.006301326,0.0025638347\n",
      "Iteration 18070: loss = 0.0062768194,0.013844982\n",
      "Iteration 18075: loss = 0.006324854,0.049985465\n",
      "Iteration 18080: loss = 0.006284054,0.011575462\n",
      "Iteration 18085: loss = 0.0062809014,0.018129837\n",
      "Iteration 18090: loss = 0.0062944926,4.14893e-05\n",
      "Iteration 18095: loss = 0.0063033276,0.006127682\n",
      "Iteration 18100: loss = 0.006299423,0.0022534432\n",
      "Iteration 18105: loss = 0.0062920838,0.0004245374\n",
      "Iteration 18110: loss = 0.006289903,0.0016301333\n",
      "Iteration 18115: loss = 0.0062931348,9.212799e-05\n",
      "Iteration 18120: loss = 0.0062962957,0.0005160949\n",
      "Iteration 18125: loss = 0.0062950323,0.00016289875\n",
      "Iteration 18130: loss = 0.0062925555,0.0001680466\n",
      "Iteration 18135: loss = 0.006292928,0.000104610415\n",
      "Iteration 18140: loss = 0.0062945243,9.138672e-05\n",
      "Iteration 18145: loss = 0.0062941504,5.379375e-05\n",
      "Iteration 18150: loss = 0.006293141,7.0608934e-05\n",
      "Iteration 18155: loss = 0.006293701,3.983198e-05\n",
      "Iteration 18160: loss = 0.006294079,5.1317264e-05\n",
      "Iteration 18165: loss = 0.0062934514,4.5537337e-05\n",
      "Iteration 18170: loss = 0.006293688,3.9573955e-05\n",
      "Iteration 18175: loss = 0.006293828,4.1427134e-05\n",
      "Iteration 18180: loss = 0.0062934277,4.2668544e-05\n",
      "Iteration 18185: loss = 0.0062937257,4.1603365e-05\n",
      "Iteration 18190: loss = 0.0062933653,4.1687217e-05\n",
      "Iteration 18195: loss = 0.0062937276,4.243684e-05\n",
      "Iteration 18200: loss = 0.006293191,4.747299e-05\n",
      "Iteration 18205: loss = 0.0062940507,6.59143e-05\n",
      "Iteration 18210: loss = 0.0062922016,0.00016809009\n",
      "Iteration 18215: loss = 0.006296426,0.00079839816\n",
      "Iteration 18220: loss = 0.006285478,0.005475628\n",
      "Iteration 18225: loss = 0.0063140932,0.036881916\n",
      "Iteration 18230: loss = 0.006271312,0.06460727\n",
      "Iteration 18235: loss = 0.00628094,0.013811606\n",
      "Iteration 18240: loss = 0.0062950677,0.0018292015\n",
      "Iteration 18245: loss = 0.0063011157,0.010279504\n",
      "Iteration 18250: loss = 0.006299544,0.006432409\n",
      "Iteration 18255: loss = 0.006293901,0.00070074695\n",
      "Iteration 18260: loss = 0.0062890896,0.000508959\n",
      "Iteration 18265: loss = 0.0062882937,0.0015127027\n",
      "Iteration 18270: loss = 0.0062909834,0.000680409\n",
      "Iteration 18275: loss = 0.0062949914,4.0158106e-05\n",
      "Iteration 18280: loss = 0.006297072,0.0002840134\n",
      "Iteration 18285: loss = 0.006296052,0.0002036325\n",
      "Iteration 18290: loss = 0.0062935683,3.8466995e-05\n",
      "Iteration 18295: loss = 0.0062919967,0.000107263724\n",
      "Iteration 18300: loss = 0.006292446,5.9743732e-05\n",
      "Iteration 18305: loss = 0.0062936395,4.6479552e-05\n",
      "Iteration 18310: loss = 0.0062940344,5.3345917e-05\n",
      "Iteration 18315: loss = 0.0062934696,3.868009e-05\n",
      "Iteration 18320: loss = 0.006293069,4.568318e-05\n",
      "Iteration 18325: loss = 0.006293394,3.822049e-05\n",
      "Iteration 18330: loss = 0.0062934104,4.0054154e-05\n",
      "Iteration 18335: loss = 0.006293133,3.82898e-05\n",
      "Iteration 18340: loss = 0.006293183,3.8404785e-05\n",
      "Iteration 18345: loss = 0.0062933415,3.8214734e-05\n",
      "Iteration 18350: loss = 0.006293084,3.7870886e-05\n",
      "Iteration 18355: loss = 0.006293019,3.7946687e-05\n",
      "Iteration 18360: loss = 0.0062931874,3.7886384e-05\n",
      "Iteration 18365: loss = 0.006293043,3.7843856e-05\n",
      "Iteration 18370: loss = 0.0062930877,3.7616395e-05\n",
      "Iteration 18375: loss = 0.006292854,3.7830567e-05\n",
      "Iteration 18380: loss = 0.006293135,3.766374e-05\n",
      "Iteration 18385: loss = 0.0062927767,3.8733226e-05\n",
      "Iteration 18390: loss = 0.00629319,4.10895e-05\n",
      "Iteration 18395: loss = 0.0062923594,5.4579752e-05\n",
      "Iteration 18400: loss = 0.006294152,0.00012499087\n",
      "Iteration 18405: loss = 0.006289316,0.00064507785\n",
      "Iteration 18410: loss = 0.0063030296,0.004916755\n",
      "Iteration 18415: loss = 0.0062651285,0.03948142\n",
      "Iteration 18420: loss = 0.0063186474,0.072118744\n",
      "Iteration 18425: loss = 0.006331504,0.021277795\n",
      "Iteration 18430: loss = 0.006317874,0.0008855052\n",
      "Iteration 18435: loss = 0.0062918155,0.0055854106\n",
      "Iteration 18440: loss = 0.006270422,0.0072948923\n",
      "Iteration 18445: loss = 0.0062622987,0.004484982\n",
      "Iteration 18450: loss = 0.006267941,0.0014919458\n",
      "Iteration 18455: loss = 0.006281894,0.00022950958\n",
      "Iteration 18460: loss = 0.006296882,0.00025844635\n",
      "Iteration 18465: loss = 0.006306344,0.0005032521\n",
      "Iteration 18470: loss = 0.0063068657,0.00038893294\n",
      "Iteration 18475: loss = 0.0063003325,0.000115264986\n",
      "Iteration 18480: loss = 0.006292038,4.087635e-05\n",
      "Iteration 18485: loss = 0.0062874337,0.00010265231\n",
      "Iteration 18490: loss = 0.0062884768,8.705425e-05\n",
      "Iteration 18495: loss = 0.0062923157,4.1517098e-05\n",
      "Iteration 18500: loss = 0.00629502,4.115607e-05\n",
      "Iteration 18505: loss = 0.0062949336,4.5862253e-05\n",
      "Iteration 18510: loss = 0.006293299,3.7112382e-05\n",
      "Iteration 18515: loss = 0.0062921494,3.9346054e-05\n",
      "Iteration 18520: loss = 0.006292133,3.8994418e-05\n",
      "Iteration 18525: loss = 0.00629292,3.627791e-05\n",
      "Iteration 18530: loss = 0.0062934407,3.631607e-05\n",
      "Iteration 18535: loss = 0.006292933,3.594548e-05\n",
      "Iteration 18540: loss = 0.0062923837,3.6707934e-05\n",
      "Iteration 18545: loss = 0.0062926635,3.6065074e-05\n",
      "Iteration 18550: loss = 0.0062929275,3.5759338e-05\n",
      "Iteration 18555: loss = 0.0062927003,3.5875117e-05\n",
      "Iteration 18560: loss = 0.0062926677,3.580119e-05\n",
      "Iteration 18565: loss = 0.0062925066,3.5889512e-05\n",
      "Iteration 18570: loss = 0.006292736,3.5625468e-05\n",
      "Iteration 18575: loss = 0.0062925015,3.5843972e-05\n",
      "Iteration 18580: loss = 0.006292412,3.5986835e-05\n",
      "Iteration 18585: loss = 0.0062930477,3.6225e-05\n",
      "Iteration 18590: loss = 0.006290933,4.4509587e-05\n",
      "Iteration 18595: loss = 0.00629686,8.998219e-05\n",
      "Iteration 18600: loss = 0.0062789917,0.00060084375\n",
      "Iteration 18605: loss = 0.0063331476,0.0054402244\n",
      "Iteration 18610: loss = 0.0062482595,0.025258394\n",
      "Iteration 18615: loss = 0.0062045394,0.058251157\n",
      "Iteration 18620: loss = 0.006254563,0.0027489974\n",
      "Iteration 18625: loss = 0.0062754005,0.016907008\n",
      "Iteration 18630: loss = 0.0062727723,0.0042368188\n",
      "Iteration 18635: loss = 0.006265935,0.0021640216\n",
      "Iteration 18640: loss = 0.006266529,0.0039576394\n",
      "Iteration 18645: loss = 0.006275279,0.00061280746\n",
      "Iteration 18650: loss = 0.006284339,0.0009777356\n",
      "Iteration 18655: loss = 0.0062867478,0.0006862134\n",
      "Iteration 18660: loss = 0.0062856465,0.00014162147\n",
      "Iteration 18665: loss = 0.0062862937,0.00035064277\n",
      "Iteration 18670: loss = 0.006289473,5.2785108e-05\n",
      "Iteration 18675: loss = 0.006291671,0.0001515435\n",
      "Iteration 18680: loss = 0.006290937,4.1060383e-05\n",
      "Iteration 18685: loss = 0.006290402,8.206009e-05\n",
      "Iteration 18690: loss = 0.0062916377,3.9653543e-05\n",
      "Iteration 18695: loss = 0.006292025,4.906999e-05\n",
      "Iteration 18700: loss = 0.006291369,4.40635e-05\n",
      "Iteration 18705: loss = 0.0062917266,3.631216e-05\n",
      "Iteration 18710: loss = 0.006292114,4.0127692e-05\n",
      "Iteration 18715: loss = 0.006291708,3.9283077e-05\n",
      "Iteration 18720: loss = 0.0062921285,3.568487e-05\n",
      "Iteration 18725: loss = 0.0062921033,3.4817844e-05\n",
      "Iteration 18730: loss = 0.006292111,3.496833e-05\n",
      "Iteration 18735: loss = 0.0062923566,3.5306482e-05\n",
      "Iteration 18740: loss = 0.0062920633,3.729916e-05\n",
      "Iteration 18745: loss = 0.0062926263,4.2792613e-05\n",
      "Iteration 18750: loss = 0.0062914565,7.3390525e-05\n",
      "Iteration 18755: loss = 0.0062939674,0.0002447336\n",
      "Iteration 18760: loss = 0.0062875357,0.0014700687\n",
      "Iteration 18765: loss = 0.0063050506,0.01115746\n",
      "Iteration 18770: loss = 0.0062628775,0.06667231\n",
      "Iteration 18775: loss = 0.0063018487,0.017406454\n",
      "Iteration 18780: loss = 0.006307926,0.031226108\n",
      "Iteration 18785: loss = 0.0062991534,0.007966467\n",
      "Iteration 18790: loss = 0.0062905024,5.0272392e-05\n",
      "Iteration 18795: loss = 0.0062848493,0.0030515061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18800: loss = 0.006284136,0.0038558026\n",
      "Iteration 18805: loss = 0.0062875636,0.0014691583\n",
      "Iteration 18810: loss = 0.0062915464,5.4569595e-05\n",
      "Iteration 18815: loss = 0.0062945834,0.00038499394\n",
      "Iteration 18820: loss = 0.0062952284,0.00053437706\n",
      "Iteration 18825: loss = 0.0062937313,0.00013958478\n",
      "Iteration 18830: loss = 0.00629204,5.555137e-05\n",
      "Iteration 18835: loss = 0.0062913024,0.00014265471\n",
      "Iteration 18840: loss = 0.0062918267,6.089302e-05\n",
      "Iteration 18845: loss = 0.0062927622,4.1908977e-05\n",
      "Iteration 18850: loss = 0.0062928493,5.7940124e-05\n",
      "Iteration 18855: loss = 0.0062922067,3.3745735e-05\n",
      "Iteration 18860: loss = 0.0062917075,4.191562e-05\n",
      "Iteration 18865: loss = 0.006291797,3.5239544e-05\n",
      "Iteration 18870: loss = 0.006292138,3.573834e-05\n",
      "Iteration 18875: loss = 0.006291998,3.3843546e-05\n",
      "Iteration 18880: loss = 0.0062917783,3.4367025e-05\n",
      "Iteration 18885: loss = 0.006291885,3.3285207e-05\n",
      "Iteration 18890: loss = 0.0062919236,3.3452656e-05\n",
      "Iteration 18895: loss = 0.0062918128,3.314551e-05\n",
      "Iteration 18900: loss = 0.0062917434,3.3244392e-05\n",
      "Iteration 18905: loss = 0.00629185,3.3206772e-05\n",
      "Iteration 18910: loss = 0.0062917024,3.314417e-05\n",
      "Iteration 18915: loss = 0.0062917513,3.2936223e-05\n",
      "Iteration 18920: loss = 0.0062916703,3.2889282e-05\n",
      "Iteration 18925: loss = 0.0062916353,3.287667e-05\n",
      "Iteration 18930: loss = 0.0062916162,3.2820288e-05\n",
      "Iteration 18935: loss = 0.0062915944,3.277309e-05\n",
      "Iteration 18940: loss = 0.006291619,3.268981e-05\n",
      "Iteration 18945: loss = 0.006291526,3.2705022e-05\n",
      "Iteration 18950: loss = 0.006291524,3.267292e-05\n",
      "Iteration 18955: loss = 0.0062913825,3.3272445e-05\n",
      "Iteration 18960: loss = 0.006291773,3.7093334e-05\n",
      "Iteration 18965: loss = 0.0062906197,8.055374e-05\n",
      "Iteration 18970: loss = 0.006294271,0.0005711631\n",
      "Iteration 18975: loss = 0.0062810895,0.007309703\n",
      "Iteration 18980: loss = 0.0063256696,0.081637785\n",
      "Iteration 18985: loss = 0.0062861275,0.0320367\n",
      "Iteration 18990: loss = 0.0062765577,0.04568804\n",
      "Iteration 18995: loss = 0.0062718107,0.030793479\n",
      "Iteration 19000: loss = 0.0062717,0.01845231\n",
      "Iteration 19005: loss = 0.0062719546,0.010889023\n",
      "Iteration 19010: loss = 0.006272959,0.006436367\n",
      "Iteration 19015: loss = 0.006275372,0.0038115303\n",
      "Iteration 19020: loss = 0.0062774518,0.0022346159\n",
      "Iteration 19025: loss = 0.0062802495,0.0012805647\n",
      "Iteration 19030: loss = 0.00628319,0.00070233375\n",
      "Iteration 19035: loss = 0.0062858667,0.0003567889\n",
      "Iteration 19040: loss = 0.0062886006,0.0001631737\n",
      "Iteration 19045: loss = 0.0062906346,6.94291e-05\n",
      "Iteration 19050: loss = 0.00629203,3.57189e-05\n",
      "Iteration 19055: loss = 0.006292792,3.2881282e-05\n",
      "Iteration 19060: loss = 0.0062929266,4.0417122e-05\n",
      "Iteration 19065: loss = 0.0062927455,4.5491783e-05\n",
      "Iteration 19070: loss = 0.006292475,4.4475582e-05\n",
      "Iteration 19075: loss = 0.0062922444,3.904153e-05\n",
      "Iteration 19080: loss = 0.0062920316,3.3389584e-05\n",
      "Iteration 19085: loss = 0.0062917382,3.1586704e-05\n",
      "Iteration 19090: loss = 0.006291483,3.2247313e-05\n",
      "Iteration 19095: loss = 0.006291257,3.301062e-05\n",
      "Iteration 19100: loss = 0.0062912498,3.2394157e-05\n",
      "Iteration 19105: loss = 0.0062914104,3.161313e-05\n",
      "Iteration 19110: loss = 0.006291533,3.1437572e-05\n",
      "Iteration 19115: loss = 0.006291536,3.1443476e-05\n",
      "Iteration 19120: loss = 0.0062913657,3.1399155e-05\n",
      "Iteration 19125: loss = 0.006291315,3.1415213e-05\n",
      "Iteration 19130: loss = 0.0062913257,3.1283533e-05\n",
      "Iteration 19135: loss = 0.006291285,3.126153e-05\n",
      "Iteration 19140: loss = 0.0062912717,3.1206648e-05\n",
      "Iteration 19145: loss = 0.0062912456,3.115989e-05\n",
      "Iteration 19150: loss = 0.006291225,3.1115986e-05\n",
      "Iteration 19155: loss = 0.006291123,3.113884e-05\n",
      "Iteration 19160: loss = 0.006291132,3.1065127e-05\n",
      "Iteration 19165: loss = 0.0062911496,3.0973068e-05\n",
      "Iteration 19170: loss = 0.0062910602,3.0984098e-05\n",
      "Iteration 19175: loss = 0.0062910696,3.0920142e-05\n",
      "Iteration 19180: loss = 0.006291036,3.0873463e-05\n",
      "Iteration 19185: loss = 0.0062910398,3.0847987e-05\n",
      "Iteration 19190: loss = 0.006290871,3.1046693e-05\n",
      "Iteration 19195: loss = 0.0062911366,3.0704803e-05\n",
      "Iteration 19200: loss = 0.0062906626,3.109679e-05\n",
      "Iteration 19205: loss = 0.006291566,3.0773106e-05\n",
      "Iteration 19210: loss = 0.006288864,3.9319224e-05\n",
      "Iteration 19215: loss = 0.006297643,9.629791e-05\n",
      "Iteration 19220: loss = 0.0062666074,0.0009680827\n",
      "Iteration 19225: loss = 0.0063655823,0.008525158\n",
      "Iteration 19230: loss = 0.0062742424,0.0017297186\n",
      "Iteration 19235: loss = 0.006239047,0.005783293\n",
      "Iteration 19240: loss = 0.0062694233,0.010578869\n",
      "Iteration 19245: loss = 0.006236332,0.018655809\n",
      "Iteration 19250: loss = 0.006283771,0.013042588\n",
      "Iteration 19255: loss = 0.0062699583,0.000701801\n",
      "Iteration 19260: loss = 0.006265596,0.0041917223\n",
      "Iteration 19265: loss = 0.0062865377,0.0024693606\n",
      "Iteration 19270: loss = 0.006282214,0.00017990074\n",
      "Iteration 19275: loss = 0.006278667,0.0011320122\n",
      "Iteration 19280: loss = 0.0062899026,0.0008755363\n",
      "Iteration 19285: loss = 0.0062849834,0.0001855108\n",
      "Iteration 19290: loss = 0.0062871915,6.3643514e-05\n",
      "Iteration 19295: loss = 0.0062906314,0.00018528228\n",
      "Iteration 19300: loss = 0.0062867557,0.00032904555\n",
      "Iteration 19305: loss = 0.0062936917,0.0005628252\n",
      "Iteration 19310: loss = 0.006284988,0.0012877029\n",
      "Iteration 19315: loss = 0.0063004834,0.003753323\n",
      "Iteration 19320: loss = 0.006274166,0.012382033\n",
      "Iteration 19325: loss = 0.0063154367,0.027098019\n",
      "Iteration 19330: loss = 0.006279155,0.010020101\n",
      "Iteration 19335: loss = 0.006284699,0.0030583572\n",
      "Iteration 19340: loss = 0.006300982,0.005601835\n",
      "Iteration 19345: loss = 0.0062932274,0.00025242698\n",
      "Iteration 19350: loss = 0.006284952,0.002526702\n",
      "Iteration 19355: loss = 0.0062918905,9.427005e-05\n",
      "Iteration 19360: loss = 0.006294263,0.0007617666\n",
      "Iteration 19365: loss = 0.0062882085,0.00044709037\n",
      "Iteration 19370: loss = 0.006290363,3.6967343e-05\n",
      "Iteration 19375: loss = 0.0062924935,0.00024215571\n",
      "Iteration 19380: loss = 0.006289029,0.00020340884\n",
      "Iteration 19385: loss = 0.0062915105,7.1562856e-05\n",
      "Iteration 19390: loss = 0.006290614,3.0319934e-05\n",
      "Iteration 19395: loss = 0.0062903664,3.7308855e-05\n",
      "Iteration 19400: loss = 0.0062913955,5.7491325e-05\n",
      "Iteration 19405: loss = 0.0062896213,0.0001056239\n",
      "Iteration 19410: loss = 0.0062925764,0.00026662968\n",
      "Iteration 19415: loss = 0.0062866807,0.0010238492\n",
      "Iteration 19420: loss = 0.006299365,0.005047697\n",
      "Iteration 19425: loss = 0.006271403,0.025738686\n",
      "Iteration 19430: loss = 0.00631467,0.04812175\n",
      "Iteration 19435: loss = 0.0062951376,0.0005025455\n",
      "Iteration 19440: loss = 0.0062814034,0.013221886\n",
      "Iteration 19445: loss = 0.0062853177,0.0045407307\n",
      "Iteration 19450: loss = 0.006295419,0.0009104679\n",
      "Iteration 19455: loss = 0.00629771,0.003320345\n",
      "Iteration 19460: loss = 0.006291753,0.00020479229\n",
      "Iteration 19465: loss = 0.0062866104,0.0008751364\n",
      "Iteration 19470: loss = 0.006287581,0.00044170377\n",
      "Iteration 19475: loss = 0.006291582,0.0001574469\n",
      "Iteration 19480: loss = 0.006292283,0.00025210425\n",
      "Iteration 19485: loss = 0.0062900544,5.6086537e-05\n",
      "Iteration 19490: loss = 0.006289656,0.00011695724\n",
      "Iteration 19495: loss = 0.006291213,4.6626752e-05\n",
      "Iteration 19500: loss = 0.0062911306,5.0480237e-05\n",
      "Iteration 19505: loss = 0.006289877,4.957666e-05\n",
      "Iteration 19510: loss = 0.0062903543,2.8927301e-05\n",
      "Iteration 19515: loss = 0.0062907697,3.7570277e-05\n",
      "Iteration 19520: loss = 0.0062901224,3.4298e-05\n",
      "Iteration 19525: loss = 0.006290441,2.873197e-05\n",
      "Iteration 19530: loss = 0.006290384,2.9012608e-05\n",
      "Iteration 19535: loss = 0.006290125,3.025675e-05\n",
      "Iteration 19540: loss = 0.0062904083,3.0251882e-05\n",
      "Iteration 19545: loss = 0.0062900987,3.1008e-05\n",
      "Iteration 19550: loss = 0.006290508,3.3423803e-05\n",
      "Iteration 19555: loss = 0.006289687,4.5236542e-05\n",
      "Iteration 19560: loss = 0.0062911455,9.599067e-05\n",
      "Iteration 19565: loss = 0.0062877885,0.0004136235\n",
      "Iteration 19570: loss = 0.0062963576,0.002651355\n",
      "Iteration 19575: loss = 0.006273409,0.01994235\n",
      "Iteration 19580: loss = 0.0063204677,0.07551127\n",
      "Iteration 19585: loss = 0.006298674,0.00013158459\n",
      "Iteration 19590: loss = 0.006282521,0.017997291\n",
      "Iteration 19595: loss = 0.006276498,0.013778566\n",
      "Iteration 19600: loss = 0.0062806117,0.0022284908\n",
      "Iteration 19605: loss = 0.0062885373,0.00037690747\n",
      "Iteration 19610: loss = 0.0062945206,0.0023308685\n",
      "Iteration 19615: loss = 0.006296264,0.0017620592\n",
      "Iteration 19620: loss = 0.0062934463,0.00023390318\n",
      "Iteration 19625: loss = 0.0062894453,0.00016240662\n",
      "Iteration 19630: loss = 0.006287612,0.0004355088\n",
      "Iteration 19635: loss = 0.006288739,0.00014450985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19640: loss = 0.0062909178,4.205232e-05\n",
      "Iteration 19645: loss = 0.0062916293,0.00011748685\n",
      "Iteration 19650: loss = 0.006290598,4.2478172e-05\n",
      "Iteration 19655: loss = 0.0062895375,4.2522537e-05\n",
      "Iteration 19660: loss = 0.00628975,4.6035624e-05\n",
      "Iteration 19665: loss = 0.00629043,2.7967617e-05\n",
      "Iteration 19670: loss = 0.0062903613,3.5388763e-05\n",
      "Iteration 19675: loss = 0.006289931,2.7737526e-05\n",
      "Iteration 19680: loss = 0.006289898,3.0858166e-05\n",
      "Iteration 19685: loss = 0.0062900693,2.747812e-05\n",
      "Iteration 19690: loss = 0.0062900227,2.864695e-05\n",
      "Iteration 19695: loss = 0.0062898654,2.7822962e-05\n",
      "Iteration 19700: loss = 0.0062899296,2.76182e-05\n",
      "Iteration 19705: loss = 0.0062899026,2.778836e-05\n",
      "Iteration 19710: loss = 0.0062898477,2.7452155e-05\n",
      "Iteration 19715: loss = 0.006289898,2.7267753e-05\n",
      "Iteration 19720: loss = 0.0062897205,2.7411766e-05\n",
      "Iteration 19725: loss = 0.006289925,2.718111e-05\n",
      "Iteration 19730: loss = 0.0062894765,2.761614e-05\n",
      "Iteration 19735: loss = 0.006290248,2.7083048e-05\n",
      "Iteration 19740: loss = 0.006288564,3.0220748e-05\n",
      "Iteration 19745: loss = 0.0062924814,3.621462e-05\n",
      "Iteration 19750: loss = 0.006281685,0.00012783796\n",
      "Iteration 19755: loss = 0.0063139326,0.0008469853\n",
      "Iteration 19760: loss = 0.006224413,0.0062050796\n",
      "Iteration 19765: loss = 0.0063328133,0.0025697127\n",
      "Iteration 19770: loss = 0.006325008,0.0072817435\n",
      "Iteration 19775: loss = 0.0063549294,0.03175343\n",
      "Iteration 19780: loss = 0.0062709185,0.07089942\n",
      "Iteration 19785: loss = 0.0062870514,0.009148427\n",
      "Iteration 19790: loss = 0.0063114036,0.003772734\n",
      "Iteration 19795: loss = 0.006320743,0.011432642\n",
      "Iteration 19800: loss = 0.0063161473,0.006427985\n",
      "Iteration 19805: loss = 0.0063065276,0.00084812054\n",
      "Iteration 19810: loss = 0.006297482,0.00056923507\n",
      "Iteration 19815: loss = 0.0062923916,0.0015076803\n",
      "Iteration 19820: loss = 0.006291501,0.0008251686\n",
      "Iteration 19825: loss = 0.006293058,6.323987e-05\n",
      "Iteration 19830: loss = 0.0062942463,0.0001954999\n",
      "Iteration 19835: loss = 0.0062935813,0.00023741079\n",
      "Iteration 19840: loss = 0.0062915254,4.0758743e-05\n",
      "Iteration 19845: loss = 0.0062896865,6.7359724e-05\n",
      "Iteration 19850: loss = 0.006289463,7.0887625e-05\n",
      "Iteration 19855: loss = 0.006290358,2.6640875e-05\n",
      "Iteration 19860: loss = 0.0062908065,4.396653e-05\n",
      "Iteration 19865: loss = 0.0062903496,2.812715e-05\n",
      "Iteration 19870: loss = 0.006289776,3.2088075e-05\n",
      "Iteration 19875: loss = 0.0062898733,2.8013343e-05\n",
      "Iteration 19880: loss = 0.0062901857,2.8183102e-05\n",
      "Iteration 19885: loss = 0.006289987,2.6699796e-05\n",
      "Iteration 19890: loss = 0.006289687,2.7626662e-05\n",
      "Iteration 19895: loss = 0.0062897936,2.6446658e-05\n",
      "Iteration 19900: loss = 0.006289747,2.6647149e-05\n",
      "Iteration 19905: loss = 0.0062895934,2.6558802e-05\n",
      "Iteration 19910: loss = 0.0062895305,2.6544534e-05\n",
      "Iteration 19915: loss = 0.0062895385,2.650062e-05\n",
      "Iteration 19920: loss = 0.006289394,2.6617057e-05\n",
      "Iteration 19925: loss = 0.006289516,2.6558924e-05\n",
      "Iteration 19930: loss = 0.006289363,2.6802121e-05\n",
      "Iteration 19935: loss = 0.006289527,2.6732607e-05\n",
      "Iteration 19940: loss = 0.006289277,2.7298476e-05\n",
      "Iteration 19945: loss = 0.0062896195,2.856743e-05\n",
      "Iteration 19950: loss = 0.006288888,3.7741003e-05\n",
      "Iteration 19955: loss = 0.006290452,8.549344e-05\n",
      "Iteration 19960: loss = 0.0062863478,0.00044241964\n",
      "Iteration 19965: loss = 0.006297745,0.0034263816\n",
      "Iteration 19970: loss = 0.0062651783,0.029645763\n",
      "Iteration 19975: loss = 0.0063246475,0.08392524\n",
      "Iteration 19980: loss = 0.006308153,0.011278478\n",
      "Iteration 19985: loss = 0.0062947036,0.0020984332\n",
      "Iteration 19990: loss = 0.006287066,0.010103421\n",
      "Iteration 19995: loss = 0.0062851184,0.00910916\n",
      "Iteration 20000: loss = 0.006287232,0.0038569209\n",
      "Iteration 20005: loss = 0.006290905,0.0005186608\n",
      "Iteration 20010: loss = 0.00629401,0.00013825134\n",
      "Iteration 20015: loss = 0.006295061,0.00072495936\n",
      "Iteration 20020: loss = 0.006293666,0.00070020463\n",
      "Iteration 20025: loss = 0.006290577,0.00022537049\n",
      "Iteration 20030: loss = 0.0062875873,3.2727054e-05\n",
      "Iteration 20035: loss = 0.0062861103,0.00012404418\n",
      "Iteration 20040: loss = 0.0062865806,0.00011734622\n",
      "Iteration 20045: loss = 0.0062884777,3.447481e-05\n",
      "Iteration 20050: loss = 0.006290171,3.7118036e-05\n",
      "Iteration 20055: loss = 0.006290601,4.471678e-05\n",
      "Iteration 20060: loss = 0.006289937,2.6038084e-05\n",
      "Iteration 20065: loss = 0.0062890737,3.1187803e-05\n",
      "Iteration 20070: loss = 0.006288877,2.9203711e-05\n",
      "Iteration 20075: loss = 0.0062891673,2.6022042e-05\n",
      "Iteration 20080: loss = 0.0062893443,2.7658078e-05\n",
      "Iteration 20085: loss = 0.006289113,2.5526742e-05\n",
      "Iteration 20090: loss = 0.0062890234,2.6408967e-05\n",
      "Iteration 20095: loss = 0.006289158,2.5337155e-05\n",
      "Iteration 20100: loss = 0.0062891603,2.5591744e-05\n",
      "Iteration 20105: loss = 0.006289005,2.5398487e-05\n",
      "Iteration 20110: loss = 0.006288936,2.5479814e-05\n",
      "Iteration 20115: loss = 0.006288968,2.5291185e-05\n",
      "Iteration 20120: loss = 0.00628905,2.5204521e-05\n",
      "Iteration 20125: loss = 0.0062889555,2.5200188e-05\n",
      "Iteration 20130: loss = 0.006288877,2.522859e-05\n",
      "Iteration 20135: loss = 0.0062889643,2.515307e-05\n",
      "Iteration 20140: loss = 0.0062887445,2.5537702e-05\n",
      "Iteration 20145: loss = 0.0062889815,2.549008e-05\n",
      "Iteration 20150: loss = 0.0062887245,2.581409e-05\n",
      "Iteration 20155: loss = 0.0062889815,2.598506e-05\n",
      "Iteration 20160: loss = 0.006288439,2.8892187e-05\n",
      "Iteration 20165: loss = 0.0062895543,4.0559862e-05\n",
      "Iteration 20170: loss = 0.0062867776,0.00012629012\n",
      "Iteration 20175: loss = 0.006294126,0.0007729319\n",
      "Iteration 20180: loss = 0.006272772,0.006757333\n",
      "Iteration 20185: loss = 0.006331286,0.052298054\n",
      "Iteration 20190: loss = 0.0062782243,0.046900615\n",
      "Iteration 20195: loss = 0.0062380377,0.034067318\n",
      "Iteration 20200: loss = 0.006230878,0.008613286\n",
      "Iteration 20205: loss = 0.006245602,0.002926587\n",
      "Iteration 20210: loss = 0.006263398,0.0029766527\n",
      "Iteration 20215: loss = 0.0062771286,0.0022030675\n",
      "Iteration 20220: loss = 0.006286431,0.0007945579\n",
      "Iteration 20225: loss = 0.0062907315,6.9454596e-05\n",
      "Iteration 20230: loss = 0.00629241,0.00010990813\n",
      "Iteration 20235: loss = 0.0062931143,0.00022774616\n",
      "Iteration 20240: loss = 0.0062929536,0.00013051291\n",
      "Iteration 20245: loss = 0.0062928894,4.0499242e-05\n",
      "Iteration 20250: loss = 0.0062928684,6.305061e-05\n",
      "Iteration 20255: loss = 0.0062927213,6.6809465e-05\n",
      "Iteration 20260: loss = 0.0062926128,3.9914186e-05\n",
      "Iteration 20265: loss = 0.0062922905,3.9957205e-05\n",
      "Iteration 20270: loss = 0.006291742,3.684099e-05\n",
      "Iteration 20275: loss = 0.006291052,2.813169e-05\n",
      "Iteration 20280: loss = 0.0062902714,2.7613223e-05\n",
      "Iteration 20285: loss = 0.0062895655,2.4722558e-05\n",
      "Iteration 20290: loss = 0.0062889718,2.505661e-05\n",
      "Iteration 20295: loss = 0.00628856,2.5310992e-05\n",
      "Iteration 20300: loss = 0.006288351,2.5554536e-05\n",
      "Iteration 20305: loss = 0.006288378,2.5304222e-05\n",
      "Iteration 20310: loss = 0.0062885974,2.4744426e-05\n",
      "Iteration 20315: loss = 0.006288755,2.4513563e-05\n",
      "Iteration 20320: loss = 0.0062888726,2.4349574e-05\n",
      "Iteration 20325: loss = 0.0062888875,2.4206447e-05\n",
      "Iteration 20330: loss = 0.0062887683,2.430438e-05\n",
      "Iteration 20335: loss = 0.006288672,2.4430065e-05\n",
      "Iteration 20340: loss = 0.0062885904,2.4491415e-05\n",
      "Iteration 20345: loss = 0.006288573,2.4407454e-05\n",
      "Iteration 20350: loss = 0.0062885596,2.4446086e-05\n",
      "Iteration 20355: loss = 0.0062885843,2.4570752e-05\n",
      "Iteration 20360: loss = 0.00628858,2.5128775e-05\n",
      "Iteration 20365: loss = 0.006288504,2.7283466e-05\n",
      "Iteration 20370: loss = 0.0062885527,3.6981208e-05\n",
      "Iteration 20375: loss = 0.006288338,9.33097e-05\n",
      "Iteration 20380: loss = 0.006288873,0.00047810187\n",
      "Iteration 20385: loss = 0.0062874667,0.0036174688\n",
      "Iteration 20390: loss = 0.006289778,0.029603906\n",
      "Iteration 20395: loss = 0.006307967,0.07669569\n",
      "Iteration 20400: loss = 0.006280638,0.009655271\n",
      "Iteration 20405: loss = 0.0062559177,0.0042228308\n",
      "Iteration 20410: loss = 0.006251503,0.0116694765\n",
      "Iteration 20415: loss = 0.0062619844,0.007551143\n",
      "Iteration 20420: loss = 0.006276598,0.0016255069\n",
      "Iteration 20425: loss = 0.0062884535,4.4577435e-05\n",
      "Iteration 20430: loss = 0.0062953024,0.0008814911\n",
      "Iteration 20435: loss = 0.006297145,0.0010218428\n",
      "Iteration 20440: loss = 0.006295573,0.0003320959\n",
      "Iteration 20445: loss = 0.0062926696,4.7867583e-05\n",
      "Iteration 20450: loss = 0.0062904838,0.00018795472\n",
      "Iteration 20455: loss = 0.0062896437,0.00014853761\n",
      "Iteration 20460: loss = 0.0062896777,2.8700404e-05\n",
      "Iteration 20465: loss = 0.0062896456,5.130519e-05\n",
      "Iteration 20470: loss = 0.006288757,4.8753504e-05\n",
      "Iteration 20475: loss = 0.006287577,2.5624238e-05\n",
      "Iteration 20480: loss = 0.0062872004,3.6249192e-05\n",
      "Iteration 20485: loss = 0.006287782,2.625748e-05\n",
      "Iteration 20490: loss = 0.006288597,2.5932419e-05\n",
      "Iteration 20495: loss = 0.006288783,2.4697156e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20500: loss = 0.0062884614,2.4312907e-05\n",
      "Iteration 20505: loss = 0.006288282,2.3997985e-05\n",
      "Iteration 20510: loss = 0.006288253,2.384447e-05\n",
      "Iteration 20515: loss = 0.006288149,2.3707213e-05\n",
      "Iteration 20520: loss = 0.0062880763,2.3843859e-05\n",
      "Iteration 20525: loss = 0.006288232,2.3396333e-05\n",
      "Iteration 20530: loss = 0.006288236,2.341321e-05\n",
      "Iteration 20535: loss = 0.0062881405,2.3380713e-05\n",
      "Iteration 20540: loss = 0.006288053,2.3432247e-05\n",
      "Iteration 20545: loss = 0.006288091,2.337217e-05\n",
      "Iteration 20550: loss = 0.0062879566,2.354477e-05\n",
      "Iteration 20555: loss = 0.0062881745,2.3503635e-05\n",
      "Iteration 20560: loss = 0.006287929,2.3973982e-05\n",
      "Iteration 20565: loss = 0.0062881163,2.4521029e-05\n",
      "Iteration 20570: loss = 0.006287643,2.8426533e-05\n",
      "Iteration 20575: loss = 0.0062886584,4.429427e-05\n",
      "Iteration 20580: loss = 0.0062863207,0.00014669242\n",
      "Iteration 20585: loss = 0.0062923003,0.0009051105\n",
      "Iteration 20590: loss = 0.0062753134,0.0075902087\n",
      "Iteration 20595: loss = 0.006320656,0.05431771\n",
      "Iteration 20600: loss = 0.006276623,0.0380574\n",
      "Iteration 20605: loss = 0.006259753,0.033429768\n",
      "Iteration 20610: loss = 0.006260171,0.0062129814\n",
      "Iteration 20615: loss = 0.0062740617,0.00051020435\n",
      "Iteration 20620: loss = 0.0062911813,0.0030230018\n",
      "Iteration 20625: loss = 0.006303637,0.003830821\n",
      "Iteration 20630: loss = 0.0063078585,0.0022155424\n",
      "Iteration 20635: loss = 0.006302163,0.00052950537\n",
      "Iteration 20640: loss = 0.0062912353,5.7677957e-05\n",
      "Iteration 20645: loss = 0.0062817973,0.00031522723\n",
      "Iteration 20650: loss = 0.0062785,0.00037132236\n",
      "Iteration 20655: loss = 0.00628237,0.00012567092\n",
      "Iteration 20660: loss = 0.0062888614,2.3908262e-05\n",
      "Iteration 20665: loss = 0.00629238,7.542563e-05\n",
      "Iteration 20670: loss = 0.0062910584,5.616141e-05\n",
      "Iteration 20675: loss = 0.006287726,2.3973658e-05\n",
      "Iteration 20680: loss = 0.0062862244,3.609994e-05\n",
      "Iteration 20685: loss = 0.0062871384,3.0828705e-05\n",
      "Iteration 20690: loss = 0.006288383,2.2623573e-05\n",
      "Iteration 20695: loss = 0.006288573,2.6019537e-05\n",
      "Iteration 20700: loss = 0.0062880465,2.2709997e-05\n",
      "Iteration 20705: loss = 0.006287553,2.4253102e-05\n",
      "Iteration 20710: loss = 0.0062875203,2.3252818e-05\n",
      "Iteration 20715: loss = 0.0062881024,2.2855631e-05\n",
      "Iteration 20720: loss = 0.006287998,2.2404207e-05\n",
      "Iteration 20725: loss = 0.006287482,2.3095237e-05\n",
      "Iteration 20730: loss = 0.0062877648,2.2472308e-05\n",
      "Iteration 20735: loss = 0.0062879096,2.239299e-05\n",
      "Iteration 20740: loss = 0.0062874896,2.2775195e-05\n",
      "Iteration 20745: loss = 0.006287817,2.2320983e-05\n",
      "Iteration 20750: loss = 0.006287576,2.2491347e-05\n",
      "Iteration 20755: loss = 0.0062877517,2.2322893e-05\n",
      "Iteration 20760: loss = 0.0062872414,2.300955e-05\n",
      "Iteration 20765: loss = 0.006288655,2.3185808e-05\n",
      "Iteration 20770: loss = 0.0062846025,4.067824e-05\n",
      "Iteration 20775: loss = 0.0062967367,0.0001630635\n",
      "Iteration 20780: loss = 0.0062576234,0.0016931279\n",
      "Iteration 20785: loss = 0.0063610286,0.010934718\n",
      "Iteration 20790: loss = 0.006312415,0.010697839\n",
      "Iteration 20795: loss = 0.0062321704,0.034998454\n",
      "Iteration 20800: loss = 0.006275061,0.01331754\n",
      "Iteration 20805: loss = 0.0062791444,0.007987845\n",
      "Iteration 20810: loss = 0.006262911,0.003617546\n",
      "Iteration 20815: loss = 0.0062681404,0.0037376825\n",
      "Iteration 20820: loss = 0.0062859445,0.0006153388\n",
      "Iteration 20825: loss = 0.006291954,0.0015978446\n",
      "Iteration 20830: loss = 0.0062852465,0.00016225647\n",
      "Iteration 20835: loss = 0.0062841014,0.00060655555\n",
      "Iteration 20840: loss = 0.0062901783,0.00012933183\n",
      "Iteration 20845: loss = 0.0062904526,0.00017011559\n",
      "Iteration 20850: loss = 0.006286531,0.00014853668\n",
      "Iteration 20855: loss = 0.0062879156,2.495261e-05\n",
      "Iteration 20860: loss = 0.006289178,8.353678e-05\n",
      "Iteration 20865: loss = 0.0062867813,5.4719105e-05\n",
      "Iteration 20870: loss = 0.0062876344,2.3174063e-05\n",
      "Iteration 20875: loss = 0.0062877084,2.8237122e-05\n",
      "Iteration 20880: loss = 0.0062866663,3.6063786e-05\n",
      "Iteration 20885: loss = 0.006287849,3.8556376e-05\n",
      "Iteration 20890: loss = 0.0062863906,4.8537062e-05\n",
      "Iteration 20895: loss = 0.006288435,7.5621116e-05\n",
      "Iteration 20900: loss = 0.0062852264,0.0001916103\n",
      "Iteration 20905: loss = 0.006291665,0.0007088195\n",
      "Iteration 20910: loss = 0.006277842,0.0036254416\n",
      "Iteration 20915: loss = 0.0063098376,0.019556401\n",
      "Iteration 20920: loss = 0.0062553207,0.05285885\n",
      "Iteration 20925: loss = 0.0062866746,0.0004264715\n",
      "Iteration 20930: loss = 0.006301198,0.016424196\n",
      "Iteration 20935: loss = 0.00629246,0.0030831548\n",
      "Iteration 20940: loss = 0.006281325,0.0017203279\n",
      "Iteration 20945: loss = 0.0062794327,0.0035929952\n",
      "Iteration 20950: loss = 0.006285492,0.00017098956\n",
      "Iteration 20955: loss = 0.0062911287,0.0009014554\n",
      "Iteration 20960: loss = 0.0062903133,0.000533813\n",
      "Iteration 20965: loss = 0.0062864763,0.000101992904\n",
      "Iteration 20970: loss = 0.0062855,0.00031680244\n",
      "Iteration 20975: loss = 0.0062878435,2.3837487e-05\n",
      "Iteration 20980: loss = 0.0062890276,0.0001402605\n",
      "Iteration 20985: loss = 0.0062874197,2.2676062e-05\n",
      "Iteration 20990: loss = 0.006286642,6.7334e-05\n",
      "Iteration 20995: loss = 0.0062876814,2.6876622e-05\n",
      "Iteration 21000: loss = 0.0062877345,3.2245916e-05\n",
      "Iteration 21005: loss = 0.0062868814,3.254438e-05\n",
      "Iteration 21010: loss = 0.006287322,2.1747226e-05\n",
      "Iteration 21015: loss = 0.0062874523,2.4130113e-05\n",
      "Iteration 21020: loss = 0.0062870034,2.55835e-05\n",
      "Iteration 21025: loss = 0.0062874197,2.3507839e-05\n",
      "Iteration 21030: loss = 0.006287074,2.2453842e-05\n",
      "Iteration 21035: loss = 0.006287213,2.1699212e-05\n",
      "Iteration 21040: loss = 0.006287091,2.1697164e-05\n",
      "Iteration 21045: loss = 0.00628721,2.1667662e-05\n",
      "Iteration 21050: loss = 0.0062870304,2.2309285e-05\n",
      "Iteration 21055: loss = 0.0062873177,2.4422694e-05\n",
      "Iteration 21060: loss = 0.006286522,3.9426974e-05\n",
      "Iteration 21065: loss = 0.006288462,0.00014333497\n",
      "Iteration 21070: loss = 0.006282924,0.0010868827\n",
      "Iteration 21075: loss = 0.0062999725,0.01039329\n",
      "Iteration 21080: loss = 0.0062541366,0.07794515\n",
      "Iteration 21085: loss = 0.006291846,0.012247117\n",
      "Iteration 21090: loss = 0.006304435,0.032591563\n",
      "Iteration 21095: loss = 0.0063026394,0.01996347\n",
      "Iteration 21100: loss = 0.006297551,0.0069959154\n",
      "Iteration 21105: loss = 0.0062912772,0.0011032459\n",
      "Iteration 21110: loss = 0.0062854816,4.01097e-05\n",
      "Iteration 21115: loss = 0.006282792,0.0006939239\n",
      "Iteration 21120: loss = 0.006282337,0.0011784853\n",
      "Iteration 21125: loss = 0.006284028,0.0009734053\n",
      "Iteration 21130: loss = 0.006286925,0.00044191437\n",
      "Iteration 21135: loss = 0.0062893494,8.712348e-05\n",
      "Iteration 21140: loss = 0.006290357,3.9818984e-05\n",
      "Iteration 21145: loss = 0.0062894174,9.666612e-05\n",
      "Iteration 21150: loss = 0.0062874462,8.699956e-05\n",
      "Iteration 21155: loss = 0.0062856805,4.0645147e-05\n",
      "Iteration 21160: loss = 0.006285245,2.8100829e-05\n",
      "Iteration 21165: loss = 0.0062863417,3.227292e-05\n",
      "Iteration 21170: loss = 0.006287625,2.7126356e-05\n",
      "Iteration 21175: loss = 0.006288055,2.1570884e-05\n",
      "Iteration 21180: loss = 0.0062873405,2.2474562e-05\n",
      "Iteration 21185: loss = 0.0062863915,2.2868026e-05\n",
      "Iteration 21190: loss = 0.0062863007,2.204116e-05\n",
      "Iteration 21195: loss = 0.0062869643,2.128033e-05\n",
      "Iteration 21200: loss = 0.0062872753,2.0780055e-05\n",
      "Iteration 21205: loss = 0.006286849,2.0922407e-05\n",
      "Iteration 21210: loss = 0.006286549,2.1276936e-05\n",
      "Iteration 21215: loss = 0.0062867994,2.082159e-05\n",
      "Iteration 21220: loss = 0.006286919,2.0728006e-05\n",
      "Iteration 21225: loss = 0.0062866323,2.093772e-05\n",
      "Iteration 21230: loss = 0.006286792,2.0833513e-05\n",
      "Iteration 21235: loss = 0.0062866006,2.0927853e-05\n",
      "Iteration 21240: loss = 0.006286694,2.0724856e-05\n",
      "Iteration 21245: loss = 0.0062867296,2.069035e-05\n",
      "Iteration 21250: loss = 0.0062865545,2.0780324e-05\n",
      "Iteration 21255: loss = 0.006286643,2.0664855e-05\n",
      "Iteration 21260: loss = 0.0062866188,2.0645968e-05\n",
      "Iteration 21265: loss = 0.006286551,2.0649968e-05\n",
      "Iteration 21270: loss = 0.006286589,2.0580608e-05\n",
      "Iteration 21275: loss = 0.006286537,2.0634192e-05\n",
      "Iteration 21280: loss = 0.0062863776,2.1163192e-05\n",
      "Iteration 21285: loss = 0.0062870034,2.2369804e-05\n",
      "Iteration 21290: loss = 0.006284918,3.599195e-05\n",
      "Iteration 21295: loss = 0.00629159,0.00012197308\n",
      "Iteration 21300: loss = 0.0062688044,0.0010589752\n",
      "Iteration 21305: loss = 0.006347528,0.01077283\n",
      "Iteration 21310: loss = 0.0061812946,0.06039626\n",
      "Iteration 21315: loss = 0.006193382,0.043420408\n",
      "Iteration 21320: loss = 0.006219384,0.027071703\n",
      "Iteration 21325: loss = 0.0062538576,0.0022611213\n",
      "Iteration 21330: loss = 0.0063029244,0.0041045076\n",
      "Iteration 21335: loss = 0.006317196,0.0063371668\n",
      "Iteration 21340: loss = 0.006291207,0.0009646068\n",
      "Iteration 21345: loss = 0.0062705236,0.00068903883\n",
      "Iteration 21350: loss = 0.006278397,0.0011069663\n",
      "Iteration 21355: loss = 0.0062951907,0.00037738407\n",
      "Iteration 21360: loss = 0.006297696,0.00018072722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21365: loss = 0.0062880083,0.00020846205\n",
      "Iteration 21370: loss = 0.0062814876,0.000111916284\n",
      "Iteration 21375: loss = 0.0062834346,7.098638e-05\n",
      "Iteration 21380: loss = 0.0062881783,6.334655e-05\n",
      "Iteration 21385: loss = 0.006289818,2.7739838e-05\n",
      "Iteration 21390: loss = 0.0062884274,4.2098756e-05\n",
      "Iteration 21395: loss = 0.0062870197,2.1937258e-05\n",
      "Iteration 21400: loss = 0.0062864553,3.0441814e-05\n",
      "Iteration 21405: loss = 0.0062861666,2.3325727e-05\n",
      "Iteration 21410: loss = 0.0062866244,2.4045228e-05\n",
      "Iteration 21415: loss = 0.0062874616,2.1023501e-05\n",
      "Iteration 21420: loss = 0.006287541,2.1398804e-05\n",
      "Iteration 21425: loss = 0.0062869634,2.173465e-05\n",
      "Iteration 21430: loss = 0.0062868497,2.116728e-05\n",
      "Iteration 21435: loss = 0.0062866793,2.129257e-05\n",
      "Iteration 21440: loss = 0.006286591,2.1351301e-05\n",
      "Iteration 21445: loss = 0.0062866933,2.1070713e-05\n",
      "Iteration 21450: loss = 0.006286793,2.0959009e-05\n",
      "Iteration 21455: loss = 0.0062867175,2.1087246e-05\n",
      "Iteration 21460: loss = 0.0062868823,2.1191321e-05\n",
      "Iteration 21465: loss = 0.006286534,2.2179298e-05\n",
      "Iteration 21470: loss = 0.006287042,2.4795074e-05\n",
      "Iteration 21475: loss = 0.006285818,4.2237007e-05\n",
      "Iteration 21480: loss = 0.0062885988,0.00015262004\n",
      "Iteration 21485: loss = 0.006280826,0.0010993142\n",
      "Iteration 21490: loss = 0.006303986,0.009903268\n",
      "Iteration 21495: loss = 0.00624289,0.07041965\n",
      "Iteration 21500: loss = 0.006300892,0.014836587\n",
      "Iteration 21505: loss = 0.006310296,0.032713346\n",
      "Iteration 21510: loss = 0.0063032745,0.015717136\n",
      "Iteration 21515: loss = 0.006294903,0.0030001358\n",
      "Iteration 21520: loss = 0.006287861,2.27501e-05\n",
      "Iteration 21525: loss = 0.006282464,0.0011775689\n",
      "Iteration 21530: loss = 0.0062806606,0.0020331466\n",
      "Iteration 21535: loss = 0.0062812003,0.0014898499\n",
      "Iteration 21540: loss = 0.0062832464,0.0004925475\n",
      "Iteration 21545: loss = 0.0062860698,3.1332616e-05\n",
      "Iteration 21550: loss = 0.006287932,0.00011495637\n",
      "Iteration 21555: loss = 0.0062885657,0.00020455444\n",
      "Iteration 21560: loss = 0.0062879366,0.0001017193\n",
      "Iteration 21565: loss = 0.006286768,2.1344069e-05\n",
      "Iteration 21570: loss = 0.006285917,4.2882708e-05\n",
      "Iteration 21575: loss = 0.006285776,4.601549e-05\n",
      "Iteration 21580: loss = 0.006286269,2.2192426e-05\n",
      "Iteration 21585: loss = 0.0062867193,2.4725774e-05\n",
      "Iteration 21590: loss = 0.006286709,2.5107318e-05\n",
      "Iteration 21595: loss = 0.006286409,2.0291081e-05\n",
      "Iteration 21600: loss = 0.0062861964,2.2286738e-05\n",
      "Iteration 21605: loss = 0.0062862835,2.072058e-05\n",
      "Iteration 21610: loss = 0.006286453,2.0598452e-05\n",
      "Iteration 21615: loss = 0.006286402,2.0388878e-05\n",
      "Iteration 21620: loss = 0.006286273,2.0327596e-05\n",
      "Iteration 21625: loss = 0.006286261,2.0246847e-05\n",
      "Iteration 21630: loss = 0.0062863254,2.022652e-05\n",
      "Iteration 21635: loss = 0.0062862462,2.010023e-05\n",
      "Iteration 21640: loss = 0.006286211,2.016727e-05\n",
      "Iteration 21645: loss = 0.0062862,2.0053754e-05\n",
      "Iteration 21650: loss = 0.0062862155,2.0043466e-05\n",
      "Iteration 21655: loss = 0.006286166,2.0016792e-05\n",
      "Iteration 21660: loss = 0.006286133,2.0028503e-05\n",
      "Iteration 21665: loss = 0.0062862006,2.0029636e-05\n",
      "Iteration 21670: loss = 0.006286064,2.0285166e-05\n",
      "Iteration 21675: loss = 0.0062862006,2.022352e-05\n",
      "Iteration 21680: loss = 0.0062859897,2.0619418e-05\n",
      "Iteration 21685: loss = 0.006286262,2.1463946e-05\n",
      "Iteration 21690: loss = 0.0062856968,2.7011876e-05\n",
      "Iteration 21695: loss = 0.006286872,5.5635137e-05\n",
      "Iteration 21700: loss = 0.006283937,0.00026135595\n",
      "Iteration 21705: loss = 0.006291958,0.0019040017\n",
      "Iteration 21710: loss = 0.0062687187,0.01674073\n",
      "Iteration 21715: loss = 0.006322985,0.08129236\n",
      "Iteration 21720: loss = 0.0062880064,0.00014176249\n",
      "Iteration 21725: loss = 0.006273544,0.017941428\n",
      "Iteration 21730: loss = 0.006272117,0.016833365\n",
      "Iteration 21735: loss = 0.0062775183,0.006318166\n",
      "Iteration 21740: loss = 0.006284284,0.0006053129\n",
      "Iteration 21745: loss = 0.006289061,0.00030060136\n",
      "Iteration 21750: loss = 0.0062914896,0.0013447499\n",
      "Iteration 21755: loss = 0.006291105,0.0012824787\n",
      "Iteration 21760: loss = 0.0062889555,0.00044959248\n",
      "Iteration 21765: loss = 0.0062862816,2.4690817e-05\n",
      "Iteration 21770: loss = 0.006284503,0.00013289702\n",
      "Iteration 21775: loss = 0.006284244,0.00018937614\n",
      "Iteration 21780: loss = 0.006285066,6.268532e-05\n",
      "Iteration 21785: loss = 0.006286278,2.2794453e-05\n",
      "Iteration 21790: loss = 0.00628682,5.291538e-05\n",
      "Iteration 21795: loss = 0.006286442,3.1187694e-05\n",
      "Iteration 21800: loss = 0.00628583,2.0849222e-05\n",
      "Iteration 21805: loss = 0.006285559,2.8231887e-05\n",
      "Iteration 21810: loss = 0.006285778,2.0428744e-05\n",
      "Iteration 21815: loss = 0.0062860805,2.113582e-05\n",
      "Iteration 21820: loss = 0.006285993,2.0253234e-05\n",
      "Iteration 21825: loss = 0.0062857787,1.9711455e-05\n",
      "Iteration 21830: loss = 0.0062857415,1.9893625e-05\n",
      "Iteration 21835: loss = 0.0062858984,1.9448315e-05\n",
      "Iteration 21840: loss = 0.006285835,1.9351199e-05\n",
      "Iteration 21845: loss = 0.0062856884,1.9515322e-05\n",
      "Iteration 21850: loss = 0.006285757,1.9262112e-05\n",
      "Iteration 21855: loss = 0.006285722,1.9262463e-05\n",
      "Iteration 21860: loss = 0.006285665,1.927499e-05\n",
      "Iteration 21865: loss = 0.006285675,1.9222893e-05\n",
      "Iteration 21870: loss = 0.006285666,1.9196415e-05\n",
      "Iteration 21875: loss = 0.0062856884,1.9138439e-05\n",
      "Iteration 21880: loss = 0.006285654,1.9134402e-05\n",
      "Iteration 21885: loss = 0.0062856046,1.9151234e-05\n",
      "Iteration 21890: loss = 0.006285584,1.9130825e-05\n",
      "Iteration 21895: loss = 0.0062856353,1.9069097e-05\n",
      "Iteration 21900: loss = 0.0062854686,1.9500627e-05\n",
      "Iteration 21905: loss = 0.0062857405,2.0896281e-05\n",
      "Iteration 21910: loss = 0.006285054,3.183227e-05\n",
      "Iteration 21915: loss = 0.006286816,0.00010690821\n",
      "Iteration 21920: loss = 0.0062816255,0.00081132184\n",
      "Iteration 21925: loss = 0.0062980684,0.008118364\n",
      "Iteration 21930: loss = 0.0062510786,0.06967878\n",
      "Iteration 21935: loss = 0.0062954016,0.024523716\n",
      "Iteration 21940: loss = 0.006305814,0.038205728\n",
      "Iteration 21945: loss = 0.006304225,0.019984875\n",
      "Iteration 21950: loss = 0.0063005215,0.006652022\n",
      "Iteration 21955: loss = 0.0062955567,0.0011678442\n",
      "Iteration 21960: loss = 0.0062901317,4.36313e-05\n",
      "Iteration 21965: loss = 0.0062863794,0.00047740943\n",
      "Iteration 21970: loss = 0.0062836683,0.00096062483\n",
      "Iteration 21975: loss = 0.006282491,0.00097049295\n",
      "Iteration 21980: loss = 0.006282803,0.0006054963\n",
      "Iteration 21985: loss = 0.006283961,0.00021412705\n",
      "Iteration 21990: loss = 0.006285431,2.9954786e-05\n",
      "Iteration 21995: loss = 0.006286398,3.9293263e-05\n",
      "Iteration 22000: loss = 0.006286739,8.175852e-05\n",
      "Iteration 22005: loss = 0.0062863063,6.527992e-05\n",
      "Iteration 22010: loss = 0.00628555,2.7088186e-05\n",
      "Iteration 22015: loss = 0.00628507,2.0858526e-05\n",
      "Iteration 22020: loss = 0.00628503,2.8843286e-05\n",
      "Iteration 22025: loss = 0.006285416,2.4125588e-05\n",
      "Iteration 22030: loss = 0.0062857666,1.8506174e-05\n",
      "Iteration 22035: loss = 0.0062856637,2.0382846e-05\n",
      "Iteration 22040: loss = 0.0062853154,1.9729016e-05\n",
      "Iteration 22045: loss = 0.0062851985,1.893824e-05\n",
      "Iteration 22050: loss = 0.0062853997,1.9031766e-05\n",
      "Iteration 22055: loss = 0.0062854737,1.855346e-05\n",
      "Iteration 22060: loss = 0.006285355,1.8745617e-05\n",
      "Iteration 22065: loss = 0.006285203,1.8731898e-05\n",
      "Iteration 22070: loss = 0.0062852614,1.8676514e-05\n",
      "Iteration 22075: loss = 0.0062853857,1.8462215e-05\n",
      "Iteration 22080: loss = 0.0062852283,1.8579723e-05\n",
      "Iteration 22085: loss = 0.0062851976,1.8561474e-05\n",
      "Iteration 22090: loss = 0.0062852134,1.850563e-05\n",
      "Iteration 22095: loss = 0.006285204,1.8483164e-05\n",
      "Iteration 22100: loss = 0.0062851957,1.8465498e-05\n",
      "Iteration 22105: loss = 0.006285155,1.8478622e-05\n",
      "Iteration 22110: loss = 0.006285167,1.8413572e-05\n",
      "Iteration 22115: loss = 0.006285113,1.8437004e-05\n",
      "Iteration 22120: loss = 0.0062850285,1.862524e-05\n",
      "Iteration 22125: loss = 0.0062852874,1.8643219e-05\n",
      "Iteration 22130: loss = 0.0062847254,1.9725707e-05\n",
      "Iteration 22135: loss = 0.006285778,2.0278176e-05\n",
      "Iteration 22140: loss = 0.0062833764,3.018372e-05\n",
      "Iteration 22145: loss = 0.006289476,6.813287e-05\n",
      "Iteration 22150: loss = 0.006271972,0.00042598206\n",
      "Iteration 22155: loss = 0.0063254866,0.0033638629\n",
      "Iteration 22160: loss = 0.0061903014,0.022294391\n",
      "Iteration 22165: loss = 0.0062946067,0.046428427\n",
      "Iteration 22170: loss = 0.006288173,0.010002634\n",
      "Iteration 22175: loss = 0.0062584956,0.015057753\n",
      "Iteration 22180: loss = 0.006255191,0.0014428941\n",
      "Iteration 22185: loss = 0.006262951,0.005922212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22190: loss = 0.006272925,0.0032715416\n",
      "Iteration 22195: loss = 0.006281248,7.199643e-05\n",
      "Iteration 22200: loss = 0.006289277,0.0012851605\n",
      "Iteration 22205: loss = 0.0062953527,0.00050536136\n",
      "Iteration 22210: loss = 0.0062963837,0.000269428\n",
      "Iteration 22215: loss = 0.006290576,0.0002550699\n",
      "Iteration 22220: loss = 0.0062824935,4.1384515e-05\n",
      "Iteration 22225: loss = 0.006279653,0.00015210311\n",
      "Iteration 22230: loss = 0.0062824753,3.9715484e-05\n",
      "Iteration 22235: loss = 0.0062850206,6.246279e-05\n",
      "Iteration 22240: loss = 0.006285058,2.160261e-05\n",
      "Iteration 22245: loss = 0.0062855915,3.103476e-05\n",
      "Iteration 22250: loss = 0.0062869843,2.419421e-05\n",
      "Iteration 22255: loss = 0.0062865852,1.9649138e-05\n",
      "Iteration 22260: loss = 0.0062857703,2.3304607e-05\n",
      "Iteration 22265: loss = 0.006286101,2.0527033e-05\n",
      "Iteration 22270: loss = 0.0062854905,1.88132e-05\n",
      "Iteration 22275: loss = 0.0062852185,1.9347875e-05\n",
      "Iteration 22280: loss = 0.0062854383,1.9880616e-05\n",
      "Iteration 22285: loss = 0.0062848744,2.1698132e-05\n",
      "Iteration 22290: loss = 0.006285542,2.3138376e-05\n",
      "Iteration 22295: loss = 0.006284497,3.068473e-05\n",
      "Iteration 22300: loss = 0.0062862816,5.5289383e-05\n",
      "Iteration 22305: loss = 0.00628267,0.00019400004\n",
      "Iteration 22310: loss = 0.006291071,0.0010159819\n",
      "Iteration 22315: loss = 0.0062695406,0.0070230053\n",
      "Iteration 22320: loss = 0.0063226987,0.042042717\n",
      "Iteration 22325: loss = 0.006257243,0.037662264\n",
      "Iteration 22330: loss = 0.0062661315,0.019782053\n",
      "Iteration 22335: loss = 0.0062861093,2.0052823e-05\n",
      "Iteration 22340: loss = 0.006297054,0.0059254835\n",
      "Iteration 22345: loss = 0.0062965355,0.0056436476\n",
      "Iteration 22350: loss = 0.0062897685,0.00086246536\n",
      "Iteration 22355: loss = 0.006283499,0.000313201\n",
      "Iteration 22360: loss = 0.0062807663,0.0012225368\n",
      "Iteration 22365: loss = 0.0062823133,0.0004553657\n",
      "Iteration 22370: loss = 0.0062856753,3.364551e-05\n",
      "Iteration 22375: loss = 0.0062871817,0.00027345543\n",
      "Iteration 22380: loss = 0.0062861503,9.3953e-05\n",
      "Iteration 22385: loss = 0.0062843463,4.038061e-05\n",
      "Iteration 22390: loss = 0.0062839054,8.259289e-05\n",
      "Iteration 22395: loss = 0.0062849056,1.8727316e-05\n",
      "Iteration 22400: loss = 0.0062856064,4.0379746e-05\n",
      "Iteration 22405: loss = 0.0062851883,2.019047e-05\n",
      "Iteration 22410: loss = 0.0062845834,2.6916372e-05\n",
      "Iteration 22415: loss = 0.006284848,1.9113424e-05\n",
      "Iteration 22420: loss = 0.006285213,2.1686292e-05\n",
      "Iteration 22425: loss = 0.006284921,1.8266594e-05\n",
      "Iteration 22430: loss = 0.006284756,1.9929972e-05\n",
      "Iteration 22435: loss = 0.006285055,1.8986231e-05\n",
      "Iteration 22440: loss = 0.006284911,1.8148416e-05\n",
      "Iteration 22445: loss = 0.0062847794,1.8759754e-05\n",
      "Iteration 22450: loss = 0.0062849936,1.8733812e-05\n",
      "Iteration 22455: loss = 0.0062847305,1.885333e-05\n",
      "Iteration 22460: loss = 0.0062849387,1.8622919e-05\n",
      "Iteration 22465: loss = 0.0062846486,1.9357907e-05\n",
      "Iteration 22470: loss = 0.0062850253,2.0495114e-05\n",
      "Iteration 22475: loss = 0.0062844246,2.58183e-05\n",
      "Iteration 22480: loss = 0.0062854774,4.3837943e-05\n",
      "Iteration 22485: loss = 0.006283239,0.00014370446\n",
      "Iteration 22490: loss = 0.0062884632,0.00076290854\n",
      "Iteration 22495: loss = 0.0062748077,0.0054479605\n",
      "Iteration 22500: loss = 0.006310445,0.03653921\n",
      "Iteration 22505: loss = 0.006259039,0.053249445\n",
      "Iteration 22510: loss = 0.006269746,0.016365359\n",
      "Iteration 22515: loss = 0.0062873396,0.00041242936\n",
      "Iteration 22520: loss = 0.006296271,0.007624193\n",
      "Iteration 22525: loss = 0.0062949494,0.00627216\n",
      "Iteration 22530: loss = 0.0062879617,0.0011396552\n",
      "Iteration 22535: loss = 0.006281402,0.00017755649\n",
      "Iteration 22540: loss = 0.006278192,0.0011705653\n",
      "Iteration 22545: loss = 0.006279455,0.0007302388\n",
      "Iteration 22550: loss = 0.0062832627,3.931728e-05\n",
      "Iteration 22555: loss = 0.0062860795,0.00017752018\n",
      "Iteration 22560: loss = 0.00628645,0.00019134663\n",
      "Iteration 22565: loss = 0.0062849796,2.1374606e-05\n",
      "Iteration 22570: loss = 0.0062837764,6.772831e-05\n",
      "Iteration 22575: loss = 0.006284075,4.6112313e-05\n",
      "Iteration 22580: loss = 0.006285101,2.0930374e-05\n",
      "Iteration 22585: loss = 0.0062853303,3.2685457e-05\n",
      "Iteration 22590: loss = 0.0062845894,1.7754834e-05\n",
      "Iteration 22595: loss = 0.0062841605,2.3752267e-05\n",
      "Iteration 22600: loss = 0.0062845103,1.774442e-05\n",
      "Iteration 22605: loss = 0.0062847063,1.9558e-05\n",
      "Iteration 22610: loss = 0.006284496,1.7743483e-05\n",
      "Iteration 22615: loss = 0.006284401,1.8475599e-05\n",
      "Iteration 22620: loss = 0.006284566,1.781513e-05\n",
      "Iteration 22625: loss = 0.0062844604,1.7654684e-05\n",
      "Iteration 22630: loss = 0.0062843356,1.8010596e-05\n",
      "Iteration 22635: loss = 0.0062844777,1.771568e-05\n",
      "Iteration 22640: loss = 0.0062844227,1.7570568e-05\n",
      "Iteration 22645: loss = 0.0062843747,1.7585453e-05\n",
      "Iteration 22650: loss = 0.006284365,1.7578044e-05\n",
      "Iteration 22655: loss = 0.006284364,1.7543192e-05\n",
      "Iteration 22660: loss = 0.0062843612,1.7502598e-05\n",
      "Iteration 22665: loss = 0.006284365,1.7487564e-05\n",
      "Iteration 22670: loss = 0.006284155,1.8080329e-05\n",
      "Iteration 22675: loss = 0.0062845307,1.9711208e-05\n",
      "Iteration 22680: loss = 0.0062837377,3.3738248e-05\n",
      "Iteration 22685: loss = 0.0062859883,0.00014500017\n",
      "Iteration 22690: loss = 0.006278746,0.0013126802\n",
      "Iteration 22695: loss = 0.0063026086,0.0142993275\n",
      "Iteration 22700: loss = 0.006241847,0.09898135\n",
      "Iteration 22705: loss = 0.0062658563,0.00053103745\n",
      "Iteration 22710: loss = 0.006286461,0.013099534\n",
      "Iteration 22715: loss = 0.006299119,0.015820848\n",
      "Iteration 22720: loss = 0.0063070175,0.011566408\n",
      "Iteration 22725: loss = 0.0063103233,0.006873751\n",
      "Iteration 22730: loss = 0.0063093654,0.0035050032\n",
      "Iteration 22735: loss = 0.0063050543,0.0014828575\n",
      "Iteration 22740: loss = 0.006298486,0.00045657856\n",
      "Iteration 22745: loss = 0.006291048,7.1813956e-05\n",
      "Iteration 22750: loss = 0.0062840316,4.5506426e-05\n",
      "Iteration 22755: loss = 0.006279245,0.00013690164\n",
      "Iteration 22760: loss = 0.0062774173,0.00018300037\n",
      "Iteration 22765: loss = 0.006278276,0.00014270195\n",
      "Iteration 22770: loss = 0.006280996,6.8337635e-05\n",
      "Iteration 22775: loss = 0.0062838593,2.3692006e-05\n",
      "Iteration 22780: loss = 0.006285655,1.8674751e-05\n",
      "Iteration 22785: loss = 0.006286001,2.6134758e-05\n",
      "Iteration 22790: loss = 0.006285472,2.5397478e-05\n",
      "Iteration 22795: loss = 0.006284727,1.8540157e-05\n",
      "Iteration 22800: loss = 0.0062841815,1.7595621e-05\n",
      "Iteration 22805: loss = 0.0062838625,1.9438585e-05\n",
      "Iteration 22810: loss = 0.00628384,1.8508728e-05\n",
      "Iteration 22815: loss = 0.006284168,1.7302273e-05\n",
      "Iteration 22820: loss = 0.0062845643,1.7315826e-05\n",
      "Iteration 22825: loss = 0.0062845214,1.711754e-05\n",
      "Iteration 22830: loss = 0.006284101,1.7312417e-05\n",
      "Iteration 22835: loss = 0.006283972,1.7565071e-05\n",
      "Iteration 22840: loss = 0.0062841917,1.7104006e-05\n",
      "Iteration 22845: loss = 0.0062843147,1.7088694e-05\n",
      "Iteration 22850: loss = 0.006284131,1.7113598e-05\n",
      "Iteration 22855: loss = 0.006284054,1.7168273e-05\n",
      "Iteration 22860: loss = 0.0062841247,1.70568e-05\n",
      "Iteration 22865: loss = 0.006284094,1.7058723e-05\n",
      "Iteration 22870: loss = 0.00628409,1.7024853e-05\n",
      "Iteration 22875: loss = 0.006284062,1.7029155e-05\n",
      "Iteration 22880: loss = 0.006283989,1.7083445e-05\n",
      "Iteration 22885: loss = 0.0062840716,1.6968395e-05\n",
      "Iteration 22890: loss = 0.006283968,1.7061495e-05\n",
      "Iteration 22895: loss = 0.0062840604,1.713077e-05\n",
      "Iteration 22900: loss = 0.00628352,1.8497492e-05\n",
      "Iteration 22905: loss = 0.006285817,2.3107797e-05\n",
      "Iteration 22910: loss = 0.0062773447,0.00011139224\n",
      "Iteration 22915: loss = 0.0063081826,0.0011225381\n",
      "Iteration 22920: loss = 0.0062124445,0.01091114\n",
      "Iteration 22925: loss = 0.0062773596,0.0112275705\n",
      "Iteration 22930: loss = 0.006349424,0.028994935\n",
      "Iteration 22935: loss = 0.006301876,0.0082768975\n",
      "Iteration 22940: loss = 0.0062918277,0.005846958\n",
      "Iteration 22945: loss = 0.006305707,0.003317847\n",
      "Iteration 22950: loss = 0.006294048,0.0016421779\n",
      "Iteration 22955: loss = 0.0062748953,0.001434329\n",
      "Iteration 22960: loss = 0.0062754056,0.00046528172\n",
      "Iteration 22965: loss = 0.0062829223,0.0008324954\n",
      "Iteration 22970: loss = 0.006278824,8.839933e-05\n",
      "Iteration 22975: loss = 0.0062757446,0.00040733253\n",
      "Iteration 22980: loss = 0.0062816455,0.00013093193\n",
      "Iteration 22985: loss = 0.0062817526,5.848579e-05\n",
      "Iteration 22990: loss = 0.0062799137,0.00012800981\n",
      "Iteration 22995: loss = 0.006283654,8.397387e-05\n",
      "Iteration 23000: loss = 0.0062820143,3.823171e-05\n",
      "Iteration 23005: loss = 0.0062831677,2.1646914e-05\n",
      "Iteration 23010: loss = 0.0062834225,2.0393141e-05\n",
      "Iteration 23015: loss = 0.006283067,2.3934512e-05\n",
      "Iteration 23020: loss = 0.0062843,3.269307e-05\n",
      "Iteration 23025: loss = 0.0062824064,7.59785e-05\n",
      "Iteration 23030: loss = 0.006286712,0.00027596732\n",
      "Iteration 23035: loss = 0.0062770457,0.0015404726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23040: loss = 0.0063019223,0.010160617\n",
      "Iteration 23045: loss = 0.0062462315,0.051908437\n",
      "Iteration 23050: loss = 0.0062994654,0.016170142\n",
      "Iteration 23055: loss = 0.006303502,0.022629008\n",
      "Iteration 23060: loss = 0.0062879436,0.0015429553\n",
      "Iteration 23065: loss = 0.0062757856,0.002709492\n",
      "Iteration 23070: loss = 0.0062733437,0.0050401203\n",
      "Iteration 23075: loss = 0.006278222,0.0012441474\n",
      "Iteration 23080: loss = 0.0062849987,0.00014525726\n",
      "Iteration 23085: loss = 0.0062879287,0.001013489\n",
      "Iteration 23090: loss = 0.0062861242,0.00037057957\n",
      "Iteration 23095: loss = 0.0062828176,4.9753296e-05\n",
      "Iteration 23100: loss = 0.006281531,0.00026102702\n",
      "Iteration 23105: loss = 0.006282982,5.019412e-05\n",
      "Iteration 23110: loss = 0.006284805,6.545422e-05\n",
      "Iteration 23115: loss = 0.006284781,5.4984346e-05\n",
      "Iteration 23120: loss = 0.006283529,2.4320227e-05\n",
      "Iteration 23125: loss = 0.00628332,3.5478966e-05\n",
      "Iteration 23130: loss = 0.0062841014,1.8658588e-05\n",
      "Iteration 23135: loss = 0.0062842513,2.276546e-05\n",
      "Iteration 23140: loss = 0.006283691,1.9018835e-05\n",
      "Iteration 23145: loss = 0.006283685,1.8405908e-05\n",
      "Iteration 23150: loss = 0.0062840055,1.866178e-05\n",
      "Iteration 23155: loss = 0.006283766,1.7085766e-05\n",
      "Iteration 23160: loss = 0.0062836893,1.747624e-05\n",
      "Iteration 23165: loss = 0.00628384,1.730678e-05\n",
      "Iteration 23170: loss = 0.0062836795,1.712322e-05\n",
      "Iteration 23175: loss = 0.0062837233,1.6956856e-05\n",
      "Iteration 23180: loss = 0.006283726,1.6937473e-05\n",
      "Iteration 23185: loss = 0.0062836916,1.6941094e-05\n",
      "Iteration 23190: loss = 0.0062837196,1.6886706e-05\n",
      "Iteration 23195: loss = 0.006283675,1.6888891e-05\n",
      "Iteration 23200: loss = 0.006283708,1.6855118e-05\n",
      "Iteration 23205: loss = 0.00628349,1.7508588e-05\n",
      "Iteration 23210: loss = 0.006283873,2.0307012e-05\n",
      "Iteration 23215: loss = 0.0062828767,4.699025e-05\n",
      "Iteration 23220: loss = 0.0062859226,0.0002914922\n",
      "Iteration 23225: loss = 0.006275736,0.0031797667\n",
      "Iteration 23230: loss = 0.006310155,0.036109377\n",
      "Iteration 23235: loss = 0.006249147,0.09669954\n",
      "Iteration 23240: loss = 0.0062569412,0.039789055\n",
      "Iteration 23245: loss = 0.0062675346,0.00900308\n",
      "Iteration 23250: loss = 0.0062759393,0.00119569\n",
      "Iteration 23255: loss = 0.0062819817,2.5810868e-05\n",
      "Iteration 23260: loss = 0.006285609,0.00030637032\n",
      "Iteration 23265: loss = 0.0062882123,0.00070700207\n",
      "Iteration 23270: loss = 0.006289032,0.0009225\n",
      "Iteration 23275: loss = 0.0062885224,0.00091520685\n",
      "Iteration 23280: loss = 0.006287416,0.00073528604\n",
      "Iteration 23285: loss = 0.0062856344,0.00048180253\n",
      "Iteration 23290: loss = 0.0062838304,0.00024626157\n",
      "Iteration 23295: loss = 0.006282384,9.273152e-05\n",
      "Iteration 23300: loss = 0.0062816627,3.0904725e-05\n",
      "Iteration 23305: loss = 0.0062817032,2.7198925e-05\n",
      "Iteration 23310: loss = 0.006282449,3.705119e-05\n",
      "Iteration 23315: loss = 0.006283499,3.601608e-05\n",
      "Iteration 23320: loss = 0.006284297,2.5584053e-05\n",
      "Iteration 23325: loss = 0.006284525,1.7805793e-05\n",
      "Iteration 23330: loss = 0.00628411,1.7055683e-05\n",
      "Iteration 23335: loss = 0.006283418,1.8565215e-05\n",
      "Iteration 23340: loss = 0.006282978,1.8368795e-05\n",
      "Iteration 23345: loss = 0.0062830686,1.7234008e-05\n",
      "Iteration 23350: loss = 0.0062834565,1.6779257e-05\n",
      "Iteration 23355: loss = 0.006283685,1.6664042e-05\n",
      "Iteration 23360: loss = 0.0062835347,1.6489359e-05\n",
      "Iteration 23365: loss = 0.0062833033,1.6647153e-05\n",
      "Iteration 23370: loss = 0.0062832865,1.6697843e-05\n",
      "Iteration 23375: loss = 0.006283337,1.6543727e-05\n",
      "Iteration 23380: loss = 0.006283498,1.6374623e-05\n",
      "Iteration 23385: loss = 0.006283254,1.6611988e-05\n",
      "Iteration 23390: loss = 0.006283252,1.6523933e-05\n",
      "Iteration 23395: loss = 0.0062833596,1.6407295e-05\n",
      "Iteration 23400: loss = 0.006283311,1.6400896e-05\n",
      "Iteration 23405: loss = 0.00628322,1.6476122e-05\n",
      "Iteration 23410: loss = 0.006283242,1.643221e-05\n",
      "Iteration 23415: loss = 0.006283267,1.6369659e-05\n",
      "Iteration 23420: loss = 0.006283228,1.6376362e-05\n",
      "Iteration 23425: loss = 0.006283104,1.6593063e-05\n",
      "Iteration 23430: loss = 0.0062834024,1.6329417e-05\n",
      "Iteration 23435: loss = 0.0062828865,1.6845916e-05\n",
      "Iteration 23440: loss = 0.0062835454,1.613661e-05\n",
      "Iteration 23445: loss = 0.0062826797,1.717828e-05\n",
      "Iteration 23450: loss = 0.0062839533,1.6563587e-05\n",
      "Iteration 23455: loss = 0.006281426,2.228662e-05\n",
      "Iteration 23460: loss = 0.006287329,3.6466943e-05\n",
      "Iteration 23465: loss = 0.0062717195,0.00020574177\n",
      "Iteration 23470: loss = 0.0063160476,0.001459995\n",
      "Iteration 23475: loss = 0.0062077665,0.008291035\n",
      "Iteration 23480: loss = 0.0063165817,0.010228353\n",
      "Iteration 23485: loss = 0.0062844437,0.047499146\n",
      "Iteration 23490: loss = 0.006346384,0.017148152\n",
      "Iteration 23495: loss = 0.006337611,0.01818688\n",
      "Iteration 23500: loss = 0.0063131843,0.0011765973\n",
      "Iteration 23505: loss = 0.0062994566,0.005662273\n",
      "Iteration 23510: loss = 0.006295383,0.003377279\n",
      "Iteration 23515: loss = 0.006298136,0.00028172781\n",
      "Iteration 23520: loss = 0.006298989,0.001408124\n",
      "Iteration 23525: loss = 0.0062945373,0.0005247109\n",
      "Iteration 23530: loss = 0.006288901,0.00017408306\n",
      "Iteration 23535: loss = 0.0062863766,0.00036792937\n",
      "Iteration 23540: loss = 0.0062874393,3.3541204e-05\n",
      "Iteration 23545: loss = 0.006287945,0.00014585826\n",
      "Iteration 23550: loss = 0.0062859743,2.5551652e-05\n",
      "Iteration 23555: loss = 0.00628423,6.4307664e-05\n",
      "Iteration 23560: loss = 0.0062843147,1.7867109e-05\n",
      "Iteration 23565: loss = 0.0062845056,3.58851e-05\n",
      "Iteration 23570: loss = 0.0062834616,1.7380671e-05\n",
      "Iteration 23575: loss = 0.006282968,2.3145945e-05\n",
      "Iteration 23580: loss = 0.006283371,2.1338465e-05\n",
      "Iteration 23585: loss = 0.006282996,1.7158109e-05\n",
      "Iteration 23590: loss = 0.006282954,1.8290508e-05\n",
      "Iteration 23595: loss = 0.006283361,1.8337301e-05\n",
      "Iteration 23600: loss = 0.006283196,1.7534578e-05\n",
      "Iteration 23605: loss = 0.006283486,1.6630107e-05\n",
      "Iteration 23610: loss = 0.0062833806,1.6462789e-05\n",
      "Iteration 23615: loss = 0.0062834495,1.6415981e-05\n",
      "Iteration 23620: loss = 0.0062832367,1.6965416e-05\n",
      "Iteration 23625: loss = 0.006283418,1.8192048e-05\n",
      "Iteration 23630: loss = 0.0062828115,2.6202646e-05\n",
      "Iteration 23635: loss = 0.0062841712,7.573013e-05\n",
      "Iteration 23640: loss = 0.006280618,0.0004824335\n",
      "Iteration 23645: loss = 0.0062913396,0.0043102484\n",
      "Iteration 23650: loss = 0.0062594754,0.039407335\n",
      "Iteration 23655: loss = 0.0063134287,0.06745234\n",
      "Iteration 23660: loss = 0.006303878,0.029353904\n",
      "Iteration 23665: loss = 0.0062857307,0.0026636892\n",
      "Iteration 23670: loss = 0.0062731304,0.0006965723\n",
      "Iteration 23675: loss = 0.006267704,0.0035955317\n",
      "Iteration 23680: loss = 0.0062678675,0.0042987196\n",
      "Iteration 23685: loss = 0.006272433,0.002880213\n",
      "Iteration 23690: loss = 0.0062782248,0.0011129075\n",
      "Iteration 23695: loss = 0.0062836297,0.00015380069\n",
      "Iteration 23700: loss = 0.006287699,6.438858e-05\n",
      "Iteration 23705: loss = 0.0062892414,0.00024622685\n",
      "Iteration 23710: loss = 0.0062885047,0.00024338407\n",
      "Iteration 23715: loss = 0.006286137,8.925495e-05\n",
      "Iteration 23720: loss = 0.0062834695,1.6230131e-05\n",
      "Iteration 23725: loss = 0.0062816306,4.2900763e-05\n",
      "Iteration 23730: loss = 0.0062811594,4.9144623e-05\n",
      "Iteration 23735: loss = 0.006281981,2.2264296e-05\n",
      "Iteration 23740: loss = 0.0062831114,1.9121395e-05\n",
      "Iteration 23745: loss = 0.006283632,2.2672928e-05\n",
      "Iteration 23750: loss = 0.006283397,1.6556716e-05\n",
      "Iteration 23755: loss = 0.006282916,1.7740724e-05\n",
      "Iteration 23760: loss = 0.0062827202,1.7949593e-05\n",
      "Iteration 23765: loss = 0.006282955,1.6304968e-05\n",
      "Iteration 23770: loss = 0.006283035,1.6720045e-05\n",
      "Iteration 23775: loss = 0.0062829256,1.6126905e-05\n",
      "Iteration 23780: loss = 0.0062828227,1.649537e-05\n",
      "Iteration 23785: loss = 0.006282916,1.609186e-05\n",
      "Iteration 23790: loss = 0.00628293,1.6146667e-05\n",
      "Iteration 23795: loss = 0.0062828646,1.6082407e-05\n",
      "Iteration 23800: loss = 0.0062827766,1.6203954e-05\n",
      "Iteration 23805: loss = 0.0062829307,1.6034854e-05\n",
      "Iteration 23810: loss = 0.006282736,1.6176817e-05\n",
      "Iteration 23815: loss = 0.006282825,1.6013051e-05\n",
      "Iteration 23820: loss = 0.006282834,1.5964919e-05\n",
      "Iteration 23825: loss = 0.006282753,1.603148e-05\n",
      "Iteration 23830: loss = 0.00628275,1.6005008e-05\n",
      "Iteration 23835: loss = 0.0062828003,1.5912368e-05\n",
      "Iteration 23840: loss = 0.006282709,1.5971375e-05\n",
      "Iteration 23845: loss = 0.006282707,1.5946225e-05\n",
      "Iteration 23850: loss = 0.006282776,1.585629e-05\n",
      "Iteration 23855: loss = 0.006282637,1.597494e-05\n",
      "Iteration 23860: loss = 0.006282763,1.582616e-05\n",
      "Iteration 23865: loss = 0.0062826066,1.5973977e-05\n",
      "Iteration 23870: loss = 0.006282778,1.5941152e-05\n",
      "Iteration 23875: loss = 0.0062825023,1.7505388e-05\n",
      "Iteration 23880: loss = 0.0062826388,3.3187982e-05\n",
      "Iteration 23885: loss = 0.0062838285,0.0002649098\n",
      "Iteration 23890: loss = 0.0062746233,0.0042324397\n",
      "Iteration 23895: loss = 0.006322719,0.065590195\n",
      "Iteration 23900: loss = 0.006256672,0.058323145\n",
      "Iteration 23905: loss = 0.006241607,0.051930044\n",
      "Iteration 23910: loss = 0.0062367576,0.029404959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23915: loss = 0.006243088,0.01389458\n",
      "Iteration 23920: loss = 0.006255682,0.005022751\n",
      "Iteration 23925: loss = 0.0062695346,0.0010557891\n",
      "Iteration 23930: loss = 0.0062818187,3.7306407e-05\n",
      "Iteration 23935: loss = 0.006289929,0.00017561547\n",
      "Iteration 23940: loss = 0.0062937704,0.00044355216\n",
      "Iteration 23945: loss = 0.0062945667,0.00052145025\n",
      "Iteration 23950: loss = 0.0062934887,0.00044336056\n",
      "Iteration 23955: loss = 0.006291654,0.0003183326\n",
      "Iteration 23960: loss = 0.0062897676,0.00020555423\n",
      "Iteration 23965: loss = 0.0062883827,0.00012658324\n",
      "Iteration 23970: loss = 0.0062871515,7.8387784e-05\n",
      "Iteration 23975: loss = 0.006286174,5.0081926e-05\n",
      "Iteration 23980: loss = 0.0062853345,3.4896522e-05\n",
      "Iteration 23985: loss = 0.0062847366,2.6698704e-05\n",
      "Iteration 23990: loss = 0.006284391,2.2240842e-05\n",
      "Iteration 23995: loss = 0.006284105,1.955422e-05\n",
      "Iteration 24000: loss = 0.0062838565,1.80308e-05\n",
      "Iteration 24005: loss = 0.006283624,1.70429e-05\n",
      "Iteration 24010: loss = 0.006283466,1.6372967e-05\n",
      "Iteration 24015: loss = 0.00628333,1.5932696e-05\n",
      "Iteration 24020: loss = 0.006283169,1.5700312e-05\n",
      "Iteration 24025: loss = 0.006283066,1.5599137e-05\n",
      "Iteration 24030: loss = 0.006282941,1.5575984e-05\n",
      "Iteration 24035: loss = 0.00628284,1.562996e-05\n",
      "Iteration 24040: loss = 0.0062828106,1.562498e-05\n",
      "Iteration 24045: loss = 0.0062827696,1.5642278e-05\n",
      "Iteration 24050: loss = 0.006282693,1.5721784e-05\n",
      "Iteration 24055: loss = 0.0062826606,1.5714493e-05\n",
      "Iteration 24060: loss = 0.006282818,1.5530062e-05\n",
      "Iteration 24065: loss = 0.0062827226,1.5606574e-05\n",
      "Iteration 24070: loss = 0.006282784,1.5512922e-05\n",
      "Iteration 24075: loss = 0.006282603,1.5691712e-05\n",
      "Iteration 24080: loss = 0.006282905,1.54516e-05\n",
      "Iteration 24085: loss = 0.0062819854,1.705109e-05\n",
      "Iteration 24090: loss = 0.0062845363,2.1191849e-05\n",
      "Iteration 24095: loss = 0.0062763155,0.000104415274\n",
      "Iteration 24100: loss = 0.006305178,0.0010389573\n",
      "Iteration 24105: loss = 0.0062152366,0.010335071\n",
      "Iteration 24110: loss = 0.006295116,0.0060587837\n",
      "Iteration 24115: loss = 0.0063029826,0.0019539134\n",
      "Iteration 24120: loss = 0.006305294,0.0017544506\n",
      "Iteration 24125: loss = 0.0063028806,0.0016734072\n",
      "Iteration 24130: loss = 0.006298387,0.00043909065\n",
      "Iteration 24135: loss = 0.006295141,0.00081502466\n",
      "Iteration 24140: loss = 0.0062939785,0.00017349582\n",
      "Iteration 24145: loss = 0.006292476,0.00032823428\n",
      "Iteration 24150: loss = 0.006289015,7.723198e-05\n",
      "Iteration 24155: loss = 0.0062867682,0.0001104885\n",
      "Iteration 24160: loss = 0.00628584,4.7675523e-05\n",
      "Iteration 24165: loss = 0.006284131,2.4416799e-05\n",
      "Iteration 24170: loss = 0.0062824995,4.1359413e-05\n",
      "Iteration 24175: loss = 0.0062821065,2.2661277e-05\n",
      "Iteration 24180: loss = 0.0062816683,2.0142172e-05\n",
      "Iteration 24185: loss = 0.006281503,2.478879e-05\n",
      "Iteration 24190: loss = 0.006282168,2.445955e-05\n",
      "Iteration 24195: loss = 0.0062823445,2.2460867e-05\n",
      "Iteration 24200: loss = 0.006283006,2.1897176e-05\n",
      "Iteration 24205: loss = 0.006282791,2.6771362e-05\n",
      "Iteration 24210: loss = 0.006283213,4.493701e-05\n",
      "Iteration 24215: loss = 0.006281994,0.00013376903\n",
      "Iteration 24220: loss = 0.0062839403,0.00061717554\n",
      "Iteration 24225: loss = 0.0062787198,0.0038196042\n",
      "Iteration 24230: loss = 0.006292699,0.024194397\n",
      "Iteration 24235: loss = 0.0062617436,0.057768553\n",
      "Iteration 24240: loss = 0.006284246,0.002525896\n",
      "Iteration 24245: loss = 0.006305182,0.009169794\n",
      "Iteration 24250: loss = 0.0063031404,0.009991998\n",
      "Iteration 24255: loss = 0.0062889676,0.0010958897\n",
      "Iteration 24260: loss = 0.0062762997,0.0009070095\n",
      "Iteration 24265: loss = 0.00627261,0.0022577217\n",
      "Iteration 24270: loss = 0.0062763183,0.0005601075\n",
      "Iteration 24275: loss = 0.00628199,0.00012689395\n",
      "Iteration 24280: loss = 0.0062845736,0.0005060222\n",
      "Iteration 24285: loss = 0.006283006,9.562907e-05\n",
      "Iteration 24290: loss = 0.006280892,8.977751e-05\n",
      "Iteration 24295: loss = 0.0062811417,0.000115806266\n",
      "Iteration 24300: loss = 0.0062831645,1.6998356e-05\n",
      "Iteration 24305: loss = 0.006283955,5.613949e-05\n",
      "Iteration 24310: loss = 0.006282779,1.6073664e-05\n",
      "Iteration 24315: loss = 0.0062818117,3.174124e-05\n",
      "Iteration 24320: loss = 0.006282199,1.6222804e-05\n",
      "Iteration 24325: loss = 0.0062826253,2.2175564e-05\n",
      "Iteration 24330: loss = 0.0062821642,1.6507998e-05\n",
      "Iteration 24335: loss = 0.006282177,1.749807e-05\n",
      "Iteration 24340: loss = 0.006282572,1.677337e-05\n",
      "Iteration 24345: loss = 0.0062823263,1.5797012e-05\n",
      "Iteration 24350: loss = 0.0062821372,1.6475085e-05\n",
      "Iteration 24355: loss = 0.0062824157,1.6527829e-05\n",
      "Iteration 24360: loss = 0.0062821456,1.652303e-05\n",
      "Iteration 24365: loss = 0.0062822946,1.5871325e-05\n",
      "Iteration 24370: loss = 0.0062821726,1.5851121e-05\n",
      "Iteration 24375: loss = 0.006282314,1.5783426e-05\n",
      "Iteration 24380: loss = 0.0062820725,1.6320962e-05\n",
      "Iteration 24385: loss = 0.0062824325,1.7531787e-05\n",
      "Iteration 24390: loss = 0.0062817074,2.6116628e-05\n",
      "Iteration 24395: loss = 0.0062834197,7.648004e-05\n",
      "Iteration 24400: loss = 0.006278662,0.0004981093\n",
      "Iteration 24405: loss = 0.0062924824,0.0042829853\n",
      "Iteration 24410: loss = 0.0062529254,0.03768991\n",
      "Iteration 24415: loss = 0.006311871,0.06495993\n",
      "Iteration 24420: loss = 0.006313771,0.025789702\n",
      "Iteration 24425: loss = 0.006301793,0.0015182466\n",
      "Iteration 24430: loss = 0.006286695,0.001744094\n",
      "Iteration 24435: loss = 0.0062735514,0.0047751293\n",
      "Iteration 24440: loss = 0.0062656947,0.0045889495\n",
      "Iteration 24445: loss = 0.00626561,0.0024690353\n",
      "Iteration 24450: loss = 0.0062713684,0.0006362999\n",
      "Iteration 24455: loss = 0.006279839,2.6068556e-05\n",
      "Iteration 24460: loss = 0.006286899,0.00020834342\n",
      "Iteration 24465: loss = 0.0062894323,0.00034813883\n",
      "Iteration 24470: loss = 0.0062873135,0.00019097219\n",
      "Iteration 24475: loss = 0.0062831007,2.9669432e-05\n",
      "Iteration 24480: loss = 0.006280327,3.5818553e-05\n",
      "Iteration 24485: loss = 0.0062800855,6.505822e-05\n",
      "Iteration 24490: loss = 0.00628143,3.204601e-05\n",
      "Iteration 24495: loss = 0.006282587,1.5580932e-05\n",
      "Iteration 24500: loss = 0.006282982,2.434721e-05\n",
      "Iteration 24505: loss = 0.006282745,1.7840168e-05\n",
      "Iteration 24510: loss = 0.0062820613,1.6175298e-05\n",
      "Iteration 24515: loss = 0.006281676,1.8165723e-05\n",
      "Iteration 24520: loss = 0.0062820376,1.5449768e-05\n",
      "Iteration 24525: loss = 0.006282501,1.5876474e-05\n",
      "Iteration 24530: loss = 0.0062822825,1.527167e-05\n",
      "Iteration 24535: loss = 0.006281886,1.5787027e-05\n",
      "Iteration 24540: loss = 0.0062820953,1.5344782e-05\n",
      "Iteration 24545: loss = 0.006282136,1.5358524e-05\n",
      "Iteration 24550: loss = 0.006282155,1.5149217e-05\n",
      "Iteration 24555: loss = 0.006281924,1.538678e-05\n",
      "Iteration 24560: loss = 0.006282125,1.5135819e-05\n",
      "Iteration 24565: loss = 0.0062818597,1.5461754e-05\n",
      "Iteration 24570: loss = 0.0062824585,1.506546e-05\n",
      "Iteration 24575: loss = 0.0062808967,1.8118728e-05\n",
      "Iteration 24580: loss = 0.006284932,2.5211397e-05\n",
      "Iteration 24585: loss = 0.0062735267,0.00013387793\n",
      "Iteration 24590: loss = 0.0063082906,0.0010703034\n",
      "Iteration 24595: loss = 0.0062143696,0.0076198583\n",
      "Iteration 24600: loss = 0.006298147,0.0017467655\n",
      "Iteration 24605: loss = 0.006336652,0.0054956484\n",
      "Iteration 24610: loss = 0.0063029653,0.008590104\n",
      "Iteration 24615: loss = 0.0063298824,0.011960213\n",
      "Iteration 24620: loss = 0.006281849,0.008577574\n",
      "Iteration 24625: loss = 0.006295974,0.0007380241\n",
      "Iteration 24630: loss = 0.006292501,0.0010544447\n",
      "Iteration 24635: loss = 0.006273523,0.0023102714\n",
      "Iteration 24640: loss = 0.0062861796,0.0010576947\n",
      "Iteration 24645: loss = 0.006277391,0.00011226685\n",
      "Iteration 24650: loss = 0.0062768334,0.00011755109\n",
      "Iteration 24655: loss = 0.006282125,0.00036365364\n",
      "Iteration 24660: loss = 0.006274819,0.0006727644\n",
      "Iteration 24665: loss = 0.0062871748,0.0012827655\n",
      "Iteration 24670: loss = 0.00627145,0.0031293028\n",
      "Iteration 24675: loss = 0.0062992014,0.008323022\n",
      "Iteration 24680: loss = 0.0062600276,0.017578065\n",
      "Iteration 24685: loss = 0.006300429,0.011256907\n",
      "Iteration 24690: loss = 0.0062841345,3.9985593e-05\n",
      "Iteration 24695: loss = 0.0062723476,0.004563429\n",
      "Iteration 24700: loss = 0.006288594,0.0013573905\n",
      "Iteration 24705: loss = 0.0062854583,0.00035119551\n",
      "Iteration 24710: loss = 0.006276537,0.0013152005\n",
      "Iteration 24715: loss = 0.006284982,0.00036326738\n",
      "Iteration 24720: loss = 0.0062826183,3.2830652e-05\n",
      "Iteration 24725: loss = 0.006279392,0.00028623446\n",
      "Iteration 24730: loss = 0.0062850607,0.0003652075\n",
      "Iteration 24735: loss = 0.0062793735,0.00033111213\n",
      "Iteration 24740: loss = 0.00628492,0.00031698684\n",
      "Iteration 24745: loss = 0.006278885,0.0004525193\n",
      "Iteration 24750: loss = 0.0062868907,0.00096241024\n",
      "Iteration 24755: loss = 0.006273623,0.0029483263\n",
      "Iteration 24760: loss = 0.006297758,0.010255876\n",
      "Iteration 24765: loss = 0.006258507,0.026230797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24770: loss = 0.006295538,0.011247985\n",
      "Iteration 24775: loss = 0.0062899147,0.0030357954\n",
      "Iteration 24780: loss = 0.0062727598,0.005378178\n",
      "Iteration 24785: loss = 0.0062797326,0.0005827948\n",
      "Iteration 24790: loss = 0.0062895115,0.0023104758\n",
      "Iteration 24795: loss = 0.006282322,3.4348093e-05\n",
      "Iteration 24800: loss = 0.0062769786,0.00097427185\n",
      "Iteration 24805: loss = 0.006283185,0.00011579669\n",
      "Iteration 24810: loss = 0.006284144,0.00019967138\n",
      "Iteration 24815: loss = 0.00627985,0.00026422433\n",
      "Iteration 24820: loss = 0.0062826886,3.6187692e-05\n",
      "Iteration 24825: loss = 0.0062825684,4.0089115e-05\n",
      "Iteration 24830: loss = 0.0062805708,9.189651e-05\n",
      "Iteration 24835: loss = 0.0062832963,8.830801e-05\n",
      "Iteration 24840: loss = 0.006280782,7.812327e-05\n",
      "Iteration 24845: loss = 0.006282985,7.777647e-05\n",
      "Iteration 24850: loss = 0.006280313,0.00012619143\n",
      "Iteration 24855: loss = 0.006284449,0.00031285468\n",
      "Iteration 24860: loss = 0.006276781,0.0011917496\n",
      "Iteration 24865: loss = 0.0062928232,0.0056139394\n",
      "Iteration 24870: loss = 0.0062593236,0.025639767\n",
      "Iteration 24875: loss = 0.006305088,0.035096027\n",
      "Iteration 24880: loss = 0.006291525,0.0015908311\n",
      "Iteration 24885: loss = 0.0062718713,0.0093751475\n",
      "Iteration 24890: loss = 0.0062727127,0.003588362\n",
      "Iteration 24895: loss = 0.0062847896,0.0009768897\n",
      "Iteration 24900: loss = 0.006288416,0.0024828003\n",
      "Iteration 24905: loss = 0.006281812,1.5762049e-05\n",
      "Iteration 24910: loss = 0.0062777437,0.0009523053\n",
      "Iteration 24915: loss = 0.0062817833,7.862589e-05\n",
      "Iteration 24920: loss = 0.0062844395,0.00032478024\n",
      "Iteration 24925: loss = 0.006281307,4.737779e-05\n",
      "Iteration 24930: loss = 0.006280241,0.0001438481\n",
      "Iteration 24935: loss = 0.0062829144,1.6734797e-05\n",
      "Iteration 24940: loss = 0.0062822555,6.82638e-05\n",
      "Iteration 24945: loss = 0.0062806145,2.645553e-05\n",
      "Iteration 24950: loss = 0.006282497,2.1993274e-05\n",
      "Iteration 24955: loss = 0.006281797,3.075393e-05\n",
      "Iteration 24960: loss = 0.0062810904,2.099837e-05\n",
      "Iteration 24965: loss = 0.0062825843,1.5035883e-05\n",
      "Iteration 24970: loss = 0.006280666,1.8939443e-05\n",
      "Iteration 24975: loss = 0.0062829372,1.9445402e-05\n",
      "Iteration 24980: loss = 0.0062797996,2.8033772e-05\n",
      "Iteration 24985: loss = 0.006284889,3.9955325e-05\n",
      "Iteration 24990: loss = 0.0062748785,0.00012195959\n",
      "Iteration 24995: loss = 0.006297371,0.00047317048\n",
      "Iteration 25000: loss = 0.006245526,0.0024175246\n",
      "Iteration 25005: loss = 0.0063323844,0.00488232\n",
      "Iteration 25010: loss = 0.0062995716,0.00215797\n",
      "Iteration 25015: loss = 0.00622234,0.016943915\n",
      "Iteration 25020: loss = 0.0063104457,0.060726084\n",
      "Iteration 25025: loss = 0.0062734485,0.00090141397\n",
      "Iteration 25030: loss = 0.0062602162,0.01876788\n",
      "Iteration 25035: loss = 0.006270899,0.008618117\n",
      "Iteration 25040: loss = 0.0062895813,0.00019394036\n",
      "Iteration 25045: loss = 0.0062998775,0.0022127095\n",
      "Iteration 25050: loss = 0.006299285,0.0026432008\n",
      "Iteration 25055: loss = 0.0062921033,0.00049522123\n",
      "Iteration 25060: loss = 0.0062841005,0.00018286028\n",
      "Iteration 25065: loss = 0.0062801396,0.00057722285\n",
      "Iteration 25070: loss = 0.0062800716,0.00014767604\n",
      "Iteration 25075: loss = 0.006281788,5.812922e-05\n",
      "Iteration 25080: loss = 0.006282181,0.00014696382\n",
      "Iteration 25085: loss = 0.006280936,2.4780631e-05\n",
      "Iteration 25090: loss = 0.006280282,5.2635125e-05\n",
      "Iteration 25095: loss = 0.0062812995,2.9545836e-05\n",
      "Iteration 25100: loss = 0.0062828083,2.1841635e-05\n",
      "Iteration 25105: loss = 0.0062828944,2.229991e-05\n",
      "Iteration 25110: loss = 0.006281894,1.8056006e-05\n",
      "Iteration 25115: loss = 0.006281612,1.7067898e-05\n",
      "Iteration 25120: loss = 0.0062818523,1.699415e-05\n",
      "Iteration 25125: loss = 0.006281666,1.5073379e-05\n",
      "Iteration 25130: loss = 0.0062816218,1.6040638e-05\n",
      "Iteration 25135: loss = 0.006281938,1.4910609e-05\n",
      "Iteration 25140: loss = 0.006281838,1.4766204e-05\n",
      "Iteration 25145: loss = 0.0062816064,1.5258504e-05\n",
      "Iteration 25150: loss = 0.006281741,1.5057192e-05\n",
      "Iteration 25155: loss = 0.0062816474,1.494396e-05\n",
      "Iteration 25160: loss = 0.006281708,1.474059e-05\n",
      "Iteration 25165: loss = 0.00628169,1.4759533e-05\n",
      "Iteration 25170: loss = 0.006281573,1.4928742e-05\n",
      "Iteration 25175: loss = 0.006281749,1.509952e-05\n",
      "Iteration 25180: loss = 0.00628141,1.6926797e-05\n",
      "Iteration 25185: loss = 0.0062821344,2.622132e-05\n",
      "Iteration 25190: loss = 0.0062800744,0.00010340711\n",
      "Iteration 25195: loss = 0.006285926,0.000755734\n",
      "Iteration 25200: loss = 0.0062681003,0.007408091\n",
      "Iteration 25205: loss = 0.006318817,0.059419688\n",
      "Iteration 25210: loss = 0.00626857,0.026774075\n",
      "Iteration 25215: loss = 0.006253214,0.03634328\n",
      "Iteration 25220: loss = 0.0062547103,0.01557489\n",
      "Iteration 25225: loss = 0.0062649404,0.0033282316\n",
      "Iteration 25230: loss = 0.0062763584,9.250425e-05\n",
      "Iteration 25235: loss = 0.0062858295,0.0005953384\n",
      "Iteration 25240: loss = 0.0062912256,0.0014578255\n",
      "Iteration 25245: loss = 0.0062915664,0.0014806224\n",
      "Iteration 25250: loss = 0.0062889117,0.00086232624\n",
      "Iteration 25255: loss = 0.0062848404,0.00024707994\n",
      "Iteration 25260: loss = 0.0062817116,1.7646544e-05\n",
      "Iteration 25265: loss = 0.006279998,7.4168216e-05\n",
      "Iteration 25270: loss = 0.006279358,0.00012767449\n",
      "Iteration 25275: loss = 0.0062798187,7.5425814e-05\n",
      "Iteration 25280: loss = 0.0062809796,1.9556079e-05\n",
      "Iteration 25285: loss = 0.006282268,2.1101418e-05\n",
      "Iteration 25290: loss = 0.006282717,2.9946734e-05\n",
      "Iteration 25295: loss = 0.0062820935,1.850011e-05\n",
      "Iteration 25300: loss = 0.006281261,1.5379792e-05\n",
      "Iteration 25305: loss = 0.006281132,1.8434028e-05\n",
      "Iteration 25310: loss = 0.006281314,1.5435724e-05\n",
      "Iteration 25315: loss = 0.0062816297,1.4907431e-05\n",
      "Iteration 25320: loss = 0.0062817303,1.5082771e-05\n",
      "Iteration 25325: loss = 0.006281384,1.4581225e-05\n",
      "Iteration 25330: loss = 0.0062813405,1.4895095e-05\n",
      "Iteration 25335: loss = 0.006281398,1.4483767e-05\n",
      "Iteration 25340: loss = 0.006281575,1.4419888e-05\n",
      "Iteration 25345: loss = 0.006281243,1.4631909e-05\n",
      "Iteration 25350: loss = 0.0062816422,1.426073e-05\n",
      "Iteration 25355: loss = 0.006280704,1.5644786e-05\n",
      "Iteration 25360: loss = 0.0062829084,1.6272705e-05\n",
      "Iteration 25365: loss = 0.0062773484,4.1787116e-05\n",
      "Iteration 25370: loss = 0.0062927883,0.00019932236\n",
      "Iteration 25375: loss = 0.0062466688,0.0018565628\n",
      "Iteration 25380: loss = 0.0063560777,0.0080863675\n",
      "Iteration 25385: loss = 0.0062924065,0.0001650637\n",
      "Iteration 25390: loss = 0.006255954,0.0010111961\n",
      "Iteration 25395: loss = 0.0062539107,0.0014107013\n",
      "Iteration 25400: loss = 0.0062576593,0.00088481535\n",
      "Iteration 25405: loss = 0.0062690447,0.00032003887\n",
      "Iteration 25410: loss = 0.006276799,5.8348178e-05\n",
      "Iteration 25415: loss = 0.0062854034,4.1100524e-05\n",
      "Iteration 25420: loss = 0.00628613,0.00014478884\n",
      "Iteration 25425: loss = 0.006292743,0.00045745712\n",
      "Iteration 25430: loss = 0.006275696,0.002483635\n",
      "Iteration 25435: loss = 0.0063112993,0.016362632\n",
      "Iteration 25440: loss = 0.006233797,0.059273154\n",
      "Iteration 25445: loss = 0.006275792,0.00058664865\n",
      "Iteration 25450: loss = 0.006294695,0.017381132\n",
      "Iteration 25455: loss = 0.0062901746,0.007616125\n",
      "Iteration 25460: loss = 0.0062793866,2.8379145e-05\n",
      "Iteration 25465: loss = 0.0062729646,0.0026183957\n",
      "Iteration 25470: loss = 0.0062744035,0.002297534\n",
      "Iteration 25475: loss = 0.006280742,0.00014593474\n",
      "Iteration 25480: loss = 0.006285975,0.00041540587\n",
      "Iteration 25485: loss = 0.006286533,0.0005317293\n",
      "Iteration 25490: loss = 0.0062833126,2.923016e-05\n",
      "Iteration 25495: loss = 0.006280593,0.00015808582\n",
      "Iteration 25500: loss = 0.0062806397,9.461384e-05\n",
      "Iteration 25505: loss = 0.0062821326,2.5581072e-05\n",
      "Iteration 25510: loss = 0.006282409,5.9998307e-05\n",
      "Iteration 25515: loss = 0.0062811826,1.5321704e-05\n",
      "Iteration 25520: loss = 0.006280633,3.437577e-05\n",
      "Iteration 25525: loss = 0.0062814443,1.4675146e-05\n",
      "Iteration 25530: loss = 0.006281988,2.1902e-05\n",
      "Iteration 25535: loss = 0.006281487,1.5086705e-05\n",
      "Iteration 25540: loss = 0.0062813177,1.6832577e-05\n",
      "Iteration 25545: loss = 0.006281615,1.5892716e-05\n",
      "Iteration 25550: loss = 0.0062813614,1.4579622e-05\n",
      "Iteration 25555: loss = 0.0062811878,1.5600739e-05\n",
      "Iteration 25560: loss = 0.006281541,1.5281134e-05\n",
      "Iteration 25565: loss = 0.0062813195,1.4869207e-05\n",
      "Iteration 25570: loss = 0.0062813773,1.4463717e-05\n",
      "Iteration 25575: loss = 0.0062812776,1.45093545e-05\n",
      "Iteration 25580: loss = 0.006281194,1.4671383e-05\n",
      "Iteration 25585: loss = 0.0062814355,1.4818499e-05\n",
      "Iteration 25590: loss = 0.0062811156,1.626577e-05\n",
      "Iteration 25595: loss = 0.006281618,2.0988859e-05\n",
      "Iteration 25600: loss = 0.0062802755,5.4142667e-05\n",
      "Iteration 25605: loss = 0.006283744,0.00027941205\n",
      "Iteration 25610: loss = 0.0062739127,0.0022992485\n",
      "Iteration 25615: loss = 0.006303309,0.020784318\n",
      "Iteration 25620: loss = 0.006242461,0.08442148\n",
      "Iteration 25625: loss = 0.006262842,0.004381505\n",
      "Iteration 25630: loss = 0.0062844665,0.005241849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25635: loss = 0.006296161,0.01174407\n",
      "Iteration 25640: loss = 0.0062990566,0.0088503845\n",
      "Iteration 25645: loss = 0.006294781,0.003578035\n",
      "Iteration 25650: loss = 0.006287508,0.0005184578\n",
      "Iteration 25655: loss = 0.0062802364,6.531669e-05\n",
      "Iteration 25660: loss = 0.0062760697,0.0005769075\n",
      "Iteration 25665: loss = 0.0062759947,0.00069363345\n",
      "Iteration 25670: loss = 0.006278399,0.00032385322\n",
      "Iteration 25675: loss = 0.0062811957,3.6418416e-05\n",
      "Iteration 25680: loss = 0.006282672,4.665007e-05\n",
      "Iteration 25685: loss = 0.0062827864,9.853892e-05\n",
      "Iteration 25690: loss = 0.0062821372,4.8341775e-05\n",
      "Iteration 25695: loss = 0.0062813633,1.4164081e-05\n",
      "Iteration 25700: loss = 0.006280819,2.9536852e-05\n",
      "Iteration 25705: loss = 0.0062806928,2.412038e-05\n",
      "Iteration 25710: loss = 0.0062812376,1.4248246e-05\n",
      "Iteration 25715: loss = 0.0062815915,1.7611814e-05\n",
      "Iteration 25720: loss = 0.0062814485,1.4830537e-05\n",
      "Iteration 25725: loss = 0.006281082,1.4654048e-05\n",
      "Iteration 25730: loss = 0.006280961,1.5188387e-05\n",
      "Iteration 25735: loss = 0.0062812683,1.4090325e-05\n",
      "Iteration 25740: loss = 0.006281235,1.44411115e-05\n",
      "Iteration 25745: loss = 0.00628108,1.4190831e-05\n",
      "Iteration 25750: loss = 0.006281045,1.4289817e-05\n",
      "Iteration 25755: loss = 0.006281179,1.4093821e-05\n",
      "Iteration 25760: loss = 0.0062809847,1.4201929e-05\n",
      "Iteration 25765: loss = 0.0062812176,1.4013731e-05\n",
      "Iteration 25770: loss = 0.006280614,1.47484025e-05\n",
      "Iteration 25775: loss = 0.006281916,1.4318167e-05\n",
      "Iteration 25780: loss = 0.006278872,2.2762943e-05\n",
      "Iteration 25785: loss = 0.0062867687,5.616209e-05\n",
      "Iteration 25790: loss = 0.0062638186,0.00045487753\n",
      "Iteration 25795: loss = 0.00633182,0.0035983168\n",
      "Iteration 25800: loss = 0.0062144473,0.0065608374\n",
      "Iteration 25805: loss = 0.006245214,0.0026583255\n",
      "Iteration 25810: loss = 0.0062693954,0.00064416626\n",
      "Iteration 25815: loss = 0.006301478,0.001517095\n",
      "Iteration 25820: loss = 0.006284846,0.004663043\n",
      "Iteration 25825: loss = 0.006324975,0.013276472\n",
      "Iteration 25830: loss = 0.0062655844,0.022604303\n",
      "Iteration 25835: loss = 0.0063028727,0.0038215318\n",
      "Iteration 25840: loss = 0.006299472,0.003895877\n",
      "Iteration 25845: loss = 0.0062731565,0.0034590848\n",
      "Iteration 25850: loss = 0.0062784017,0.00034885277\n",
      "Iteration 25855: loss = 0.0062878374,0.0018611229\n",
      "Iteration 25860: loss = 0.006277634,0.000110954395\n",
      "Iteration 25865: loss = 0.0062753484,0.00047541762\n",
      "Iteration 25870: loss = 0.006283157,0.00045144954\n",
      "Iteration 25875: loss = 0.006279316,4.332707e-05\n",
      "Iteration 25880: loss = 0.0062795007,7.8964134e-05\n",
      "Iteration 25885: loss = 0.0062838015,0.00016487713\n",
      "Iteration 25890: loss = 0.006279783,0.00015993866\n",
      "Iteration 25895: loss = 0.006283831,0.00013221064\n",
      "Iteration 25900: loss = 0.006279458,0.00015583847\n",
      "Iteration 25905: loss = 0.0062841214,0.00027636884\n",
      "Iteration 25910: loss = 0.0062761833,0.0007677751\n",
      "Iteration 25915: loss = 0.0062904716,0.0028196159\n",
      "Iteration 25920: loss = 0.0062621864,0.011982525\n",
      "Iteration 25925: loss = 0.0063114925,0.0326009\n",
      "Iteration 25930: loss = 0.0062701497,0.008245587\n",
      "Iteration 25935: loss = 0.006268487,0.008358243\n",
      "Iteration 25940: loss = 0.0062866635,0.0019712166\n",
      "Iteration 25945: loss = 0.0062890816,0.0038066877\n",
      "Iteration 25950: loss = 0.0062775295,0.0002928179\n",
      "Iteration 25955: loss = 0.0062747933,0.0016200843\n",
      "Iteration 25960: loss = 0.006283028,8.347796e-05\n",
      "Iteration 25965: loss = 0.006285548,0.00059574284\n",
      "Iteration 25970: loss = 0.0062800776,9.613397e-05\n",
      "Iteration 25975: loss = 0.00627905,0.00017199464\n",
      "Iteration 25980: loss = 0.0062825414,0.00012756008\n",
      "Iteration 25985: loss = 0.0062812367,1.6981607e-05\n",
      "Iteration 25990: loss = 0.006280013,7.4627154e-05\n",
      "Iteration 25995: loss = 0.0062821507,4.5008113e-05\n",
      "Iteration 26000: loss = 0.0062808413,1.5859832e-05\n",
      "Iteration 26005: loss = 0.006280664,1.8114313e-05\n",
      "Iteration 26010: loss = 0.0062815547,2.5815069e-05\n",
      "Iteration 26015: loss = 0.0062804036,3.353503e-05\n",
      "Iteration 26020: loss = 0.0062818923,4.4556575e-05\n",
      "Iteration 26025: loss = 0.0062795975,8.739804e-05\n",
      "Iteration 26030: loss = 0.006283391,0.0002504366\n",
      "Iteration 26035: loss = 0.006275944,0.0010606023\n",
      "Iteration 26040: loss = 0.006292518,0.005496123\n",
      "Iteration 26045: loss = 0.006255955,0.027549403\n",
      "Iteration 26050: loss = 0.006306013,0.03713661\n",
      "Iteration 26055: loss = 0.0062947157,0.003554764\n",
      "Iteration 26060: loss = 0.0062728575,0.007352721\n",
      "Iteration 26065: loss = 0.006268186,0.006188094\n",
      "Iteration 26070: loss = 0.00627883,4.4362627e-05\n",
      "Iteration 26075: loss = 0.006288057,0.002494382\n",
      "Iteration 26080: loss = 0.006286047,0.0006667799\n",
      "Iteration 26085: loss = 0.006278543,0.00033676907\n",
      "Iteration 26090: loss = 0.006277081,0.0005757986\n",
      "Iteration 26095: loss = 0.006281569,2.2137221e-05\n",
      "Iteration 26100: loss = 0.0062834993,0.00025218012\n",
      "Iteration 26105: loss = 0.006280582,1.4484209e-05\n",
      "Iteration 26110: loss = 0.0062795323,0.00010764722\n",
      "Iteration 26115: loss = 0.0062815454,1.4352804e-05\n",
      "Iteration 26120: loss = 0.0062815114,4.6353594e-05\n",
      "Iteration 26125: loss = 0.006280305,2.2151407e-05\n",
      "Iteration 26130: loss = 0.0062809163,1.8593712e-05\n",
      "Iteration 26135: loss = 0.0062811077,2.3786102e-05\n",
      "Iteration 26140: loss = 0.006280614,1.5881802e-05\n",
      "Iteration 26145: loss = 0.0062808977,1.4132983e-05\n",
      "Iteration 26150: loss = 0.006280727,1.6210673e-05\n",
      "Iteration 26155: loss = 0.0062810555,1.5777849e-05\n",
      "Iteration 26160: loss = 0.006280082,1.651885e-05\n",
      "Iteration 26165: loss = 0.00628227,1.7213362e-05\n",
      "Iteration 26170: loss = 0.006277114,3.9800918e-05\n",
      "Iteration 26175: loss = 0.006290225,0.00014488706\n",
      "Iteration 26180: loss = 0.006254231,0.0010937437\n",
      "Iteration 26185: loss = 0.006344447,0.00570702\n",
      "Iteration 26190: loss = 0.0062504844,0.0014129842\n",
      "Iteration 26195: loss = 0.0062474445,0.003554116\n",
      "Iteration 26200: loss = 0.006240384,0.008051917\n",
      "Iteration 26205: loss = 0.0063263886,0.043455098\n",
      "Iteration 26210: loss = 0.00626172,0.028108701\n",
      "Iteration 26215: loss = 0.006265525,0.022037933\n",
      "Iteration 26220: loss = 0.006287321,0.0007946644\n",
      "Iteration 26225: loss = 0.006301462,0.0035201337\n",
      "Iteration 26230: loss = 0.006301814,0.0053418707\n",
      "Iteration 26235: loss = 0.0062946957,0.0016329399\n",
      "Iteration 26240: loss = 0.006286941,8.338303e-05\n",
      "Iteration 26245: loss = 0.006281795,0.0008802364\n",
      "Iteration 26250: loss = 0.0062813717,0.0006206381\n",
      "Iteration 26255: loss = 0.006283553,3.1735173e-05\n",
      "Iteration 26260: loss = 0.0062850416,0.00017321478\n",
      "Iteration 26265: loss = 0.006283933,0.000135894\n",
      "Iteration 26270: loss = 0.0062813945,1.4291058e-05\n",
      "Iteration 26275: loss = 0.0062799007,6.8465575e-05\n",
      "Iteration 26280: loss = 0.0062802043,2.3828312e-05\n",
      "Iteration 26285: loss = 0.0062810928,2.6890453e-05\n",
      "Iteration 26290: loss = 0.006281029,2.1979657e-05\n",
      "Iteration 26295: loss = 0.006280566,1.76664e-05\n",
      "Iteration 26300: loss = 0.0062807775,1.7207783e-05\n",
      "Iteration 26305: loss = 0.0062813875,1.5516733e-05\n",
      "Iteration 26310: loss = 0.006281243,1.3962222e-05\n",
      "Iteration 26315: loss = 0.006280892,1.5115276e-05\n",
      "Iteration 26320: loss = 0.006280986,1.3833589e-05\n",
      "Iteration 26325: loss = 0.0062809493,1.40313805e-05\n",
      "Iteration 26330: loss = 0.0062807933,1.4151927e-05\n",
      "Iteration 26335: loss = 0.0062809424,1.3795499e-05\n",
      "Iteration 26340: loss = 0.0062808953,1.3765313e-05\n",
      "Iteration 26345: loss = 0.0062808096,1.3897034e-05\n",
      "Iteration 26350: loss = 0.0062809214,1.4054511e-05\n",
      "Iteration 26355: loss = 0.0062807077,1.450677e-05\n",
      "Iteration 26360: loss = 0.0062809368,1.4774828e-05\n",
      "Iteration 26365: loss = 0.0062804967,1.7527138e-05\n",
      "Iteration 26370: loss = 0.0062813503,2.7736874e-05\n",
      "Iteration 26375: loss = 0.0062793572,9.7773314e-05\n",
      "Iteration 26380: loss = 0.0062843547,0.00058991776\n",
      "Iteration 26385: loss = 0.0062702727,0.0048913253\n",
      "Iteration 26390: loss = 0.0063099214,0.037997298\n",
      "Iteration 26395: loss = 0.006252058,0.05368744\n",
      "Iteration 26400: loss = 0.0062576258,0.02407445\n",
      "Iteration 26405: loss = 0.0062740203,0.00081266946\n",
      "Iteration 26410: loss = 0.006287238,0.0025070088\n",
      "Iteration 26415: loss = 0.0062921154,0.005612386\n",
      "Iteration 26420: loss = 0.006289588,0.004015575\n",
      "Iteration 26425: loss = 0.0062840073,0.0011497631\n",
      "Iteration 26430: loss = 0.0062787235,3.096574e-05\n",
      "Iteration 26435: loss = 0.006276933,0.0003506748\n",
      "Iteration 26440: loss = 0.0062784613,0.00054101925\n",
      "Iteration 26445: loss = 0.006281165,0.00022477334\n",
      "Iteration 26450: loss = 0.00628236,1.6825868e-05\n",
      "Iteration 26455: loss = 0.0062816604,7.648591e-05\n",
      "Iteration 26460: loss = 0.0062804692,8.3582774e-05\n",
      "Iteration 26465: loss = 0.006280148,1.9486075e-05\n",
      "Iteration 26470: loss = 0.006280801,2.5567344e-05\n",
      "Iteration 26475: loss = 0.0062811007,2.8097726e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26480: loss = 0.0062806886,1.3584438e-05\n",
      "Iteration 26485: loss = 0.0062804315,1.8819266e-05\n",
      "Iteration 26490: loss = 0.006280776,1.4721718e-05\n",
      "Iteration 26495: loss = 0.0062807724,1.4506447e-05\n",
      "Iteration 26500: loss = 0.006280503,1.4437408e-05\n",
      "Iteration 26505: loss = 0.006280529,1.376821e-05\n",
      "Iteration 26510: loss = 0.0062807365,1.3707759e-05\n",
      "Iteration 26515: loss = 0.006280529,1.3632618e-05\n",
      "Iteration 26520: loss = 0.0062804874,1.3625653e-05\n",
      "Iteration 26525: loss = 0.0062805717,1.3489834e-05\n",
      "Iteration 26530: loss = 0.0062805736,1.3437401e-05\n",
      "Iteration 26535: loss = 0.0062804813,1.3496355e-05\n",
      "Iteration 26540: loss = 0.0062804488,1.3506353e-05\n",
      "Iteration 26545: loss = 0.006280632,1.3428804e-05\n",
      "Iteration 26550: loss = 0.006280087,1.4153233e-05\n",
      "Iteration 26555: loss = 0.0062812273,1.3576215e-05\n",
      "Iteration 26560: loss = 0.0062785163,2.0396707e-05\n",
      "Iteration 26565: loss = 0.006285671,4.625357e-05\n",
      "Iteration 26570: loss = 0.0062644333,0.00038157377\n",
      "Iteration 26575: loss = 0.006329745,0.0032604297\n",
      "Iteration 26580: loss = 0.006200593,0.009485177\n",
      "Iteration 26585: loss = 0.0062634465,0.010433182\n",
      "Iteration 26590: loss = 0.006237719,0.027105734\n",
      "Iteration 26595: loss = 0.0063104555,0.014808196\n",
      "Iteration 26600: loss = 0.0063005914,0.003663084\n",
      "Iteration 26605: loss = 0.0062784986,0.005178608\n",
      "Iteration 26610: loss = 0.00628116,0.0017900985\n",
      "Iteration 26615: loss = 0.00629399,0.0016033587\n",
      "Iteration 26620: loss = 0.0062905266,0.0007589588\n",
      "Iteration 26625: loss = 0.006280744,0.0006736388\n",
      "Iteration 26630: loss = 0.0062825195,0.00020769364\n",
      "Iteration 26635: loss = 0.006287591,0.00037566377\n",
      "Iteration 26640: loss = 0.006284272,2.4834506e-05\n",
      "Iteration 26645: loss = 0.006281545,0.0001763893\n",
      "Iteration 26650: loss = 0.006284563,7.1175804e-05\n",
      "Iteration 26655: loss = 0.006283136,1.9554176e-05\n",
      "Iteration 26660: loss = 0.006281233,5.454055e-05\n",
      "Iteration 26665: loss = 0.006282825,4.885721e-05\n",
      "Iteration 26670: loss = 0.0062806383,3.0121359e-05\n",
      "Iteration 26675: loss = 0.006281257,1.770266e-05\n",
      "Iteration 26680: loss = 0.006280391,1.552722e-05\n",
      "Iteration 26685: loss = 0.0062806117,1.4869738e-05\n",
      "Iteration 26690: loss = 0.006280216,1.6021957e-05\n",
      "Iteration 26695: loss = 0.0062809433,1.9483523e-05\n",
      "Iteration 26700: loss = 0.006279707,4.4811743e-05\n",
      "Iteration 26705: loss = 0.0062832013,0.00021271745\n",
      "Iteration 26710: loss = 0.006273927,0.001631113\n",
      "Iteration 26715: loss = 0.0063011725,0.014227867\n",
      "Iteration 26720: loss = 0.0062371944,0.07537535\n",
      "Iteration 26725: loss = 0.0062782723,0.00046218775\n",
      "Iteration 26730: loss = 0.0062963963,0.017625721\n",
      "Iteration 26735: loss = 0.006295984,0.015419323\n",
      "Iteration 26740: loss = 0.0062893797,0.0055129076\n",
      "Iteration 26745: loss = 0.0062811985,0.00045997844\n",
      "Iteration 26750: loss = 0.0062752943,0.00035696733\n",
      "Iteration 26755: loss = 0.0062727914,0.0013297779\n",
      "Iteration 26760: loss = 0.0062739137,0.0011896631\n",
      "Iteration 26765: loss = 0.0062775724,0.00039163706\n",
      "Iteration 26770: loss = 0.006281383,1.5762333e-05\n",
      "Iteration 26775: loss = 0.006283922,0.00012840933\n",
      "Iteration 26780: loss = 0.0062841065,0.00016629904\n",
      "Iteration 26785: loss = 0.0062825624,4.7613135e-05\n",
      "Iteration 26790: loss = 0.006280584,1.8482608e-05\n",
      "Iteration 26795: loss = 0.0062795584,4.6628e-05\n",
      "Iteration 26800: loss = 0.006279841,2.4110228e-05\n",
      "Iteration 26805: loss = 0.006280614,1.521798e-05\n",
      "Iteration 26810: loss = 0.006280932,2.0486419e-05\n",
      "Iteration 26815: loss = 0.0062806252,1.3822824e-05\n",
      "Iteration 26820: loss = 0.0062803277,1.5407613e-05\n",
      "Iteration 26825: loss = 0.006280424,1.4395042e-05\n",
      "Iteration 26830: loss = 0.0062807123,1.3802209e-05\n",
      "Iteration 26835: loss = 0.0062805903,1.3644908e-05\n",
      "Iteration 26840: loss = 0.006280294,1.3861485e-05\n",
      "Iteration 26845: loss = 0.0062803533,1.3512543e-05\n",
      "Iteration 26850: loss = 0.0062804553,1.3383173e-05\n",
      "Iteration 26855: loss = 0.006280478,1.3311618e-05\n",
      "Iteration 26860: loss = 0.0062804013,1.3350555e-05\n",
      "Iteration 26865: loss = 0.006280314,1.342205e-05\n",
      "Iteration 26870: loss = 0.006280403,1.3432509e-05\n",
      "Iteration 26875: loss = 0.0062802187,1.3645455e-05\n",
      "Iteration 26880: loss = 0.0062804143,1.3352844e-05\n",
      "Iteration 26885: loss = 0.0062802513,1.3455692e-05\n",
      "Iteration 26890: loss = 0.006280351,1.3391725e-05\n",
      "Iteration 26895: loss = 0.006280123,1.3861091e-05\n",
      "Iteration 26900: loss = 0.0062805596,1.4476027e-05\n",
      "Iteration 26905: loss = 0.006279768,1.9056013e-05\n",
      "Iteration 26910: loss = 0.006281191,3.627611e-05\n",
      "Iteration 26915: loss = 0.006277883,0.00015612866\n",
      "Iteration 26920: loss = 0.0062865834,0.0010313129\n",
      "Iteration 26925: loss = 0.0062617008,0.008751821\n",
      "Iteration 26930: loss = 0.0063251387,0.05628528\n",
      "Iteration 26935: loss = 0.006276537,0.017804695\n",
      "Iteration 26940: loss = 0.0062429495,0.028551102\n",
      "Iteration 26945: loss = 0.0062358216,0.009626955\n",
      "Iteration 26950: loss = 0.006253975,0.0011055524\n",
      "Iteration 26955: loss = 0.006280102,0.001054205\n",
      "Iteration 26960: loss = 0.0063003935,0.002150187\n",
      "Iteration 26965: loss = 0.006308031,0.0019281267\n",
      "Iteration 26970: loss = 0.0063029476,0.0009139112\n",
      "Iteration 26975: loss = 0.0062902435,0.00014362326\n",
      "Iteration 26980: loss = 0.0062772688,4.1235504e-05\n",
      "Iteration 26985: loss = 0.0062706224,0.00019572908\n",
      "Iteration 26990: loss = 0.0062715006,0.0001795412\n",
      "Iteration 26995: loss = 0.006276742,5.7555528e-05\n",
      "Iteration 27000: loss = 0.0062816306,1.4061811e-05\n",
      "Iteration 27005: loss = 0.0062836506,3.4736124e-05\n",
      "Iteration 27010: loss = 0.0062829903,2.8900326e-05\n",
      "Iteration 27015: loss = 0.006281059,1.3190042e-05\n",
      "Iteration 27020: loss = 0.006279279,1.7996865e-05\n",
      "Iteration 27025: loss = 0.0062789693,1.8337445e-05\n",
      "Iteration 27030: loss = 0.0062802355,1.3264337e-05\n",
      "Iteration 27035: loss = 0.0062812087,1.3633502e-05\n",
      "Iteration 27040: loss = 0.006280755,1.31288725e-05\n",
      "Iteration 27045: loss = 0.0062799617,1.3680237e-05\n",
      "Iteration 27050: loss = 0.0062799715,1.3605149e-05\n",
      "Iteration 27055: loss = 0.0062804446,1.2954231e-05\n",
      "Iteration 27060: loss = 0.006280489,1.2919889e-05\n",
      "Iteration 27065: loss = 0.006280057,1.3306017e-05\n",
      "Iteration 27070: loss = 0.006280193,1.3086852e-05\n",
      "Iteration 27075: loss = 0.0062803742,1.2945784e-05\n",
      "Iteration 27080: loss = 0.0062798555,1.3617004e-05\n",
      "Iteration 27085: loss = 0.0062808306,1.347414e-05\n",
      "Iteration 27090: loss = 0.0062786997,2.0745405e-05\n",
      "Iteration 27095: loss = 0.006283906,5.6067023e-05\n",
      "Iteration 27100: loss = 0.0062690875,0.00045219614\n",
      "Iteration 27105: loss = 0.0063139047,0.004234517\n",
      "Iteration 27110: loss = 0.006233696,0.022040427\n",
      "Iteration 27115: loss = 0.0062048007,0.036005564\n",
      "Iteration 27120: loss = 0.006262342,0.0009906795\n",
      "Iteration 27125: loss = 0.0062862784,0.010577872\n",
      "Iteration 27130: loss = 0.006277244,0.00034950717\n",
      "Iteration 27135: loss = 0.006266633,0.003181415\n",
      "Iteration 27140: loss = 0.006272814,0.0007148527\n",
      "Iteration 27145: loss = 0.006283195,0.00079490914\n",
      "Iteration 27150: loss = 0.0062824725,0.00044592447\n",
      "Iteration 27155: loss = 0.0062770937,0.000260221\n",
      "Iteration 27160: loss = 0.0062783957,0.0001693129\n",
      "Iteration 27165: loss = 0.0062830932,0.00014224858\n",
      "Iteration 27170: loss = 0.006282316,3.2826898e-05\n",
      "Iteration 27175: loss = 0.006280247,9.603019e-05\n",
      "Iteration 27180: loss = 0.0062823906,1.9396146e-05\n",
      "Iteration 27185: loss = 0.0062826383,3.027024e-05\n",
      "Iteration 27190: loss = 0.0062809126,3.9853432e-05\n",
      "Iteration 27195: loss = 0.0062819286,2.0210111e-05\n",
      "Iteration 27200: loss = 0.0062810914,1.3227556e-05\n",
      "Iteration 27205: loss = 0.0062804352,1.5912961e-05\n",
      "Iteration 27210: loss = 0.006280842,1.943472e-05\n",
      "Iteration 27215: loss = 0.0062795207,2.7617469e-05\n",
      "Iteration 27220: loss = 0.0062810853,4.6509496e-05\n",
      "Iteration 27225: loss = 0.0062782266,0.00013032685\n",
      "Iteration 27230: loss = 0.006284121,0.0005219362\n",
      "Iteration 27235: loss = 0.0062710927,0.002866695\n",
      "Iteration 27240: loss = 0.0063026436,0.01676154\n",
      "Iteration 27245: loss = 0.006244497,0.051309057\n",
      "Iteration 27250: loss = 0.0062806164,0.00047241335\n",
      "Iteration 27255: loss = 0.006297262,0.01582503\n",
      "Iteration 27260: loss = 0.0062886314,0.0042425822\n",
      "Iteration 27265: loss = 0.006275533,0.0007100298\n",
      "Iteration 27270: loss = 0.0062710047,0.003495551\n",
      "Iteration 27275: loss = 0.0062766126,0.000778625\n",
      "Iteration 27280: loss = 0.0062839375,0.0002948352\n",
      "Iteration 27285: loss = 0.0062856637,0.000790271\n",
      "Iteration 27290: loss = 0.0062817167,4.0949053e-05\n",
      "Iteration 27295: loss = 0.006278003,0.00024060252\n",
      "Iteration 27300: loss = 0.0062787235,9.6875665e-05\n",
      "Iteration 27305: loss = 0.0062811635,5.9271442e-05\n",
      "Iteration 27310: loss = 0.006281125,5.7175053e-05\n",
      "Iteration 27315: loss = 0.0062794983,2.8314093e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27320: loss = 0.0062795733,2.9403433e-05\n",
      "Iteration 27325: loss = 0.0062808157,2.2900065e-05\n",
      "Iteration 27330: loss = 0.0062804855,1.480429e-05\n",
      "Iteration 27335: loss = 0.006279713,2.0697822e-05\n",
      "Iteration 27340: loss = 0.006280167,1.3431258e-05\n",
      "Iteration 27345: loss = 0.006280234,1.40743805e-05\n",
      "Iteration 27350: loss = 0.006279839,1.556119e-05\n",
      "Iteration 27355: loss = 0.006280303,1.3937139e-05\n",
      "Iteration 27360: loss = 0.0062799826,1.3239376e-05\n",
      "Iteration 27365: loss = 0.0062800064,1.29983255e-05\n",
      "Iteration 27370: loss = 0.0062800436,1.2998082e-05\n",
      "Iteration 27375: loss = 0.00628,1.3064116e-05\n",
      "Iteration 27380: loss = 0.0062799775,1.301719e-05\n",
      "Iteration 27385: loss = 0.006279947,1.2992004e-05\n",
      "Iteration 27390: loss = 0.006280059,1.3015008e-05\n",
      "Iteration 27395: loss = 0.006279722,1.4498244e-05\n",
      "Iteration 27400: loss = 0.006280458,2.3255348e-05\n",
      "Iteration 27405: loss = 0.0062782266,0.000121911355\n",
      "Iteration 27410: loss = 0.0062856674,0.0012472849\n",
      "Iteration 27415: loss = 0.006259592,0.015974356\n",
      "Iteration 27420: loss = 0.0063295006,0.10551974\n",
      "Iteration 27425: loss = 0.006302973,0.004980502\n",
      "Iteration 27430: loss = 0.0062856055,0.0017528393\n",
      "Iteration 27435: loss = 0.0062738354,0.0049706968\n",
      "Iteration 27440: loss = 0.006267439,0.005254012\n",
      "Iteration 27445: loss = 0.006264692,0.0042621167\n",
      "Iteration 27450: loss = 0.006265513,0.0032310844\n",
      "Iteration 27455: loss = 0.006267347,0.002315966\n",
      "Iteration 27460: loss = 0.0062699523,0.0015115232\n",
      "Iteration 27465: loss = 0.0062726885,0.0009318269\n",
      "Iteration 27470: loss = 0.0062749847,0.0004933463\n",
      "Iteration 27475: loss = 0.0062768017,0.0002174118\n",
      "Iteration 27480: loss = 0.00627807,6.929714e-05\n",
      "Iteration 27485: loss = 0.0062792595,1.6939923e-05\n",
      "Iteration 27490: loss = 0.0062805153,1.8481865e-05\n",
      "Iteration 27495: loss = 0.0062814555,3.131633e-05\n",
      "Iteration 27500: loss = 0.0062815803,3.486514e-05\n",
      "Iteration 27505: loss = 0.00628108,2.61488e-05\n",
      "Iteration 27510: loss = 0.0062804637,1.5961223e-05\n",
      "Iteration 27515: loss = 0.0062799896,1.276586e-05\n",
      "Iteration 27520: loss = 0.00627972,1.4599014e-05\n",
      "Iteration 27525: loss = 0.006279601,1.5137596e-05\n",
      "Iteration 27530: loss = 0.0062797274,1.3516102e-05\n",
      "Iteration 27535: loss = 0.0062799565,1.2657123e-05\n",
      "Iteration 27540: loss = 0.0062801354,1.2825002e-05\n",
      "Iteration 27545: loss = 0.0062800148,1.2786948e-05\n",
      "Iteration 27550: loss = 0.006279892,1.2659035e-05\n",
      "Iteration 27555: loss = 0.0062798723,1.2695603e-05\n",
      "Iteration 27560: loss = 0.0062797745,1.278102e-05\n",
      "Iteration 27565: loss = 0.0062798057,1.2691183e-05\n",
      "Iteration 27570: loss = 0.0062798862,1.2587498e-05\n",
      "Iteration 27575: loss = 0.0062796567,1.2837105e-05\n",
      "Iteration 27580: loss = 0.0062800613,1.2467215e-05\n",
      "Iteration 27585: loss = 0.0062792622,1.3512525e-05\n",
      "Iteration 27590: loss = 0.0062807915,1.3332174e-05\n",
      "Iteration 27595: loss = 0.0062772497,2.4949479e-05\n",
      "Iteration 27600: loss = 0.006286441,7.4703246e-05\n",
      "Iteration 27605: loss = 0.006260302,0.00060905056\n",
      "Iteration 27610: loss = 0.006333437,0.0042873784\n",
      "Iteration 27615: loss = 0.0062305187,0.0042508817\n",
      "Iteration 27620: loss = 0.006236509,0.0027620047\n",
      "Iteration 27625: loss = 0.006267233,0.00031449532\n",
      "Iteration 27630: loss = 0.006288349,9.441311e-05\n",
      "Iteration 27635: loss = 0.006295106,0.00038102365\n",
      "Iteration 27640: loss = 0.006296802,0.0003705886\n",
      "Iteration 27645: loss = 0.0062904446,0.00016862858\n",
      "Iteration 27650: loss = 0.0062835496,2.8428172e-05\n",
      "Iteration 27655: loss = 0.006277817,2.397924e-05\n",
      "Iteration 27660: loss = 0.006274527,5.9262657e-05\n",
      "Iteration 27665: loss = 0.0062764357,5.804851e-05\n",
      "Iteration 27670: loss = 0.0062766913,8.592255e-05\n",
      "Iteration 27675: loss = 0.0062853037,0.0003890513\n",
      "Iteration 27680: loss = 0.006271003,0.0027070416\n",
      "Iteration 27685: loss = 0.006311115,0.019753817\n",
      "Iteration 27690: loss = 0.006232508,0.063946836\n",
      "Iteration 27695: loss = 0.0062637203,0.0012153103\n",
      "Iteration 27700: loss = 0.006284781,0.01086774\n",
      "Iteration 27705: loss = 0.006289862,0.01133886\n",
      "Iteration 27710: loss = 0.0062863645,0.002281888\n",
      "Iteration 27715: loss = 0.0062804944,0.00019046992\n",
      "Iteration 27720: loss = 0.0062775468,0.001873376\n",
      "Iteration 27725: loss = 0.0062786755,0.0013450255\n",
      "Iteration 27730: loss = 0.006282518,0.00011426577\n",
      "Iteration 27735: loss = 0.006285088,0.00021537046\n",
      "Iteration 27740: loss = 0.006283903,0.00033231164\n",
      "Iteration 27745: loss = 0.0062801675,4.3674223e-05\n",
      "Iteration 27750: loss = 0.006277153,7.120365e-05\n",
      "Iteration 27755: loss = 0.0062772515,8.350348e-05\n",
      "Iteration 27760: loss = 0.0062795025,1.3386461e-05\n",
      "Iteration 27765: loss = 0.0062811505,3.7042937e-05\n",
      "Iteration 27770: loss = 0.0062808353,1.5589947e-05\n",
      "Iteration 27775: loss = 0.006279744,2.0234052e-05\n",
      "Iteration 27780: loss = 0.0062795323,1.5370786e-05\n",
      "Iteration 27785: loss = 0.0062798965,1.5297248e-05\n",
      "Iteration 27790: loss = 0.0062797666,1.3192159e-05\n",
      "Iteration 27795: loss = 0.0062796012,1.4182724e-05\n",
      "Iteration 27800: loss = 0.0062799305,1.2479287e-05\n",
      "Iteration 27805: loss = 0.0062798993,1.2854679e-05\n",
      "Iteration 27810: loss = 0.006279607,1.2845153e-05\n",
      "Iteration 27815: loss = 0.006279668,1.26523e-05\n",
      "Iteration 27820: loss = 0.0062798653,1.2654265e-05\n",
      "Iteration 27825: loss = 0.0062796003,1.29003365e-05\n",
      "Iteration 27830: loss = 0.006279703,1.2716753e-05\n",
      "Iteration 27835: loss = 0.0062795826,1.2816834e-05\n",
      "Iteration 27840: loss = 0.006279809,1.2755967e-05\n",
      "Iteration 27845: loss = 0.0062793945,1.3912858e-05\n",
      "Iteration 27850: loss = 0.006279999,1.6025728e-05\n",
      "Iteration 27855: loss = 0.0062787994,3.0863346e-05\n",
      "Iteration 27860: loss = 0.006281492,0.00010988672\n",
      "Iteration 27865: loss = 0.006274402,0.0007356044\n",
      "Iteration 27870: loss = 0.0062944884,0.006000359\n",
      "Iteration 27875: loss = 0.0062408955,0.044823553\n",
      "Iteration 27880: loss = 0.006300874,0.03758119\n",
      "Iteration 27885: loss = 0.006322328,0.027045853\n",
      "Iteration 27890: loss = 0.0063118488,0.0037528726\n",
      "Iteration 27895: loss = 0.0062861443,0.000769038\n",
      "Iteration 27900: loss = 0.0062621343,0.0035110486\n",
      "Iteration 27905: loss = 0.0062518935,0.0037373644\n",
      "Iteration 27910: loss = 0.006257517,0.0017779468\n",
      "Iteration 27915: loss = 0.0062713646,0.0002466344\n",
      "Iteration 27920: loss = 0.006284121,6.4365086e-05\n",
      "Iteration 27925: loss = 0.006289534,0.00034247016\n",
      "Iteration 27930: loss = 0.0062879683,0.0002937173\n",
      "Iteration 27935: loss = 0.0062829847,6.655375e-05\n",
      "Iteration 27940: loss = 0.0062785237,2.0856533e-05\n",
      "Iteration 27945: loss = 0.0062763444,7.126119e-05\n",
      "Iteration 27950: loss = 0.0062768944,4.4139622e-05\n",
      "Iteration 27955: loss = 0.0062795524,1.2548279e-05\n",
      "Iteration 27960: loss = 0.0062815263,2.286928e-05\n",
      "Iteration 27965: loss = 0.006280894,1.6678778e-05\n",
      "Iteration 27970: loss = 0.0062791943,1.3301586e-05\n",
      "Iteration 27975: loss = 0.0062787496,1.6522157e-05\n",
      "Iteration 27980: loss = 0.0062794406,1.2623623e-05\n",
      "Iteration 27985: loss = 0.006280036,1.27900075e-05\n",
      "Iteration 27990: loss = 0.006279833,1.2306574e-05\n",
      "Iteration 27995: loss = 0.0062792837,1.27960975e-05\n",
      "Iteration 28000: loss = 0.0062794616,1.25161805e-05\n",
      "Iteration 28005: loss = 0.0062795472,1.2460925e-05\n",
      "Iteration 28010: loss = 0.006279783,1.2236529e-05\n",
      "Iteration 28015: loss = 0.0062787794,1.4121968e-05\n",
      "Iteration 28020: loss = 0.006281216,1.7618751e-05\n",
      "Iteration 28025: loss = 0.0062744464,7.0858056e-05\n",
      "Iteration 28030: loss = 0.0062947213,0.00048414353\n",
      "Iteration 28035: loss = 0.0062349755,0.0044239587\n",
      "Iteration 28040: loss = 0.006330686,0.0107862465\n",
      "Iteration 28045: loss = 0.00634171,0.011466132\n",
      "Iteration 28050: loss = 0.0062923455,0.003725405\n",
      "Iteration 28055: loss = 0.0062828637,8.5151456e-05\n",
      "Iteration 28060: loss = 0.006281948,0.0016990963\n",
      "Iteration 28065: loss = 0.006262742,0.0015696614\n",
      "Iteration 28070: loss = 0.006271345,0.00048655045\n",
      "Iteration 28075: loss = 0.006269897,0.00018252603\n",
      "Iteration 28080: loss = 0.006269373,0.000288022\n",
      "Iteration 28085: loss = 0.006278746,0.0004328902\n",
      "Iteration 28090: loss = 0.0062731113,0.00073756673\n",
      "Iteration 28095: loss = 0.0062879715,0.0015776259\n",
      "Iteration 28100: loss = 0.0062697795,0.0043189484\n",
      "Iteration 28105: loss = 0.006301991,0.011674947\n",
      "Iteration 28110: loss = 0.006257884,0.018960096\n",
      "Iteration 28115: loss = 0.006291192,0.00415786\n",
      "Iteration 28120: loss = 0.006287619,0.0021524208\n",
      "Iteration 28125: loss = 0.0062688254,0.003963454\n",
      "Iteration 28130: loss = 0.0062797517,2.687137e-05\n",
      "Iteration 28135: loss = 0.0062852115,0.0013278441\n",
      "Iteration 28140: loss = 0.006274913,0.00075293623\n",
      "Iteration 28145: loss = 0.0062794643,1.3798328e-05\n",
      "Iteration 28150: loss = 0.006282686,0.00032640755\n",
      "Iteration 28155: loss = 0.0062766825,0.00037101202\n",
      "Iteration 28160: loss = 0.0062817894,0.00016900185\n",
      "Iteration 28165: loss = 0.0062785842,5.266844e-05\n",
      "Iteration 28170: loss = 0.0062798928,1.7486178e-05\n",
      "Iteration 28175: loss = 0.0062794522,1.2607162e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28180: loss = 0.0062794047,1.3189885e-05\n",
      "Iteration 28185: loss = 0.006279869,1.6061904e-05\n",
      "Iteration 28190: loss = 0.006278755,3.439904e-05\n",
      "Iteration 28195: loss = 0.0062813624,0.00013936614\n",
      "Iteration 28200: loss = 0.006274473,0.00094429526\n",
      "Iteration 28205: loss = 0.006294034,0.007760844\n",
      "Iteration 28210: loss = 0.006243061,0.05325842\n",
      "Iteration 28215: loss = 0.0062969686,0.022104735\n",
      "Iteration 28220: loss = 0.006304545,0.028747743\n",
      "Iteration 28225: loss = 0.006294114,0.007094259\n",
      "Iteration 28230: loss = 0.0062827487,0.00010045333\n",
      "Iteration 28235: loss = 0.006274734,0.0022300722\n",
      "Iteration 28240: loss = 0.006272392,0.0033223494\n",
      "Iteration 28245: loss = 0.006274981,0.0017539698\n",
      "Iteration 28250: loss = 0.0062794345,0.00022640437\n",
      "Iteration 28255: loss = 0.006282816,9.996465e-05\n",
      "Iteration 28260: loss = 0.006283115,0.0003902707\n",
      "Iteration 28265: loss = 0.0062809107,0.00024108413\n",
      "Iteration 28270: loss = 0.006278031,2.8123126e-05\n",
      "Iteration 28275: loss = 0.0062770247,6.0461167e-05\n",
      "Iteration 28280: loss = 0.0062783957,7.048086e-05\n",
      "Iteration 28285: loss = 0.0062804637,1.7468297e-05\n",
      "Iteration 28290: loss = 0.006281037,2.3927547e-05\n",
      "Iteration 28295: loss = 0.0062797735,2.2349286e-05\n",
      "Iteration 28300: loss = 0.006278492,1.4618424e-05\n",
      "Iteration 28305: loss = 0.006278712,1.8262388e-05\n",
      "Iteration 28310: loss = 0.006279923,1.2308689e-05\n",
      "Iteration 28315: loss = 0.0062800325,1.34890515e-05\n",
      "Iteration 28320: loss = 0.006279107,1.2761848e-05\n",
      "Iteration 28325: loss = 0.006278927,1.3353805e-05\n",
      "Iteration 28330: loss = 0.006279619,1.2046125e-05\n",
      "Iteration 28335: loss = 0.006279549,1.2196912e-05\n",
      "Iteration 28340: loss = 0.0062790345,1.2587146e-05\n",
      "Iteration 28345: loss = 0.0062793605,1.2130335e-05\n",
      "Iteration 28350: loss = 0.0062794085,1.2079046e-05\n",
      "Iteration 28355: loss = 0.0062791486,1.2309082e-05\n",
      "Iteration 28360: loss = 0.0062792846,1.2141325e-05\n",
      "Iteration 28365: loss = 0.006279265,1.21453595e-05\n",
      "Iteration 28370: loss = 0.006279181,1.22175015e-05\n",
      "Iteration 28375: loss = 0.006279281,1.209628e-05\n",
      "Iteration 28380: loss = 0.0062790797,1.23013815e-05\n",
      "Iteration 28385: loss = 0.0062793833,1.2007885e-05\n",
      "Iteration 28390: loss = 0.0062787323,1.2873017e-05\n",
      "Iteration 28395: loss = 0.0062801577,1.2916309e-05\n",
      "Iteration 28400: loss = 0.0062766573,2.6765932e-05\n",
      "Iteration 28405: loss = 0.0062859957,0.00010107784\n",
      "Iteration 28410: loss = 0.0062585175,0.00093567197\n",
      "Iteration 28415: loss = 0.0063419887,0.009008324\n",
      "Iteration 28420: loss = 0.006183157,0.050944224\n",
      "Iteration 28425: loss = 0.00620807,0.048188716\n",
      "Iteration 28430: loss = 0.0062309406,0.028756676\n",
      "Iteration 28435: loss = 0.006247135,0.003537093\n",
      "Iteration 28440: loss = 0.006283877,0.0022420264\n",
      "Iteration 28445: loss = 0.0063025304,0.006028794\n",
      "Iteration 28450: loss = 0.0062867445,0.002825717\n",
      "Iteration 28455: loss = 0.006266877,0.00042840594\n",
      "Iteration 28460: loss = 0.0062662163,0.00059076375\n",
      "Iteration 28465: loss = 0.006281057,0.0005777333\n",
      "Iteration 28470: loss = 0.0062906747,0.00033680478\n",
      "Iteration 28475: loss = 0.006287689,8.88542e-05\n",
      "Iteration 28480: loss = 0.006279263,9.534216e-05\n",
      "Iteration 28485: loss = 0.0062748655,0.00010700791\n",
      "Iteration 28490: loss = 0.0062769335,3.2835007e-05\n",
      "Iteration 28495: loss = 0.0062808334,3.4657958e-05\n",
      "Iteration 28500: loss = 0.0062823873,2.393327e-05\n",
      "Iteration 28505: loss = 0.006281398,1.672885e-05\n",
      "Iteration 28510: loss = 0.0062800907,1.9133493e-05\n",
      "Iteration 28515: loss = 0.0062797046,1.2990919e-05\n",
      "Iteration 28520: loss = 0.0062796506,1.613877e-05\n",
      "Iteration 28525: loss = 0.0062793926,1.3374727e-05\n",
      "Iteration 28530: loss = 0.00627955,1.3853158e-05\n",
      "Iteration 28535: loss = 0.0062800855,1.2320583e-05\n",
      "Iteration 28540: loss = 0.006280232,1.24465805e-05\n",
      "Iteration 28545: loss = 0.006279923,1.2507069e-05\n",
      "Iteration 28550: loss = 0.006279914,1.2312017e-05\n",
      "Iteration 28555: loss = 0.006279768,1.2407145e-05\n",
      "Iteration 28560: loss = 0.006279692,1.2462022e-05\n",
      "Iteration 28565: loss = 0.0062796692,1.2457145e-05\n",
      "Iteration 28570: loss = 0.006279632,1.2450606e-05\n",
      "Iteration 28575: loss = 0.006279664,1.2407019e-05\n",
      "Iteration 28580: loss = 0.006279634,1.2395221e-05\n",
      "Iteration 28585: loss = 0.0062796343,1.2365428e-05\n",
      "Iteration 28590: loss = 0.006279612,1.2352326e-05\n",
      "Iteration 28595: loss = 0.0062795733,1.239389e-05\n",
      "Iteration 28600: loss = 0.0062796497,1.2362177e-05\n",
      "Iteration 28605: loss = 0.0062793554,1.3372312e-05\n",
      "Iteration 28610: loss = 0.00628015,1.845032e-05\n",
      "Iteration 28615: loss = 0.0062776203,8.161376e-05\n",
      "Iteration 28620: loss = 0.006286047,0.0008019517\n",
      "Iteration 28625: loss = 0.006255626,0.010778194\n",
      "Iteration 28630: loss = 0.0063482015,0.094134085\n",
      "Iteration 28635: loss = 0.006286047,0.00019398714\n",
      "Iteration 28640: loss = 0.0062712766,0.0100540565\n",
      "Iteration 28645: loss = 0.006266912,0.011752073\n",
      "Iteration 28650: loss = 0.0062656845,0.009142211\n",
      "Iteration 28655: loss = 0.0062659853,0.006338384\n",
      "Iteration 28660: loss = 0.0062673986,0.0041942773\n",
      "Iteration 28665: loss = 0.006270017,0.0027081557\n",
      "Iteration 28670: loss = 0.0062725935,0.0016835819\n",
      "Iteration 28675: loss = 0.0062748943,0.0009957186\n",
      "Iteration 28680: loss = 0.006276502,0.00053796277\n",
      "Iteration 28685: loss = 0.006277807,0.00024901185\n",
      "Iteration 28690: loss = 0.0062786615,9.1286915e-05\n",
      "Iteration 28695: loss = 0.006279238,2.43066e-05\n",
      "Iteration 28700: loss = 0.0062797465,1.2472662e-05\n",
      "Iteration 28705: loss = 0.0062800176,2.3196842e-05\n",
      "Iteration 28710: loss = 0.0062801153,3.168641e-05\n",
      "Iteration 28715: loss = 0.00628005,2.8738934e-05\n",
      "Iteration 28720: loss = 0.0062798224,1.9410114e-05\n",
      "Iteration 28725: loss = 0.0062795696,1.3172594e-05\n",
      "Iteration 28730: loss = 0.006279336,1.2385739e-05\n",
      "Iteration 28735: loss = 0.0062791873,1.4021237e-05\n",
      "Iteration 28740: loss = 0.0062791924,1.3808001e-05\n",
      "Iteration 28745: loss = 0.0062793004,1.2497447e-05\n",
      "Iteration 28750: loss = 0.0062794015,1.2113308e-05\n",
      "Iteration 28755: loss = 0.0062794294,1.2344207e-05\n",
      "Iteration 28760: loss = 0.0062793926,1.2193269e-05\n",
      "Iteration 28765: loss = 0.006279297,1.2114297e-05\n",
      "Iteration 28770: loss = 0.0062792473,1.2208859e-05\n",
      "Iteration 28775: loss = 0.0062792636,1.2124481e-05\n",
      "Iteration 28780: loss = 0.0062792744,1.2081751e-05\n",
      "Iteration 28785: loss = 0.006279276,1.2076549e-05\n",
      "Iteration 28790: loss = 0.0062792413,1.2066039e-05\n",
      "Iteration 28795: loss = 0.006279217,1.2067691e-05\n",
      "Iteration 28800: loss = 0.0062791873,1.2072047e-05\n",
      "Iteration 28805: loss = 0.0062791775,1.2062474e-05\n",
      "Iteration 28810: loss = 0.0062791687,1.2054438e-05\n",
      "Iteration 28815: loss = 0.006279173,1.2039045e-05\n",
      "Iteration 28820: loss = 0.006279079,1.2141798e-05\n",
      "Iteration 28825: loss = 0.006279171,1.2082903e-05\n",
      "Iteration 28830: loss = 0.0062790834,1.2085128e-05\n",
      "Iteration 28835: loss = 0.006279076,1.2070578e-05\n",
      "Iteration 28840: loss = 0.006279094,1.2037019e-05\n",
      "Iteration 28845: loss = 0.006279083,1.201395e-05\n",
      "Iteration 28850: loss = 0.006279057,1.2036611e-05\n",
      "Iteration 28855: loss = 0.006279073,1.2002405e-05\n",
      "Iteration 28860: loss = 0.006279065,1.1970865e-05\n",
      "Iteration 28865: loss = 0.006279005,1.2035851e-05\n",
      "Iteration 28870: loss = 0.006279106,1.2164683e-05\n",
      "Iteration 28875: loss = 0.006278862,1.3364237e-05\n",
      "Iteration 28880: loss = 0.0062794,1.821048e-05\n",
      "Iteration 28885: loss = 0.006277915,5.7643156e-05\n",
      "Iteration 28890: loss = 0.0062819947,0.00038001515\n",
      "Iteration 28895: loss = 0.0062696156,0.003652347\n",
      "Iteration 28900: loss = 0.006307922,0.03448818\n",
      "Iteration 28905: loss = 0.0062431865,0.06972251\n",
      "Iteration 28910: loss = 0.006254438,0.028370451\n",
      "Iteration 28915: loss = 0.0062701236,0.002796595\n",
      "Iteration 28920: loss = 0.006281695,0.00038262023\n",
      "Iteration 28925: loss = 0.006288097,0.0026850917\n",
      "Iteration 28930: loss = 0.0062895324,0.0036349955\n",
      "Iteration 28935: loss = 0.0062878584,0.0028460566\n",
      "Iteration 28940: loss = 0.006283516,0.0014142204\n",
      "Iteration 28945: loss = 0.0062789214,0.00037378765\n",
      "Iteration 28950: loss = 0.006275404,4.473708e-05\n",
      "Iteration 28955: loss = 0.0062740627,0.00012243766\n",
      "Iteration 28960: loss = 0.006274929,0.00019760836\n",
      "Iteration 28965: loss = 0.0062774085,0.00012568687\n",
      "Iteration 28970: loss = 0.00628008,3.4186578e-05\n",
      "Iteration 28975: loss = 0.0062815375,1.8352432e-05\n",
      "Iteration 28980: loss = 0.006281353,3.2722135e-05\n",
      "Iteration 28985: loss = 0.0062798136,2.3294986e-05\n",
      "Iteration 28990: loss = 0.0062782033,1.4194313e-05\n",
      "Iteration 28995: loss = 0.006277727,1.7592456e-05\n",
      "Iteration 29000: loss = 0.0062784343,1.5261063e-05\n",
      "Iteration 29005: loss = 0.006279482,1.1644425e-05\n",
      "Iteration 29010: loss = 0.0062797065,1.2277159e-05\n",
      "Iteration 29015: loss = 0.006279129,1.1928167e-05\n",
      "Iteration 29020: loss = 0.0062785163,1.2538281e-05\n",
      "Iteration 29025: loss = 0.0062786355,1.2428945e-05\n",
      "Iteration 29030: loss = 0.0062791626,1.1601043e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29035: loss = 0.0062791635,1.16546735e-05\n",
      "Iteration 29040: loss = 0.0062787198,1.1991646e-05\n",
      "Iteration 29045: loss = 0.0062787626,1.1900622e-05\n",
      "Iteration 29050: loss = 0.006278966,1.1659119e-05\n",
      "Iteration 29055: loss = 0.006278916,1.1693932e-05\n",
      "Iteration 29060: loss = 0.006278807,1.1775774e-05\n",
      "Iteration 29065: loss = 0.0062788352,1.1735809e-05\n",
      "Iteration 29070: loss = 0.0062788506,1.1694565e-05\n",
      "Iteration 29075: loss = 0.0062787817,1.1753115e-05\n",
      "Iteration 29080: loss = 0.0062788143,1.1687808e-05\n",
      "Iteration 29085: loss = 0.0062787016,1.1838478e-05\n",
      "Iteration 29090: loss = 0.0062789745,1.1802176e-05\n",
      "Iteration 29095: loss = 0.006278396,1.2778519e-05\n",
      "Iteration 29100: loss = 0.006279353,1.3009625e-05\n",
      "Iteration 29105: loss = 0.0062774806,2.0247571e-05\n",
      "Iteration 29110: loss = 0.0062816325,4.612761e-05\n",
      "Iteration 29115: loss = 0.0062712934,0.00025399707\n",
      "Iteration 29120: loss = 0.006299334,0.0017813726\n",
      "Iteration 29125: loss = 0.006221477,0.014850713\n",
      "Iteration 29130: loss = 0.0063630776,0.058181036\n",
      "Iteration 29135: loss = 0.006366728,0.015947135\n",
      "Iteration 29140: loss = 0.006337485,0.019421114\n",
      "Iteration 29145: loss = 0.0063350815,0.007700408\n",
      "Iteration 29150: loss = 0.006329002,0.0031772084\n",
      "Iteration 29155: loss = 0.0063027795,0.0025207151\n",
      "Iteration 29160: loss = 0.006273641,0.00093657937\n",
      "Iteration 29165: loss = 0.006258097,0.0006914697\n",
      "Iteration 29170: loss = 0.006260332,0.00083689677\n",
      "Iteration 29175: loss = 0.006273854,0.0001611186\n",
      "Iteration 29180: loss = 0.006285267,8.690993e-05\n",
      "Iteration 29185: loss = 0.0062887357,0.0001918123\n",
      "Iteration 29190: loss = 0.0062850006,4.8144037e-05\n",
      "Iteration 29195: loss = 0.006280288,5.921787e-05\n",
      "Iteration 29200: loss = 0.0062784534,1.6994763e-05\n",
      "Iteration 29205: loss = 0.006278133,3.445437e-05\n",
      "Iteration 29210: loss = 0.0062776245,2.083155e-05\n",
      "Iteration 29215: loss = 0.0062775486,2.3462351e-05\n",
      "Iteration 29220: loss = 0.0062788366,1.3004872e-05\n",
      "Iteration 29225: loss = 0.0062796734,1.409578e-05\n",
      "Iteration 29230: loss = 0.0062794616,1.2932913e-05\n",
      "Iteration 29235: loss = 0.0062796953,1.1843516e-05\n",
      "Iteration 29240: loss = 0.006279813,1.2151978e-05\n",
      "Iteration 29245: loss = 0.0062794876,1.1974072e-05\n",
      "Iteration 29250: loss = 0.0062794704,1.1719364e-05\n",
      "Iteration 29255: loss = 0.006279323,1.1778383e-05\n",
      "Iteration 29260: loss = 0.0062792352,1.181013e-05\n",
      "Iteration 29265: loss = 0.0062791654,1.18477965e-05\n",
      "Iteration 29270: loss = 0.006279103,1.1879178e-05\n",
      "Iteration 29275: loss = 0.0062790886,1.18774215e-05\n",
      "Iteration 29280: loss = 0.006279066,1.1870206e-05\n",
      "Iteration 29285: loss = 0.0062790033,1.1924544e-05\n",
      "Iteration 29290: loss = 0.0062791184,1.2007686e-05\n",
      "Iteration 29295: loss = 0.006278748,1.4136502e-05\n",
      "Iteration 29300: loss = 0.006279838,3.167507e-05\n",
      "Iteration 29305: loss = 0.006275993,0.00027564666\n",
      "Iteration 29310: loss = 0.0062904176,0.0038042718\n",
      "Iteration 29315: loss = 0.006237196,0.05343143\n",
      "Iteration 29320: loss = 0.0063152444,0.059901275\n",
      "Iteration 29325: loss = 0.006311662,0.051470526\n",
      "Iteration 29330: loss = 0.006302212,0.030426277\n",
      "Iteration 29335: loss = 0.006297557,0.017726274\n",
      "Iteration 29340: loss = 0.0062956954,0.010440593\n",
      "Iteration 29345: loss = 0.0062940293,0.0061862953\n",
      "Iteration 29350: loss = 0.0062926323,0.003706513\n",
      "Iteration 29355: loss = 0.006289771,0.0022099798\n",
      "Iteration 29360: loss = 0.006287044,0.0013033802\n",
      "Iteration 29365: loss = 0.006284676,0.00078704266\n",
      "Iteration 29370: loss = 0.0062829405,0.00047573718\n",
      "Iteration 29375: loss = 0.0062818495,0.00027528865\n",
      "Iteration 29380: loss = 0.0062809805,0.00015406992\n",
      "Iteration 29385: loss = 0.006280421,8.2372906e-05\n",
      "Iteration 29390: loss = 0.0062798676,4.0044986e-05\n",
      "Iteration 29395: loss = 0.00627947,1.9062874e-05\n",
      "Iteration 29400: loss = 0.0062791477,1.24498365e-05\n",
      "Iteration 29405: loss = 0.006278891,1.2629565e-05\n",
      "Iteration 29410: loss = 0.00627879,1.4221343e-05\n",
      "Iteration 29415: loss = 0.006278735,1.515502e-05\n",
      "Iteration 29420: loss = 0.0062787724,1.4471344e-05\n",
      "Iteration 29425: loss = 0.0062788464,1.3086971e-05\n",
      "Iteration 29430: loss = 0.006278973,1.1812959e-05\n",
      "Iteration 29435: loss = 0.006279049,1.156389e-05\n",
      "Iteration 29440: loss = 0.0062790723,1.1689444e-05\n",
      "Iteration 29445: loss = 0.0062790196,1.1683062e-05\n",
      "Iteration 29450: loss = 0.006278958,1.1617199e-05\n",
      "Iteration 29455: loss = 0.0062789037,1.1591615e-05\n",
      "Iteration 29460: loss = 0.006278874,1.1615158e-05\n",
      "Iteration 29465: loss = 0.006278842,1.1630198e-05\n",
      "Iteration 29470: loss = 0.0062788236,1.16274e-05\n",
      "Iteration 29475: loss = 0.0062788003,1.1615992e-05\n",
      "Iteration 29480: loss = 0.006278809,1.15931125e-05\n",
      "Iteration 29485: loss = 0.006278731,1.1672454e-05\n",
      "Iteration 29490: loss = 0.0062787854,1.159274e-05\n",
      "Iteration 29495: loss = 0.0062787645,1.1589846e-05\n",
      "Iteration 29500: loss = 0.0062787184,1.1604438e-05\n",
      "Iteration 29505: loss = 0.0062786913,1.16272695e-05\n",
      "Iteration 29510: loss = 0.0062786867,1.1595905e-05\n",
      "Iteration 29515: loss = 0.0062786858,1.1585931e-05\n",
      "Iteration 29520: loss = 0.0062786713,1.1573217e-05\n",
      "Iteration 29525: loss = 0.0062786783,1.1551975e-05\n",
      "Iteration 29530: loss = 0.006278668,1.1551226e-05\n",
      "Iteration 29535: loss = 0.006278647,1.1557622e-05\n",
      "Iteration 29540: loss = 0.006278639,1.1549955e-05\n",
      "Iteration 29545: loss = 0.0062785163,1.1756147e-05\n",
      "Iteration 29550: loss = 0.0062786634,1.1683192e-05\n",
      "Iteration 29555: loss = 0.0062785135,1.1778746e-05\n",
      "Iteration 29560: loss = 0.0062786527,1.1640304e-05\n",
      "Iteration 29565: loss = 0.006278455,1.1911698e-05\n",
      "Iteration 29570: loss = 0.006278699,1.1998267e-05\n",
      "Iteration 29575: loss = 0.0062782597,1.3715603e-05\n",
      "Iteration 29580: loss = 0.0062789447,1.74272e-05\n",
      "Iteration 29585: loss = 0.0062776026,3.9399543e-05\n",
      "Iteration 29590: loss = 0.0062807966,0.00016897816\n",
      "Iteration 29595: loss = 0.0062723067,0.0011952025\n",
      "Iteration 29600: loss = 0.0062963148,0.009805884\n",
      "Iteration 29605: loss = 0.0062376913,0.059086658\n",
      "Iteration 29610: loss = 0.0062866732,0.010003457\n",
      "Iteration 29615: loss = 0.0063014464,0.025515666\n",
      "Iteration 29620: loss = 0.006300852,0.008620128\n",
      "Iteration 29625: loss = 0.0062921233,0.00052273524\n",
      "Iteration 29630: loss = 0.0062819556,0.0015680471\n",
      "Iteration 29635: loss = 0.0062735076,0.0027643538\n",
      "Iteration 29640: loss = 0.006269483,0.0015346715\n",
      "Iteration 29645: loss = 0.0062714373,0.00023190881\n",
      "Iteration 29650: loss = 0.006275971,0.00017276403\n",
      "Iteration 29655: loss = 0.0062809554,0.00036824495\n",
      "Iteration 29660: loss = 0.006282845,0.00016200703\n",
      "Iteration 29665: loss = 0.0062811426,2.018936e-05\n",
      "Iteration 29670: loss = 0.0062780534,7.640295e-05\n",
      "Iteration 29675: loss = 0.0062763165,5.35841e-05\n",
      "Iteration 29680: loss = 0.0062773973,1.595356e-05\n",
      "Iteration 29685: loss = 0.006279409,2.786548e-05\n",
      "Iteration 29690: loss = 0.0062799496,1.4953373e-05\n",
      "Iteration 29695: loss = 0.0062787198,1.4515308e-05\n",
      "Iteration 29700: loss = 0.0062776934,1.5835822e-05\n",
      "Iteration 29705: loss = 0.0062783323,1.2055216e-05\n",
      "Iteration 29710: loss = 0.006279091,1.2160097e-05\n",
      "Iteration 29715: loss = 0.0062786737,1.1357247e-05\n",
      "Iteration 29720: loss = 0.0062781535,1.2095474e-05\n",
      "Iteration 29725: loss = 0.0062786103,1.146823e-05\n",
      "Iteration 29730: loss = 0.00627867,1.1158153e-05\n",
      "Iteration 29735: loss = 0.0062783114,1.16312995e-05\n",
      "Iteration 29740: loss = 0.0062785014,1.13029e-05\n",
      "Iteration 29745: loss = 0.006278517,1.1218932e-05\n",
      "Iteration 29750: loss = 0.0062783062,1.14700815e-05\n",
      "Iteration 29755: loss = 0.0062785945,1.1280458e-05\n",
      "Iteration 29760: loss = 0.006278189,1.17193895e-05\n",
      "Iteration 29765: loss = 0.0062786154,1.133587e-05\n",
      "Iteration 29770: loss = 0.006278049,1.1996388e-05\n",
      "Iteration 29775: loss = 0.0062788497,1.1717092e-05\n",
      "Iteration 29780: loss = 0.006277391,1.53363e-05\n",
      "Iteration 29785: loss = 0.006280437,2.3385446e-05\n",
      "Iteration 29790: loss = 0.0062728855,0.00010374717\n",
      "Iteration 29795: loss = 0.006293783,0.00065455935\n",
      "Iteration 29800: loss = 0.0062323757,0.0057221213\n",
      "Iteration 29805: loss = 0.0063780607,0.034783617\n",
      "Iteration 29810: loss = 0.006307077,0.04887233\n",
      "Iteration 29815: loss = 0.0063047423,0.0044236872\n",
      "Iteration 29820: loss = 0.006350608,0.009013389\n",
      "Iteration 29825: loss = 0.0063485303,0.009940043\n",
      "Iteration 29830: loss = 0.006301584,0.0010204798\n",
      "Iteration 29835: loss = 0.0062555764,0.0011550805\n",
      "Iteration 29840: loss = 0.0062485985,0.002217484\n",
      "Iteration 29845: loss = 0.0062695243,0.00029648206\n",
      "Iteration 29850: loss = 0.006290743,0.00026367346\n",
      "Iteration 29855: loss = 0.0062935106,0.00046021855\n",
      "Iteration 29860: loss = 0.006282784,2.9389541e-05\n",
      "Iteration 29865: loss = 0.006273622,0.00012530136\n",
      "Iteration 29870: loss = 0.006273085,8.406471e-05\n",
      "Iteration 29875: loss = 0.006278152,3.7825685e-05\n",
      "Iteration 29880: loss = 0.0062810816,2.8261611e-05\n",
      "Iteration 29885: loss = 0.006280799,2.1466269e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29890: loss = 0.0062801535,1.9784262e-05\n",
      "Iteration 29895: loss = 0.0062798182,1.45809645e-05\n",
      "Iteration 29900: loss = 0.006278726,1.3148922e-05\n",
      "Iteration 29905: loss = 0.0062778317,1.636376e-05\n",
      "Iteration 29910: loss = 0.0062785256,1.2599252e-05\n",
      "Iteration 29915: loss = 0.0062788785,1.1929317e-05\n",
      "Iteration 29920: loss = 0.0062788893,1.2017302e-05\n",
      "Iteration 29925: loss = 0.0062791924,1.1370814e-05\n",
      "Iteration 29930: loss = 0.0062791095,1.1347921e-05\n",
      "Iteration 29935: loss = 0.00627896,1.1463578e-05\n",
      "Iteration 29940: loss = 0.0062788953,1.1451595e-05\n",
      "Iteration 29945: loss = 0.00627879,1.1512969e-05\n",
      "Iteration 29950: loss = 0.0062787044,1.1590124e-05\n",
      "Iteration 29955: loss = 0.0062787086,1.1553525e-05\n",
      "Iteration 29960: loss = 0.0062786355,1.1627182e-05\n",
      "Iteration 29965: loss = 0.006278762,1.1635809e-05\n",
      "Iteration 29970: loss = 0.006278383,1.3434459e-05\n",
      "Iteration 29975: loss = 0.0062795337,2.6638938e-05\n",
      "Iteration 29980: loss = 0.006275656,0.0001974373\n",
      "Iteration 29985: loss = 0.0062895473,0.002439483\n",
      "Iteration 29990: loss = 0.0062388317,0.033668555\n",
      "Iteration 29995: loss = 0.0063386,0.09550101\n",
      "Iteration 30000: loss = 0.0063167424,0.046198826\n",
      "Iteration 30005: loss = 0.006298763,0.017914625\n",
      "Iteration 30010: loss = 0.006289394,0.007650044\n",
      "Iteration 30015: loss = 0.0062859557,0.0036156373\n",
      "Iteration 30020: loss = 0.0062848907,0.001787669\n",
      "Iteration 30025: loss = 0.0062847524,0.00086596084\n",
      "Iteration 30030: loss = 0.006283277,0.00038079484\n",
      "Iteration 30035: loss = 0.0062814723,0.00013808235\n",
      "Iteration 30040: loss = 0.0062798616,3.589451e-05\n",
      "Iteration 30045: loss = 0.006278451,1.1866754e-05\n",
      "Iteration 30050: loss = 0.006277761,2.4083143e-05\n",
      "Iteration 30055: loss = 0.0062774247,4.548904e-05\n",
      "Iteration 30060: loss = 0.0062773973,6.0646078e-05\n",
      "Iteration 30065: loss = 0.0062775053,6.157384e-05\n",
      "Iteration 30070: loss = 0.006277744,4.9654634e-05\n",
      "Iteration 30075: loss = 0.0062780096,3.3352924e-05\n",
      "Iteration 30080: loss = 0.006278278,1.9108249e-05\n",
      "Iteration 30085: loss = 0.0062785526,1.2204342e-05\n",
      "Iteration 30090: loss = 0.006278746,1.15469775e-05\n",
      "Iteration 30095: loss = 0.00627882,1.2716476e-05\n",
      "Iteration 30100: loss = 0.006278802,1.2821962e-05\n",
      "Iteration 30105: loss = 0.0062787104,1.189321e-05\n",
      "Iteration 30110: loss = 0.006278552,1.1315258e-05\n",
      "Iteration 30115: loss = 0.0062784455,1.1533246e-05\n",
      "Iteration 30120: loss = 0.0062783994,1.1667533e-05\n",
      "Iteration 30125: loss = 0.006278413,1.1442197e-05\n",
      "Iteration 30130: loss = 0.0062784534,1.1330148e-05\n",
      "Iteration 30135: loss = 0.006278438,1.1348165e-05\n",
      "Iteration 30140: loss = 0.006278422,1.1336047e-05\n",
      "Iteration 30145: loss = 0.006278386,1.1326971e-05\n",
      "Iteration 30150: loss = 0.006278364,1.1334054e-05\n",
      "Iteration 30155: loss = 0.006278357,1.1320599e-05\n",
      "Iteration 30160: loss = 0.0062783584,1.1310238e-05\n",
      "Iteration 30165: loss = 0.0062783286,1.1313634e-05\n",
      "Iteration 30170: loss = 0.0062783095,1.1311689e-05\n",
      "Iteration 30175: loss = 0.0062783062,1.131171e-05\n",
      "Iteration 30180: loss = 0.0062782895,1.1294933e-05\n",
      "Iteration 30185: loss = 0.0062782434,1.1334407e-05\n",
      "Iteration 30190: loss = 0.006278316,1.1299994e-05\n",
      "Iteration 30195: loss = 0.0062782075,1.1352742e-05\n",
      "Iteration 30200: loss = 0.0062782355,1.1283396e-05\n",
      "Iteration 30205: loss = 0.006278248,1.1280616e-05\n",
      "Iteration 30210: loss = 0.0062782057,1.1262375e-05\n",
      "Iteration 30215: loss = 0.0062781945,1.1268334e-05\n",
      "Iteration 30220: loss = 0.006278196,1.1246896e-05\n",
      "Iteration 30225: loss = 0.0062781814,1.1242398e-05\n",
      "Iteration 30230: loss = 0.0062781624,1.1246578e-05\n",
      "Iteration 30235: loss = 0.0062781437,1.1235832e-05\n",
      "Iteration 30240: loss = 0.006278144,1.1220186e-05\n",
      "Iteration 30245: loss = 0.006278079,1.1294579e-05\n",
      "Iteration 30250: loss = 0.0062782285,1.1485909e-05\n",
      "Iteration 30255: loss = 0.0062778858,1.3871842e-05\n",
      "Iteration 30260: loss = 0.006278791,2.8739987e-05\n",
      "Iteration 30265: loss = 0.0062759635,0.0001829472\n",
      "Iteration 30270: loss = 0.0062851203,0.001897638\n",
      "Iteration 30275: loss = 0.0062538665,0.02302118\n",
      "Iteration 30280: loss = 0.006326576,0.09911804\n",
      "Iteration 30285: loss = 0.006302972,0.01951371\n",
      "Iteration 30290: loss = 0.0062860064,0.00083705265\n",
      "Iteration 30295: loss = 0.0062769502,0.0005373591\n",
      "Iteration 30300: loss = 0.0062727984,0.0018009365\n",
      "Iteration 30305: loss = 0.0062706308,0.002381057\n",
      "Iteration 30310: loss = 0.006270358,0.0022458558\n",
      "Iteration 30315: loss = 0.00627066,0.0018378\n",
      "Iteration 30320: loss = 0.0062724375,0.0012728792\n",
      "Iteration 30325: loss = 0.006274667,0.0007641749\n",
      "Iteration 30330: loss = 0.0062769596,0.0003631987\n",
      "Iteration 30335: loss = 0.0062791035,0.0001263573\n",
      "Iteration 30340: loss = 0.006280521,2.8375776e-05\n",
      "Iteration 30345: loss = 0.006281072,2.1504218e-05\n",
      "Iteration 30350: loss = 0.0062805824,3.702885e-05\n",
      "Iteration 30355: loss = 0.0062795454,4.032758e-05\n",
      "Iteration 30360: loss = 0.006278304,2.8693988e-05\n",
      "Iteration 30365: loss = 0.006277375,1.731785e-05\n",
      "Iteration 30370: loss = 0.00627707,1.3978235e-05\n",
      "Iteration 30375: loss = 0.0062774364,1.4452515e-05\n",
      "Iteration 30380: loss = 0.006278152,1.310948e-05\n",
      "Iteration 30385: loss = 0.0062786923,1.124993e-05\n",
      "Iteration 30390: loss = 0.006278642,1.0891526e-05\n",
      "Iteration 30395: loss = 0.0062782387,1.1203441e-05\n",
      "Iteration 30400: loss = 0.0062778587,1.1479474e-05\n",
      "Iteration 30405: loss = 0.0062778313,1.13906e-05\n",
      "Iteration 30410: loss = 0.006278062,1.1085231e-05\n",
      "Iteration 30415: loss = 0.006278198,1.0945394e-05\n",
      "Iteration 30420: loss = 0.006278128,1.0944391e-05\n",
      "Iteration 30425: loss = 0.00627796,1.1109585e-05\n",
      "Iteration 30430: loss = 0.00627796,1.1079685e-05\n",
      "Iteration 30435: loss = 0.0062780324,1.0973647e-05\n",
      "Iteration 30440: loss = 0.0062780157,1.0974634e-05\n",
      "Iteration 30445: loss = 0.0062779295,1.1041497e-05\n",
      "Iteration 30450: loss = 0.006277988,1.0985204e-05\n",
      "Iteration 30455: loss = 0.0062778224,1.1204807e-05\n",
      "Iteration 30460: loss = 0.006278056,1.0923221e-05\n",
      "Iteration 30465: loss = 0.0062778345,1.1095868e-05\n",
      "Iteration 30470: loss = 0.0062779174,1.0987555e-05\n",
      "Iteration 30475: loss = 0.0062779174,1.0978895e-05\n",
      "Iteration 30480: loss = 0.0062778853,1.0973205e-05\n",
      "Iteration 30485: loss = 0.006277846,1.1016449e-05\n",
      "Iteration 30490: loss = 0.0062779,1.0953132e-05\n",
      "Iteration 30495: loss = 0.0062778224,1.1005477e-05\n",
      "Iteration 30500: loss = 0.0062778126,1.0978181e-05\n",
      "Iteration 30505: loss = 0.0062778476,1.0928481e-05\n",
      "Iteration 30510: loss = 0.006277776,1.1004277e-05\n",
      "Iteration 30515: loss = 0.006277709,1.140217e-05\n",
      "Iteration 30520: loss = 0.0062781554,1.3425972e-05\n",
      "Iteration 30525: loss = 0.0062763635,3.6431804e-05\n",
      "Iteration 30530: loss = 0.0062832762,0.0002550562\n",
      "Iteration 30535: loss = 0.006255271,0.003131568\n",
      "Iteration 30540: loss = 0.0063637397,0.03739973\n",
      "Iteration 30545: loss = 0.0062261424,0.065157436\n",
      "Iteration 30550: loss = 0.0061982214,0.02208194\n",
      "Iteration 30555: loss = 0.006277103,0.00047946986\n",
      "Iteration 30560: loss = 0.0063551483,0.007661736\n",
      "Iteration 30565: loss = 0.006320258,0.003677083\n",
      "Iteration 30570: loss = 0.006249896,0.0037478271\n",
      "Iteration 30575: loss = 0.0062645986,0.0012919715\n",
      "Iteration 30580: loss = 0.006302484,0.0007204195\n",
      "Iteration 30585: loss = 0.006288413,0.00021687591\n",
      "Iteration 30590: loss = 0.0062633436,0.00046779343\n",
      "Iteration 30595: loss = 0.006270801,0.0002113025\n",
      "Iteration 30600: loss = 0.0062862686,0.00011192258\n",
      "Iteration 30605: loss = 0.0062833154,3.4984878e-05\n",
      "Iteration 30610: loss = 0.006275557,7.308066e-05\n",
      "Iteration 30615: loss = 0.0062769395,3.6047324e-05\n",
      "Iteration 30620: loss = 0.006281225,1.6708731e-05\n",
      "Iteration 30625: loss = 0.006280601,2.1859942e-05\n",
      "Iteration 30630: loss = 0.0062776892,1.5894248e-05\n",
      "Iteration 30635: loss = 0.0062774844,1.5920214e-05\n",
      "Iteration 30640: loss = 0.006279106,1.2744365e-05\n",
      "Iteration 30645: loss = 0.006279617,1.16114115e-05\n",
      "Iteration 30650: loss = 0.0062788273,1.1953545e-05\n",
      "Iteration 30655: loss = 0.0062782527,1.2071032e-05\n",
      "Iteration 30660: loss = 0.0062783696,1.1910427e-05\n",
      "Iteration 30665: loss = 0.00627874,1.1287822e-05\n",
      "Iteration 30670: loss = 0.006278843,1.1227436e-05\n",
      "Iteration 30675: loss = 0.0062786345,1.1329207e-05\n",
      "Iteration 30680: loss = 0.0062784576,1.14639115e-05\n",
      "Iteration 30685: loss = 0.0062784287,1.1479322e-05\n",
      "Iteration 30690: loss = 0.0062784054,1.1534596e-05\n",
      "Iteration 30695: loss = 0.0062785707,1.138484e-05\n",
      "Iteration 30700: loss = 0.0062784306,1.1454764e-05\n",
      "Iteration 30705: loss = 0.006278489,1.1405111e-05\n",
      "Iteration 30710: loss = 0.0062782993,1.1667748e-05\n",
      "Iteration 30715: loss = 0.006278438,1.1625807e-05\n",
      "Iteration 30720: loss = 0.006278182,1.2486653e-05\n",
      "Iteration 30725: loss = 0.0062786057,1.3900908e-05\n",
      "Iteration 30730: loss = 0.006277809,2.2788796e-05\n",
      "Iteration 30735: loss = 0.0062794467,6.4842694e-05\n",
      "Iteration 30740: loss = 0.0062753893,0.00035776832\n",
      "Iteration 30745: loss = 0.006286211,0.0026146208\n",
      "Iteration 30750: loss = 0.0062558777,0.021168383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30755: loss = 0.006317605,0.06597888\n",
      "Iteration 30760: loss = 0.00628777,0.003683034\n",
      "Iteration 30765: loss = 0.006265992,0.005987584\n",
      "Iteration 30770: loss = 0.006261256,0.011455962\n",
      "Iteration 30775: loss = 0.006266711,0.00574418\n",
      "Iteration 30780: loss = 0.006274661,0.00066361483\n",
      "Iteration 30785: loss = 0.0062813046,0.00027042997\n",
      "Iteration 30790: loss = 0.0062838024,0.00117828\n",
      "Iteration 30795: loss = 0.006282885,0.0008586349\n",
      "Iteration 30800: loss = 0.006279906,0.00012665511\n",
      "Iteration 30805: loss = 0.006276963,6.5394204e-05\n",
      "Iteration 30810: loss = 0.0062760096,0.00020656105\n",
      "Iteration 30815: loss = 0.0062769707,8.040593e-05\n",
      "Iteration 30820: loss = 0.006278666,1.4482735e-05\n",
      "Iteration 30825: loss = 0.006279418,5.3738546e-05\n",
      "Iteration 30830: loss = 0.0062788525,2.1959357e-05\n",
      "Iteration 30835: loss = 0.0062778774,1.6295866e-05\n",
      "Iteration 30840: loss = 0.006277691,2.1343587e-05\n",
      "Iteration 30845: loss = 0.006278198,1.1207315e-05\n",
      "Iteration 30850: loss = 0.0062784776,1.48561985e-05\n",
      "Iteration 30855: loss = 0.006278163,1.1219024e-05\n",
      "Iteration 30860: loss = 0.0062779593,1.2411254e-05\n",
      "Iteration 30865: loss = 0.006278127,1.1180145e-05\n",
      "Iteration 30870: loss = 0.006278198,1.1345382e-05\n",
      "Iteration 30875: loss = 0.0062780865,1.1179494e-05\n",
      "Iteration 30880: loss = 0.0062780348,1.1300421e-05\n",
      "Iteration 30885: loss = 0.006278109,1.1162744e-05\n",
      "Iteration 30890: loss = 0.006278002,1.1219582e-05\n",
      "Iteration 30895: loss = 0.006278077,1.1186735e-05\n",
      "Iteration 30900: loss = 0.0062779435,1.1358623e-05\n",
      "Iteration 30905: loss = 0.0062780567,1.1217726e-05\n",
      "Iteration 30910: loss = 0.0062779156,1.1294921e-05\n",
      "Iteration 30915: loss = 0.0062780264,1.1220809e-05\n",
      "Iteration 30920: loss = 0.0062778513,1.1603733e-05\n",
      "Iteration 30925: loss = 0.0062781335,1.22135425e-05\n",
      "Iteration 30930: loss = 0.006277546,1.6791271e-05\n",
      "Iteration 30935: loss = 0.0062787873,3.7483565e-05\n",
      "Iteration 30940: loss = 0.006275688,0.0001908377\n",
      "Iteration 30945: loss = 0.0062841303,0.0014002463\n",
      "Iteration 30950: loss = 0.006259574,0.01240779\n",
      "Iteration 30955: loss = 0.006320371,0.06728457\n",
      "Iteration 30960: loss = 0.006273285,0.0016698627\n",
      "Iteration 30965: loss = 0.006257836,0.020096393\n",
      "Iteration 30970: loss = 0.0062597618,0.014523394\n",
      "Iteration 30975: loss = 0.0062688347,0.0039865472\n",
      "Iteration 30980: loss = 0.0062771128,6.264135e-05\n",
      "Iteration 30985: loss = 0.0062823626,0.00088417024\n",
      "Iteration 30990: loss = 0.0062836683,0.0015705208\n",
      "Iteration 30995: loss = 0.00628243,0.0010416153\n",
      "Iteration 31000: loss = 0.0062798555,0.00019680437\n",
      "Iteration 31005: loss = 0.006277258,3.6877067e-05\n",
      "Iteration 31010: loss = 0.0062760455,0.00018870005\n",
      "Iteration 31015: loss = 0.0062763435,0.00015034639\n",
      "Iteration 31020: loss = 0.00627782,2.1297723e-05\n",
      "Iteration 31025: loss = 0.00627902,3.0545307e-05\n",
      "Iteration 31030: loss = 0.006279148,4.2989428e-05\n",
      "Iteration 31035: loss = 0.006278373,1.3648215e-05\n",
      "Iteration 31040: loss = 0.0062776743,1.6596687e-05\n",
      "Iteration 31045: loss = 0.006277623,1.7462242e-05\n",
      "Iteration 31050: loss = 0.006278014,1.083872e-05\n",
      "Iteration 31055: loss = 0.0062782704,1.3419702e-05\n",
      "Iteration 31060: loss = 0.006278068,1.0950322e-05\n",
      "Iteration 31065: loss = 0.006277816,1.2004564e-05\n",
      "Iteration 31070: loss = 0.0062778755,1.11261725e-05\n",
      "Iteration 31075: loss = 0.0062780785,1.1300429e-05\n",
      "Iteration 31080: loss = 0.0062779463,1.078482e-05\n",
      "Iteration 31085: loss = 0.0062777963,1.1265263e-05\n",
      "Iteration 31090: loss = 0.006277908,1.0803786e-05\n",
      "Iteration 31095: loss = 0.006277894,1.0815532e-05\n",
      "Iteration 31100: loss = 0.006277748,1.1087774e-05\n",
      "Iteration 31105: loss = 0.006277856,1.08730965e-05\n",
      "Iteration 31110: loss = 0.0062777232,1.0937378e-05\n",
      "Iteration 31115: loss = 0.0062778364,1.086807e-05\n",
      "Iteration 31120: loss = 0.0062776604,1.1132953e-05\n",
      "Iteration 31125: loss = 0.0062778178,1.1048021e-05\n",
      "Iteration 31130: loss = 0.0062775966,1.1459144e-05\n",
      "Iteration 31135: loss = 0.0062779114,1.2150303e-05\n",
      "Iteration 31140: loss = 0.0062772967,1.6982169e-05\n",
      "Iteration 31145: loss = 0.0062786117,4.0948784e-05\n",
      "Iteration 31150: loss = 0.006275374,0.00021370908\n",
      "Iteration 31155: loss = 0.006284081,0.0015464681\n",
      "Iteration 31160: loss = 0.0062590707,0.013248703\n",
      "Iteration 31165: loss = 0.0063190185,0.066511646\n",
      "Iteration 31170: loss = 0.006275577,0.0009557169\n",
      "Iteration 31175: loss = 0.006259436,0.018699277\n",
      "Iteration 31180: loss = 0.006261278,0.013790394\n",
      "Iteration 31185: loss = 0.0062709413,0.003435345\n",
      "Iteration 31190: loss = 0.0062799403,4.2488566e-05\n",
      "Iteration 31195: loss = 0.006285543,0.0010446325\n",
      "Iteration 31200: loss = 0.006286213,0.001686896\n",
      "Iteration 31205: loss = 0.0062836255,0.0008795001\n",
      "Iteration 31210: loss = 0.0062793866,8.682567e-05\n",
      "Iteration 31215: loss = 0.006275885,9.034534e-05\n",
      "Iteration 31220: loss = 0.0062746834,0.00023740667\n",
      "Iteration 31225: loss = 0.0062756278,0.00010567063\n",
      "Iteration 31230: loss = 0.006277544,1.1535747e-05\n",
      "Iteration 31235: loss = 0.006278757,5.1388644e-05\n",
      "Iteration 31240: loss = 0.006278597,3.3565673e-05\n",
      "Iteration 31245: loss = 0.0062776227,1.1222992e-05\n",
      "Iteration 31250: loss = 0.0062771305,2.3180595e-05\n",
      "Iteration 31255: loss = 0.0062774904,1.289462e-05\n",
      "Iteration 31260: loss = 0.006277973,1.2357935e-05\n",
      "Iteration 31265: loss = 0.0062779286,1.2435458e-05\n",
      "Iteration 31270: loss = 0.0062775244,1.1023182e-05\n",
      "Iteration 31275: loss = 0.006277423,1.16799365e-05\n",
      "Iteration 31280: loss = 0.0062776883,1.0721685e-05\n",
      "Iteration 31285: loss = 0.006277681,1.0727912e-05\n",
      "Iteration 31290: loss = 0.0062774946,1.0829524e-05\n",
      "Iteration 31295: loss = 0.006277548,1.0620493e-05\n",
      "Iteration 31300: loss = 0.0062775766,1.0651713e-05\n",
      "Iteration 31305: loss = 0.006277427,1.0835831e-05\n",
      "Iteration 31310: loss = 0.006277581,1.0645901e-05\n",
      "Iteration 31315: loss = 0.006277492,1.0622793e-05\n",
      "Iteration 31320: loss = 0.0062774173,1.06663665e-05\n",
      "Iteration 31325: loss = 0.006277412,1.0656622e-05\n",
      "Iteration 31330: loss = 0.0062774904,1.05642985e-05\n",
      "Iteration 31335: loss = 0.006277463,1.0571022e-05\n",
      "Iteration 31340: loss = 0.0062773637,1.06430325e-05\n",
      "Iteration 31345: loss = 0.006277391,1.0600908e-05\n",
      "Iteration 31350: loss = 0.0062774136,1.0565735e-05\n",
      "Iteration 31355: loss = 0.0062773395,1.063302e-05\n",
      "Iteration 31360: loss = 0.006277481,1.0690528e-05\n",
      "Iteration 31365: loss = 0.0062770117,1.3265611e-05\n",
      "Iteration 31370: loss = 0.0062783267,3.5754267e-05\n",
      "Iteration 31375: loss = 0.006273825,0.00036154682\n",
      "Iteration 31380: loss = 0.0062906058,0.005170685\n",
      "Iteration 31385: loss = 0.006231073,0.06875121\n",
      "Iteration 31390: loss = 0.00629247,0.028561225\n",
      "Iteration 31395: loss = 0.006311121,0.038853697\n",
      "Iteration 31400: loss = 0.006317017,0.025535205\n",
      "Iteration 31405: loss = 0.0063185333,0.015606071\n",
      "Iteration 31410: loss = 0.0063138963,0.0090585295\n",
      "Iteration 31415: loss = 0.006305658,0.005171203\n",
      "Iteration 31420: loss = 0.006298777,0.002873556\n",
      "Iteration 31425: loss = 0.0062919687,0.0015961098\n",
      "Iteration 31430: loss = 0.0062881857,0.0009066609\n",
      "Iteration 31435: loss = 0.00628501,0.0005333929\n",
      "Iteration 31440: loss = 0.006284158,0.0003238246\n",
      "Iteration 31445: loss = 0.0062829778,0.00020459351\n",
      "Iteration 31450: loss = 0.006282535,0.00013460281\n",
      "Iteration 31455: loss = 0.0062814723,9.071392e-05\n",
      "Iteration 31460: loss = 0.0062805465,6.1415245e-05\n",
      "Iteration 31465: loss = 0.006279677,4.1779414e-05\n",
      "Iteration 31470: loss = 0.0062790285,2.7434224e-05\n",
      "Iteration 31475: loss = 0.006278639,1.7603996e-05\n",
      "Iteration 31480: loss = 0.006278151,1.2387922e-05\n",
      "Iteration 31485: loss = 0.006277708,1.0606941e-05\n",
      "Iteration 31490: loss = 0.006277269,1.0850331e-05\n",
      "Iteration 31495: loss = 0.0062771724,1.1389955e-05\n",
      "Iteration 31500: loss = 0.00627711,1.1540399e-05\n",
      "Iteration 31505: loss = 0.0062771686,1.1240004e-05\n",
      "Iteration 31510: loss = 0.0062772254,1.08524255e-05\n",
      "Iteration 31515: loss = 0.0062773447,1.0532277e-05\n",
      "Iteration 31520: loss = 0.0062774327,1.0465083e-05\n",
      "Iteration 31525: loss = 0.0062774327,1.0452448e-05\n",
      "Iteration 31530: loss = 0.00627739,1.0464162e-05\n",
      "Iteration 31535: loss = 0.006277365,1.0458609e-05\n",
      "Iteration 31540: loss = 0.006277291,1.0496473e-05\n",
      "Iteration 31545: loss = 0.0062772403,1.0541983e-05\n",
      "Iteration 31550: loss = 0.0062772515,1.0512096e-05\n",
      "Iteration 31555: loss = 0.006277209,1.0527239e-05\n",
      "Iteration 31560: loss = 0.0062773353,1.04056235e-05\n",
      "Iteration 31565: loss = 0.006277068,1.0684378e-05\n",
      "Iteration 31570: loss = 0.0062775207,1.0356436e-05\n",
      "Iteration 31575: loss = 0.00627641,1.2508777e-05\n",
      "Iteration 31580: loss = 0.0062794485,1.755517e-05\n",
      "Iteration 31585: loss = 0.006270131,0.00010250666\n",
      "Iteration 31590: loss = 0.006299999,0.00087359466\n",
      "Iteration 31595: loss = 0.006215738,0.006989127\n",
      "Iteration 31600: loss = 0.006295813,0.0024500422\n",
      "Iteration 31605: loss = 0.006317064,0.0019700816\n",
      "Iteration 31610: loss = 0.0063163876,0.0022064063\n",
      "Iteration 31615: loss = 0.0062993183,0.00083700387\n",
      "Iteration 31620: loss = 0.0062888823,0.0003375413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31625: loss = 0.0062834844,0.00021305606\n",
      "Iteration 31630: loss = 0.0062746257,3.6172565e-05\n",
      "Iteration 31635: loss = 0.0062710512,8.44687e-05\n",
      "Iteration 31640: loss = 0.006272185,0.00013395325\n",
      "Iteration 31645: loss = 0.0062711947,0.000103479055\n",
      "Iteration 31650: loss = 0.006275387,5.162137e-05\n",
      "Iteration 31655: loss = 0.0062765125,2.8882521e-05\n",
      "Iteration 31660: loss = 0.006279645,3.321769e-05\n",
      "Iteration 31665: loss = 0.0062782676,7.019138e-05\n",
      "Iteration 31670: loss = 0.0062811486,0.00021279017\n",
      "Iteration 31675: loss = 0.006272879,0.0009707149\n",
      "Iteration 31680: loss = 0.0062886286,0.005396429\n",
      "Iteration 31685: loss = 0.006250216,0.028475607\n",
      "Iteration 31690: loss = 0.0063046473,0.034290798\n",
      "Iteration 31695: loss = 0.006289698,0.0061841765\n",
      "Iteration 31700: loss = 0.0062668636,0.0042658295\n",
      "Iteration 31705: loss = 0.0062642484,0.007283486\n",
      "Iteration 31710: loss = 0.006275717,0.00043938277\n",
      "Iteration 31715: loss = 0.0062859077,0.0013833786\n",
      "Iteration 31720: loss = 0.006285324,0.001409944\n",
      "Iteration 31725: loss = 0.0062776837,1.040429e-05\n",
      "Iteration 31730: loss = 0.0062729795,0.00055112725\n",
      "Iteration 31735: loss = 0.0062749633,0.00013183356\n",
      "Iteration 31740: loss = 0.006278949,0.0001177137\n",
      "Iteration 31745: loss = 0.006279007,0.00010143511\n",
      "Iteration 31750: loss = 0.0062765474,3.6787605e-05\n",
      "Iteration 31755: loss = 0.0062764417,4.9619262e-05\n",
      "Iteration 31760: loss = 0.006278179,2.2413346e-05\n",
      "Iteration 31765: loss = 0.0062779784,2.0128813e-05\n",
      "Iteration 31770: loss = 0.0062766564,2.2552942e-05\n",
      "Iteration 31775: loss = 0.0062771947,1.0650543e-05\n",
      "Iteration 31780: loss = 0.006277679,1.50100805e-05\n",
      "Iteration 31785: loss = 0.0062769894,1.3686154e-05\n",
      "Iteration 31790: loss = 0.0062773805,1.0518852e-05\n",
      "Iteration 31795: loss = 0.006277304,1.05177405e-05\n",
      "Iteration 31800: loss = 0.006277021,1.1388723e-05\n",
      "Iteration 31805: loss = 0.006277418,1.1574769e-05\n",
      "Iteration 31810: loss = 0.0062770103,1.2280928e-05\n",
      "Iteration 31815: loss = 0.0062774885,1.3054718e-05\n",
      "Iteration 31820: loss = 0.006276626,1.867433e-05\n",
      "Iteration 31825: loss = 0.006278139,3.934508e-05\n",
      "Iteration 31830: loss = 0.006274978,0.0001641387\n",
      "Iteration 31835: loss = 0.0062827147,0.0009810232\n",
      "Iteration 31840: loss = 0.0062616,0.007484693\n",
      "Iteration 31845: loss = 0.0063147093,0.04535651\n",
      "Iteration 31850: loss = 0.0062595196,0.022588808\n",
      "Iteration 31855: loss = 0.0062490967,0.023903323\n",
      "Iteration 31860: loss = 0.0062584844,0.0027592382\n",
      "Iteration 31865: loss = 0.006276457,0.0012032428\n",
      "Iteration 31870: loss = 0.0062904116,0.0042586727\n",
      "Iteration 31875: loss = 0.006293725,0.002737751\n",
      "Iteration 31880: loss = 0.0062859226,0.00033268868\n",
      "Iteration 31885: loss = 0.0062744874,0.00020584668\n",
      "Iteration 31890: loss = 0.0062684193,0.0006597595\n",
      "Iteration 31895: loss = 0.0062713814,0.00027333928\n",
      "Iteration 31900: loss = 0.006278208,1.1629307e-05\n",
      "Iteration 31905: loss = 0.006281253,0.00012973294\n",
      "Iteration 31910: loss = 0.0062791347,6.667596e-05\n",
      "Iteration 31915: loss = 0.0062761926,1.4668949e-05\n",
      "Iteration 31920: loss = 0.006275713,4.572343e-05\n",
      "Iteration 31925: loss = 0.0062767616,1.3929224e-05\n",
      "Iteration 31930: loss = 0.0062777246,1.8291725e-05\n",
      "Iteration 31935: loss = 0.0062777144,1.3351334e-05\n",
      "Iteration 31940: loss = 0.0062767914,1.2677941e-05\n",
      "Iteration 31945: loss = 0.0062765256,1.2474886e-05\n",
      "Iteration 31950: loss = 0.0062774546,1.096615e-05\n",
      "Iteration 31955: loss = 0.006277167,1.0535308e-05\n",
      "Iteration 31960: loss = 0.0062767747,1.1164173e-05\n",
      "Iteration 31965: loss = 0.0062769693,1.0322173e-05\n",
      "Iteration 31970: loss = 0.006277148,1.0462983e-05\n",
      "Iteration 31975: loss = 0.006276892,1.0542039e-05\n",
      "Iteration 31980: loss = 0.0062768683,1.0391366e-05\n",
      "Iteration 31985: loss = 0.0062772166,1.0120261e-05\n",
      "Iteration 31990: loss = 0.0062763225,1.1394761e-05\n",
      "Iteration 31995: loss = 0.0062785316,1.2798548e-05\n",
      "Iteration 32000: loss = 0.0062718173,5.65863e-05\n",
      "Iteration 32005: loss = 0.0062945113,0.00047648046\n",
      "Iteration 32010: loss = 0.00622016,0.005215488\n",
      "Iteration 32015: loss = 0.006327659,0.0045485944\n",
      "Iteration 32020: loss = 0.0063417344,0.00597607\n",
      "Iteration 32025: loss = 0.006296621,0.010230713\n",
      "Iteration 32030: loss = 0.0063400827,0.02338488\n",
      "Iteration 32035: loss = 0.006277496,0.012945764\n",
      "Iteration 32040: loss = 0.0062856576,0.0024488547\n",
      "Iteration 32045: loss = 0.006307528,0.00553307\n",
      "Iteration 32050: loss = 0.006293289,0.00054922287\n",
      "Iteration 32055: loss = 0.006277716,0.0023104236\n",
      "Iteration 32060: loss = 0.0062845442,9.337042e-05\n",
      "Iteration 32065: loss = 0.006289343,0.00091678\n",
      "Iteration 32070: loss = 0.006280027,0.00012947904\n",
      "Iteration 32075: loss = 0.0062780958,0.00020570653\n",
      "Iteration 32080: loss = 0.0062829503,0.0002274173\n",
      "Iteration 32085: loss = 0.0062785405,2.6186593e-05\n",
      "Iteration 32090: loss = 0.0062775402,4.5884753e-05\n",
      "Iteration 32095: loss = 0.0062799733,8.223912e-05\n",
      "Iteration 32100: loss = 0.0062762736,6.883932e-05\n",
      "Iteration 32105: loss = 0.00627868,4.3618973e-05\n",
      "Iteration 32110: loss = 0.006276149,3.8773436e-05\n",
      "Iteration 32115: loss = 0.006278287,4.7576043e-05\n",
      "Iteration 32120: loss = 0.0062749176,0.00011139288\n",
      "Iteration 32125: loss = 0.006280811,0.00037335407\n",
      "Iteration 32130: loss = 0.0062684356,0.0017853254\n",
      "Iteration 32135: loss = 0.006297073,0.009757048\n",
      "Iteration 32140: loss = 0.0062390137,0.03901548\n",
      "Iteration 32145: loss = 0.006294207,0.01145301\n",
      "Iteration 32150: loss = 0.0062970747,0.013842942\n",
      "Iteration 32155: loss = 0.0062760455,4.353449e-05\n",
      "Iteration 32160: loss = 0.0062653166,0.0052450113\n",
      "Iteration 32165: loss = 0.0062705767,0.0017187373\n",
      "Iteration 32170: loss = 0.0062810206,0.00036490016\n",
      "Iteration 32175: loss = 0.0062838453,0.0012992357\n",
      "Iteration 32180: loss = 0.006278712,4.939598e-05\n",
      "Iteration 32185: loss = 0.006274156,0.00040192116\n",
      "Iteration 32190: loss = 0.006275616,0.00012075114\n",
      "Iteration 32195: loss = 0.0062789842,0.00010713046\n",
      "Iteration 32200: loss = 0.0062784716,6.808176e-05\n",
      "Iteration 32205: loss = 0.0062760073,5.106787e-05\n",
      "Iteration 32210: loss = 0.00627635,2.6132186e-05\n",
      "Iteration 32215: loss = 0.0062778625,3.377423e-05\n",
      "Iteration 32220: loss = 0.0062771034,1.0439809e-05\n",
      "Iteration 32225: loss = 0.0062764934,2.1562906e-05\n",
      "Iteration 32230: loss = 0.0062774345,1.46722905e-05\n",
      "Iteration 32235: loss = 0.0062770764,1.0244921e-05\n",
      "Iteration 32240: loss = 0.0062767505,1.2869021e-05\n",
      "Iteration 32245: loss = 0.006277243,1.2890378e-05\n",
      "Iteration 32250: loss = 0.006276743,1.2027029e-05\n",
      "Iteration 32255: loss = 0.006277112,1.0965367e-05\n",
      "Iteration 32260: loss = 0.0062767793,1.1320668e-05\n",
      "Iteration 32265: loss = 0.006277142,1.17603995e-05\n",
      "Iteration 32270: loss = 0.006276543,1.5478397e-05\n",
      "Iteration 32275: loss = 0.006277703,3.1152485e-05\n",
      "Iteration 32280: loss = 0.006274965,0.00013785146\n",
      "Iteration 32285: loss = 0.0062819435,0.0009002484\n",
      "Iteration 32290: loss = 0.0062623373,0.0074827117\n",
      "Iteration 32295: loss = 0.006313915,0.049019378\n",
      "Iteration 32300: loss = 0.006257711,0.021322744\n",
      "Iteration 32305: loss = 0.00625174,0.027244486\n",
      "Iteration 32310: loss = 0.006262537,0.006019435\n",
      "Iteration 32315: loss = 0.006276841,0.0001464252\n",
      "Iteration 32320: loss = 0.006284641,0.0025697716\n",
      "Iteration 32325: loss = 0.006285554,0.0032446426\n",
      "Iteration 32330: loss = 0.0062818862,0.0013779438\n",
      "Iteration 32335: loss = 0.0062770904,9.086025e-05\n",
      "Iteration 32340: loss = 0.0062739593,0.00020340721\n",
      "Iteration 32345: loss = 0.0062739723,0.0004259657\n",
      "Iteration 32350: loss = 0.0062763058,0.00017239404\n",
      "Iteration 32355: loss = 0.0062784445,1.1414705e-05\n",
      "Iteration 32360: loss = 0.006278701,7.607168e-05\n",
      "Iteration 32365: loss = 0.0062773465,5.4667733e-05\n",
      "Iteration 32370: loss = 0.006276133,1.2355815e-05\n",
      "Iteration 32375: loss = 0.006276345,2.7376498e-05\n",
      "Iteration 32380: loss = 0.0062772166,1.6032112e-05\n",
      "Iteration 32385: loss = 0.006277401,1.1727733e-05\n",
      "Iteration 32390: loss = 0.006276855,1.4376812e-05\n",
      "Iteration 32395: loss = 0.006276682,1.0434829e-05\n",
      "Iteration 32400: loss = 0.006276969,1.1828502e-05\n",
      "Iteration 32405: loss = 0.006277015,9.972749e-06\n",
      "Iteration 32410: loss = 0.0062767775,1.08494605e-05\n",
      "Iteration 32415: loss = 0.0062767365,1.02975555e-05\n",
      "Iteration 32420: loss = 0.0062769256,1.019529e-05\n",
      "Iteration 32425: loss = 0.0062767644,1.0296114e-05\n",
      "Iteration 32430: loss = 0.00627674,1.012726e-05\n",
      "Iteration 32435: loss = 0.0062768706,1.00329835e-05\n",
      "Iteration 32440: loss = 0.006276691,1.0257957e-05\n",
      "Iteration 32445: loss = 0.0062767225,1.0282639e-05\n",
      "Iteration 32450: loss = 0.0062768213,1.01426685e-05\n",
      "Iteration 32455: loss = 0.006276509,1.04460005e-05\n",
      "Iteration 32460: loss = 0.0062771006,1.0030885e-05\n",
      "Iteration 32465: loss = 0.0062758275,1.2069602e-05\n",
      "Iteration 32470: loss = 0.0062786783,1.3837772e-05\n",
      "Iteration 32475: loss = 0.0062713944,5.369861e-05\n",
      "Iteration 32480: loss = 0.0062919357,0.000314139\n",
      "Iteration 32485: loss = 0.0062317383,0.0028656763\n",
      "Iteration 32490: loss = 0.0063590226,0.009673199\n",
      "Iteration 32495: loss = 0.006280057,0.020720456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32500: loss = 0.006327911,0.04817119\n",
      "Iteration 32505: loss = 0.0062814853,0.00060120743\n",
      "Iteration 32510: loss = 0.0062594246,0.010292936\n",
      "Iteration 32515: loss = 0.0062648044,0.007024452\n",
      "Iteration 32520: loss = 0.0062774443,0.00011750024\n",
      "Iteration 32525: loss = 0.0062886346,0.0017931067\n",
      "Iteration 32530: loss = 0.0062891985,0.0016712475\n",
      "Iteration 32535: loss = 0.0062835184,8.2213504e-05\n",
      "Iteration 32540: loss = 0.0062786243,0.0004662643\n",
      "Iteration 32545: loss = 0.0062783323,0.00034143508\n",
      "Iteration 32550: loss = 0.0062809275,2.9373743e-05\n",
      "Iteration 32555: loss = 0.0062817414,0.00016321713\n",
      "Iteration 32560: loss = 0.006279726,2.4024599e-05\n",
      "Iteration 32565: loss = 0.006277451,5.7497975e-05\n",
      "Iteration 32570: loss = 0.0062775915,2.2049237e-05\n",
      "Iteration 32575: loss = 0.0062784143,2.4880364e-05\n",
      "Iteration 32580: loss = 0.0062777363,1.27312e-05\n",
      "Iteration 32585: loss = 0.0062767393,1.7626353e-05\n",
      "Iteration 32590: loss = 0.0062769153,1.0385864e-05\n",
      "Iteration 32595: loss = 0.006277152,1.345278e-05\n",
      "Iteration 32600: loss = 0.006276574,1.1729264e-05\n",
      "Iteration 32605: loss = 0.0062766504,1.0670108e-05\n",
      "Iteration 32610: loss = 0.0062768986,1.1365306e-05\n",
      "Iteration 32615: loss = 0.0062766145,1.143992e-05\n",
      "Iteration 32620: loss = 0.006276933,1.0489702e-05\n",
      "Iteration 32625: loss = 0.0062768296,1.03075845e-05\n",
      "Iteration 32630: loss = 0.0062769637,1.0118881e-05\n",
      "Iteration 32635: loss = 0.0062768194,1.0416441e-05\n",
      "Iteration 32640: loss = 0.0062770112,1.0747924e-05\n",
      "Iteration 32645: loss = 0.0062765297,1.3347864e-05\n",
      "Iteration 32650: loss = 0.006277366,2.331352e-05\n",
      "Iteration 32655: loss = 0.006275229,9.670925e-05\n",
      "Iteration 32660: loss = 0.006280989,0.00066424656\n",
      "Iteration 32665: loss = 0.006264008,0.0060665803\n",
      "Iteration 32670: loss = 0.00631249,0.047155898\n",
      "Iteration 32675: loss = 0.006251291,0.031743474\n",
      "Iteration 32680: loss = 0.006250849,0.031270437\n",
      "Iteration 32685: loss = 0.0062637255,0.008208576\n",
      "Iteration 32690: loss = 0.0062777908,0.00028610695\n",
      "Iteration 32695: loss = 0.006285425,0.0010954227\n",
      "Iteration 32700: loss = 0.006287483,0.0026482327\n",
      "Iteration 32705: loss = 0.006285614,0.0022060815\n",
      "Iteration 32710: loss = 0.006281734,0.00093191554\n",
      "Iteration 32715: loss = 0.0062773996,0.00010020302\n",
      "Iteration 32720: loss = 0.006274039,8.065679e-05\n",
      "Iteration 32725: loss = 0.006272784,0.00024116313\n",
      "Iteration 32730: loss = 0.0062736105,0.00017958067\n",
      "Iteration 32735: loss = 0.006275917,3.3200733e-05\n",
      "Iteration 32740: loss = 0.006277936,1.945247e-05\n",
      "Iteration 32745: loss = 0.0062786606,4.425314e-05\n",
      "Iteration 32750: loss = 0.0062779994,2.1663682e-05\n",
      "Iteration 32755: loss = 0.0062768306,1.0807316e-05\n",
      "Iteration 32760: loss = 0.0062762047,1.8564495e-05\n",
      "Iteration 32765: loss = 0.006276419,1.25354545e-05\n",
      "Iteration 32770: loss = 0.006276999,1.0810524e-05\n",
      "Iteration 32775: loss = 0.0062771593,1.1668813e-05\n",
      "Iteration 32780: loss = 0.006276825,9.9379795e-06\n",
      "Iteration 32785: loss = 0.006276619,1.0884677e-05\n",
      "Iteration 32790: loss = 0.0062767253,1.00037905e-05\n",
      "Iteration 32795: loss = 0.0062768403,1.014022e-05\n",
      "Iteration 32800: loss = 0.0062767174,9.940837e-06\n",
      "Iteration 32805: loss = 0.0062766075,1.0154053e-05\n",
      "Iteration 32810: loss = 0.006276754,9.900351e-06\n",
      "Iteration 32815: loss = 0.006276706,9.909846e-06\n",
      "Iteration 32820: loss = 0.006276607,9.979414e-06\n",
      "Iteration 32825: loss = 0.006276632,9.926439e-06\n",
      "Iteration 32830: loss = 0.0062766164,9.933033e-06\n",
      "Iteration 32835: loss = 0.0062765814,9.954938e-06\n",
      "Iteration 32840: loss = 0.0062765866,9.92666e-06\n",
      "Iteration 32845: loss = 0.0062765717,9.9265935e-06\n",
      "Iteration 32850: loss = 0.0062765535,9.931257e-06\n",
      "Iteration 32855: loss = 0.006276537,9.942056e-06\n",
      "Iteration 32860: loss = 0.006276604,1.00278685e-05\n",
      "Iteration 32865: loss = 0.006276296,1.1100501e-05\n",
      "Iteration 32870: loss = 0.0062770066,1.4976824e-05\n",
      "Iteration 32875: loss = 0.0062750545,5.0852246e-05\n",
      "Iteration 32880: loss = 0.006280565,0.0003322614\n",
      "Iteration 32885: loss = 0.0062636877,0.0032265938\n",
      "Iteration 32890: loss = 0.0063159238,0.030892782\n",
      "Iteration 32895: loss = 0.0062322654,0.06959017\n",
      "Iteration 32900: loss = 0.006216397,0.025069263\n",
      "Iteration 32905: loss = 0.00623768,0.003118189\n",
      "Iteration 32910: loss = 0.006268999,0.0011137559\n",
      "Iteration 32915: loss = 0.00629165,0.0024207074\n",
      "Iteration 32920: loss = 0.0063013174,0.0025763325\n",
      "Iteration 32925: loss = 0.006306837,0.0020956954\n",
      "Iteration 32930: loss = 0.0063040294,0.001470095\n",
      "Iteration 32935: loss = 0.0063000363,0.0008840387\n",
      "Iteration 32940: loss = 0.0062930104,0.00044103694\n",
      "Iteration 32945: loss = 0.0062869736,0.00017546833\n",
      "Iteration 32950: loss = 0.0062812087,5.100088e-05\n",
      "Iteration 32955: loss = 0.006277386,1.36900435e-05\n",
      "Iteration 32960: loss = 0.0062748063,2.0852105e-05\n",
      "Iteration 32965: loss = 0.0062734443,3.497982e-05\n",
      "Iteration 32970: loss = 0.00627302,3.7154517e-05\n",
      "Iteration 32975: loss = 0.0062737013,2.6707456e-05\n",
      "Iteration 32980: loss = 0.0062750806,1.52188595e-05\n",
      "Iteration 32985: loss = 0.0062766955,9.978545e-06\n",
      "Iteration 32990: loss = 0.0062775803,1.0275608e-05\n",
      "Iteration 32995: loss = 0.006277653,1.0675094e-05\n",
      "Iteration 33000: loss = 0.006277232,1.0094858e-05\n",
      "Iteration 33005: loss = 0.0062766825,9.82309e-06\n",
      "Iteration 33010: loss = 0.0062762112,1.0490279e-05\n",
      "Iteration 33015: loss = 0.0062761083,1.0588701e-05\n",
      "Iteration 33020: loss = 0.0062764497,9.994931e-06\n",
      "Iteration 33025: loss = 0.0062766825,9.785317e-06\n",
      "Iteration 33030: loss = 0.006276677,9.767446e-06\n",
      "Iteration 33035: loss = 0.006276404,9.988602e-06\n",
      "Iteration 33040: loss = 0.0062763095,1.01100795e-05\n",
      "Iteration 33045: loss = 0.006276585,1.0077683e-05\n",
      "Iteration 33050: loss = 0.006276066,1.1078404e-05\n",
      "Iteration 33055: loss = 0.006277137,1.1447689e-05\n",
      "Iteration 33060: loss = 0.0062751696,1.971062e-05\n",
      "Iteration 33065: loss = 0.006279007,5.0194718e-05\n",
      "Iteration 33070: loss = 0.006269573,0.00028440496\n",
      "Iteration 33075: loss = 0.0062946505,0.001931313\n",
      "Iteration 33080: loss = 0.0062409583,0.012033297\n",
      "Iteration 33085: loss = 0.006265745,0.025614036\n",
      "Iteration 33090: loss = 0.0063047,0.0034596454\n",
      "Iteration 33095: loss = 0.006309444,0.00766808\n",
      "Iteration 33100: loss = 0.006289104,0.001361805\n",
      "Iteration 33105: loss = 0.0062801815,0.0029325543\n",
      "Iteration 33110: loss = 0.006284838,0.00034576815\n",
      "Iteration 33115: loss = 0.0062825023,0.0009872145\n",
      "Iteration 33120: loss = 0.006271146,0.0002518189\n",
      "Iteration 33125: loss = 0.006269868,0.00029776045\n",
      "Iteration 33130: loss = 0.0062754345,0.00024942725\n",
      "Iteration 33135: loss = 0.0062749577,2.6363605e-05\n",
      "Iteration 33140: loss = 0.0062744543,0.0001293408\n",
      "Iteration 33145: loss = 0.006278841,5.4009335e-05\n",
      "Iteration 33150: loss = 0.0062781777,1.1360122e-05\n",
      "Iteration 33155: loss = 0.0062767416,3.5042565e-05\n",
      "Iteration 33160: loss = 0.0062777437,4.4805685e-05\n",
      "Iteration 33165: loss = 0.0062750573,4.7070764e-05\n",
      "Iteration 33170: loss = 0.006277155,4.6689354e-05\n",
      "Iteration 33175: loss = 0.006275229,6.8707675e-05\n",
      "Iteration 33180: loss = 0.0062788506,0.00014662328\n",
      "Iteration 33185: loss = 0.006272966,0.0005058357\n",
      "Iteration 33190: loss = 0.0062847175,0.0023026878\n",
      "Iteration 33195: loss = 0.0062579154,0.012051299\n",
      "Iteration 33200: loss = 0.0063089556,0.038393736\n",
      "Iteration 33205: loss = 0.0062673893,0.005428031\n",
      "Iteration 33210: loss = 0.006259385,0.013564106\n",
      "Iteration 33215: loss = 0.0062750936,2.964915e-05\n",
      "Iteration 33220: loss = 0.006286291,0.0043863114\n",
      "Iteration 33225: loss = 0.0062814415,0.0013802164\n",
      "Iteration 33230: loss = 0.0062722433,0.0005030658\n",
      "Iteration 33235: loss = 0.0062713977,0.0010887523\n",
      "Iteration 33240: loss = 0.0062779654,1.2081271e-05\n",
      "Iteration 33245: loss = 0.0062807784,0.000435048\n",
      "Iteration 33250: loss = 0.0062766764,1.8467366e-05\n",
      "Iteration 33255: loss = 0.0062735663,0.00017767926\n",
      "Iteration 33260: loss = 0.006276289,1.1454099e-05\n",
      "Iteration 33265: loss = 0.00627836,7.38175e-05\n",
      "Iteration 33270: loss = 0.006276294,1.2877231e-05\n",
      "Iteration 33275: loss = 0.00627552,2.8117473e-05\n",
      "Iteration 33280: loss = 0.0062769824,2.095272e-05\n",
      "Iteration 33285: loss = 0.0062765013,9.677215e-06\n",
      "Iteration 33290: loss = 0.0062761027,1.3928217e-05\n",
      "Iteration 33295: loss = 0.006276682,1.345144e-05\n",
      "Iteration 33300: loss = 0.00627611,1.1664275e-05\n",
      "Iteration 33305: loss = 0.0062764906,9.90436e-06\n",
      "Iteration 33310: loss = 0.00627632,9.748947e-06\n",
      "Iteration 33315: loss = 0.0062762205,9.951475e-06\n",
      "Iteration 33320: loss = 0.00627647,1.00756915e-05\n",
      "Iteration 33325: loss = 0.006276017,1.1817272e-05\n",
      "Iteration 33330: loss = 0.0062767826,1.7491433e-05\n",
      "Iteration 33335: loss = 0.006274935,6.325618e-05\n",
      "Iteration 33340: loss = 0.0062799365,0.00040939837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33345: loss = 0.0062651555,0.003677525\n",
      "Iteration 33350: loss = 0.006308898,0.03208591\n",
      "Iteration 33355: loss = 0.0062390813,0.061166164\n",
      "Iteration 33360: loss = 0.006239193,0.022867816\n",
      "Iteration 33365: loss = 0.006259793,0.0011145297\n",
      "Iteration 33370: loss = 0.0062839887,0.0016005671\n",
      "Iteration 33375: loss = 0.006295512,0.0044122823\n",
      "Iteration 33380: loss = 0.006295597,0.0041757226\n",
      "Iteration 33385: loss = 0.0062895208,0.002188056\n",
      "Iteration 33390: loss = 0.0062825903,0.0005334142\n",
      "Iteration 33395: loss = 0.0062770247,1.1531134e-05\n",
      "Iteration 33400: loss = 0.0062728617,0.00018530361\n",
      "Iteration 33405: loss = 0.0062708235,0.00032001853\n",
      "Iteration 33410: loss = 0.006272062,0.00018461229\n",
      "Iteration 33415: loss = 0.006275223,2.8293152e-05\n",
      "Iteration 33420: loss = 0.006277412,2.1335174e-05\n",
      "Iteration 33425: loss = 0.0062781065,4.936128e-05\n",
      "Iteration 33430: loss = 0.0062776194,2.5544994e-05\n",
      "Iteration 33435: loss = 0.006276117,9.919606e-06\n",
      "Iteration 33440: loss = 0.0062753395,1.8488478e-05\n",
      "Iteration 33445: loss = 0.0062756483,1.4076148e-05\n",
      "Iteration 33450: loss = 0.006276308,9.814457e-06\n",
      "Iteration 33455: loss = 0.006276725,1.1562453e-05\n",
      "Iteration 33460: loss = 0.0062761945,9.776827e-06\n",
      "Iteration 33465: loss = 0.0062760185,1.0419273e-05\n",
      "Iteration 33470: loss = 0.0062757484,1.0340737e-05\n",
      "Iteration 33475: loss = 0.006276831,9.715945e-06\n",
      "Iteration 33480: loss = 0.0062747486,1.4273053e-05\n",
      "Iteration 33485: loss = 0.006280162,3.3194996e-05\n",
      "Iteration 33490: loss = 0.0062625255,0.0003076754\n",
      "Iteration 33495: loss = 0.006320331,0.0029370727\n",
      "Iteration 33500: loss = 0.006213753,0.007061483\n",
      "Iteration 33505: loss = 0.006232213,0.002834324\n",
      "Iteration 33510: loss = 0.0062624793,0.00037110507\n",
      "Iteration 33515: loss = 0.0062802793,0.00013511402\n",
      "Iteration 33520: loss = 0.0062843785,0.00018435152\n",
      "Iteration 33525: loss = 0.0062890835,0.00021724562\n",
      "Iteration 33530: loss = 0.006289447,0.00022949348\n",
      "Iteration 33535: loss = 0.0062856846,0.00018557352\n",
      "Iteration 33540: loss = 0.0062850933,0.00011383102\n",
      "Iteration 33545: loss = 0.0062791556,7.673374e-05\n",
      "Iteration 33550: loss = 0.0062795,9.6813674e-05\n",
      "Iteration 33555: loss = 0.006271905,0.0002688514\n",
      "Iteration 33560: loss = 0.006280747,0.0009906371\n",
      "Iteration 33565: loss = 0.0062596295,0.004729468\n",
      "Iteration 33570: loss = 0.0063053635,0.021252032\n",
      "Iteration 33575: loss = 0.006241679,0.032388967\n",
      "Iteration 33580: loss = 0.0062697176,0.00084405666\n",
      "Iteration 33585: loss = 0.006292103,0.008995954\n",
      "Iteration 33590: loss = 0.0062848586,0.0025287317\n",
      "Iteration 33595: loss = 0.0062703807,0.0013070861\n",
      "Iteration 33600: loss = 0.006269152,0.002058967\n",
      "Iteration 33605: loss = 0.0062779537,3.9182494e-05\n",
      "Iteration 33610: loss = 0.0062819677,0.0008740061\n",
      "Iteration 33615: loss = 0.0062772054,1.6740034e-05\n",
      "Iteration 33620: loss = 0.0062733446,0.00034050888\n",
      "Iteration 33625: loss = 0.006276062,1.2301443e-05\n",
      "Iteration 33630: loss = 0.006278178,0.0001375237\n",
      "Iteration 33635: loss = 0.0062759337,1.3811052e-05\n",
      "Iteration 33640: loss = 0.006275192,5.1761494e-05\n",
      "Iteration 33645: loss = 0.0062771533,3.0711795e-05\n",
      "Iteration 33650: loss = 0.00627644,9.8909095e-06\n",
      "Iteration 33655: loss = 0.0062757544,2.0827583e-05\n",
      "Iteration 33660: loss = 0.006276781,1.770946e-05\n",
      "Iteration 33665: loss = 0.0062759877,1.1877358e-05\n",
      "Iteration 33670: loss = 0.0062762257,9.697067e-06\n",
      "Iteration 33675: loss = 0.006276298,9.931271e-06\n",
      "Iteration 33680: loss = 0.0062760008,1.11712425e-05\n",
      "Iteration 33685: loss = 0.006276546,1.3341862e-05\n",
      "Iteration 33690: loss = 0.0062755663,2.346224e-05\n",
      "Iteration 33695: loss = 0.0062773903,6.4025255e-05\n",
      "Iteration 33700: loss = 0.006273148,0.00031890982\n",
      "Iteration 33705: loss = 0.0062838704,0.0020852033\n",
      "Iteration 33710: loss = 0.006255178,0.01567343\n",
      "Iteration 33715: loss = 0.006316216,0.059186444\n",
      "Iteration 33720: loss = 0.006279649,4.828484e-05\n",
      "Iteration 33725: loss = 0.0062592407,0.014822462\n",
      "Iteration 33730: loss = 0.0062601953,0.0105468305\n",
      "Iteration 33735: loss = 0.0062712673,0.00094444456\n",
      "Iteration 33740: loss = 0.006280307,0.00087767834\n",
      "Iteration 33745: loss = 0.0062831384,0.0021856704\n",
      "Iteration 33750: loss = 0.006279915,0.0010293975\n",
      "Iteration 33755: loss = 0.00627573,3.485387e-05\n",
      "Iteration 33760: loss = 0.006273117,0.00030657192\n",
      "Iteration 33765: loss = 0.0062744454,0.0003088432\n",
      "Iteration 33770: loss = 0.0062773484,2.3379702e-05\n",
      "Iteration 33775: loss = 0.006278396,7.404531e-05\n",
      "Iteration 33780: loss = 0.006276759,6.789e-05\n",
      "Iteration 33785: loss = 0.0062752427,1.2368856e-05\n",
      "Iteration 33790: loss = 0.006275824,3.4039294e-05\n",
      "Iteration 33795: loss = 0.0062768105,1.2393871e-05\n",
      "Iteration 33800: loss = 0.006276598,1.5636278e-05\n",
      "Iteration 33805: loss = 0.0062760147,1.2494337e-05\n",
      "Iteration 33810: loss = 0.0062762126,1.1583059e-05\n",
      "Iteration 33815: loss = 0.0062765176,9.990956e-06\n",
      "Iteration 33820: loss = 0.006276093,1.0663142e-05\n",
      "Iteration 33825: loss = 0.0062761107,9.542144e-06\n",
      "Iteration 33830: loss = 0.0062762443,1.0045286e-05\n",
      "Iteration 33835: loss = 0.006275996,9.6839085e-06\n",
      "Iteration 33840: loss = 0.0062762196,9.395852e-06\n",
      "Iteration 33845: loss = 0.0062760706,9.540547e-06\n",
      "Iteration 33850: loss = 0.006275965,9.563184e-06\n",
      "Iteration 33855: loss = 0.0062762666,9.292918e-06\n",
      "Iteration 33860: loss = 0.006275715,9.89062e-06\n",
      "Iteration 33865: loss = 0.0062765204,9.320832e-06\n",
      "Iteration 33870: loss = 0.006274938,1.2220933e-05\n",
      "Iteration 33875: loss = 0.0062784776,1.6564034e-05\n",
      "Iteration 33880: loss = 0.006269129,8.48546e-05\n",
      "Iteration 33885: loss = 0.006296422,0.0005787509\n",
      "Iteration 33890: loss = 0.0062206066,0.004424442\n",
      "Iteration 33895: loss = 0.0063282796,0.0034348813\n",
      "Iteration 33900: loss = 0.0063006976,0.008496954\n",
      "Iteration 33905: loss = 0.006347457,0.037706394\n",
      "Iteration 33910: loss = 0.006258086,0.029874325\n",
      "Iteration 33915: loss = 0.0062615904,0.017489322\n",
      "Iteration 33920: loss = 0.006281052,0.00017160828\n",
      "Iteration 33925: loss = 0.00629504,0.004676203\n",
      "Iteration 33930: loss = 0.0062983963,0.0048277937\n",
      "Iteration 33935: loss = 0.006291326,0.00095749745\n",
      "Iteration 33940: loss = 0.0062832516,0.00027232303\n",
      "Iteration 33945: loss = 0.0062783547,0.0010173976\n",
      "Iteration 33950: loss = 0.006278969,0.00047383635\n",
      "Iteration 33955: loss = 0.006281502,3.6583086e-05\n",
      "Iteration 33960: loss = 0.006282648,0.00020929234\n",
      "Iteration 33965: loss = 0.0062809195,0.000102037775\n",
      "Iteration 33970: loss = 0.006278042,2.1253798e-05\n",
      "Iteration 33975: loss = 0.0062767775,6.538453e-05\n",
      "Iteration 33980: loss = 0.006277457,1.3686351e-05\n",
      "Iteration 33985: loss = 0.006278254,2.4594852e-05\n",
      "Iteration 33990: loss = 0.0062775915,1.3411494e-05\n",
      "Iteration 33995: loss = 0.006276574,1.49267125e-05\n",
      "Iteration 34000: loss = 0.0062765316,1.13357955e-05\n",
      "Iteration 34005: loss = 0.006276934,1.1740712e-05\n",
      "Iteration 34010: loss = 0.006276574,9.562719e-06\n",
      "Iteration 34015: loss = 0.0062761214,1.14793e-05\n",
      "Iteration 34020: loss = 0.00627636,9.636605e-06\n",
      "Iteration 34025: loss = 0.0062762997,9.813454e-06\n",
      "Iteration 34030: loss = 0.006275996,1.0493009e-05\n",
      "Iteration 34035: loss = 0.0062762685,9.902128e-06\n",
      "Iteration 34040: loss = 0.006276076,9.81931e-06\n",
      "Iteration 34045: loss = 0.006276157,9.575697e-06\n",
      "Iteration 34050: loss = 0.006276162,9.543538e-06\n",
      "Iteration 34055: loss = 0.0062761153,9.59261e-06\n",
      "Iteration 34060: loss = 0.006276217,9.592201e-06\n",
      "Iteration 34065: loss = 0.0062759854,1.0225756e-05\n",
      "Iteration 34070: loss = 0.0062763765,1.1725994e-05\n",
      "Iteration 34075: loss = 0.0062753134,2.5248451e-05\n",
      "Iteration 34080: loss = 0.0062780194,0.00011644745\n",
      "Iteration 34085: loss = 0.006270287,0.0009572987\n",
      "Iteration 34090: loss = 0.0062941336,0.009233149\n",
      "Iteration 34095: loss = 0.006230155,0.065779\n",
      "Iteration 34100: loss = 0.0062837605,0.0066982326\n",
      "Iteration 34105: loss = 0.0062983376,0.024904294\n",
      "Iteration 34110: loss = 0.006296061,0.016228156\n",
      "Iteration 34115: loss = 0.0062882155,0.005388592\n",
      "Iteration 34120: loss = 0.006280201,0.00062633876\n",
      "Iteration 34125: loss = 0.006275061,0.00012416963\n",
      "Iteration 34130: loss = 0.006271798,0.0008450858\n",
      "Iteration 34135: loss = 0.0062717,0.0011099317\n",
      "Iteration 34140: loss = 0.0062731225,0.0007511317\n",
      "Iteration 34145: loss = 0.006275732,0.00023854135\n",
      "Iteration 34150: loss = 0.0062781177,1.645686e-05\n",
      "Iteration 34155: loss = 0.006279214,5.8948994e-05\n",
      "Iteration 34160: loss = 0.00627869,9.839173e-05\n",
      "Iteration 34165: loss = 0.006277125,4.949461e-05\n",
      "Iteration 34170: loss = 0.006275467,1.2325033e-05\n",
      "Iteration 34175: loss = 0.0062746406,2.2104248e-05\n",
      "Iteration 34180: loss = 0.0062751058,2.2491435e-05\n",
      "Iteration 34185: loss = 0.006276184,1.0586995e-05\n",
      "Iteration 34190: loss = 0.0062769414,1.0693357e-05\n",
      "Iteration 34195: loss = 0.0062768236,1.175491e-05\n",
      "Iteration 34200: loss = 0.0062760883,9.4047155e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34205: loss = 0.0062755714,1.1147319e-05\n",
      "Iteration 34210: loss = 0.006275812,9.862876e-06\n",
      "Iteration 34215: loss = 0.0062761935,9.405473e-06\n",
      "Iteration 34220: loss = 0.006276235,9.416299e-06\n",
      "Iteration 34225: loss = 0.0062759253,9.494634e-06\n",
      "Iteration 34230: loss = 0.00627584,9.633445e-06\n",
      "Iteration 34235: loss = 0.006276012,9.396631e-06\n",
      "Iteration 34240: loss = 0.006276012,9.329562e-06\n",
      "Iteration 34245: loss = 0.0062758937,9.411349e-06\n",
      "Iteration 34250: loss = 0.006275819,9.47115e-06\n",
      "Iteration 34255: loss = 0.006275872,9.393647e-06\n",
      "Iteration 34260: loss = 0.006275929,9.323348e-06\n",
      "Iteration 34265: loss = 0.006275822,9.405234e-06\n",
      "Iteration 34270: loss = 0.0062757786,9.414658e-06\n",
      "Iteration 34275: loss = 0.0062758587,9.329678e-06\n",
      "Iteration 34280: loss = 0.006275773,9.386579e-06\n",
      "Iteration 34285: loss = 0.0062757316,9.413138e-06\n",
      "Iteration 34290: loss = 0.006275926,9.372745e-06\n",
      "Iteration 34295: loss = 0.0062754187,1.0593856e-05\n",
      "Iteration 34300: loss = 0.0062764375,1.3670954e-05\n",
      "Iteration 34305: loss = 0.0062739006,4.5434826e-05\n",
      "Iteration 34310: loss = 0.006280784,0.0002736199\n",
      "Iteration 34315: loss = 0.006260321,0.002533024\n",
      "Iteration 34320: loss = 0.006322071,0.02375026\n",
      "Iteration 34325: loss = 0.006217072,0.07261193\n",
      "Iteration 34330: loss = 0.0061906367,0.016434055\n",
      "Iteration 34335: loss = 0.006221497,0.005190327\n",
      "Iteration 34340: loss = 0.006241286,0.005978953\n",
      "Iteration 34345: loss = 0.0062446673,0.00475982\n",
      "Iteration 34350: loss = 0.006241364,0.0027960592\n",
      "Iteration 34355: loss = 0.0062441654,0.0015958893\n",
      "Iteration 34360: loss = 0.0062521175,0.0009284133\n",
      "Iteration 34365: loss = 0.0062637334,0.0005040652\n",
      "Iteration 34370: loss = 0.006273946,0.00017727366\n",
      "Iteration 34375: loss = 0.006281297,4.8122052e-05\n",
      "Iteration 34380: loss = 0.006284245,9.900301e-05\n",
      "Iteration 34385: loss = 0.0062840586,0.000111997215\n",
      "Iteration 34390: loss = 0.0062820367,4.9526752e-05\n",
      "Iteration 34395: loss = 0.006279525,2.3789065e-05\n",
      "Iteration 34400: loss = 0.0062776557,2.2050379e-05\n",
      "Iteration 34405: loss = 0.0062766564,1.05806375e-05\n",
      "Iteration 34410: loss = 0.006276198,1.1393075e-05\n",
      "Iteration 34415: loss = 0.0062758066,1.2345492e-05\n",
      "Iteration 34420: loss = 0.0062755067,1.09296025e-05\n",
      "Iteration 34425: loss = 0.0062754476,1.17059635e-05\n",
      "Iteration 34430: loss = 0.0062755686,1.0572948e-05\n",
      "Iteration 34435: loss = 0.006275712,1.0708812e-05\n",
      "Iteration 34440: loss = 0.0062757134,1.0119803e-05\n",
      "Iteration 34445: loss = 0.00627573,1.0116716e-05\n",
      "Iteration 34450: loss = 0.0062758434,9.830228e-06\n",
      "Iteration 34455: loss = 0.0062758774,9.699208e-06\n",
      "Iteration 34460: loss = 0.006275866,9.690913e-06\n",
      "Iteration 34465: loss = 0.0062759607,9.6424255e-06\n",
      "Iteration 34470: loss = 0.006275877,9.771558e-06\n",
      "Iteration 34475: loss = 0.00627597,9.620468e-06\n",
      "Iteration 34480: loss = 0.006275901,9.579174e-06\n",
      "Iteration 34485: loss = 0.0062759235,9.46906e-06\n",
      "Iteration 34490: loss = 0.0062759235,9.480065e-06\n",
      "Iteration 34495: loss = 0.0062758937,9.502725e-06\n",
      "Iteration 34500: loss = 0.006275883,9.473748e-06\n",
      "Iteration 34505: loss = 0.0062758513,9.470375e-06\n",
      "Iteration 34510: loss = 0.0062758643,9.452686e-06\n",
      "Iteration 34515: loss = 0.0062758108,9.5383675e-06\n",
      "Iteration 34520: loss = 0.0062758983,9.965621e-06\n",
      "Iteration 34525: loss = 0.006275562,1.5452297e-05\n",
      "Iteration 34530: loss = 0.006276622,8.0066675e-05\n",
      "Iteration 34535: loss = 0.006272651,0.0010313367\n",
      "Iteration 34540: loss = 0.0062883594,0.015829096\n",
      "Iteration 34545: loss = 0.006237259,0.12052214\n",
      "Iteration 34550: loss = 0.006265908,0.016499138\n",
      "Iteration 34555: loss = 0.0062842877,0.0015572252\n",
      "Iteration 34560: loss = 0.006293025,0.00027547544\n",
      "Iteration 34565: loss = 0.006290689,0.00031277435\n",
      "Iteration 34570: loss = 0.006282961,0.00026148098\n",
      "Iteration 34575: loss = 0.0062779505,0.00017449546\n",
      "Iteration 34580: loss = 0.0062751113,0.00011786168\n",
      "Iteration 34585: loss = 0.006272295,8.8059474e-05\n",
      "Iteration 34590: loss = 0.0062736473,5.0184564e-05\n",
      "Iteration 34595: loss = 0.0062737097,3.0438085e-05\n",
      "Iteration 34600: loss = 0.006273993,2.9465524e-05\n",
      "Iteration 34605: loss = 0.006275404,2.4861714e-05\n",
      "Iteration 34610: loss = 0.0062753693,2.0644115e-05\n",
      "Iteration 34615: loss = 0.006276037,2.157638e-05\n",
      "Iteration 34620: loss = 0.006276604,2.4175033e-05\n",
      "Iteration 34625: loss = 0.006276622,2.4765406e-05\n",
      "Iteration 34630: loss = 0.006276931,2.2499584e-05\n",
      "Iteration 34635: loss = 0.0062768683,1.8703937e-05\n",
      "Iteration 34640: loss = 0.006276743,1.4612982e-05\n",
      "Iteration 34645: loss = 0.006276509,1.0879649e-05\n",
      "Iteration 34650: loss = 0.0062762965,9.418118e-06\n",
      "Iteration 34655: loss = 0.0062761013,9.387196e-06\n",
      "Iteration 34660: loss = 0.0062759877,9.847896e-06\n",
      "Iteration 34665: loss = 0.0062759425,9.909498e-06\n",
      "Iteration 34670: loss = 0.0062759654,9.60774e-06\n",
      "Iteration 34675: loss = 0.0062760296,9.343839e-06\n",
      "Iteration 34680: loss = 0.0062760976,9.274952e-06\n",
      "Iteration 34685: loss = 0.00627609,9.280725e-06\n",
      "Iteration 34690: loss = 0.0062760166,9.278945e-06\n",
      "Iteration 34695: loss = 0.0062759444,9.352483e-06\n",
      "Iteration 34700: loss = 0.0062759244,9.345271e-06\n",
      "Iteration 34705: loss = 0.0062759076,9.328029e-06\n",
      "Iteration 34710: loss = 0.00627592,9.282633e-06\n",
      "Iteration 34715: loss = 0.006275927,9.27146e-06\n",
      "Iteration 34720: loss = 0.006275904,9.272618e-06\n",
      "Iteration 34725: loss = 0.0062758806,9.272897e-06\n",
      "Iteration 34730: loss = 0.006275859,9.277072e-06\n",
      "Iteration 34735: loss = 0.0062758233,9.288485e-06\n",
      "Iteration 34740: loss = 0.006275807,9.294982e-06\n",
      "Iteration 34745: loss = 0.006275794,9.285076e-06\n",
      "Iteration 34750: loss = 0.0062757824,9.278967e-06\n",
      "Iteration 34755: loss = 0.0062757805,9.261666e-06\n",
      "Iteration 34760: loss = 0.006275741,9.280346e-06\n",
      "Iteration 34765: loss = 0.0062757167,9.292084e-06\n",
      "Iteration 34770: loss = 0.006275745,9.256747e-06\n",
      "Iteration 34775: loss = 0.0062756,9.523643e-06\n",
      "Iteration 34780: loss = 0.0062757656,9.358801e-06\n",
      "Iteration 34785: loss = 0.006275604,9.413302e-06\n",
      "Iteration 34790: loss = 0.0062757456,9.257622e-06\n",
      "Iteration 34795: loss = 0.0062755267,9.504617e-06\n",
      "Iteration 34800: loss = 0.0062757176,9.443329e-06\n",
      "Iteration 34805: loss = 0.0062754224,1.0327847e-05\n",
      "Iteration 34810: loss = 0.0062759384,1.1778967e-05\n",
      "Iteration 34815: loss = 0.006274851,2.1910086e-05\n",
      "Iteration 34820: loss = 0.006277246,7.092896e-05\n",
      "Iteration 34825: loss = 0.0062713143,0.0004291372\n",
      "Iteration 34830: loss = 0.006287548,0.0033184686\n",
      "Iteration 34835: loss = 0.0062424527,0.026779367\n",
      "Iteration 34840: loss = 0.006319858,0.056926753\n",
      "Iteration 34845: loss = 0.006308623,0.011827181\n",
      "Iteration 34850: loss = 0.006284608,0.0009802941\n",
      "Iteration 34855: loss = 0.0062609892,0.006760755\n",
      "Iteration 34860: loss = 0.006246544,0.0062717455\n",
      "Iteration 34865: loss = 0.006248997,0.0024451117\n",
      "Iteration 34870: loss = 0.0062644687,0.0002202618\n",
      "Iteration 34875: loss = 0.0062815663,0.00025406727\n",
      "Iteration 34880: loss = 0.006290142,0.0006884431\n",
      "Iteration 34885: loss = 0.006287336,0.0004258047\n",
      "Iteration 34890: loss = 0.0062784515,4.9775674e-05\n",
      "Iteration 34895: loss = 0.006271508,5.1458606e-05\n",
      "Iteration 34900: loss = 0.00627053,0.000117493226\n",
      "Iteration 34905: loss = 0.0062732473,4.6252888e-05\n",
      "Iteration 34910: loss = 0.006276106,1.0477169e-05\n",
      "Iteration 34915: loss = 0.006277455,2.9489147e-05\n",
      "Iteration 34920: loss = 0.006276937,1.4483934e-05\n",
      "Iteration 34925: loss = 0.006275009,1.152917e-05\n",
      "Iteration 34930: loss = 0.0062741004,1.6546506e-05\n",
      "Iteration 34935: loss = 0.0062753647,9.3613835e-06\n",
      "Iteration 34940: loss = 0.006276285,1.066161e-05\n",
      "Iteration 34945: loss = 0.0062754857,9.30321e-06\n",
      "Iteration 34950: loss = 0.0062750275,1.0327911e-05\n",
      "Iteration 34955: loss = 0.0062752995,9.381581e-06\n",
      "Iteration 34960: loss = 0.00627559,9.383584e-06\n",
      "Iteration 34965: loss = 0.006275501,9.133586e-06\n",
      "Iteration 34970: loss = 0.0062749186,9.925363e-06\n",
      "Iteration 34975: loss = 0.0062759914,9.151287e-06\n",
      "Iteration 34980: loss = 0.0062742694,1.2357924e-05\n",
      "Iteration 34985: loss = 0.0062777833,1.965703e-05\n",
      "Iteration 34990: loss = 0.006268363,0.00011572493\n",
      "Iteration 34995: loss = 0.0062963455,0.000879608\n",
      "Iteration 35000: loss = 0.0062229126,0.0063328343\n",
      "Iteration 35005: loss = 0.006288031,0.006612366\n",
      "Iteration 35010: loss = 0.006336419,0.011528477\n",
      "Iteration 35015: loss = 0.006296337,0.0060419356\n",
      "Iteration 35020: loss = 0.0062959953,0.0005313074\n",
      "Iteration 35025: loss = 0.0062921173,0.0014949753\n",
      "Iteration 35030: loss = 0.006268075,0.001687499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35035: loss = 0.0062762504,0.0005184174\n",
      "Iteration 35040: loss = 0.0062690135,7.926961e-05\n",
      "Iteration 35045: loss = 0.0062669204,0.0002117042\n",
      "Iteration 35050: loss = 0.006274125,0.00039640834\n",
      "Iteration 35055: loss = 0.006267388,0.00065870397\n",
      "Iteration 35060: loss = 0.006281967,0.0012923684\n",
      "Iteration 35065: loss = 0.0062647946,0.0033341895\n",
      "Iteration 35070: loss = 0.006296478,0.0089698\n",
      "Iteration 35075: loss = 0.006252689,0.016675876\n",
      "Iteration 35080: loss = 0.006292034,0.006541569\n",
      "Iteration 35085: loss = 0.0062814075,0.0007214854\n",
      "Iteration 35090: loss = 0.006264067,0.004121926\n",
      "Iteration 35095: loss = 0.0062789246,0.00039352063\n",
      "Iteration 35100: loss = 0.006280152,0.0007462621\n",
      "Iteration 35105: loss = 0.006269578,0.0010125518\n",
      "Iteration 35110: loss = 0.0062770955,9.4457675e-05\n",
      "Iteration 35115: loss = 0.006277522,0.000118621654\n",
      "Iteration 35120: loss = 0.0062725437,0.0003239619\n",
      "Iteration 35125: loss = 0.0062786774,0.0002734843\n",
      "Iteration 35130: loss = 0.006273305,0.00017897172\n",
      "Iteration 35135: loss = 0.006277529,0.00013135732\n",
      "Iteration 35140: loss = 0.0062732585,0.00016473884\n",
      "Iteration 35145: loss = 0.006278772,0.00034366903\n",
      "Iteration 35150: loss = 0.006269458,0.0011376548\n",
      "Iteration 35155: loss = 0.006287812,0.0046987575\n",
      "Iteration 35160: loss = 0.0062513463,0.019089827\n",
      "Iteration 35165: loss = 0.0063036084,0.027839066\n",
      "Iteration 35170: loss = 0.0062783808,6.927713e-05\n",
      "Iteration 35175: loss = 0.006260826,0.009240042\n",
      "Iteration 35180: loss = 0.006272349,0.00045645662\n",
      "Iteration 35185: loss = 0.006284231,0.0028162594\n",
      "Iteration 35190: loss = 0.0062787565,0.0005553355\n",
      "Iteration 35195: loss = 0.006269822,0.00087919226\n",
      "Iteration 35200: loss = 0.006272799,0.00027716273\n",
      "Iteration 35205: loss = 0.006279431,0.0003626862\n",
      "Iteration 35210: loss = 0.006276972,5.857859e-05\n",
      "Iteration 35215: loss = 0.0062728194,0.00020027376\n",
      "Iteration 35220: loss = 0.0062754326,1.0623447e-05\n",
      "Iteration 35225: loss = 0.006276918,6.992878e-05\n",
      "Iteration 35230: loss = 0.006274454,4.9522852e-05\n",
      "Iteration 35235: loss = 0.006275531,9.336908e-06\n",
      "Iteration 35240: loss = 0.0062758066,1.8235522e-05\n",
      "Iteration 35245: loss = 0.0062746326,2.6775178e-05\n",
      "Iteration 35250: loss = 0.0062761344,2.3337649e-05\n",
      "Iteration 35255: loss = 0.0062747044,2.2343993e-05\n",
      "Iteration 35260: loss = 0.0062758992,2.2566646e-05\n",
      "Iteration 35265: loss = 0.0062744464,3.7544272e-05\n",
      "Iteration 35270: loss = 0.006276974,9.4721356e-05\n",
      "Iteration 35275: loss = 0.0062718135,0.0003861531\n",
      "Iteration 35280: loss = 0.0062833056,0.002041291\n",
      "Iteration 35285: loss = 0.0062556327,0.012579815\n",
      "Iteration 35290: loss = 0.0063115438,0.045316987\n",
      "Iteration 35295: loss = 0.006271319,0.0032164017\n",
      "Iteration 35300: loss = 0.006253015,0.016732242\n",
      "Iteration 35305: loss = 0.00626219,0.0024316371\n",
      "Iteration 35310: loss = 0.0062808134,0.0014822552\n",
      "Iteration 35315: loss = 0.006288409,0.003474565\n",
      "Iteration 35320: loss = 0.006281398,0.00051783526\n",
      "Iteration 35325: loss = 0.0062707434,0.0003992994\n",
      "Iteration 35330: loss = 0.0062689683,0.0007689843\n",
      "Iteration 35335: loss = 0.006274684,3.2246036e-05\n",
      "Iteration 35340: loss = 0.00627866,0.00021673633\n",
      "Iteration 35345: loss = 0.0062769693,9.624931e-05\n",
      "Iteration 35350: loss = 0.006274114,4.4112167e-05\n",
      "Iteration 35355: loss = 0.006273909,6.2501706e-05\n",
      "Iteration 35360: loss = 0.00627566,1.6265594e-05\n",
      "Iteration 35365: loss = 0.006276269,2.8353335e-05\n",
      "Iteration 35370: loss = 0.0062747444,1.4831427e-05\n",
      "Iteration 35375: loss = 0.0062746704,1.4405232e-05\n",
      "Iteration 35380: loss = 0.006275808,1.37738425e-05\n",
      "Iteration 35385: loss = 0.0062749493,9.351777e-06\n",
      "Iteration 35390: loss = 0.006275267,1.09218e-05\n",
      "Iteration 35395: loss = 0.006274547,1.1532253e-05\n",
      "Iteration 35400: loss = 0.006276963,1.2181921e-05\n",
      "Iteration 35405: loss = 0.0062703243,4.5528926e-05\n",
      "Iteration 35410: loss = 0.0062883818,0.00024007673\n",
      "Iteration 35415: loss = 0.0062379506,0.0020215663\n",
      "Iteration 35420: loss = 0.0063415393,0.0058562304\n",
      "Iteration 35425: loss = 0.0062920884,0.0004803432\n",
      "Iteration 35430: loss = 0.006263167,0.00053966435\n",
      "Iteration 35435: loss = 0.0062461733,0.0013391303\n",
      "Iteration 35440: loss = 0.006265197,0.0019655088\n",
      "Iteration 35445: loss = 0.0062478003,0.006284896\n",
      "Iteration 35450: loss = 0.0063112383,0.022680627\n",
      "Iteration 35455: loss = 0.00625132,0.02160748\n",
      "Iteration 35460: loss = 0.0062736995,0.0019118185\n",
      "Iteration 35465: loss = 0.0062983446,0.0067037684\n",
      "Iteration 35470: loss = 0.0062885038,0.0015870206\n",
      "Iteration 35475: loss = 0.0062712114,0.0017386457\n",
      "Iteration 35480: loss = 0.0062712324,0.0009925386\n",
      "Iteration 35485: loss = 0.0062795323,0.00048364015\n",
      "Iteration 35490: loss = 0.006278187,0.00040125984\n",
      "Iteration 35495: loss = 0.006271426,0.00023958152\n",
      "Iteration 35500: loss = 0.006272509,0.00010534562\n",
      "Iteration 35505: loss = 0.006277029,0.0001610833\n",
      "Iteration 35510: loss = 0.0062753656,9.198148e-06\n",
      "Iteration 35515: loss = 0.006274307,7.178201e-05\n",
      "Iteration 35520: loss = 0.006276878,4.1495103e-05\n",
      "Iteration 35525: loss = 0.006275523,9.301347e-06\n",
      "Iteration 35530: loss = 0.0062747994,1.7907767e-05\n",
      "Iteration 35535: loss = 0.0062758946,2.4370454e-05\n",
      "Iteration 35540: loss = 0.00627443,2.5983829e-05\n",
      "Iteration 35545: loss = 0.006276088,2.6502485e-05\n",
      "Iteration 35550: loss = 0.0062744035,3.904482e-05\n",
      "Iteration 35555: loss = 0.006276859,8.048618e-05\n",
      "Iteration 35560: loss = 0.006272185,0.0002787343\n",
      "Iteration 35565: loss = 0.0062815733,0.0012450374\n",
      "Iteration 35570: loss = 0.0062601506,0.006969491\n",
      "Iteration 35575: loss = 0.006307332,0.03175066\n",
      "Iteration 35580: loss = 0.00625217,0.022305312\n",
      "Iteration 35585: loss = 0.0062577724,0.009902162\n",
      "Iteration 35590: loss = 0.006279116,0.0014298157\n",
      "Iteration 35595: loss = 0.0062875743,0.0062770564\n",
      "Iteration 35600: loss = 0.006279524,0.00082835276\n",
      "Iteration 35605: loss = 0.0062701968,0.0009039069\n",
      "Iteration 35610: loss = 0.0062698796,0.0012930685\n",
      "Iteration 35615: loss = 0.006276468,1.1452129e-05\n",
      "Iteration 35620: loss = 0.0062796026,0.00046992133\n",
      "Iteration 35625: loss = 0.0062759365,8.729938e-05\n",
      "Iteration 35630: loss = 0.0062723304,0.0001289329\n",
      "Iteration 35635: loss = 0.006274207,6.5551845e-05\n",
      "Iteration 35640: loss = 0.0062773214,4.7880192e-05\n",
      "Iteration 35645: loss = 0.0062759514,2.5281877e-05\n",
      "Iteration 35650: loss = 0.006273662,3.3353346e-05\n",
      "Iteration 35655: loss = 0.0062750424,1.0108542e-05\n",
      "Iteration 35660: loss = 0.0062762112,1.9425108e-05\n",
      "Iteration 35665: loss = 0.0062746587,1.2050527e-05\n",
      "Iteration 35670: loss = 0.0062748254,9.918467e-06\n",
      "Iteration 35675: loss = 0.0062756035,1.1274303e-05\n",
      "Iteration 35680: loss = 0.0062747523,1.147709e-05\n",
      "Iteration 35685: loss = 0.006275218,9.637982e-06\n",
      "Iteration 35690: loss = 0.0062750163,9.204288e-06\n",
      "Iteration 35695: loss = 0.0062750545,9.044297e-06\n",
      "Iteration 35700: loss = 0.0062749125,9.247505e-06\n",
      "Iteration 35705: loss = 0.006275224,9.279559e-06\n",
      "Iteration 35710: loss = 0.006274659,1.0896018e-05\n",
      "Iteration 35715: loss = 0.0062757605,1.6841477e-05\n",
      "Iteration 35720: loss = 0.006272973,6.8930785e-05\n",
      "Iteration 35725: loss = 0.0062803184,0.00045814013\n",
      "Iteration 35730: loss = 0.006258951,0.004275025\n",
      "Iteration 35735: loss = 0.0063202423,0.036839817\n",
      "Iteration 35740: loss = 0.006244531,0.049204893\n",
      "Iteration 35745: loss = 0.0062087304,0.028336465\n",
      "Iteration 35750: loss = 0.006224157,0.0060107405\n",
      "Iteration 35755: loss = 0.0062528686,0.0008320613\n",
      "Iteration 35760: loss = 0.0062744264,0.0009512591\n",
      "Iteration 35765: loss = 0.006288644,0.0011809979\n",
      "Iteration 35770: loss = 0.0062966687,0.0010820982\n",
      "Iteration 35775: loss = 0.0062993453,0.0008891497\n",
      "Iteration 35780: loss = 0.0062979218,0.0006765186\n",
      "Iteration 35785: loss = 0.0062937737,0.000453494\n",
      "Iteration 35790: loss = 0.0062889657,0.00026437023\n",
      "Iteration 35795: loss = 0.0062842374,0.00014202838\n",
      "Iteration 35800: loss = 0.0062806667,6.536572e-05\n",
      "Iteration 35805: loss = 0.0062781028,2.2207456e-05\n",
      "Iteration 35810: loss = 0.0062760934,1.04705105e-05\n",
      "Iteration 35815: loss = 0.006274281,1.20306995e-05\n",
      "Iteration 35820: loss = 0.006273065,1.707096e-05\n",
      "Iteration 35825: loss = 0.0062728063,1.8857512e-05\n",
      "Iteration 35830: loss = 0.006273389,1.5275333e-05\n",
      "Iteration 35835: loss = 0.006274236,1.1240906e-05\n",
      "Iteration 35840: loss = 0.006274964,9.147135e-06\n",
      "Iteration 35845: loss = 0.0062754466,8.95708e-06\n",
      "Iteration 35850: loss = 0.006275629,9.013782e-06\n",
      "Iteration 35855: loss = 0.006275386,8.867116e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35860: loss = 0.00627498,9.040642e-06\n",
      "Iteration 35865: loss = 0.00627473,9.361581e-06\n",
      "Iteration 35870: loss = 0.006274721,9.324452e-06\n",
      "Iteration 35875: loss = 0.006274901,9.070281e-06\n",
      "Iteration 35880: loss = 0.0062749856,8.96999e-06\n",
      "Iteration 35885: loss = 0.0062749754,8.970118e-06\n",
      "Iteration 35890: loss = 0.0062749106,9.001681e-06\n",
      "Iteration 35895: loss = 0.006274799,9.102146e-06\n",
      "Iteration 35900: loss = 0.006274815,9.062315e-06\n",
      "Iteration 35905: loss = 0.00627489,8.974888e-06\n",
      "Iteration 35910: loss = 0.006274786,9.079077e-06\n",
      "Iteration 35915: loss = 0.0062749223,9.007648e-06\n",
      "Iteration 35920: loss = 0.0062745507,9.973622e-06\n",
      "Iteration 35925: loss = 0.0062755756,1.5356902e-05\n",
      "Iteration 35930: loss = 0.0062721125,9.42503e-05\n",
      "Iteration 35935: loss = 0.006284559,0.0011403143\n",
      "Iteration 35940: loss = 0.0062438897,0.015729347\n",
      "Iteration 35945: loss = 0.0062511526,0.07225235\n",
      "Iteration 35950: loss = 0.0062931697,0.0014895155\n",
      "Iteration 35955: loss = 0.006304556,0.009386242\n",
      "Iteration 35960: loss = 0.006292717,0.0115545085\n",
      "Iteration 35965: loss = 0.0062763556,0.0065580606\n",
      "Iteration 35970: loss = 0.006268138,0.0020637626\n",
      "Iteration 35975: loss = 0.0062610502,0.00042415806\n",
      "Iteration 35980: loss = 0.006261528,0.0006128212\n",
      "Iteration 35985: loss = 0.0062658773,0.0008622471\n",
      "Iteration 35990: loss = 0.006270008,0.00053679507\n",
      "Iteration 35995: loss = 0.0062750657,0.00011388078\n",
      "Iteration 36000: loss = 0.006278941,2.0867943e-05\n",
      "Iteration 36005: loss = 0.0062799565,9.74193e-05\n",
      "Iteration 36010: loss = 0.0062797456,7.959685e-05\n",
      "Iteration 36015: loss = 0.006278333,1.7828545e-05\n",
      "Iteration 36020: loss = 0.006276731,1.9539879e-05\n",
      "Iteration 36025: loss = 0.006276225,2.6316917e-05\n",
      "Iteration 36030: loss = 0.0062764282,1.1023738e-05\n",
      "Iteration 36035: loss = 0.006276699,1.112584e-05\n",
      "Iteration 36040: loss = 0.0062766243,1.1937756e-05\n",
      "Iteration 36045: loss = 0.0062760436,8.65691e-06\n",
      "Iteration 36050: loss = 0.006275713,1.0282071e-05\n",
      "Iteration 36055: loss = 0.0062758117,8.835241e-06\n",
      "Iteration 36060: loss = 0.006275904,9.084354e-06\n",
      "Iteration 36065: loss = 0.006275762,8.72254e-06\n",
      "Iteration 36070: loss = 0.0062755826,9.029929e-06\n",
      "Iteration 36075: loss = 0.0062756115,8.767964e-06\n",
      "Iteration 36080: loss = 0.0062756077,8.801678e-06\n",
      "Iteration 36085: loss = 0.0062754513,8.936064e-06\n",
      "Iteration 36090: loss = 0.0062754937,8.840471e-06\n",
      "Iteration 36095: loss = 0.006275356,8.953197e-06\n",
      "Iteration 36100: loss = 0.006275412,8.908678e-06\n",
      "Iteration 36105: loss = 0.0062752687,9.074418e-06\n",
      "Iteration 36110: loss = 0.0062753647,8.9432415e-06\n",
      "Iteration 36115: loss = 0.0062751956,9.094958e-06\n",
      "Iteration 36120: loss = 0.006275318,9.010784e-06\n",
      "Iteration 36125: loss = 0.006275125,9.341773e-06\n",
      "Iteration 36130: loss = 0.006275406,9.702181e-06\n",
      "Iteration 36135: loss = 0.0062747723,1.382791e-05\n",
      "Iteration 36140: loss = 0.006276066,3.235433e-05\n",
      "Iteration 36145: loss = 0.006272774,0.00016683723\n",
      "Iteration 36150: loss = 0.0062816404,0.0012123093\n",
      "Iteration 36155: loss = 0.006255875,0.010796311\n",
      "Iteration 36160: loss = 0.0063214605,0.06104633\n",
      "Iteration 36165: loss = 0.0062702796,0.0030670862\n",
      "Iteration 36170: loss = 0.006255463,0.020792197\n",
      "Iteration 36175: loss = 0.0062578605,0.013128312\n",
      "Iteration 36180: loss = 0.006266557,0.0029746522\n",
      "Iteration 36185: loss = 0.0062745376,1.1921073e-05\n",
      "Iteration 36190: loss = 0.006279793,0.000978007\n",
      "Iteration 36195: loss = 0.0062809656,0.0016248807\n",
      "Iteration 36200: loss = 0.0062792893,0.00095298217\n",
      "Iteration 36205: loss = 0.0062758676,0.00015643949\n",
      "Iteration 36210: loss = 0.0062728934,4.2828247e-05\n",
      "Iteration 36215: loss = 0.0062718126,0.00019838772\n",
      "Iteration 36220: loss = 0.006272847,0.00013397672\n",
      "Iteration 36225: loss = 0.006274953,1.537129e-05\n",
      "Iteration 36230: loss = 0.00627657,2.9591149e-05\n",
      "Iteration 36235: loss = 0.0062767514,3.959743e-05\n",
      "Iteration 36240: loss = 0.006275689,1.0350706e-05\n",
      "Iteration 36245: loss = 0.0062746457,1.5992498e-05\n",
      "Iteration 36250: loss = 0.0062745004,1.5403193e-05\n",
      "Iteration 36255: loss = 0.006275006,9.089575e-06\n",
      "Iteration 36260: loss = 0.006275315,1.1353612e-05\n",
      "Iteration 36265: loss = 0.006275169,8.878518e-06\n",
      "Iteration 36270: loss = 0.006274883,1.01789965e-05\n",
      "Iteration 36275: loss = 0.0062749316,9.07611e-06\n",
      "Iteration 36280: loss = 0.006275088,9.331569e-06\n",
      "Iteration 36285: loss = 0.0062749167,8.889188e-06\n",
      "Iteration 36290: loss = 0.0062748753,9.084842e-06\n",
      "Iteration 36295: loss = 0.006274881,8.893723e-06\n",
      "Iteration 36300: loss = 0.0062749353,8.878034e-06\n",
      "Iteration 36305: loss = 0.0062749214,8.818317e-06\n",
      "Iteration 36310: loss = 0.006274844,8.907099e-06\n",
      "Iteration 36315: loss = 0.006274829,8.8796105e-06\n",
      "Iteration 36320: loss = 0.006274844,8.848621e-06\n",
      "Iteration 36325: loss = 0.0062747668,8.945171e-06\n",
      "Iteration 36330: loss = 0.006274954,9.041171e-06\n",
      "Iteration 36335: loss = 0.0062745907,9.773823e-06\n",
      "Iteration 36340: loss = 0.006274896,1.0042295e-05\n",
      "Iteration 36345: loss = 0.0062744278,1.4073208e-05\n",
      "Iteration 36350: loss = 0.0062756077,3.188188e-05\n",
      "Iteration 36355: loss = 0.006272387,0.00016155359\n",
      "Iteration 36360: loss = 0.0062812045,0.0011708587\n",
      "Iteration 36365: loss = 0.006255452,0.010427087\n",
      "Iteration 36370: loss = 0.0063204076,0.06031556\n",
      "Iteration 36375: loss = 0.0062733106,0.004078239\n",
      "Iteration 36380: loss = 0.0062475298,0.021542141\n",
      "Iteration 36385: loss = 0.006242458,0.013126021\n",
      "Iteration 36390: loss = 0.0062545803,0.003002568\n",
      "Iteration 36395: loss = 0.006271979,2.2564702e-05\n",
      "Iteration 36400: loss = 0.0062848707,0.00091877603\n",
      "Iteration 36405: loss = 0.0062879194,0.0015847582\n",
      "Iteration 36410: loss = 0.0062843733,0.0009974911\n",
      "Iteration 36415: loss = 0.006278695,0.00020476268\n",
      "Iteration 36420: loss = 0.0062741432,1.8788836e-05\n",
      "Iteration 36425: loss = 0.006271132,0.00016709563\n",
      "Iteration 36430: loss = 0.0062706973,0.00015007869\n",
      "Iteration 36435: loss = 0.0062731854,2.9150822e-05\n",
      "Iteration 36440: loss = 0.00627603,1.8042752e-05\n",
      "Iteration 36445: loss = 0.006276496,3.86376e-05\n",
      "Iteration 36450: loss = 0.006275486,1.48866175e-05\n",
      "Iteration 36455: loss = 0.006274273,1.1832109e-05\n",
      "Iteration 36460: loss = 0.006273752,1.721844e-05\n",
      "Iteration 36465: loss = 0.0062744934,9.243147e-06\n",
      "Iteration 36470: loss = 0.006275167,1.0253349e-05\n",
      "Iteration 36475: loss = 0.0062748156,9.240671e-06\n",
      "Iteration 36480: loss = 0.006274558,9.1976135e-06\n",
      "Iteration 36485: loss = 0.006274173,9.645868e-06\n",
      "Iteration 36490: loss = 0.006275213,8.707068e-06\n",
      "Iteration 36495: loss = 0.0062734787,1.2243444e-05\n",
      "Iteration 36500: loss = 0.0062776427,2.0884741e-05\n",
      "Iteration 36505: loss = 0.0062654777,0.00014559466\n",
      "Iteration 36510: loss = 0.006302584,0.001188308\n",
      "Iteration 36515: loss = 0.0062119775,0.006581512\n",
      "Iteration 36520: loss = 0.006273534,0.0004576113\n",
      "Iteration 36525: loss = 0.006307062,0.0012476632\n",
      "Iteration 36530: loss = 0.0063061374,0.0012250228\n",
      "Iteration 36535: loss = 0.0062935613,0.0006807306\n",
      "Iteration 36540: loss = 0.0062866244,0.00022571243\n",
      "Iteration 36545: loss = 0.0062728147,8.7279e-05\n",
      "Iteration 36550: loss = 0.0062708748,0.0001215902\n",
      "Iteration 36555: loss = 0.006265526,0.00017686342\n",
      "Iteration 36560: loss = 0.006271931,0.00022772331\n",
      "Iteration 36565: loss = 0.006267375,0.0005491669\n",
      "Iteration 36570: loss = 0.006285642,0.0021744121\n",
      "Iteration 36575: loss = 0.006255582,0.010346521\n",
      "Iteration 36580: loss = 0.0063143685,0.031671304\n",
      "Iteration 36585: loss = 0.006263118,0.0077000842\n",
      "Iteration 36590: loss = 0.006258993,0.009511429\n",
      "Iteration 36595: loss = 0.0062787025,0.00078165863\n",
      "Iteration 36600: loss = 0.0062849964,0.004440296\n",
      "Iteration 36605: loss = 0.00627402,4.5430745e-05\n",
      "Iteration 36610: loss = 0.0062665474,0.0015624305\n",
      "Iteration 36615: loss = 0.006271845,0.00017441889\n",
      "Iteration 36620: loss = 0.006278796,0.00050022476\n",
      "Iteration 36625: loss = 0.006276908,9.027995e-05\n",
      "Iteration 36630: loss = 0.0062729013,0.00021627716\n",
      "Iteration 36635: loss = 0.006274682,1.8726249e-05\n",
      "Iteration 36640: loss = 0.006276618,0.00010170779\n",
      "Iteration 36645: loss = 0.0062742108,1.5404561e-05\n",
      "Iteration 36650: loss = 0.0062737525,3.1673946e-05\n",
      "Iteration 36655: loss = 0.006275568,3.338223e-05\n",
      "Iteration 36660: loss = 0.0062744725,1.2541752e-05\n",
      "Iteration 36665: loss = 0.0062746275,1.0054175e-05\n",
      "Iteration 36670: loss = 0.006275162,1.4360252e-05\n",
      "Iteration 36675: loss = 0.0062741176,1.8360579e-05\n",
      "Iteration 36680: loss = 0.0062752347,1.9493682e-05\n",
      "Iteration 36685: loss = 0.0062739043,2.8610191e-05\n",
      "Iteration 36690: loss = 0.0062759076,5.6888362e-05\n",
      "Iteration 36695: loss = 0.00627226,0.00019079543\n",
      "Iteration 36700: loss = 0.0062797205,0.00085610506\n",
      "Iteration 36705: loss = 0.0062623885,0.004934465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36710: loss = 0.006302616,0.025839757\n",
      "Iteration 36715: loss = 0.0062449,0.034430698\n",
      "Iteration 36720: loss = 0.006260749,0.005464348\n",
      "Iteration 36725: loss = 0.006284075,0.0043574185\n",
      "Iteration 36730: loss = 0.0062889173,0.0069309874\n",
      "Iteration 36735: loss = 0.006277822,0.00040732807\n",
      "Iteration 36740: loss = 0.0062682647,0.0013130682\n",
      "Iteration 36745: loss = 0.0062688794,0.0014047244\n",
      "Iteration 36750: loss = 0.0062757055,1.31995075e-05\n",
      "Iteration 36755: loss = 0.006278919,0.0004950249\n",
      "Iteration 36760: loss = 0.006275576,0.0001533613\n",
      "Iteration 36765: loss = 0.006272167,9.2661794e-05\n",
      "Iteration 36770: loss = 0.006273687,0.00011331152\n",
      "Iteration 36775: loss = 0.00627636,2.1179807e-05\n",
      "Iteration 36780: loss = 0.006275209,5.058394e-05\n",
      "Iteration 36785: loss = 0.006273354,1.7196737e-05\n",
      "Iteration 36790: loss = 0.0062746187,2.2324572e-05\n",
      "Iteration 36795: loss = 0.0062755304,1.5636322e-05\n",
      "Iteration 36800: loss = 0.006273975,1.1041433e-05\n",
      "Iteration 36805: loss = 0.006274376,1.4217379e-05\n",
      "Iteration 36810: loss = 0.0062751505,9.764653e-06\n",
      "Iteration 36815: loss = 0.0062739756,9.837491e-06\n",
      "Iteration 36820: loss = 0.0062748226,9.611079e-06\n",
      "Iteration 36825: loss = 0.0062744073,9.91812e-06\n",
      "Iteration 36830: loss = 0.0062743737,9.624441e-06\n",
      "Iteration 36835: loss = 0.006274696,9.26522e-06\n",
      "Iteration 36840: loss = 0.0062739816,1.04299625e-05\n",
      "Iteration 36845: loss = 0.006275257,1.084092e-05\n",
      "Iteration 36850: loss = 0.0062725153,2.251623e-05\n",
      "Iteration 36855: loss = 0.006279092,6.794572e-05\n",
      "Iteration 36860: loss = 0.006261337,0.0004573655\n",
      "Iteration 36865: loss = 0.006312841,0.0034899132\n",
      "Iteration 36870: loss = 0.0061866227,0.023209648\n",
      "Iteration 36875: loss = 0.0062872064,0.044181097\n",
      "Iteration 36880: loss = 0.0062873303,0.0003559124\n",
      "Iteration 36885: loss = 0.006241623,0.011658179\n",
      "Iteration 36890: loss = 0.006229952,0.0045910366\n",
      "Iteration 36895: loss = 0.006246107,0.0029425977\n",
      "Iteration 36900: loss = 0.0062664915,0.0029857894\n",
      "Iteration 36905: loss = 0.006281957,0.00029476304\n",
      "Iteration 36910: loss = 0.0062860674,0.0005852225\n",
      "Iteration 36915: loss = 0.0062842867,0.00072169036\n",
      "Iteration 36920: loss = 0.00628114,5.1135892e-05\n",
      "Iteration 36925: loss = 0.006276578,0.00017969581\n",
      "Iteration 36930: loss = 0.0062718727,8.943127e-05\n",
      "Iteration 36935: loss = 0.006269626,8.146395e-05\n",
      "Iteration 36940: loss = 0.006272556,4.2049473e-05\n",
      "Iteration 36945: loss = 0.0062768073,1.887794e-05\n",
      "Iteration 36950: loss = 0.0062778764,1.8169956e-05\n",
      "Iteration 36955: loss = 0.0062765316,1.861777e-05\n",
      "Iteration 36960: loss = 0.006276319,1.0216267e-05\n",
      "Iteration 36965: loss = 0.0062763127,1.2771159e-05\n",
      "Iteration 36970: loss = 0.0062750396,9.463938e-06\n",
      "Iteration 36975: loss = 0.0062747393,9.9584795e-06\n",
      "Iteration 36980: loss = 0.006275076,1.013937e-05\n",
      "Iteration 36985: loss = 0.0062746867,9.74653e-06\n",
      "Iteration 36990: loss = 0.0062749423,8.977453e-06\n",
      "Iteration 36995: loss = 0.006274993,8.802932e-06\n",
      "Iteration 37000: loss = 0.006274902,9.000965e-06\n",
      "Iteration 37005: loss = 0.006275187,9.02882e-06\n",
      "Iteration 37010: loss = 0.006274762,1.01000105e-05\n",
      "Iteration 37015: loss = 0.0062755053,1.2806544e-05\n",
      "Iteration 37020: loss = 0.0062738433,3.3257078e-05\n",
      "Iteration 37025: loss = 0.006277763,0.00015054776\n",
      "Iteration 37030: loss = 0.0062672063,0.0011048273\n",
      "Iteration 37035: loss = 0.006297398,0.009219657\n",
      "Iteration 37040: loss = 0.006222309,0.055651825\n",
      "Iteration 37045: loss = 0.0062884414,0.006908005\n",
      "Iteration 37050: loss = 0.006301699,0.022788042\n",
      "Iteration 37055: loss = 0.006292065,0.010115276\n",
      "Iteration 37060: loss = 0.0062789805,0.00087026466\n",
      "Iteration 37065: loss = 0.0062695644,0.0005931187\n",
      "Iteration 37070: loss = 0.0062663225,0.0021821223\n",
      "Iteration 37075: loss = 0.006267583,0.0017298518\n",
      "Iteration 37080: loss = 0.006271793,0.000436115\n",
      "Iteration 37085: loss = 0.006276009,1.34392285e-05\n",
      "Iteration 37090: loss = 0.0062781903,0.000242191\n",
      "Iteration 37095: loss = 0.0062778764,0.00022768039\n",
      "Iteration 37100: loss = 0.0062758774,3.4759076e-05\n",
      "Iteration 37105: loss = 0.0062739905,3.093891e-05\n",
      "Iteration 37110: loss = 0.0062734447,6.164827e-05\n",
      "Iteration 37115: loss = 0.006274322,1.5938707e-05\n",
      "Iteration 37120: loss = 0.0062752888,1.5741183e-05\n",
      "Iteration 37125: loss = 0.006275331,1.7889091e-05\n",
      "Iteration 37130: loss = 0.006274702,8.754915e-06\n",
      "Iteration 37135: loss = 0.006274346,1.2753184e-05\n",
      "Iteration 37140: loss = 0.006274584,9.077086e-06\n",
      "Iteration 37145: loss = 0.006274828,9.482626e-06\n",
      "Iteration 37150: loss = 0.006274697,8.821406e-06\n",
      "Iteration 37155: loss = 0.006274486,9.258265e-06\n",
      "Iteration 37160: loss = 0.006274574,8.751225e-06\n",
      "Iteration 37165: loss = 0.006274625,8.797117e-06\n",
      "Iteration 37170: loss = 0.006274586,8.711231e-06\n",
      "Iteration 37175: loss = 0.0062745265,8.775663e-06\n",
      "Iteration 37180: loss = 0.00627453,8.721623e-06\n",
      "Iteration 37185: loss = 0.0062745432,8.71322e-06\n",
      "Iteration 37190: loss = 0.0062745064,8.7124145e-06\n",
      "Iteration 37195: loss = 0.006274479,8.7249e-06\n",
      "Iteration 37200: loss = 0.0062745176,8.701198e-06\n",
      "Iteration 37205: loss = 0.0062743486,9.044955e-06\n",
      "Iteration 37210: loss = 0.0062745544,9.21935e-06\n",
      "Iteration 37215: loss = 0.006274183,1.0360825e-05\n",
      "Iteration 37220: loss = 0.0062748343,1.4018198e-05\n",
      "Iteration 37225: loss = 0.00627337,4.046684e-05\n",
      "Iteration 37230: loss = 0.0062769153,0.00020436553\n",
      "Iteration 37235: loss = 0.006267198,0.0015744347\n",
      "Iteration 37240: loss = 0.0062953657,0.013424193\n",
      "Iteration 37245: loss = 0.006230094,0.06530264\n",
      "Iteration 37250: loss = 0.0062727532,0.00010321739\n",
      "Iteration 37255: loss = 0.0062926826,0.014743802\n",
      "Iteration 37260: loss = 0.00629425,0.012781382\n",
      "Iteration 37265: loss = 0.00628244,0.0032902586\n",
      "Iteration 37270: loss = 0.006271793,0.00012350384\n",
      "Iteration 37275: loss = 0.0062661893,0.0009617631\n",
      "Iteration 37280: loss = 0.006265385,0.0015076812\n",
      "Iteration 37285: loss = 0.0062691686,0.00074879866\n",
      "Iteration 37290: loss = 0.006273654,8.212381e-05\n",
      "Iteration 37295: loss = 0.006277256,9.375848e-05\n",
      "Iteration 37300: loss = 0.006278115,0.0002007981\n",
      "Iteration 37305: loss = 0.0062766764,7.920363e-05\n",
      "Iteration 37310: loss = 0.0062745395,1.0937689e-05\n",
      "Iteration 37315: loss = 0.0062732757,4.7875226e-05\n",
      "Iteration 37320: loss = 0.006273702,2.866609e-05\n",
      "Iteration 37325: loss = 0.0062747486,8.978189e-06\n",
      "Iteration 37330: loss = 0.006275268,1.7405237e-05\n",
      "Iteration 37335: loss = 0.0062748822,9.814674e-06\n",
      "Iteration 37340: loss = 0.0062743635,1.0550349e-05\n",
      "Iteration 37345: loss = 0.006274355,1.0110671e-05\n",
      "Iteration 37350: loss = 0.0062746727,8.791933e-06\n",
      "Iteration 37355: loss = 0.0062746317,9.025563e-06\n",
      "Iteration 37360: loss = 0.0062744035,8.663161e-06\n",
      "Iteration 37365: loss = 0.0062743113,8.891673e-06\n",
      "Iteration 37370: loss = 0.006274466,8.589497e-06\n",
      "Iteration 37375: loss = 0.0062743872,8.547409e-06\n",
      "Iteration 37380: loss = 0.006274277,8.732058e-06\n",
      "Iteration 37385: loss = 0.006274412,8.61769e-06\n",
      "Iteration 37390: loss = 0.00627425,8.681784e-06\n",
      "Iteration 37395: loss = 0.006274253,8.616986e-06\n",
      "Iteration 37400: loss = 0.006274302,8.578361e-06\n",
      "Iteration 37405: loss = 0.006274231,8.621588e-06\n",
      "Iteration 37410: loss = 0.0062741893,8.618117e-06\n",
      "Iteration 37415: loss = 0.006274221,8.568723e-06\n",
      "Iteration 37420: loss = 0.0062741735,8.598453e-06\n",
      "Iteration 37425: loss = 0.0062741376,8.615407e-06\n",
      "Iteration 37430: loss = 0.0062741856,8.557861e-06\n",
      "Iteration 37435: loss = 0.0062740794,8.647442e-06\n",
      "Iteration 37440: loss = 0.0062741474,8.562323e-06\n",
      "Iteration 37445: loss = 0.0062740636,8.638941e-06\n",
      "Iteration 37450: loss = 0.0062741493,8.686384e-06\n",
      "Iteration 37455: loss = 0.0062737423,1.1013948e-05\n",
      "Iteration 37460: loss = 0.0062752403,4.0136812e-05\n",
      "Iteration 37465: loss = 0.006269053,0.00059954595\n",
      "Iteration 37470: loss = 0.006295958,0.011633916\n",
      "Iteration 37475: loss = 0.0062055946,0.13374408\n",
      "Iteration 37480: loss = 0.0062251296,0.015482562\n",
      "Iteration 37485: loss = 0.00623695,0.005489045\n",
      "Iteration 37490: loss = 0.006235027,0.0057160608\n",
      "Iteration 37495: loss = 0.006233241,0.0070706448\n",
      "Iteration 37500: loss = 0.006233443,0.007319945\n",
      "Iteration 37505: loss = 0.006239724,0.005478736\n",
      "Iteration 37510: loss = 0.006250365,0.0028804082\n",
      "Iteration 37515: loss = 0.0062602167,0.0010288538\n",
      "Iteration 37520: loss = 0.006269707,0.00020788106\n",
      "Iteration 37525: loss = 0.006274669,1.9053328e-05\n",
      "Iteration 37530: loss = 0.006277828,4.7027315e-05\n",
      "Iteration 37535: loss = 0.0062786434,8.8276225e-05\n",
      "Iteration 37540: loss = 0.006278509,9.495972e-05\n",
      "Iteration 37545: loss = 0.006278103,7.687174e-05\n",
      "Iteration 37550: loss = 0.006277433,5.46978e-05\n",
      "Iteration 37555: loss = 0.0062768017,3.6756497e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37560: loss = 0.0062763556,2.4715346e-05\n",
      "Iteration 37565: loss = 0.0062758257,1.7583032e-05\n",
      "Iteration 37570: loss = 0.006275414,1.3417546e-05\n",
      "Iteration 37575: loss = 0.006275043,1.1430716e-05\n",
      "Iteration 37580: loss = 0.0062747835,1.0145308e-05\n",
      "Iteration 37585: loss = 0.006274635,9.435106e-06\n",
      "Iteration 37590: loss = 0.006274536,9.036974e-06\n",
      "Iteration 37595: loss = 0.006274419,8.803966e-06\n",
      "Iteration 37600: loss = 0.006274311,8.705012e-06\n",
      "Iteration 37605: loss = 0.0062742196,8.643301e-06\n",
      "Iteration 37610: loss = 0.006274173,8.621945e-06\n",
      "Iteration 37615: loss = 0.0062741153,8.615034e-06\n",
      "Iteration 37620: loss = 0.0062740804,8.609656e-06\n",
      "Iteration 37625: loss = 0.006274031,8.6405935e-06\n",
      "Iteration 37630: loss = 0.0062739975,8.6468e-06\n",
      "Iteration 37635: loss = 0.006274,8.620488e-06\n",
      "Iteration 37640: loss = 0.0062739565,8.644064e-06\n",
      "Iteration 37645: loss = 0.006273959,8.626558e-06\n",
      "Iteration 37650: loss = 0.0062739155,8.656989e-06\n",
      "Iteration 37655: loss = 0.006273905,8.655126e-06\n",
      "Iteration 37660: loss = 0.006273905,8.639852e-06\n",
      "Iteration 37665: loss = 0.006273936,8.615497e-06\n",
      "Iteration 37670: loss = 0.0062737335,8.840172e-06\n",
      "Iteration 37675: loss = 0.0062742135,8.518876e-06\n",
      "Iteration 37680: loss = 0.0062731397,1.0208891e-05\n",
      "Iteration 37685: loss = 0.006275303,1.1292507e-05\n",
      "Iteration 37690: loss = 0.006270166,3.8419617e-05\n",
      "Iteration 37695: loss = 0.0062841363,0.00020360605\n",
      "Iteration 37700: loss = 0.006244088,0.0017929224\n",
      "Iteration 37705: loss = 0.0063315234,0.0073234085\n",
      "Iteration 37710: loss = 0.006289415,0.0011005662\n",
      "Iteration 37715: loss = 0.0062689367,0.0013593207\n",
      "Iteration 37720: loss = 0.006260332,0.000515441\n",
      "Iteration 37725: loss = 0.006258862,0.00089534896\n",
      "Iteration 37730: loss = 0.0062628873,0.00026148988\n",
      "Iteration 37735: loss = 0.006269777,0.00020840779\n",
      "Iteration 37740: loss = 0.006276237,8.68403e-05\n",
      "Iteration 37745: loss = 0.0062792744,5.937907e-05\n",
      "Iteration 37750: loss = 0.0062791235,9.246962e-05\n",
      "Iteration 37755: loss = 0.0062763006,1.6236369e-05\n",
      "Iteration 37760: loss = 0.00627349,1.9936082e-05\n",
      "Iteration 37765: loss = 0.0062722513,3.6063713e-05\n",
      "Iteration 37770: loss = 0.0062728566,2.3167966e-05\n",
      "Iteration 37775: loss = 0.0062743505,1.1010548e-05\n",
      "Iteration 37780: loss = 0.0062751095,8.984927e-06\n",
      "Iteration 37785: loss = 0.0062747,8.501061e-06\n",
      "Iteration 37790: loss = 0.0062738317,9.538988e-06\n",
      "Iteration 37795: loss = 0.0062736436,1.1128282e-05\n",
      "Iteration 37800: loss = 0.006274169,1.6285474e-05\n",
      "Iteration 37805: loss = 0.0062742955,5.1570307e-05\n",
      "Iteration 37810: loss = 0.0062742066,0.00030007117\n",
      "Iteration 37815: loss = 0.0062735206,0.0023451422\n",
      "Iteration 37820: loss = 0.006277192,0.019393781\n",
      "Iteration 37825: loss = 0.0062489263,0.061985634\n",
      "Iteration 37830: loss = 0.006286327,0.0041253604\n",
      "Iteration 37835: loss = 0.006312724,0.0059998883\n",
      "Iteration 37840: loss = 0.006313247,0.010381315\n",
      "Iteration 37845: loss = 0.0062992107,0.004936726\n",
      "Iteration 37850: loss = 0.006282992,0.00050883094\n",
      "Iteration 37855: loss = 0.0062707835,0.00027115966\n",
      "Iteration 37860: loss = 0.006264858,0.0010819172\n",
      "Iteration 37865: loss = 0.0062649637,0.00076486246\n",
      "Iteration 37870: loss = 0.006268284,0.00012353942\n",
      "Iteration 37875: loss = 0.0062720627,8.704073e-05\n",
      "Iteration 37880: loss = 0.0062740273,0.00018867031\n",
      "Iteration 37885: loss = 0.006274143,6.575847e-05\n",
      "Iteration 37890: loss = 0.006273391,1.4102051e-05\n",
      "Iteration 37895: loss = 0.0062734843,4.7785114e-05\n",
      "Iteration 37900: loss = 0.006274572,1.7388034e-05\n",
      "Iteration 37905: loss = 0.006275507,1.2715231e-05\n",
      "Iteration 37910: loss = 0.006275244,1.5499334e-05\n",
      "Iteration 37915: loss = 0.006274224,8.486974e-06\n",
      "Iteration 37920: loss = 0.006273555,1.2747376e-05\n",
      "Iteration 37925: loss = 0.0062738075,9.026776e-06\n",
      "Iteration 37930: loss = 0.0062742154,9.77574e-06\n",
      "Iteration 37935: loss = 0.0062742108,8.407648e-06\n",
      "Iteration 37940: loss = 0.00627416,8.819148e-06\n",
      "Iteration 37945: loss = 0.0062741823,8.422417e-06\n",
      "Iteration 37950: loss = 0.0062740226,8.5925385e-06\n",
      "Iteration 37955: loss = 0.0062738974,8.735378e-06\n",
      "Iteration 37960: loss = 0.00627408,8.485742e-06\n",
      "Iteration 37965: loss = 0.0062739975,8.560966e-06\n",
      "Iteration 37970: loss = 0.006274075,8.543002e-06\n",
      "Iteration 37975: loss = 0.006273842,8.793471e-06\n",
      "Iteration 37980: loss = 0.0062739966,8.624986e-06\n",
      "Iteration 37985: loss = 0.006273855,8.83109e-06\n",
      "Iteration 37990: loss = 0.0062740124,8.850909e-06\n",
      "Iteration 37995: loss = 0.0062736757,9.861642e-06\n",
      "Iteration 38000: loss = 0.0062742955,1.287005e-05\n",
      "Iteration 38005: loss = 0.0062728063,3.6592846e-05\n",
      "Iteration 38010: loss = 0.00627647,0.00018971945\n",
      "Iteration 38015: loss = 0.006266547,0.0015025144\n",
      "Iteration 38020: loss = 0.0062953853,0.01312204\n",
      "Iteration 38025: loss = 0.006228356,0.066655196\n",
      "Iteration 38030: loss = 0.0062679253,0.00013072893\n",
      "Iteration 38035: loss = 0.006292556,0.013596981\n",
      "Iteration 38040: loss = 0.006299095,0.013284211\n",
      "Iteration 38045: loss = 0.0062939017,0.0053439476\n",
      "Iteration 38050: loss = 0.006282765,0.00063791475\n",
      "Iteration 38055: loss = 0.006271711,0.00015654627\n",
      "Iteration 38060: loss = 0.0062651415,0.0009855974\n",
      "Iteration 38065: loss = 0.0062651425,0.0010366733\n",
      "Iteration 38070: loss = 0.0062691425,0.00041042568\n",
      "Iteration 38075: loss = 0.00627356,2.2458182e-05\n",
      "Iteration 38080: loss = 0.006276062,7.7378325e-05\n",
      "Iteration 38085: loss = 0.0062766783,0.00013868314\n",
      "Iteration 38090: loss = 0.0062757093,5.079566e-05\n",
      "Iteration 38095: loss = 0.006273877,9.45321e-06\n",
      "Iteration 38100: loss = 0.0062724724,3.4980323e-05\n",
      "Iteration 38105: loss = 0.006272882,2.1580583e-05\n",
      "Iteration 38110: loss = 0.0062741414,8.589885e-06\n",
      "Iteration 38115: loss = 0.0062744473,1.3950605e-05\n",
      "Iteration 38120: loss = 0.0062740967,9.22608e-06\n",
      "Iteration 38125: loss = 0.0062736073,9.669433e-06\n",
      "Iteration 38130: loss = 0.0062734764,9.707853e-06\n",
      "Iteration 38135: loss = 0.006273905,8.45451e-06\n",
      "Iteration 38140: loss = 0.006273879,8.762603e-06\n",
      "Iteration 38145: loss = 0.006273695,8.590116e-06\n",
      "Iteration 38150: loss = 0.0062736548,8.638575e-06\n",
      "Iteration 38155: loss = 0.006273731,8.476548e-06\n",
      "Iteration 38160: loss = 0.006273763,8.460793e-06\n",
      "Iteration 38165: loss = 0.0062736645,8.5046195e-06\n",
      "Iteration 38170: loss = 0.0062736585,8.496634e-06\n",
      "Iteration 38175: loss = 0.0062735914,8.565126e-06\n",
      "Iteration 38180: loss = 0.006273905,8.321602e-06\n",
      "Iteration 38185: loss = 0.0062726433,1.0851465e-05\n",
      "Iteration 38190: loss = 0.0062774685,2.6593492e-05\n",
      "Iteration 38195: loss = 0.0062571126,0.0004265622\n",
      "Iteration 38200: loss = 0.006340515,0.006402731\n",
      "Iteration 38205: loss = 0.0062386706,0.0023967177\n",
      "Iteration 38210: loss = 0.006230435,0.0028115883\n",
      "Iteration 38215: loss = 0.006252198,0.0020042148\n",
      "Iteration 38220: loss = 0.006244885,0.0024411543\n",
      "Iteration 38225: loss = 0.0062841624,0.0060765594\n",
      "Iteration 38230: loss = 0.006242007,0.015517257\n",
      "Iteration 38235: loss = 0.0063058008,0.0130781345\n",
      "Iteration 38240: loss = 0.006283177,9.4479845e-05\n",
      "Iteration 38245: loss = 0.006267387,0.0048944256\n",
      "Iteration 38250: loss = 0.0062880423,0.00076739007\n",
      "Iteration 38255: loss = 0.0062871124,0.0010321368\n",
      "Iteration 38260: loss = 0.0062708836,0.0010318493\n",
      "Iteration 38265: loss = 0.0062762457,1.2878696e-05\n",
      "Iteration 38270: loss = 0.006279755,0.0004063577\n",
      "Iteration 38275: loss = 0.006270738,0.00029332182\n",
      "Iteration 38280: loss = 0.006275106,2.8497787e-05\n",
      "Iteration 38285: loss = 0.006274972,3.1687254e-05\n",
      "Iteration 38290: loss = 0.006271711,9.6001815e-05\n",
      "Iteration 38295: loss = 0.006276248,0.00012722291\n",
      "Iteration 38300: loss = 0.0062708627,0.00018719379\n",
      "Iteration 38305: loss = 0.0062780846,0.00035303226\n",
      "Iteration 38310: loss = 0.0062669697,0.0009852332\n",
      "Iteration 38315: loss = 0.0062872036,0.0034746367\n",
      "Iteration 38320: loss = 0.0062488173,0.013127149\n",
      "Iteration 38325: loss = 0.0063079116,0.025035903\n",
      "Iteration 38330: loss = 0.006265836,0.0019060901\n",
      "Iteration 38335: loss = 0.006257277,0.007211293\n",
      "Iteration 38340: loss = 0.0062799435,0.000838747\n",
      "Iteration 38345: loss = 0.0062845014,0.0026099756\n",
      "Iteration 38350: loss = 0.006270421,0.0003661397\n",
      "Iteration 38355: loss = 0.0062683127,0.00089845026\n",
      "Iteration 38360: loss = 0.0062774383,0.00028625666\n",
      "Iteration 38365: loss = 0.0062768664,0.00019147214\n",
      "Iteration 38370: loss = 0.0062708817,0.0002723053\n",
      "Iteration 38375: loss = 0.006274192,8.574971e-06\n",
      "Iteration 38380: loss = 0.006275944,9.883093e-05\n",
      "Iteration 38385: loss = 0.0062723733,8.0771984e-05\n",
      "Iteration 38390: loss = 0.006274555,1.5461297e-05\n",
      "Iteration 38395: loss = 0.00627441,1.2015925e-05\n",
      "Iteration 38400: loss = 0.006273069,2.9628318e-05\n",
      "Iteration 38405: loss = 0.006275116,4.2277334e-05\n",
      "Iteration 38410: loss = 0.0062724,6.852523e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38415: loss = 0.006276166,0.00013760898\n",
      "Iteration 38420: loss = 0.006269852,0.00042294324\n",
      "Iteration 38425: loss = 0.0062819705,0.0016809052\n",
      "Iteration 38430: loss = 0.00625621,0.008098841\n",
      "Iteration 38435: loss = 0.0063067484,0.028872784\n",
      "Iteration 38440: loss = 0.006253607,0.013973242\n",
      "Iteration 38445: loss = 0.006258985,0.007330508\n",
      "Iteration 38450: loss = 0.006282164,0.0021820061\n",
      "Iteration 38455: loss = 0.006286021,0.0044070943\n",
      "Iteration 38460: loss = 0.00627291,2.1438522e-05\n",
      "Iteration 38465: loss = 0.0062658344,0.001774595\n",
      "Iteration 38470: loss = 0.0062715206,0.00011624966\n",
      "Iteration 38475: loss = 0.0062779547,0.00056921545\n",
      "Iteration 38480: loss = 0.0062753814,8.992009e-05\n",
      "Iteration 38485: loss = 0.0062712035,0.0002248001\n",
      "Iteration 38490: loss = 0.0062731877,2.5807518e-05\n",
      "Iteration 38495: loss = 0.006275822,0.000108461965\n",
      "Iteration 38500: loss = 0.006273657,1.0287028e-05\n",
      "Iteration 38505: loss = 0.0062727965,4.0710263e-05\n",
      "Iteration 38510: loss = 0.006274579,2.8161789e-05\n",
      "Iteration 38515: loss = 0.006273674,8.629066e-06\n",
      "Iteration 38520: loss = 0.0062732287,1.591233e-05\n",
      "Iteration 38525: loss = 0.0062743165,1.8192795e-05\n",
      "Iteration 38530: loss = 0.0062732156,1.6679014e-05\n",
      "Iteration 38535: loss = 0.006274136,1.33947815e-05\n",
      "Iteration 38540: loss = 0.0062732534,1.4360911e-05\n",
      "Iteration 38545: loss = 0.006274222,1.7561222e-05\n",
      "Iteration 38550: loss = 0.0062727095,3.581973e-05\n",
      "Iteration 38555: loss = 0.006275505,0.000112223286\n",
      "Iteration 38560: loss = 0.0062692673,0.0005749798\n",
      "Iteration 38565: loss = 0.0062845745,0.0036325902\n",
      "Iteration 38570: loss = 0.0062462003,0.023500126\n",
      "Iteration 38575: loss = 0.0063097035,0.044422988\n",
      "Iteration 38580: loss = 0.006288253,0.004986463\n",
      "Iteration 38585: loss = 0.0062650456,0.004682676\n",
      "Iteration 38590: loss = 0.006260343,0.008525844\n",
      "Iteration 38595: loss = 0.006268157,0.001555315\n",
      "Iteration 38600: loss = 0.006276583,0.00052222254\n",
      "Iteration 38605: loss = 0.006279435,0.0016247318\n",
      "Iteration 38610: loss = 0.0062760576,0.0004977713\n",
      "Iteration 38615: loss = 0.0062715015,0.000101358724\n",
      "Iteration 38620: loss = 0.006270545,0.00036781322\n",
      "Iteration 38625: loss = 0.006273733,8.702441e-05\n",
      "Iteration 38630: loss = 0.006276302,5.8071855e-05\n",
      "Iteration 38635: loss = 0.0062752715,8.18719e-05\n",
      "Iteration 38640: loss = 0.0062725223,1.412537e-05\n",
      "Iteration 38645: loss = 0.0062723006,4.0838004e-05\n",
      "Iteration 38650: loss = 0.006274331,9.414115e-06\n",
      "Iteration 38655: loss = 0.006274927,1.9586958e-05\n",
      "Iteration 38660: loss = 0.0062734354,9.163989e-06\n",
      "Iteration 38665: loss = 0.006273156,1.3427804e-05\n",
      "Iteration 38670: loss = 0.0062741744,8.266939e-06\n",
      "Iteration 38675: loss = 0.006274,9.714274e-06\n",
      "Iteration 38680: loss = 0.006273187,1.0124481e-05\n",
      "Iteration 38685: loss = 0.0062738117,8.205459e-06\n",
      "Iteration 38690: loss = 0.006273804,8.536188e-06\n",
      "Iteration 38695: loss = 0.0062733446,9.037235e-06\n",
      "Iteration 38700: loss = 0.006273748,8.272154e-06\n",
      "Iteration 38705: loss = 0.006273409,8.517186e-06\n",
      "Iteration 38710: loss = 0.0062737125,8.275828e-06\n",
      "Iteration 38715: loss = 0.006273175,9.043904e-06\n",
      "Iteration 38720: loss = 0.006273975,8.874848e-06\n",
      "Iteration 38725: loss = 0.006272552,1.296099e-05\n",
      "Iteration 38730: loss = 0.0062754457,2.6056246e-05\n",
      "Iteration 38735: loss = 0.0062685125,0.00013975162\n",
      "Iteration 38740: loss = 0.006286614,0.00096005324\n",
      "Iteration 38745: loss = 0.0062359017,0.008561779\n",
      "Iteration 38750: loss = 0.006353243,0.051230896\n",
      "Iteration 38755: loss = 0.0063142846,0.016182866\n",
      "Iteration 38760: loss = 0.006267177,0.015827931\n",
      "Iteration 38765: loss = 0.0062791887,0.0032946377\n",
      "Iteration 38770: loss = 0.006308649,0.0014813871\n",
      "Iteration 38775: loss = 0.006320411,0.0036752538\n",
      "Iteration 38780: loss = 0.0063085654,0.0024192883\n",
      "Iteration 38785: loss = 0.0062842485,0.0003236473\n",
      "Iteration 38790: loss = 0.006264918,0.00017542238\n",
      "Iteration 38795: loss = 0.0062583815,0.0005690482\n",
      "Iteration 38800: loss = 0.006262951,0.0002741946\n",
      "Iteration 38805: loss = 0.0062719206,2.5281255e-05\n",
      "Iteration 38810: loss = 0.0062781447,7.851213e-05\n",
      "Iteration 38815: loss = 0.0062795854,5.011006e-05\n",
      "Iteration 38820: loss = 0.0062775817,2.9357268e-05\n",
      "Iteration 38825: loss = 0.006275696,3.0982537e-05\n",
      "Iteration 38830: loss = 0.006274782,8.236043e-06\n",
      "Iteration 38835: loss = 0.0062742434,1.449377e-05\n",
      "Iteration 38840: loss = 0.0062733893,1.0840496e-05\n",
      "Iteration 38845: loss = 0.0062729293,1.2679787e-05\n",
      "Iteration 38850: loss = 0.006273279,1.0125914e-05\n",
      "Iteration 38855: loss = 0.0062738056,9.696458e-06\n",
      "Iteration 38860: loss = 0.006273819,8.584453e-06\n",
      "Iteration 38865: loss = 0.006273866,8.679613e-06\n",
      "Iteration 38870: loss = 0.0062740967,8.292498e-06\n",
      "Iteration 38875: loss = 0.0062740594,8.216292e-06\n",
      "Iteration 38880: loss = 0.006273996,8.285343e-06\n",
      "Iteration 38885: loss = 0.0062740003,8.23653e-06\n",
      "Iteration 38890: loss = 0.0062739775,8.2451725e-06\n",
      "Iteration 38895: loss = 0.0062738997,8.293614e-06\n",
      "Iteration 38900: loss = 0.006273864,8.304424e-06\n",
      "Iteration 38905: loss = 0.006273849,8.305997e-06\n",
      "Iteration 38910: loss = 0.006273795,8.328199e-06\n",
      "Iteration 38915: loss = 0.006273756,8.335865e-06\n",
      "Iteration 38920: loss = 0.006273731,8.345437e-06\n",
      "Iteration 38925: loss = 0.006273717,8.342502e-06\n",
      "Iteration 38930: loss = 0.006273676,8.374587e-06\n",
      "Iteration 38935: loss = 0.0062737595,8.4136045e-06\n",
      "Iteration 38940: loss = 0.0062733893,1.0438588e-05\n",
      "Iteration 38945: loss = 0.006274625,3.20146e-05\n",
      "Iteration 38950: loss = 0.006269677,0.00040681745\n",
      "Iteration 38955: loss = 0.0062902323,0.006985788\n",
      "Iteration 38960: loss = 0.006215136,0.09410118\n",
      "Iteration 38965: loss = 0.0062702303,0.00085523166\n",
      "Iteration 38970: loss = 0.0062804227,0.0064688814\n",
      "Iteration 38975: loss = 0.006288553,0.007293157\n",
      "Iteration 38980: loss = 0.0062863105,0.0025831794\n",
      "Iteration 38985: loss = 0.006278401,0.0010521194\n",
      "Iteration 38990: loss = 0.0062760203,0.00029549462\n",
      "Iteration 38995: loss = 0.006276217,0.00016540095\n",
      "Iteration 39000: loss = 0.006272968,0.0001409462\n",
      "Iteration 39005: loss = 0.006271519,0.00020218582\n",
      "Iteration 39010: loss = 0.0062726666,0.00015889306\n",
      "Iteration 39015: loss = 0.006272596,0.00014155178\n",
      "Iteration 39020: loss = 0.006271554,0.000101934245\n",
      "Iteration 39025: loss = 0.006272136,6.4819564e-05\n",
      "Iteration 39030: loss = 0.0062731444,4.471504e-05\n",
      "Iteration 39035: loss = 0.006273484,2.555957e-05\n",
      "Iteration 39040: loss = 0.00627375,1.7722894e-05\n",
      "Iteration 39045: loss = 0.0062744133,1.1109716e-05\n",
      "Iteration 39050: loss = 0.0062746634,8.965646e-06\n",
      "Iteration 39055: loss = 0.0062746895,7.902988e-06\n",
      "Iteration 39060: loss = 0.006274694,8.139666e-06\n",
      "Iteration 39065: loss = 0.0062747006,8.274948e-06\n",
      "Iteration 39070: loss = 0.006274638,8.5220045e-06\n",
      "Iteration 39075: loss = 0.006274503,8.527009e-06\n",
      "Iteration 39080: loss = 0.0062744305,8.322383e-06\n",
      "Iteration 39085: loss = 0.006274344,8.120718e-06\n",
      "Iteration 39090: loss = 0.00627422,8.0647515e-06\n",
      "Iteration 39095: loss = 0.006274099,8.130551e-06\n",
      "Iteration 39100: loss = 0.006274039,8.191359e-06\n",
      "Iteration 39105: loss = 0.006273994,8.2097495e-06\n",
      "Iteration 39110: loss = 0.00627395,8.21137e-06\n",
      "Iteration 39115: loss = 0.006273929,8.188543e-06\n",
      "Iteration 39120: loss = 0.0062739053,8.197227e-06\n",
      "Iteration 39125: loss = 0.0062738736,8.200762e-06\n",
      "Iteration 39130: loss = 0.0062738494,8.210562e-06\n",
      "Iteration 39135: loss = 0.006273821,8.215098e-06\n",
      "Iteration 39140: loss = 0.006273801,8.212723e-06\n",
      "Iteration 39145: loss = 0.0062737707,8.220948e-06\n",
      "Iteration 39150: loss = 0.0062737414,8.23305e-06\n",
      "Iteration 39155: loss = 0.006273704,8.250041e-06\n",
      "Iteration 39160: loss = 0.006273691,8.241662e-06\n",
      "Iteration 39165: loss = 0.0062736534,8.26184e-06\n",
      "Iteration 39170: loss = 0.006273622,8.270956e-06\n",
      "Iteration 39175: loss = 0.0062736175,8.259673e-06\n",
      "Iteration 39180: loss = 0.0062735328,8.362619e-06\n",
      "Iteration 39185: loss = 0.0062736184,8.264461e-06\n",
      "Iteration 39190: loss = 0.0062735435,8.280547e-06\n",
      "Iteration 39195: loss = 0.0062735067,8.309024e-06\n",
      "Iteration 39200: loss = 0.006273492,8.289676e-06\n",
      "Iteration 39205: loss = 0.006273519,8.263246e-06\n",
      "Iteration 39210: loss = 0.0062734834,8.271705e-06\n",
      "Iteration 39215: loss = 0.0062734424,8.30274e-06\n",
      "Iteration 39220: loss = 0.006273426,8.282323e-06\n",
      "Iteration 39225: loss = 0.0062734205,8.280714e-06\n",
      "Iteration 39230: loss = 0.006273409,8.27509e-06\n",
      "Iteration 39235: loss = 0.0062733744,8.300644e-06\n",
      "Iteration 39240: loss = 0.00627338,8.278587e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39245: loss = 0.006273337,8.301413e-06\n",
      "Iteration 39250: loss = 0.006273356,8.281654e-06\n",
      "Iteration 39255: loss = 0.006273288,8.35113e-06\n",
      "Iteration 39260: loss = 0.006273389,8.387915e-06\n",
      "Iteration 39265: loss = 0.0062729903,1.0213673e-05\n",
      "Iteration 39270: loss = 0.0062741525,2.3539971e-05\n",
      "Iteration 39275: loss = 0.006270218,0.0002022339\n",
      "Iteration 39280: loss = 0.006284461,0.0026052368\n",
      "Iteration 39285: loss = 0.0062328973,0.03571954\n",
      "Iteration 39290: loss = 0.0063236933,0.074039765\n",
      "Iteration 39295: loss = 0.0063265995,0.044825044\n",
      "Iteration 39300: loss = 0.006324487,0.021042578\n",
      "Iteration 39305: loss = 0.0063214875,0.01088611\n",
      "Iteration 39310: loss = 0.0063155927,0.006216104\n",
      "Iteration 39315: loss = 0.0063091666,0.003730343\n",
      "Iteration 39320: loss = 0.006302467,0.0022562514\n",
      "Iteration 39325: loss = 0.0062962393,0.001352567\n",
      "Iteration 39330: loss = 0.0062906314,0.00079270866\n",
      "Iteration 39335: loss = 0.0062859864,0.0004527017\n",
      "Iteration 39340: loss = 0.0062821563,0.00024711594\n",
      "Iteration 39345: loss = 0.006279262,0.00012882252\n",
      "Iteration 39350: loss = 0.0062772427,6.187327e-05\n",
      "Iteration 39355: loss = 0.0062758145,2.6819009e-05\n",
      "Iteration 39360: loss = 0.006274732,1.1732314e-05\n",
      "Iteration 39365: loss = 0.0062737376,8.243974e-06\n",
      "Iteration 39370: loss = 0.006272942,9.702282e-06\n",
      "Iteration 39375: loss = 0.0062722825,1.2523074e-05\n",
      "Iteration 39380: loss = 0.00627201,1.4071793e-05\n",
      "Iteration 39385: loss = 0.006272087,1.3457646e-05\n",
      "Iteration 39390: loss = 0.006272396,1.1600668e-05\n",
      "Iteration 39395: loss = 0.0062727723,9.7307e-06\n",
      "Iteration 39400: loss = 0.006273108,8.566063e-06\n",
      "Iteration 39405: loss = 0.006273341,8.298184e-06\n",
      "Iteration 39410: loss = 0.0062734745,8.310046e-06\n",
      "Iteration 39415: loss = 0.006273489,8.320802e-06\n",
      "Iteration 39420: loss = 0.0062733293,8.2714805e-06\n",
      "Iteration 39425: loss = 0.0062731807,8.366399e-06\n",
      "Iteration 39430: loss = 0.006273055,8.486718e-06\n",
      "Iteration 39435: loss = 0.0062730704,8.465015e-06\n",
      "Iteration 39440: loss = 0.006273094,8.410948e-06\n",
      "Iteration 39445: loss = 0.006273096,8.378651e-06\n",
      "Iteration 39450: loss = 0.006273119,8.339691e-06\n",
      "Iteration 39455: loss = 0.0062731337,8.308271e-06\n",
      "Iteration 39460: loss = 0.006273097,8.334494e-06\n",
      "Iteration 39465: loss = 0.0062730876,8.324717e-06\n",
      "Iteration 39470: loss = 0.0062730475,8.351132e-06\n",
      "Iteration 39475: loss = 0.006273069,8.313576e-06\n",
      "Iteration 39480: loss = 0.006273009,8.359747e-06\n",
      "Iteration 39485: loss = 0.0062730648,8.2929e-06\n",
      "Iteration 39490: loss = 0.006272943,8.409226e-06\n",
      "Iteration 39495: loss = 0.006273164,8.249345e-06\n",
      "Iteration 39500: loss = 0.0062724897,9.651764e-06\n",
      "Iteration 39505: loss = 0.0062748077,1.4808761e-05\n",
      "Iteration 39510: loss = 0.006266236,0.000118633485\n",
      "Iteration 39515: loss = 0.006298043,0.0014422438\n",
      "Iteration 39520: loss = 0.006209312,0.012666485\n",
      "Iteration 39525: loss = 0.0062308027,0.01434329\n",
      "Iteration 39530: loss = 0.0062775407,0.0031144128\n",
      "Iteration 39535: loss = 0.006279617,0.0018903859\n",
      "Iteration 39540: loss = 0.006262465,0.0026679423\n",
      "Iteration 39545: loss = 0.0062712766,2.7977352e-05\n",
      "Iteration 39550: loss = 0.006277803,0.0011116078\n",
      "Iteration 39555: loss = 0.0062681176,0.00034984908\n",
      "Iteration 39560: loss = 0.0062702913,7.42198e-05\n",
      "Iteration 39565: loss = 0.0062757987,0.00032198502\n",
      "Iteration 39570: loss = 0.006270115,0.00018797335\n",
      "Iteration 39575: loss = 0.0062741167,2.87113e-05\n",
      "Iteration 39580: loss = 0.0062739844,1.2654457e-05\n",
      "Iteration 39585: loss = 0.006272808,4.624917e-05\n",
      "Iteration 39590: loss = 0.006276039,8.9875146e-05\n",
      "Iteration 39595: loss = 0.0062717847,0.00019882926\n",
      "Iteration 39600: loss = 0.0062791347,0.0005571965\n",
      "Iteration 39605: loss = 0.0062654964,0.0021364107\n",
      "Iteration 39610: loss = 0.0062930477,0.009054587\n",
      "Iteration 39615: loss = 0.0062431875,0.027292417\n",
      "Iteration 39620: loss = 0.0062915403,0.009044977\n",
      "Iteration 39625: loss = 0.0062885336,0.0062859003\n",
      "Iteration 39630: loss = 0.0062647965,0.0022913883\n",
      "Iteration 39635: loss = 0.0062633115,0.0031397806\n",
      "Iteration 39640: loss = 0.006276766,0.0003489595\n",
      "Iteration 39645: loss = 0.006280318,0.0013640085\n",
      "Iteration 39650: loss = 0.006271789,8.848629e-05\n",
      "Iteration 39655: loss = 0.0062693604,0.000527858\n",
      "Iteration 39660: loss = 0.006275015,7.630384e-05\n",
      "Iteration 39665: loss = 0.0062757335,0.00015175497\n",
      "Iteration 39670: loss = 0.006271603,0.000104741914\n",
      "Iteration 39675: loss = 0.006273005,1.4082614e-05\n",
      "Iteration 39680: loss = 0.006274764,6.387688e-05\n",
      "Iteration 39685: loss = 0.0062725083,3.1746025e-05\n",
      "Iteration 39690: loss = 0.006273348,8.2873485e-06\n",
      "Iteration 39695: loss = 0.0062738922,1.6529892e-05\n",
      "Iteration 39700: loss = 0.006272528,2.6556294e-05\n",
      "Iteration 39705: loss = 0.006274112,2.837777e-05\n",
      "Iteration 39710: loss = 0.006272277,3.772304e-05\n",
      "Iteration 39715: loss = 0.0062746,6.216664e-05\n",
      "Iteration 39720: loss = 0.006270885,0.00017129416\n",
      "Iteration 39725: loss = 0.006277948,0.00065804744\n",
      "Iteration 39730: loss = 0.0062625925,0.0033686527\n",
      "Iteration 39735: loss = 0.0062973327,0.017260088\n",
      "Iteration 39740: loss = 0.0062397444,0.037315175\n",
      "Iteration 39745: loss = 0.006271178,6.746324e-05\n",
      "Iteration 39750: loss = 0.0062906365,0.010626612\n",
      "Iteration 39755: loss = 0.006281987,0.0028719946\n",
      "Iteration 39760: loss = 0.0062669907,0.00095236057\n",
      "Iteration 39765: loss = 0.0062640794,0.0025350668\n",
      "Iteration 39770: loss = 0.006272187,8.394689e-05\n",
      "Iteration 39775: loss = 0.0062789717,0.00071288063\n",
      "Iteration 39780: loss = 0.0062767775,0.00027517232\n",
      "Iteration 39785: loss = 0.00627118,0.00013555809\n",
      "Iteration 39790: loss = 0.0062706466,0.0001735842\n",
      "Iteration 39795: loss = 0.006274086,3.6714755e-05\n",
      "Iteration 39800: loss = 0.00627472,7.119329e-05\n",
      "Iteration 39805: loss = 0.006272432,2.8461192e-05\n",
      "Iteration 39810: loss = 0.0062725474,2.4162227e-05\n",
      "Iteration 39815: loss = 0.006273994,2.487148e-05\n",
      "Iteration 39820: loss = 0.0062731397,8.289014e-06\n",
      "Iteration 39825: loss = 0.0062726773,1.4787406e-05\n",
      "Iteration 39830: loss = 0.0062735346,1.1855676e-05\n",
      "Iteration 39835: loss = 0.006273024,8.6273785e-06\n",
      "Iteration 39840: loss = 0.006272936,8.806332e-06\n",
      "Iteration 39845: loss = 0.006273279,9.471045e-06\n",
      "Iteration 39850: loss = 0.006272755,1.1155711e-05\n",
      "Iteration 39855: loss = 0.0062733865,1.1716444e-05\n",
      "Iteration 39860: loss = 0.006272461,1.7457629e-05\n",
      "Iteration 39865: loss = 0.0062740264,3.5682086e-05\n",
      "Iteration 39870: loss = 0.0062708594,0.00013534093\n",
      "Iteration 39875: loss = 0.006277971,0.0007119184\n",
      "Iteration 39880: loss = 0.006260132,0.004775024\n",
      "Iteration 39885: loss = 0.006304407,0.028849863\n",
      "Iteration 39890: loss = 0.006242225,0.03550582\n",
      "Iteration 39895: loss = 0.006251198,0.011142851\n",
      "Iteration 39900: loss = 0.0062736706,0.0008415569\n",
      "Iteration 39905: loss = 0.0062880716,0.0067006005\n",
      "Iteration 39910: loss = 0.006287083,0.003493848\n",
      "Iteration 39915: loss = 0.006276879,6.210501e-05\n",
      "Iteration 39920: loss = 0.0062668263,0.00089420687\n",
      "Iteration 39925: loss = 0.0062651765,0.0009904627\n",
      "Iteration 39930: loss = 0.0062708394,7.620683e-05\n",
      "Iteration 39935: loss = 0.006276638,0.00017698851\n",
      "Iteration 39940: loss = 0.006276559,0.00021254228\n",
      "Iteration 39945: loss = 0.00627291,9.3213785e-06\n",
      "Iteration 39950: loss = 0.006270958,8.5860105e-05\n",
      "Iteration 39955: loss = 0.0062723267,2.7704496e-05\n",
      "Iteration 39960: loss = 0.006273813,2.2962195e-05\n",
      "Iteration 39965: loss = 0.0062737265,2.1001162e-05\n",
      "Iteration 39970: loss = 0.0062726624,1.250759e-05\n",
      "Iteration 39975: loss = 0.006272322,1.4589583e-05\n",
      "Iteration 39980: loss = 0.0062733013,1.0347159e-05\n",
      "Iteration 39985: loss = 0.0062732394,9.253557e-06\n",
      "Iteration 39990: loss = 0.0062725083,1.0555752e-05\n",
      "Iteration 39995: loss = 0.006272983,8.148292e-06\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    PINN_solver.model.load_weights('./checkpoints/pinn_solver')\n",
    "except:\n",
    "    optim = tf.keras.optimizers.Adam(epsilon=1e-30)\n",
    "    PINN_solver.train(N=N, optimizer=optim, method = 'original')\n",
    "    \n",
    "    PINN_solver.model.save_weights('./checkpoints/pinn_solver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PINN with PCGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN_solver_pcgrad = PINN(x_u, y_u, x_r, init_model())\n",
    "PINN_solver_pcgrad.model.set_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 00000: loss = 0.08280759,1988.7423\n",
      "Iteration 00005: loss = 0.08258636,1754.5768\n",
      "Iteration 00010: loss = 0.08362034,1622.8054\n",
      "Iteration 00015: loss = 0.085416526,1518.5045\n",
      "Iteration 00020: loss = 0.08721436,1403.4705\n",
      "Iteration 00025: loss = 0.08859796,1281.263\n",
      "Iteration 00030: loss = 0.08952454,1181.5056\n",
      "Iteration 00035: loss = 0.09006065,1110.3749\n",
      "Iteration 00040: loss = 0.09035504,1044.9104\n",
      "Iteration 00045: loss = 0.09054089,974.3496\n",
      "Iteration 00050: loss = 0.09069526,906.0065\n",
      "Iteration 00055: loss = 0.09072634,842.3459\n",
      "Iteration 00060: loss = 0.09040085,773.0621\n",
      "Iteration 00065: loss = 0.089378774,692.53595\n",
      "Iteration 00070: loss = 0.08702236,594.9193\n",
      "Iteration 00075: loss = 0.08155605,467.4381\n",
      "Iteration 00080: loss = 0.068560205,303.5728\n",
      "Iteration 00085: loss = 0.04347405,139.26865\n",
      "Iteration 00090: loss = 0.019150583,64.95764\n",
      "Iteration 00095: loss = 0.009298088,60.81485\n",
      "Iteration 00100: loss = 0.0059491047,59.98584\n",
      "Iteration 00105: loss = 0.0048983437,55.026073\n",
      "Iteration 00110: loss = 0.004833007,45.647392\n",
      "Iteration 00115: loss = 0.0050894734,36.822197\n",
      "Iteration 00120: loss = 0.005326198,30.568314\n",
      "Iteration 00125: loss = 0.0055257925,26.091827\n",
      "Iteration 00130: loss = 0.0057403757,22.356956\n",
      "Iteration 00135: loss = 0.005899564,19.238255\n",
      "Iteration 00140: loss = 0.0059404015,16.652227\n",
      "Iteration 00145: loss = 0.0059221964,14.402556\n",
      "Iteration 00150: loss = 0.005919984,12.2915325\n",
      "Iteration 00155: loss = 0.0059413016,10.396293\n",
      "Iteration 00160: loss = 0.005984168,8.776823\n",
      "Iteration 00165: loss = 0.006072111,7.403702\n",
      "Iteration 00170: loss = 0.0062102764,6.272536\n",
      "Iteration 00175: loss = 0.0063735875,5.425587\n",
      "Iteration 00180: loss = 0.006542425,4.825395\n",
      "Iteration 00185: loss = 0.006709507,4.4014955\n",
      "Iteration 00190: loss = 0.0068622767,4.101564\n",
      "Iteration 00195: loss = 0.006987793,3.8821783\n",
      "Iteration 00200: loss = 0.0070803165,3.7060125\n",
      "Iteration 00205: loss = 0.0070975926,3.5430195\n",
      "Iteration 00210: loss = 0.007068205,3.389039\n",
      "Iteration 00215: loss = 0.00705337,3.24689\n",
      "Iteration 00220: loss = 0.007053545,3.112401\n",
      "Iteration 00225: loss = 0.0070633288,2.984889\n",
      "Iteration 00230: loss = 0.007078577,2.864788\n",
      "Iteration 00235: loss = 0.0070977923,2.7519379\n",
      "Iteration 00240: loss = 0.0071198526,2.645849\n",
      "Iteration 00245: loss = 0.0071432963,2.5460687\n",
      "Iteration 00250: loss = 0.007167559,2.4521925\n",
      "Iteration 00255: loss = 0.0071924217,2.3639283\n",
      "Iteration 00260: loss = 0.007217452,2.2810674\n",
      "Iteration 00265: loss = 0.007242439,2.2033732\n",
      "Iteration 00270: loss = 0.00726744,2.1305776\n",
      "Iteration 00275: loss = 0.007291518,2.0624034\n",
      "Iteration 00280: loss = 0.007308996,1.998524\n",
      "Iteration 00285: loss = 0.007321961,1.9386601\n",
      "Iteration 00290: loss = 0.007336535,1.8825321\n",
      "Iteration 00295: loss = 0.0073520765,1.8298507\n",
      "Iteration 00300: loss = 0.0073647615,1.7803485\n",
      "Iteration 00305: loss = 0.0073746196,1.733752\n",
      "Iteration 00310: loss = 0.007384039,1.6898043\n",
      "Iteration 00315: loss = 0.007393288,1.6482611\n",
      "Iteration 00320: loss = 0.0074009937,1.6089057\n",
      "Iteration 00325: loss = 0.007406937,1.5715362\n",
      "Iteration 00330: loss = 0.007411953,1.5359689\n",
      "Iteration 00335: loss = 0.0074162926,1.5020391\n",
      "Iteration 00340: loss = 0.0074195345,1.4696016\n",
      "Iteration 00345: loss = 0.0074215867,1.4385238\n",
      "Iteration 00350: loss = 0.0074228104,1.4086903\n",
      "Iteration 00355: loss = 0.00742338,1.3799938\n",
      "Iteration 00360: loss = 0.0074231904,1.3523455\n",
      "Iteration 00365: loss = 0.0074222595,1.325662\n",
      "Iteration 00370: loss = 0.007420756,1.2998682\n",
      "Iteration 00375: loss = 0.0074187852,1.2749014\n",
      "Iteration 00380: loss = 0.0074163345,1.2507021\n",
      "Iteration 00385: loss = 0.00741345,1.2272192\n",
      "Iteration 00390: loss = 0.0074102045,1.2044048\n",
      "Iteration 00395: loss = 0.0074066725,1.1822195\n",
      "Iteration 00400: loss = 0.007402846,1.1606237\n",
      "Iteration 00405: loss = 0.007398778,1.1395863\n",
      "Iteration 00410: loss = 0.0073945117,1.1190739\n",
      "Iteration 00415: loss = 0.0073900796,1.0990616\n",
      "Iteration 00420: loss = 0.007385506,1.0795246\n",
      "Iteration 00425: loss = 0.007380806,1.060438\n",
      "Iteration 00430: loss = 0.0073760096,1.041784\n",
      "Iteration 00435: loss = 0.007371146,1.0235416\n",
      "Iteration 00440: loss = 0.0073662293,1.0056976\n",
      "Iteration 00445: loss = 0.0073612556,0.98823225\n",
      "Iteration 00450: loss = 0.0073562707,0.9711339\n",
      "Iteration 00455: loss = 0.0073512555,0.95438755\n",
      "Iteration 00460: loss = 0.007346229,0.93798345\n",
      "Iteration 00465: loss = 0.007341214,0.92190856\n",
      "Iteration 00470: loss = 0.007336221,0.9061545\n",
      "Iteration 00475: loss = 0.007331236,0.8907077\n",
      "Iteration 00480: loss = 0.0073262732,0.8755634\n",
      "Iteration 00485: loss = 0.0073213466,0.8607116\n",
      "Iteration 00490: loss = 0.0073164515,0.84614384\n",
      "Iteration 00495: loss = 0.0073115914,0.831854\n",
      "Iteration 00500: loss = 0.007306771,0.81783587\n",
      "Iteration 00505: loss = 0.0073019913,0.804082\n",
      "Iteration 00510: loss = 0.0072972737,0.7905855\n",
      "Iteration 00515: loss = 0.0072925743,0.7773422\n",
      "Iteration 00520: loss = 0.00728793,0.76434577\n",
      "Iteration 00525: loss = 0.007283335,0.75159115\n",
      "Iteration 00530: loss = 0.0072787893,0.73907256\n",
      "Iteration 00535: loss = 0.00727428,0.7267894\n",
      "Iteration 00540: loss = 0.007269817,0.71473247\n",
      "Iteration 00545: loss = 0.0072654025,0.7028999\n",
      "Iteration 00550: loss = 0.007261025,0.69128716\n",
      "Iteration 00555: loss = 0.007256698,0.67988956\n",
      "Iteration 00560: loss = 0.007252406,0.66870254\n",
      "Iteration 00565: loss = 0.0072481507,0.6577232\n",
      "Iteration 00570: loss = 0.0072439336,0.6469493\n",
      "Iteration 00575: loss = 0.0072397776,0.63637644\n",
      "Iteration 00580: loss = 0.0072356365,0.6259991\n",
      "Iteration 00585: loss = 0.0072315224,0.61581576\n",
      "Iteration 00590: loss = 0.007227454,0.60582423\n",
      "Iteration 00595: loss = 0.007223396,0.5960171\n",
      "Iteration 00600: loss = 0.007219393,0.5863958\n",
      "Iteration 00605: loss = 0.007215412,0.57695305\n",
      "Iteration 00610: loss = 0.007211443,0.5676886\n",
      "Iteration 00615: loss = 0.0072075017,0.5585973\n",
      "Iteration 00620: loss = 0.0072035957,0.54967695\n",
      "Iteration 00625: loss = 0.0071997065,0.5409241\n",
      "Iteration 00630: loss = 0.007195829,0.5323354\n",
      "Iteration 00635: loss = 0.0071919686,0.523908\n",
      "Iteration 00640: loss = 0.007188124,0.5156401\n",
      "Iteration 00645: loss = 0.007184299,0.5075268\n",
      "Iteration 00650: loss = 0.007180489,0.49956632\n",
      "Iteration 00655: loss = 0.007176688,0.49175495\n",
      "Iteration 00660: loss = 0.0071728867,0.48408955\n",
      "Iteration 00665: loss = 0.007169112,0.47656873\n",
      "Iteration 00670: loss = 0.007165341,0.4691888\n",
      "Iteration 00675: loss = 0.0071615814,0.46194488\n",
      "Iteration 00680: loss = 0.0071578226,0.45483676\n",
      "Iteration 00685: loss = 0.0071540806,0.44786206\n",
      "Iteration 00690: loss = 0.0071503283,0.44101653\n",
      "Iteration 00695: loss = 0.0071465853,0.43429604\n",
      "Iteration 00700: loss = 0.007142845,0.42770153\n",
      "Iteration 00705: loss = 0.007139105,0.42122814\n",
      "Iteration 00710: loss = 0.007135374,0.41487318\n",
      "Iteration 00715: loss = 0.007131648,0.40863442\n",
      "Iteration 00720: loss = 0.0071279095,0.4025101\n",
      "Iteration 00725: loss = 0.00712418,0.39649612\n",
      "Iteration 00730: loss = 0.007120449,0.3905937\n",
      "Iteration 00735: loss = 0.0071167215,0.3847953\n",
      "Iteration 00740: loss = 0.007112996,0.37910265\n",
      "Iteration 00745: loss = 0.0071092583,0.37351173\n",
      "Iteration 00750: loss = 0.007105529,0.36802077\n",
      "Iteration 00755: loss = 0.0071017966,0.3626278\n",
      "Iteration 00760: loss = 0.00709807,0.35733098\n",
      "Iteration 00765: loss = 0.007094335,0.35212773\n",
      "Iteration 00770: loss = 0.007090608,0.34701627\n",
      "Iteration 00775: loss = 0.007086871,0.34199464\n",
      "Iteration 00780: loss = 0.0070831287,0.33706072\n",
      "Iteration 00785: loss = 0.0070794052,0.33221394\n",
      "Iteration 00790: loss = 0.0070756623,0.3274495\n",
      "Iteration 00795: loss = 0.0070719286,0.32276905\n",
      "Iteration 00800: loss = 0.007068189,0.31816846\n",
      "Iteration 00805: loss = 0.007064462,0.31364858\n",
      "Iteration 00810: loss = 0.007060719,0.30920509\n",
      "Iteration 00815: loss = 0.0070569883,0.30483747\n",
      "Iteration 00820: loss = 0.0070532598,0.30054528\n",
      "Iteration 00825: loss = 0.0070495256,0.2963267\n",
      "Iteration 00830: loss = 0.007045792,0.29217884\n",
      "Iteration 00835: loss = 0.007042071,0.28810096\n",
      "Iteration 00840: loss = 0.0070383414,0.284093\n",
      "Iteration 00845: loss = 0.0070346035,0.28015178\n",
      "Iteration 00850: loss = 0.0070308833,0.27627742\n",
      "Iteration 00855: loss = 0.0070271795,0.27246904\n",
      "Iteration 00860: loss = 0.0070234532,0.26872322\n",
      "Iteration 00865: loss = 0.007019752,0.2650413\n",
      "Iteration 00870: loss = 0.0070160325,0.26142\n",
      "Iteration 00875: loss = 0.007012321,0.25786003\n",
      "Iteration 00880: loss = 0.0070086312,0.25435984\n",
      "Iteration 00885: loss = 0.0070049316,0.25091797\n",
      "Iteration 00890: loss = 0.0070012473,0.2475335\n",
      "Iteration 00895: loss = 0.006997566,0.24420515\n",
      "Iteration 00900: loss = 0.006993875,0.24093136\n",
      "Iteration 00905: loss = 0.006990215,0.23771358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 00910: loss = 0.0069865403,0.23454924\n",
      "Iteration 00915: loss = 0.006982898,0.23143697\n",
      "Iteration 00920: loss = 0.0069792415,0.22837806\n",
      "Iteration 00925: loss = 0.0069756038,0.22536859\n",
      "Iteration 00930: loss = 0.006971963,0.22241025\n",
      "Iteration 00935: loss = 0.006968348,0.21950094\n",
      "Iteration 00940: loss = 0.0069647226,0.21664023\n",
      "Iteration 00945: loss = 0.006961131,0.21382758\n",
      "Iteration 00950: loss = 0.0069575273,0.21106218\n",
      "Iteration 00955: loss = 0.0069539417,0.20834346\n",
      "Iteration 00960: loss = 0.0069503635,0.2056693\n",
      "Iteration 00965: loss = 0.0069467966,0.20304117\n",
      "Iteration 00970: loss = 0.0069432533,0.20045575\n",
      "Iteration 00975: loss = 0.006939706,0.19791442\n",
      "Iteration 00980: loss = 0.0069361706,0.1954166\n",
      "Iteration 00985: loss = 0.006932653,0.19296026\n",
      "Iteration 00990: loss = 0.006929156,0.19054538\n",
      "Iteration 00995: loss = 0.006925659,0.18817134\n",
      "Iteration 01000: loss = 0.0069221854,0.18583785\n",
      "Iteration 01005: loss = 0.006918719,0.1835438\n",
      "Iteration 01010: loss = 0.006915267,0.18128854\n",
      "Iteration 01015: loss = 0.006911833,0.17907138\n",
      "Iteration 01020: loss = 0.0069084116,0.1768929\n",
      "Iteration 01025: loss = 0.0069050007,0.17475063\n",
      "Iteration 01030: loss = 0.0069016223,0.17264551\n",
      "Iteration 01035: loss = 0.0068982504,0.17057613\n",
      "Iteration 01040: loss = 0.006894887,0.16854167\n",
      "Iteration 01045: loss = 0.0068915486,0.16654257\n",
      "Iteration 01050: loss = 0.0068882215,0.16457802\n",
      "Iteration 01055: loss = 0.0068849125,0.16264692\n",
      "Iteration 01060: loss = 0.006881638,0.16074838\n",
      "Iteration 01065: loss = 0.006878357,0.15888268\n",
      "Iteration 01070: loss = 0.006875109,0.1570497\n",
      "Iteration 01075: loss = 0.006871875,0.15524772\n",
      "Iteration 01080: loss = 0.0068686674,0.15347666\n",
      "Iteration 01085: loss = 0.006865471,0.15173659\n",
      "Iteration 01090: loss = 0.0068623065,0.15002611\n",
      "Iteration 01095: loss = 0.0068591344,0.14834476\n",
      "Iteration 01100: loss = 0.0068559996,0.14669287\n",
      "Iteration 01105: loss = 0.0068528927,0.14506924\n",
      "Iteration 01110: loss = 0.0068497956,0.14347334\n",
      "Iteration 01115: loss = 0.006846729,0.14190498\n",
      "Iteration 01120: loss = 0.006843677,0.14036387\n",
      "Iteration 01125: loss = 0.00684064,0.13884842\n",
      "Iteration 01130: loss = 0.0068376274,0.13735947\n",
      "Iteration 01135: loss = 0.0068346574,0.13589558\n",
      "Iteration 01140: loss = 0.0068316907,0.13445698\n",
      "Iteration 01145: loss = 0.00682875,0.133043\n",
      "Iteration 01150: loss = 0.006825832,0.13165285\n",
      "Iteration 01155: loss = 0.0068229423,0.13028619\n",
      "Iteration 01160: loss = 0.006820073,0.12894304\n",
      "Iteration 01165: loss = 0.006817223,0.12762198\n",
      "Iteration 01170: loss = 0.006814404,0.12632386\n",
      "Iteration 01175: loss = 0.0068115913,0.12504715\n",
      "Iteration 01180: loss = 0.0068088095,0.12379189\n",
      "Iteration 01185: loss = 0.0068060677,0.122557655\n",
      "Iteration 01190: loss = 0.0068033296,0.121343665\n",
      "Iteration 01195: loss = 0.0068006185,0.120149925\n",
      "Iteration 01200: loss = 0.0067979335,0.11897584\n",
      "Iteration 01205: loss = 0.006795296,0.11782116\n",
      "Iteration 01210: loss = 0.006792663,0.1166853\n",
      "Iteration 01215: loss = 0.006790051,0.1155678\n",
      "Iteration 01220: loss = 0.006787464,0.11446862\n",
      "Iteration 01225: loss = 0.006784906,0.11338702\n",
      "Iteration 01230: loss = 0.006782374,0.11232291\n",
      "Iteration 01235: loss = 0.00677987,0.11127553\n",
      "Iteration 01240: loss = 0.00677738,0.110245146\n",
      "Iteration 01245: loss = 0.0067749205,0.10923094\n",
      "Iteration 01250: loss = 0.0067724786,0.10823239\n",
      "Iteration 01255: loss = 0.0067700637,0.10724957\n",
      "Iteration 01260: loss = 0.0067676767,0.106282026\n",
      "Iteration 01265: loss = 0.006765324,0.10532953\n",
      "Iteration 01270: loss = 0.0067629856,0.104391344\n",
      "Iteration 01275: loss = 0.006760666,0.1034675\n",
      "Iteration 01280: loss = 0.0067583774,0.10255759\n",
      "Iteration 01285: loss = 0.006756108,0.10166152\n",
      "Iteration 01290: loss = 0.006753862,0.10077875\n",
      "Iteration 01295: loss = 0.006751653,0.09990893\n",
      "Iteration 01300: loss = 0.006749455,0.09905211\n",
      "Iteration 01305: loss = 0.0067472993,0.0982077\n",
      "Iteration 01310: loss = 0.00674513,0.0973755\n",
      "Iteration 01315: loss = 0.00674301,0.09655525\n",
      "Iteration 01320: loss = 0.006740904,0.095746785\n",
      "Iteration 01325: loss = 0.0067388355,0.09494973\n",
      "Iteration 01330: loss = 0.006736779,0.09416397\n",
      "Iteration 01335: loss = 0.006734748,0.093389064\n",
      "Iteration 01340: loss = 0.006732739,0.09262504\n",
      "Iteration 01345: loss = 0.0067307525,0.09187144\n",
      "Iteration 01350: loss = 0.0067287893,0.09112822\n",
      "Iteration 01355: loss = 0.006726844,0.090395\n",
      "Iteration 01360: loss = 0.0067249197,0.08967158\n",
      "Iteration 01365: loss = 0.0067230184,0.08895781\n",
      "Iteration 01370: loss = 0.006721144,0.08825354\n",
      "Iteration 01375: loss = 0.0067192893,0.08755864\n",
      "Iteration 01380: loss = 0.0067174523,0.08687262\n",
      "Iteration 01385: loss = 0.006715639,0.08619554\n",
      "Iteration 01390: loss = 0.0067138453,0.085527204\n",
      "Iteration 01395: loss = 0.006712068,0.08486728\n",
      "Iteration 01400: loss = 0.0067103095,0.08421584\n",
      "Iteration 01405: loss = 0.006708577,0.083572395\n",
      "Iteration 01410: loss = 0.006706864,0.082937054\n",
      "Iteration 01415: loss = 0.006705158,0.0823095\n",
      "Iteration 01420: loss = 0.006703496,0.08168983\n",
      "Iteration 01425: loss = 0.0067018247,0.08107759\n",
      "Iteration 01430: loss = 0.0067001916,0.080472685\n",
      "Iteration 01435: loss = 0.0066985744,0.07987513\n",
      "Iteration 01440: loss = 0.0066969707,0.07928479\n",
      "Iteration 01445: loss = 0.0066953874,0.07870126\n",
      "Iteration 01450: loss = 0.006693817,0.07812467\n",
      "Iteration 01455: loss = 0.006692271,0.0775548\n",
      "Iteration 01460: loss = 0.0066907387,0.07699151\n",
      "Iteration 01465: loss = 0.0066892183,0.07643476\n",
      "Iteration 01470: loss = 0.0066877324,0.07588431\n",
      "Iteration 01475: loss = 0.0066862362,0.07534017\n",
      "Iteration 01480: loss = 0.006684767,0.07480215\n",
      "Iteration 01485: loss = 0.006683321,0.07427019\n",
      "Iteration 01490: loss = 0.006681887,0.07374415\n",
      "Iteration 01495: loss = 0.0066804662,0.07322391\n",
      "Iteration 01500: loss = 0.0066790697,0.072709344\n",
      "Iteration 01505: loss = 0.0066776765,0.07220051\n",
      "Iteration 01510: loss = 0.0066763056,0.07169719\n",
      "Iteration 01515: loss = 0.0066749468,0.07119921\n",
      "Iteration 01520: loss = 0.0066735987,0.07070672\n",
      "Iteration 01525: loss = 0.0066722645,0.07021938\n",
      "Iteration 01530: loss = 0.0066709495,0.06973731\n",
      "Iteration 01535: loss = 0.0066696615,0.06926031\n",
      "Iteration 01540: loss = 0.0066683684,0.06878826\n",
      "Iteration 01545: loss = 0.0066670887,0.06832121\n",
      "Iteration 01550: loss = 0.006665826,0.06785895\n",
      "Iteration 01555: loss = 0.0066645704,0.06740153\n",
      "Iteration 01560: loss = 0.0066633373,0.066948764\n",
      "Iteration 01565: loss = 0.00666211,0.06650061\n",
      "Iteration 01570: loss = 0.006660903,0.06605711\n",
      "Iteration 01575: loss = 0.006659699,0.065618105\n",
      "Iteration 01580: loss = 0.0066585126,0.0651834\n",
      "Iteration 01585: loss = 0.0066573382,0.06475312\n",
      "Iteration 01590: loss = 0.00665617,0.06432717\n",
      "Iteration 01595: loss = 0.0066550174,0.06390541\n",
      "Iteration 01600: loss = 0.0066538746,0.06348786\n",
      "Iteration 01605: loss = 0.0066527366,0.06307441\n",
      "Iteration 01610: loss = 0.006651622,0.06266498\n",
      "Iteration 01615: loss = 0.006650506,0.062259585\n",
      "Iteration 01620: loss = 0.006649414,0.061858155\n",
      "Iteration 01625: loss = 0.006648324,0.06146056\n",
      "Iteration 01630: loss = 0.0066472343,0.061066803\n",
      "Iteration 01635: loss = 0.006646166,0.060676817\n",
      "Iteration 01640: loss = 0.0066451095,0.060290564\n",
      "Iteration 01645: loss = 0.006644055,0.05990798\n",
      "Iteration 01650: loss = 0.0066430196,0.059529036\n",
      "Iteration 01655: loss = 0.0066419835,0.05915363\n",
      "Iteration 01660: loss = 0.0066409614,0.05878174\n",
      "Iteration 01665: loss = 0.0066399537,0.058413465\n",
      "Iteration 01670: loss = 0.0066389428,0.058048513\n",
      "Iteration 01675: loss = 0.0066379444,0.057686966\n",
      "Iteration 01680: loss = 0.0066369604,0.05732879\n",
      "Iteration 01685: loss = 0.006635973,0.056973882\n",
      "Iteration 01690: loss = 0.0066349967,0.056622278\n",
      "Iteration 01695: loss = 0.0066340477,0.056273863\n",
      "Iteration 01700: loss = 0.006633083,0.055928685\n",
      "Iteration 01705: loss = 0.0066321357,0.055586554\n",
      "Iteration 01710: loss = 0.0066311955,0.05524756\n",
      "Iteration 01715: loss = 0.006630266,0.05491162\n",
      "Iteration 01720: loss = 0.0066293404,0.054578733\n",
      "Iteration 01725: loss = 0.0066284216,0.05424882\n",
      "Iteration 01730: loss = 0.0066275145,0.05392185\n",
      "Iteration 01735: loss = 0.0066266097,0.05359772\n",
      "Iteration 01740: loss = 0.0066257142,0.053276546\n",
      "Iteration 01745: loss = 0.0066248267,0.052958187\n",
      "Iteration 01750: loss = 0.0066239364,0.052642617\n",
      "Iteration 01755: loss = 0.0066230656,0.052329823\n",
      "Iteration 01760: loss = 0.006622205,0.052019846\n",
      "Iteration 01765: loss = 0.006621336,0.051712506\n",
      "Iteration 01770: loss = 0.006620476,0.05140778\n",
      "Iteration 01775: loss = 0.006619627,0.05110574\n",
      "Iteration 01780: loss = 0.0066187843,0.05080635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 01785: loss = 0.0066179503,0.05050946\n",
      "Iteration 01790: loss = 0.0066171177,0.050215237\n",
      "Iteration 01795: loss = 0.0066162893,0.049923487\n",
      "Iteration 01800: loss = 0.006615478,0.049634144\n",
      "Iteration 01805: loss = 0.0066146604,0.049347363\n",
      "Iteration 01810: loss = 0.0066138487,0.049063012\n",
      "Iteration 01815: loss = 0.0066130427,0.048780978\n",
      "Iteration 01820: loss = 0.0066122618,0.0485014\n",
      "Iteration 01825: loss = 0.006611462,0.0482241\n",
      "Iteration 01830: loss = 0.0066106766,0.047949158\n",
      "Iteration 01835: loss = 0.0066099013,0.047676507\n",
      "Iteration 01840: loss = 0.0066091307,0.04740616\n",
      "Iteration 01845: loss = 0.006608358,0.047137998\n",
      "Iteration 01850: loss = 0.006607592,0.046872057\n",
      "Iteration 01855: loss = 0.0066068284,0.046608314\n",
      "Iteration 01860: loss = 0.006606072,0.046346743\n",
      "Iteration 01865: loss = 0.006605329,0.04608734\n",
      "Iteration 01870: loss = 0.006604579,0.045830064\n",
      "Iteration 01875: loss = 0.0066038445,0.04557485\n",
      "Iteration 01880: loss = 0.0066031083,0.045321763\n",
      "Iteration 01885: loss = 0.006602382,0.045070622\n",
      "Iteration 01890: loss = 0.006601652,0.044821635\n",
      "Iteration 01895: loss = 0.0066009318,0.044574555\n",
      "Iteration 01900: loss = 0.006600223,0.044329498\n",
      "Iteration 01905: loss = 0.0065995003,0.0440864\n",
      "Iteration 01910: loss = 0.006598797,0.043845188\n",
      "Iteration 01915: loss = 0.0065981024,0.043605976\n",
      "Iteration 01920: loss = 0.0065974,0.043368626\n",
      "Iteration 01925: loss = 0.0065967026,0.04313316\n",
      "Iteration 01930: loss = 0.00659601,0.04289953\n",
      "Iteration 01935: loss = 0.0065953215,0.042667788\n",
      "Iteration 01940: loss = 0.0065946435,0.04243781\n",
      "Iteration 01945: loss = 0.0065939683,0.04220967\n",
      "Iteration 01950: loss = 0.0065932833,0.04198332\n",
      "Iteration 01955: loss = 0.0065926253,0.041758705\n",
      "Iteration 01960: loss = 0.006591949,0.0415358\n",
      "Iteration 01965: loss = 0.006591296,0.04131467\n",
      "Iteration 01970: loss = 0.006590642,0.041095175\n",
      "Iteration 01975: loss = 0.00658999,0.040877435\n",
      "Iteration 01980: loss = 0.0065893326,0.04066136\n",
      "Iteration 01985: loss = 0.006588679,0.040446833\n",
      "Iteration 01990: loss = 0.0065880474,0.04023403\n",
      "Iteration 01995: loss = 0.006587407,0.040022917\n",
      "Iteration 02000: loss = 0.00658676,0.039813273\n",
      "Iteration 02005: loss = 0.0065861517,0.039605286\n",
      "Iteration 02010: loss = 0.0065855156,0.03939879\n",
      "Iteration 02015: loss = 0.0065848846,0.03919391\n",
      "Iteration 02020: loss = 0.006584255,0.038990576\n",
      "Iteration 02025: loss = 0.006583637,0.038788747\n",
      "Iteration 02030: loss = 0.006583013,0.038588356\n",
      "Iteration 02035: loss = 0.0065824254,0.038389534\n",
      "Iteration 02040: loss = 0.0065818126,0.03819213\n",
      "Iteration 02045: loss = 0.0065811933,0.03799623\n",
      "Iteration 02050: loss = 0.0065805963,0.037801754\n",
      "Iteration 02055: loss = 0.0065799975,0.03760868\n",
      "Iteration 02060: loss = 0.006579404,0.037417024\n",
      "Iteration 02065: loss = 0.0065788124,0.037226796\n",
      "Iteration 02070: loss = 0.0065782196,0.037037976\n",
      "Iteration 02075: loss = 0.006577648,0.036850512\n",
      "Iteration 02080: loss = 0.0065770657,0.03666438\n",
      "Iteration 02085: loss = 0.006576471,0.036479577\n",
      "Iteration 02090: loss = 0.006575888,0.03629611\n",
      "Iteration 02095: loss = 0.0065753334,0.036114\n",
      "Iteration 02100: loss = 0.006574753,0.03593318\n",
      "Iteration 02105: loss = 0.0065741837,0.035753652\n",
      "Iteration 02110: loss = 0.006573634,0.03557536\n",
      "Iteration 02115: loss = 0.006573066,0.03539839\n",
      "Iteration 02120: loss = 0.006572502,0.035222642\n",
      "Iteration 02125: loss = 0.0065719415,0.0350482\n",
      "Iteration 02130: loss = 0.006571391,0.034874894\n",
      "Iteration 02135: loss = 0.0065708384,0.034702845\n",
      "Iteration 02140: loss = 0.006570298,0.034531966\n",
      "Iteration 02145: loss = 0.006569758,0.034362365\n",
      "Iteration 02150: loss = 0.006569211,0.034193892\n",
      "Iteration 02155: loss = 0.0065686717,0.034026597\n",
      "Iteration 02160: loss = 0.006568137,0.033860475\n",
      "Iteration 02165: loss = 0.0065676067,0.033695504\n",
      "Iteration 02170: loss = 0.006567078,0.03353164\n",
      "Iteration 02175: loss = 0.00656655,0.033368893\n",
      "Iteration 02180: loss = 0.0065660253,0.033207268\n",
      "Iteration 02185: loss = 0.0065655033,0.033046804\n",
      "Iteration 02190: loss = 0.0065649785,0.032887377\n",
      "Iteration 02195: loss = 0.0065644593,0.032729093\n",
      "Iteration 02200: loss = 0.0065639443,0.032571834\n",
      "Iteration 02205: loss = 0.006563431,0.03241564\n",
      "Iteration 02210: loss = 0.0065629296,0.03226052\n",
      "Iteration 02215: loss = 0.0065624244,0.032106474\n",
      "Iteration 02220: loss = 0.006561913,0.03195337\n",
      "Iteration 02225: loss = 0.0065614074,0.03180137\n",
      "Iteration 02230: loss = 0.0065609044,0.031650342\n",
      "Iteration 02235: loss = 0.006560408,0.03150032\n",
      "Iteration 02240: loss = 0.00655991,0.03135132\n",
      "Iteration 02245: loss = 0.006559424,0.031203348\n",
      "Iteration 02250: loss = 0.0065589375,0.03105625\n",
      "Iteration 02255: loss = 0.006558441,0.030910227\n",
      "Iteration 02260: loss = 0.0065579577,0.030765085\n",
      "Iteration 02265: loss = 0.0065574734,0.030620897\n",
      "Iteration 02270: loss = 0.006556992,0.030477758\n",
      "Iteration 02275: loss = 0.0065565095,0.030335415\n",
      "Iteration 02280: loss = 0.0065560364,0.030194115\n",
      "Iteration 02285: loss = 0.0065555577,0.03005363\n",
      "Iteration 02290: loss = 0.0065550846,0.029914135\n",
      "Iteration 02295: loss = 0.0065546124,0.029775508\n",
      "Iteration 02300: loss = 0.006554154,0.029637754\n",
      "Iteration 02305: loss = 0.006553678,0.029500902\n",
      "Iteration 02310: loss = 0.006553222,0.029364929\n",
      "Iteration 02315: loss = 0.0065527563,0.029229864\n",
      "Iteration 02320: loss = 0.0065522944,0.029095585\n",
      "Iteration 02325: loss = 0.0065518413,0.028962158\n",
      "Iteration 02330: loss = 0.006551385,0.02882965\n",
      "Iteration 02335: loss = 0.006550934,0.028697927\n",
      "Iteration 02340: loss = 0.0065504736,0.02856708\n",
      "Iteration 02345: loss = 0.006550027,0.028437011\n",
      "Iteration 02350: loss = 0.00654959,0.028307822\n",
      "Iteration 02355: loss = 0.0065491344,0.028179336\n",
      "Iteration 02360: loss = 0.0065487013,0.02805174\n",
      "Iteration 02365: loss = 0.006548247,0.027924925\n",
      "Iteration 02370: loss = 0.006547811,0.027798865\n",
      "Iteration 02375: loss = 0.0065473826,0.027673632\n",
      "Iteration 02380: loss = 0.0065469425,0.027549125\n",
      "Iteration 02385: loss = 0.006546514,0.027425464\n",
      "Iteration 02390: loss = 0.00654608,0.027302463\n",
      "Iteration 02395: loss = 0.006545644,0.027180294\n",
      "Iteration 02400: loss = 0.006545222,0.027058858\n",
      "Iteration 02405: loss = 0.0065447898,0.026938166\n",
      "Iteration 02410: loss = 0.006544368,0.0268182\n",
      "Iteration 02415: loss = 0.0065439492,0.026698992\n",
      "Iteration 02420: loss = 0.0065435222,0.026580501\n",
      "Iteration 02425: loss = 0.0065430985,0.026462663\n",
      "Iteration 02430: loss = 0.0065426887,0.026345603\n",
      "Iteration 02435: loss = 0.006542278,0.026229233\n",
      "Iteration 02440: loss = 0.006541858,0.026113588\n",
      "Iteration 02445: loss = 0.0065414435,0.025998581\n",
      "Iteration 02450: loss = 0.006541045,0.025884304\n",
      "Iteration 02455: loss = 0.0065406263,0.025770675\n",
      "Iteration 02460: loss = 0.0065402184,0.025657695\n",
      "Iteration 02465: loss = 0.006539809,0.02554544\n",
      "Iteration 02470: loss = 0.0065393974,0.02543366\n",
      "Iteration 02475: loss = 0.006538968,0.025322314\n",
      "Iteration 02480: loss = 0.006538547,0.025211334\n",
      "Iteration 02485: loss = 0.006538112,0.025100732\n",
      "Iteration 02490: loss = 0.0065376875,0.024990436\n",
      "Iteration 02495: loss = 0.0065372568,0.024880413\n",
      "Iteration 02500: loss = 0.0065368265,0.024770686\n",
      "Iteration 02505: loss = 0.0065363985,0.024661219\n",
      "Iteration 02510: loss = 0.006535968,0.024552064\n",
      "Iteration 02515: loss = 0.006535532,0.024443215\n",
      "Iteration 02520: loss = 0.0065351054,0.024334628\n",
      "Iteration 02525: loss = 0.0065346826,0.024226312\n",
      "Iteration 02530: loss = 0.0065342556,0.02411843\n",
      "Iteration 02535: loss = 0.0065338355,0.024010822\n",
      "Iteration 02540: loss = 0.0065334155,0.023903582\n",
      "Iteration 02545: loss = 0.006532988,0.023796637\n",
      "Iteration 02550: loss = 0.0065325606,0.023690054\n",
      "Iteration 02555: loss = 0.0065321475,0.023583896\n",
      "Iteration 02560: loss = 0.0065317242,0.02347807\n",
      "Iteration 02565: loss = 0.00653131,0.023372605\n",
      "Iteration 02570: loss = 0.00653089,0.023267575\n",
      "Iteration 02575: loss = 0.006530465,0.023162926\n",
      "Iteration 02580: loss = 0.006530049,0.02305863\n",
      "Iteration 02585: loss = 0.006529638,0.02295475\n",
      "Iteration 02590: loss = 0.0065292283,0.022851273\n",
      "Iteration 02595: loss = 0.006528811,0.02274819\n",
      "Iteration 02600: loss = 0.0065283985,0.022645459\n",
      "Iteration 02605: loss = 0.0065279924,0.0225432\n",
      "Iteration 02610: loss = 0.006527579,0.02244131\n",
      "Iteration 02615: loss = 0.0065271594,0.022339888\n",
      "Iteration 02620: loss = 0.0065267566,0.022238798\n",
      "Iteration 02625: loss = 0.0065263603,0.02213815\n",
      "Iteration 02630: loss = 0.0065259547,0.022037957\n",
      "Iteration 02635: loss = 0.0065255505,0.021938104\n",
      "Iteration 02640: loss = 0.006525149,0.021838762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 02645: loss = 0.0065247435,0.021739788\n",
      "Iteration 02650: loss = 0.0065243472,0.021641228\n",
      "Iteration 02655: loss = 0.0065239393,0.021543076\n",
      "Iteration 02660: loss = 0.006523552,0.021445379\n",
      "Iteration 02665: loss = 0.006523157,0.021348042\n",
      "Iteration 02670: loss = 0.00652276,0.021251146\n",
      "Iteration 02675: loss = 0.0065223537,0.021154728\n",
      "Iteration 02680: loss = 0.006521963,0.021058638\n",
      "Iteration 02685: loss = 0.00652157,0.020963013\n",
      "Iteration 02690: loss = 0.0065211863,0.020867832\n",
      "Iteration 02695: loss = 0.0065207877,0.02077306\n",
      "Iteration 02700: loss = 0.0065204096,0.020678662\n",
      "Iteration 02705: loss = 0.0065200087,0.020584732\n",
      "Iteration 02710: loss = 0.006519628,0.020491224\n",
      "Iteration 02715: loss = 0.006519245,0.020398036\n",
      "Iteration 02720: loss = 0.006518858,0.020305358\n",
      "Iteration 02725: loss = 0.006518474,0.020213112\n",
      "Iteration 02730: loss = 0.0065180957,0.020121206\n",
      "Iteration 02735: loss = 0.006517707,0.02002979\n",
      "Iteration 02740: loss = 0.0065173325,0.01993876\n",
      "Iteration 02745: loss = 0.0065169595,0.019848168\n",
      "Iteration 02750: loss = 0.006516585,0.019757938\n",
      "Iteration 02755: loss = 0.0065162014,0.019668167\n",
      "Iteration 02760: loss = 0.0065158284,0.019578762\n",
      "Iteration 02765: loss = 0.006515457,0.019489776\n",
      "Iteration 02770: loss = 0.006515084,0.019401222\n",
      "Iteration 02775: loss = 0.006514693,0.019313043\n",
      "Iteration 02780: loss = 0.0065143327,0.019225309\n",
      "Iteration 02785: loss = 0.006513961,0.019137975\n",
      "Iteration 02790: loss = 0.0065135914,0.019051082\n",
      "Iteration 02795: loss = 0.0065132226,0.018964505\n",
      "Iteration 02800: loss = 0.0065128575,0.018878423\n",
      "Iteration 02805: loss = 0.006512502,0.01879268\n",
      "Iteration 02810: loss = 0.0065121297,0.018707337\n",
      "Iteration 02815: loss = 0.0065117665,0.018622417\n",
      "Iteration 02820: loss = 0.0065114084,0.018537907\n",
      "Iteration 02825: loss = 0.0065110438,0.018453782\n",
      "Iteration 02830: loss = 0.006510684,0.018370075\n",
      "Iteration 02835: loss = 0.0065103215,0.018286709\n",
      "Iteration 02840: loss = 0.006509976,0.01820379\n",
      "Iteration 02845: loss = 0.006509611,0.018121215\n",
      "Iteration 02850: loss = 0.0065092538,0.01803907\n",
      "Iteration 02855: loss = 0.0065089012,0.017957274\n",
      "Iteration 02860: loss = 0.006508537,0.017875895\n",
      "Iteration 02865: loss = 0.0065081925,0.01779487\n",
      "Iteration 02870: loss = 0.006507834,0.017714296\n",
      "Iteration 02875: loss = 0.0065074847,0.01763406\n",
      "Iteration 02880: loss = 0.0065071317,0.0175542\n",
      "Iteration 02885: loss = 0.006506786,0.017474761\n",
      "Iteration 02890: loss = 0.006506438,0.017395653\n",
      "Iteration 02895: loss = 0.0065060887,0.017316941\n",
      "Iteration 02900: loss = 0.0065057445,0.017238565\n",
      "Iteration 02905: loss = 0.0065053967,0.017160622\n",
      "Iteration 02910: loss = 0.0065050586,0.017083017\n",
      "Iteration 02915: loss = 0.006504707,0.01700581\n",
      "Iteration 02920: loss = 0.006504375,0.01692897\n",
      "Iteration 02925: loss = 0.0065040365,0.016852485\n",
      "Iteration 02930: loss = 0.0065036775,0.016776387\n",
      "Iteration 02935: loss = 0.0065033417,0.016700642\n",
      "Iteration 02940: loss = 0.0065030134,0.016625216\n",
      "Iteration 02945: loss = 0.006502668,0.016550219\n",
      "Iteration 02950: loss = 0.006502326,0.016475553\n",
      "Iteration 02955: loss = 0.0065019955,0.016401256\n",
      "Iteration 02960: loss = 0.0065016574,0.016327288\n",
      "Iteration 02965: loss = 0.006501326,0.016253714\n",
      "Iteration 02970: loss = 0.006500996,0.016180472\n",
      "Iteration 02975: loss = 0.006500659,0.016107574\n",
      "Iteration 02980: loss = 0.0065003266,0.016035065\n",
      "Iteration 02985: loss = 0.0065000015,0.015962882\n",
      "Iteration 02990: loss = 0.006499663,0.015891032\n",
      "Iteration 02995: loss = 0.0064993394,0.015819566\n",
      "Iteration 03000: loss = 0.0064990036,0.01574843\n",
      "Iteration 03005: loss = 0.00649869,0.015677616\n",
      "Iteration 03010: loss = 0.006498369,0.015607174\n",
      "Iteration 03015: loss = 0.006498027,0.015537042\n",
      "Iteration 03020: loss = 0.006497714,0.01546726\n",
      "Iteration 03025: loss = 0.006497385,0.015397849\n",
      "Iteration 03030: loss = 0.006497057,0.01532872\n",
      "Iteration 03035: loss = 0.0064967438,0.015259966\n",
      "Iteration 03040: loss = 0.006496417,0.015191531\n",
      "Iteration 03045: loss = 0.0064960974,0.015123404\n",
      "Iteration 03050: loss = 0.006495781,0.015055656\n",
      "Iteration 03055: loss = 0.006495451,0.0149882\n",
      "Iteration 03060: loss = 0.0064951438,0.014921053\n",
      "Iteration 03065: loss = 0.0064948252,0.0148542635\n",
      "Iteration 03070: loss = 0.0064945067,0.0147878025\n",
      "Iteration 03075: loss = 0.006494187,0.014721685\n",
      "Iteration 03080: loss = 0.0064938795,0.014655819\n",
      "Iteration 03085: loss = 0.0064935647,0.0145903025\n",
      "Iteration 03090: loss = 0.006493242,0.014525127\n",
      "Iteration 03095: loss = 0.0064929263,0.014460221\n",
      "Iteration 03100: loss = 0.006492615,0.014395659\n",
      "Iteration 03105: loss = 0.006492306,0.014331393\n",
      "Iteration 03110: loss = 0.0064920043,0.014267428\n",
      "Iteration 03115: loss = 0.006491693,0.014203779\n",
      "Iteration 03120: loss = 0.006491384,0.014140492\n",
      "Iteration 03125: loss = 0.0064910757,0.014077468\n",
      "Iteration 03130: loss = 0.006490778,0.014014762\n",
      "Iteration 03135: loss = 0.006490469,0.013952351\n",
      "Iteration 03140: loss = 0.006490156,0.013890239\n",
      "Iteration 03145: loss = 0.00648986,0.013828428\n",
      "Iteration 03150: loss = 0.006489555,0.013766908\n",
      "Iteration 03155: loss = 0.0064892494,0.013705707\n",
      "Iteration 03160: loss = 0.0064889393,0.013644805\n",
      "Iteration 03165: loss = 0.006488642,0.013584139\n",
      "Iteration 03170: loss = 0.006488344,0.0135237975\n",
      "Iteration 03175: loss = 0.0064880247,0.013463761\n",
      "Iteration 03180: loss = 0.0064877323,0.013404021\n",
      "Iteration 03185: loss = 0.0064874305,0.01334457\n",
      "Iteration 03190: loss = 0.00648714,0.013285384\n",
      "Iteration 03195: loss = 0.006486831,0.013226513\n",
      "Iteration 03200: loss = 0.006486546,0.013167911\n",
      "Iteration 03205: loss = 0.0064862403,0.013109579\n",
      "Iteration 03210: loss = 0.0064859535,0.013051526\n",
      "Iteration 03215: loss = 0.006485651,0.012993791\n",
      "Iteration 03220: loss = 0.00648535,0.012936285\n",
      "Iteration 03225: loss = 0.006485067,0.012879087\n",
      "Iteration 03230: loss = 0.006484773,0.012822166\n",
      "Iteration 03235: loss = 0.006484481,0.012765526\n",
      "Iteration 03240: loss = 0.0064841863,0.012709131\n",
      "Iteration 03245: loss = 0.0064838957,0.012653018\n",
      "Iteration 03250: loss = 0.0064836075,0.012597155\n",
      "Iteration 03255: loss = 0.0064833243,0.012541578\n",
      "Iteration 03260: loss = 0.0064830226,0.012486283\n",
      "Iteration 03265: loss = 0.0064827404,0.012431244\n",
      "Iteration 03270: loss = 0.0064824526,0.012376495\n",
      "Iteration 03275: loss = 0.006482166,0.012321936\n",
      "Iteration 03280: loss = 0.0064818854,0.012267709\n",
      "Iteration 03285: loss = 0.006481577,0.012213756\n",
      "Iteration 03290: loss = 0.0064812936,0.012160006\n",
      "Iteration 03295: loss = 0.0064810175,0.012106512\n",
      "Iteration 03300: loss = 0.006480733,0.012053331\n",
      "Iteration 03305: loss = 0.0064804424,0.012000333\n",
      "Iteration 03310: loss = 0.006480157,0.0119476635\n",
      "Iteration 03315: loss = 0.0064798784,0.0118951835\n",
      "Iteration 03320: loss = 0.0064795963,0.011843009\n",
      "Iteration 03325: loss = 0.006479326,0.011791059\n",
      "Iteration 03330: loss = 0.0064790435,0.011739366\n",
      "Iteration 03335: loss = 0.0064787585,0.011687884\n",
      "Iteration 03340: loss = 0.0064784787,0.0116366595\n",
      "Iteration 03345: loss = 0.006478189,0.011585675\n",
      "Iteration 03350: loss = 0.006477916,0.011534976\n",
      "Iteration 03355: loss = 0.0064776293,0.011484427\n",
      "Iteration 03360: loss = 0.006477365,0.01143418\n",
      "Iteration 03365: loss = 0.006477082,0.011384184\n",
      "Iteration 03370: loss = 0.006476808,0.011334398\n",
      "Iteration 03375: loss = 0.0064765387,0.01128487\n",
      "Iteration 03380: loss = 0.0064762603,0.011235547\n",
      "Iteration 03385: loss = 0.0064759706,0.0111864675\n",
      "Iteration 03390: loss = 0.006475709,0.011137597\n",
      "Iteration 03395: loss = 0.0064754356,0.011089009\n",
      "Iteration 03400: loss = 0.006475152,0.011040613\n",
      "Iteration 03405: loss = 0.0064748935,0.010992441\n",
      "Iteration 03410: loss = 0.0064746193,0.010944528\n",
      "Iteration 03415: loss = 0.0064743403,0.01089681\n",
      "Iteration 03420: loss = 0.0064740703,0.010849338\n",
      "Iteration 03425: loss = 0.0064738006,0.0108021\n",
      "Iteration 03430: loss = 0.0064735305,0.010755066\n",
      "Iteration 03435: loss = 0.006473258,0.010708271\n",
      "Iteration 03440: loss = 0.0064729904,0.010661665\n",
      "Iteration 03445: loss = 0.006472716,0.010615307\n",
      "Iteration 03450: loss = 0.0064724586,0.01056916\n",
      "Iteration 03455: loss = 0.006472183,0.010523183\n",
      "Iteration 03460: loss = 0.0064719263,0.010477494\n",
      "Iteration 03465: loss = 0.006471653,0.010431976\n",
      "Iteration 03470: loss = 0.006471398,0.010386705\n",
      "Iteration 03475: loss = 0.006471126,0.010341607\n",
      "Iteration 03480: loss = 0.006470859,0.010296771\n",
      "Iteration 03485: loss = 0.0064705885,0.010252109\n",
      "Iteration 03490: loss = 0.0064703394,0.0102076605\n",
      "Iteration 03495: loss = 0.0064700674,0.01016343\n",
      "Iteration 03500: loss = 0.0064697987,0.010119392\n",
      "Iteration 03505: loss = 0.0064695417,0.010075573\n",
      "Iteration 03510: loss = 0.006469274,0.010031923\n",
      "Iteration 03515: loss = 0.0064690164,0.009988516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 03520: loss = 0.006468762,0.009945305\n",
      "Iteration 03525: loss = 0.0064684995,0.0099023\n",
      "Iteration 03530: loss = 0.006468227,0.009859463\n",
      "Iteration 03535: loss = 0.0064679836,0.009816861\n",
      "Iteration 03540: loss = 0.006467718,0.009774413\n",
      "Iteration 03545: loss = 0.006467458,0.009732205\n",
      "Iteration 03550: loss = 0.006467207,0.009690169\n",
      "Iteration 03555: loss = 0.0064669386,0.009648345\n",
      "Iteration 03560: loss = 0.006466685,0.009606728\n",
      "Iteration 03565: loss = 0.0064664357,0.009565271\n",
      "Iteration 03570: loss = 0.006466174,0.009524013\n",
      "Iteration 03575: loss = 0.0064659123,0.009482954\n",
      "Iteration 03580: loss = 0.0064656637,0.009442084\n",
      "Iteration 03585: loss = 0.0064654015,0.009401392\n",
      "Iteration 03590: loss = 0.0064651505,0.009360907\n",
      "Iteration 03595: loss = 0.0064648935,0.009320625\n",
      "Iteration 03600: loss = 0.0064646504,0.009280489\n",
      "Iteration 03605: loss = 0.006464387,0.009240549\n",
      "Iteration 03610: loss = 0.006464138,0.009200783\n",
      "Iteration 03615: loss = 0.0064638783,0.009161231\n",
      "Iteration 03620: loss = 0.0064636315,0.009121847\n",
      "Iteration 03625: loss = 0.0064633843,0.009082656\n",
      "Iteration 03630: loss = 0.0064631314,0.009043612\n",
      "Iteration 03635: loss = 0.0064628813,0.009004769\n",
      "Iteration 03640: loss = 0.00646263,0.008966081\n",
      "Iteration 03645: loss = 0.0064623826,0.008927593\n",
      "Iteration 03650: loss = 0.006462129,0.00888928\n",
      "Iteration 03655: loss = 0.00646189,0.008851138\n",
      "Iteration 03660: loss = 0.0064616357,0.008813175\n",
      "Iteration 03665: loss = 0.0064613917,0.008775378\n",
      "Iteration 03670: loss = 0.0064611384,0.008737734\n",
      "Iteration 03675: loss = 0.006460901,0.008700322\n",
      "Iteration 03680: loss = 0.006460642,0.008663042\n",
      "Iteration 03685: loss = 0.006460398,0.008625953\n",
      "Iteration 03690: loss = 0.0064601465,0.008588993\n",
      "Iteration 03695: loss = 0.006459905,0.008552256\n",
      "Iteration 03700: loss = 0.0064596585,0.008515656\n",
      "Iteration 03705: loss = 0.0064594015,0.008479258\n",
      "Iteration 03710: loss = 0.006459167,0.008442968\n",
      "Iteration 03715: loss = 0.0064589265,0.008406887\n",
      "Iteration 03720: loss = 0.0064586946,0.008370964\n",
      "Iteration 03725: loss = 0.0064584464,0.0083352085\n",
      "Iteration 03730: loss = 0.006458206,0.008299604\n",
      "Iteration 03735: loss = 0.0064579584,0.008264146\n",
      "Iteration 03740: loss = 0.006457715,0.008228868\n",
      "Iteration 03745: loss = 0.0064574727,0.008193769\n",
      "Iteration 03750: loss = 0.006457235,0.008158815\n",
      "Iteration 03755: loss = 0.0064569847,0.008123992\n",
      "Iteration 03760: loss = 0.006456753,0.008089352\n",
      "Iteration 03765: loss = 0.0064565153,0.008054869\n",
      "Iteration 03770: loss = 0.0064562634,0.008020532\n",
      "Iteration 03775: loss = 0.006456032,0.007986338\n",
      "Iteration 03780: loss = 0.0064558,0.007952318\n",
      "Iteration 03785: loss = 0.006455554,0.0079184305\n",
      "Iteration 03790: loss = 0.0064553167,0.007884689\n",
      "Iteration 03795: loss = 0.006455081,0.007851118\n",
      "Iteration 03800: loss = 0.006454852,0.0078176875\n",
      "Iteration 03805: loss = 0.0064546037,0.007784441\n",
      "Iteration 03810: loss = 0.0064543504,0.0077513275\n",
      "Iteration 03815: loss = 0.0064541274,0.0077183703\n",
      "Iteration 03820: loss = 0.006453889,0.007685574\n",
      "Iteration 03825: loss = 0.006453665,0.0076528797\n",
      "Iteration 03830: loss = 0.0064534373,0.007620372\n",
      "Iteration 03835: loss = 0.0064531895,0.007588053\n",
      "Iteration 03840: loss = 0.006452953,0.007555833\n",
      "Iteration 03845: loss = 0.006452715,0.0075237965\n",
      "Iteration 03850: loss = 0.006452481,0.007491908\n",
      "Iteration 03855: loss = 0.0064522736,0.007460124\n",
      "Iteration 03860: loss = 0.006452014,0.0074285017\n",
      "Iteration 03865: loss = 0.006451798,0.007396963\n",
      "Iteration 03870: loss = 0.0064515644,0.00736557\n",
      "Iteration 03875: loss = 0.006451338,0.007334352\n",
      "Iteration 03880: loss = 0.0064511173,0.0073032295\n",
      "Iteration 03885: loss = 0.0064508454,0.0072723366\n",
      "Iteration 03890: loss = 0.0064506344,0.0072415415\n",
      "Iteration 03895: loss = 0.006450403,0.007210975\n",
      "Iteration 03900: loss = 0.006450154,0.0071805115\n",
      "Iteration 03905: loss = 0.0064499765,0.007150132\n",
      "Iteration 03910: loss = 0.0064496933,0.0071199937\n",
      "Iteration 03915: loss = 0.0064495094,0.007089827\n",
      "Iteration 03920: loss = 0.0064492556,0.0070598894\n",
      "Iteration 03925: loss = 0.00644906,0.0070300596\n",
      "Iteration 03930: loss = 0.0064487183,0.007000635\n",
      "Iteration 03935: loss = 0.006448604,0.006971116\n",
      "Iteration 03940: loss = 0.0064483066,0.006941855\n",
      "Iteration 03945: loss = 0.006448123,0.0069125653\n",
      "Iteration 03950: loss = 0.0064478847,0.0068834685\n",
      "Iteration 03955: loss = 0.006447608,0.006854648\n",
      "Iteration 03960: loss = 0.0064474605,0.006825876\n",
      "Iteration 03965: loss = 0.006447144,0.0067975176\n",
      "Iteration 03970: loss = 0.0064469646,0.006769038\n",
      "Iteration 03975: loss = 0.0064467792,0.0067405785\n",
      "Iteration 03980: loss = 0.0064464207,0.0067125736\n",
      "Iteration 03985: loss = 0.0064463005,0.0066846\n",
      "Iteration 03990: loss = 0.0064459494,0.006657361\n",
      "Iteration 03995: loss = 0.006445773,0.0066298526\n",
      "Iteration 04000: loss = 0.006445646,0.0066022477\n",
      "Iteration 04005: loss = 0.0064452644,0.0065750806\n",
      "Iteration 04010: loss = 0.0064452663,0.0065477076\n",
      "Iteration 04015: loss = 0.0064446796,0.0065214704\n",
      "Iteration 04020: loss = 0.0064446623,0.0064947265\n",
      "Iteration 04025: loss = 0.0064444,0.0064683603\n",
      "Iteration 04030: loss = 0.006444061,0.006442342\n",
      "Iteration 04035: loss = 0.0064439154,0.0064162402\n",
      "Iteration 04040: loss = 0.0064437534,0.006390445\n",
      "Iteration 04045: loss = 0.0064433985,0.0063648494\n",
      "Iteration 04050: loss = 0.0064433734,0.0063389107\n",
      "Iteration 04055: loss = 0.0064428914,0.00631404\n",
      "Iteration 04060: loss = 0.0064426307,0.0062891673\n",
      "Iteration 04065: loss = 0.006442621,0.006264182\n",
      "Iteration 04070: loss = 0.006442237,0.0062395437\n",
      "Iteration 04075: loss = 0.0064422097,0.0062143668\n",
      "Iteration 04080: loss = 0.0064418227,0.0061900923\n",
      "Iteration 04085: loss = 0.006441577,0.006165835\n",
      "Iteration 04090: loss = 0.006441591,0.006141268\n",
      "Iteration 04095: loss = 0.0064409743,0.0061179567\n",
      "Iteration 04100: loss = 0.006440958,0.006093866\n",
      "Iteration 04105: loss = 0.0064407326,0.006070289\n",
      "Iteration 04110: loss = 0.0064403336,0.0060471855\n",
      "Iteration 04115: loss = 0.0064405357,0.0060231267\n",
      "Iteration 04120: loss = 0.0064398996,0.006000343\n",
      "Iteration 04125: loss = 0.006439766,0.0059771594\n",
      "Iteration 04130: loss = 0.0064396937,0.0059542283\n",
      "Iteration 04135: loss = 0.0064391443,0.0059320675\n",
      "Iteration 04140: loss = 0.0064392686,0.005908978\n",
      "Iteration 04145: loss = 0.0064388136,0.0058871163\n",
      "Iteration 04150: loss = 0.0064384905,0.0058652386\n",
      "Iteration 04155: loss = 0.0064386674,0.0058426624\n",
      "Iteration 04160: loss = 0.0064379596,0.005821555\n",
      "Iteration 04165: loss = 0.0064380816,0.005799267\n",
      "Iteration 04170: loss = 0.0064378106,0.0057776966\n",
      "Iteration 04175: loss = 0.0064373333,0.0057566585\n",
      "Iteration 04180: loss = 0.0064373785,0.005734941\n",
      "Iteration 04185: loss = 0.00643687,0.0057142647\n",
      "Iteration 04190: loss = 0.006436994,0.005692535\n",
      "Iteration 04195: loss = 0.006436374,0.0056724544\n",
      "Iteration 04200: loss = 0.0064364895,0.005651122\n",
      "Iteration 04205: loss = 0.0064361948,0.00563062\n",
      "Iteration 04210: loss = 0.006435735,0.005610638\n",
      "Iteration 04215: loss = 0.0064359275,0.005589596\n",
      "Iteration 04220: loss = 0.0064351805,0.005570419\n",
      "Iteration 04225: loss = 0.006435433,0.005549426\n",
      "Iteration 04230: loss = 0.0064347605,0.0055303616\n",
      "Iteration 04235: loss = 0.006435105,0.0055094296\n",
      "Iteration 04240: loss = 0.0064344727,0.005490345\n",
      "Iteration 04245: loss = 0.0064343554,0.005470561\n",
      "Iteration 04250: loss = 0.0064343847,0.0054506394\n",
      "Iteration 04255: loss = 0.0064337277,0.0054321387\n",
      "Iteration 04260: loss = 0.0064338353,0.0054123495\n",
      "Iteration 04265: loss = 0.0064334744,0.0053935596\n",
      "Iteration 04270: loss = 0.006433081,0.0053750277\n",
      "Iteration 04275: loss = 0.0064332536,0.005355412\n",
      "Iteration 04280: loss = 0.006432444,0.0053381203\n",
      "Iteration 04285: loss = 0.0064328904,0.0053181136\n",
      "Iteration 04290: loss = 0.0064319284,0.005301331\n",
      "Iteration 04295: loss = 0.00643241,0.0052814917\n",
      "Iteration 04300: loss = 0.0064314567,0.005264949\n",
      "Iteration 04305: loss = 0.006431986,0.0052451706\n",
      "Iteration 04310: loss = 0.0064310445,0.0052287634\n",
      "Iteration 04315: loss = 0.0064314418,0.0052094003\n",
      "Iteration 04320: loss = 0.0064307586,0.005192538\n",
      "Iteration 04325: loss = 0.006430862,0.0051740985\n",
      "Iteration 04330: loss = 0.0064303987,0.005157098\n",
      "Iteration 04335: loss = 0.0064302175,0.005139601\n",
      "Iteration 04340: loss = 0.0064301603,0.0051217815\n",
      "Iteration 04345: loss = 0.006429534,0.0051056873\n",
      "Iteration 04350: loss = 0.006429806,0.0050873226\n",
      "Iteration 04355: loss = 0.006429242,0.005070996\n",
      "Iteration 04360: loss = 0.0064297207,0.005052424\n",
      "Iteration 04365: loss = 0.0064285975,0.0050377245\n",
      "Iteration 04370: loss = 0.0064291335,0.0050188047\n",
      "Iteration 04375: loss = 0.0064283093,0.0050033093\n",
      "Iteration 04380: loss = 0.0064286552,0.0049849185\n",
      "Iteration 04385: loss = 0.00642772,0.0049701342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 04390: loss = 0.006428283,0.0049516144\n",
      "Iteration 04395: loss = 0.006427053,0.0049381424\n",
      "Iteration 04400: loss = 0.006427616,0.0049196673\n",
      "Iteration 04405: loss = 0.006426712,0.004905373\n",
      "Iteration 04410: loss = 0.0064272396,0.00488728\n",
      "Iteration 04415: loss = 0.0064262724,0.0048732897\n",
      "Iteration 04420: loss = 0.006426669,0.004855797\n",
      "Iteration 04425: loss = 0.006426249,0.0048401877\n",
      "Iteration 04430: loss = 0.0064262557,0.0048239245\n",
      "Iteration 04435: loss = 0.006425639,0.004809371\n",
      "Iteration 04440: loss = 0.0064256233,0.004793236\n",
      "Iteration 04445: loss = 0.00642503,0.00477877\n",
      "Iteration 04450: loss = 0.0064250208,0.004762956\n",
      "Iteration 04455: loss = 0.00642457,0.004748341\n",
      "Iteration 04460: loss = 0.006424449,0.004732965\n",
      "Iteration 04465: loss = 0.006424338,0.004717515\n",
      "Iteration 04470: loss = 0.006423963,0.0047029755\n",
      "Iteration 04475: loss = 0.0064243716,0.0046860278\n",
      "Iteration 04480: loss = 0.006423441,0.004673521\n",
      "Iteration 04485: loss = 0.006423731,0.0046569216\n",
      "Iteration 04490: loss = 0.0064228717,0.004644287\n",
      "Iteration 04495: loss = 0.0064231213,0.004627988\n",
      "Iteration 04500: loss = 0.006422236,0.0046157436\n",
      "Iteration 04505: loss = 0.006422702,0.004599027\n",
      "Iteration 04510: loss = 0.00642172,0.0045874137\n",
      "Iteration 04515: loss = 0.006422185,0.004570597\n",
      "Iteration 04520: loss = 0.0064212247,0.0045589916\n",
      "Iteration 04525: loss = 0.0064216317,0.004542773\n",
      "Iteration 04530: loss = 0.0064208205,0.0045305546\n",
      "Iteration 04535: loss = 0.0064214407,0.004513768\n",
      "Iteration 04540: loss = 0.0064201974,0.0045036166\n",
      "Iteration 04545: loss = 0.0064208503,0.0044866875\n",
      "Iteration 04550: loss = 0.0064196177,0.004476467\n",
      "Iteration 04555: loss = 0.006420255,0.004459595\n",
      "Iteration 04560: loss = 0.006419089,0.00444941\n",
      "Iteration 04565: loss = 0.0064198654,0.0044323322\n",
      "Iteration 04570: loss = 0.006418466,0.004423532\n",
      "Iteration 04575: loss = 0.006419208,0.004406078\n",
      "Iteration 04580: loss = 0.006417951,0.004396896\n",
      "Iteration 04585: loss = 0.0064185113,0.0043804846\n",
      "Iteration 04590: loss = 0.006417467,0.0043704547\n",
      "Iteration 04595: loss = 0.00641813,0.0043535526\n",
      "Iteration 04600: loss = 0.0064169876,0.004344679\n",
      "Iteration 04605: loss = 0.0064174025,0.004328777\n",
      "Iteration 04610: loss = 0.0064163953,0.0043191696\n",
      "Iteration 04615: loss = 0.0064167064,0.0043037543\n",
      "Iteration 04620: loss = 0.0064158845,0.0042937924\n",
      "Iteration 04625: loss = 0.0064161564,0.0042784642\n",
      "Iteration 04630: loss = 0.0064153927,0.0042688614\n",
      "Iteration 04635: loss = 0.0064152475,0.0042552883\n",
      "Iteration 04640: loss = 0.006414943,0.004243559\n",
      "Iteration 04645: loss = 0.0064144204,0.0042320276\n",
      "Iteration 04650: loss = 0.006414406,0.0042190915\n",
      "Iteration 04655: loss = 0.0064136684,0.0042087976\n",
      "Iteration 04660: loss = 0.0064140987,0.004194006\n",
      "Iteration 04665: loss = 0.006412964,0.00418563\n",
      "Iteration 04670: loss = 0.0064137946,0.0041691316\n",
      "Iteration 04675: loss = 0.0064120307,0.0041647926\n",
      "Iteration 04680: loss = 0.006412933,0.0041471384\n",
      "Iteration 04685: loss = 0.0064112428,0.0041425815\n",
      "Iteration 04690: loss = 0.0064121094,0.004124515\n",
      "Iteration 04695: loss = 0.0064106914,0.0041195117\n",
      "Iteration 04700: loss = 0.0064111627,0.0041031665\n",
      "Iteration 04705: loss = 0.006410102,0.0040965145\n",
      "Iteration 04710: loss = 0.006410138,0.0040821116\n",
      "Iteration 04715: loss = 0.0064100865,0.004070874\n",
      "Iteration 04720: loss = 0.0064089256,0.004063924\n",
      "Iteration 04725: loss = 0.0064096893,0.0040475843\n",
      "Iteration 04730: loss = 0.006407713,0.0040469468\n",
      "Iteration 04735: loss = 0.0064089787,0.0040263482\n",
      "Iteration 04740: loss = 0.0064069186,0.004027059\n",
      "Iteration 04745: loss = 0.00640758,0.0040077735\n",
      "Iteration 04750: loss = 0.006406717,0.004002155\n",
      "Iteration 04755: loss = 0.006406022,0.0039923927\n",
      "Iteration 04760: loss = 0.0064065685,0.0039777043\n",
      "Iteration 04765: loss = 0.0064045307,0.0039795367\n",
      "Iteration 04770: loss = 0.006406,0.003956751\n",
      "Iteration 04775: loss = 0.006403577,0.003962725\n",
      "Iteration 04780: loss = 0.006404226,0.0039419746\n",
      "Iteration 04785: loss = 0.006403295,0.0039379722\n",
      "Iteration 04790: loss = 0.0064023416,0.0039306516\n",
      "Iteration 04795: loss = 0.006403237,0.003913737\n",
      "Iteration 04800: loss = 0.006400788,0.0039214455\n",
      "Iteration 04805: loss = 0.0064023486,0.0038947398\n",
      "Iteration 04810: loss = 0.0064000036,0.0039048966\n",
      "Iteration 04815: loss = 0.006399805,0.0038878697\n",
      "Iteration 04820: loss = 0.0064002327,0.0038769017\n",
      "Iteration 04825: loss = 0.0063973106,0.0038913991\n",
      "Iteration 04830: loss = 0.0063991793,0.0038595798\n",
      "Iteration 04835: loss = 0.0063964482,0.0038774766\n",
      "Iteration 04840: loss = 0.006396355,0.003856492\n",
      "Iteration 04845: loss = 0.006396877,0.0038448276\n",
      "Iteration 04850: loss = 0.006393684,0.0038681051\n",
      "Iteration 04855: loss = 0.006395782,0.0038283411\n",
      "Iteration 04860: loss = 0.006393032,0.0038534892\n",
      "Iteration 04865: loss = 0.0063920356,0.0038391915\n",
      "Iteration 04870: loss = 0.006393907,0.0038127773\n",
      "Iteration 04875: loss = 0.0063895187,0.0038646772\n",
      "Iteration 04880: loss = 0.006390806,0.0038140458\n",
      "Iteration 04885: loss = 0.0063912007,0.0038047065\n",
      "Iteration 04890: loss = 0.006387217,0.003855093\n",
      "Iteration 04895: loss = 0.0063891322,0.0037961393\n",
      "Iteration 04900: loss = 0.0063891546,0.0037765745\n",
      "Iteration 04905: loss = 0.006389329,0.0037645432\n",
      "Iteration 04910: loss = 0.006388837,0.003772635\n",
      "Iteration 04915: loss = 0.006385838,0.0037977886\n",
      "Iteration 04920: loss = 0.0063848584,0.0037879741\n",
      "Iteration 04925: loss = 0.0063833017,0.0038097533\n",
      "Iteration 04930: loss = 0.0063797445,0.0038714833\n",
      "Iteration 04935: loss = 0.0063782143,0.0038755639\n",
      "Iteration 04940: loss = 0.006375304,0.003920814\n",
      "Iteration 04945: loss = 0.006373457,0.0039518047\n",
      "Iteration 04950: loss = 0.006371015,0.003997729\n",
      "Iteration 04955: loss = 0.0063686273,0.0040430482\n",
      "Iteration 04960: loss = 0.0063653816,0.0041161925\n",
      "Iteration 04965: loss = 0.006363338,0.004163015\n",
      "Iteration 04970: loss = 0.006359374,0.004286267\n",
      "Iteration 04975: loss = 0.00635694,0.0043691024\n",
      "Iteration 04980: loss = 0.0063522104,0.0045448523\n",
      "Iteration 04985: loss = 0.0063493154,0.0046769623\n",
      "Iteration 04990: loss = 0.0063438583,0.0049550505\n",
      "Iteration 04995: loss = 0.006340966,0.0051637776\n",
      "Iteration 05000: loss = 0.0063343537,0.0056019034\n",
      "Iteration 05005: loss = 0.006331809,0.005865436\n",
      "Iteration 05010: loss = 0.006324478,0.0064045163\n",
      "Iteration 05015: loss = 0.006322445,0.0065803113\n",
      "Iteration 05020: loss = 0.006315027,0.006949297\n",
      "Iteration 05025: loss = 0.0063134246,0.0068385624\n",
      "Iteration 05030: loss = 0.0063067055,0.0070278486\n",
      "Iteration 05035: loss = 0.0063056196,0.00695248\n",
      "Iteration 05040: loss = 0.0062995707,0.0073444513\n",
      "Iteration 05045: loss = 0.006299147,0.0074892254\n",
      "Iteration 05050: loss = 0.006292905,0.008062897\n",
      "Iteration 05055: loss = 0.0062930468,0.008069072\n",
      "Iteration 05060: loss = 0.00628667,0.008278412\n",
      "Iteration 05065: loss = 0.006286779,0.007880633\n",
      "Iteration 05070: loss = 0.0062811766,0.007953478\n",
      "Iteration 05075: loss = 0.0062812865,0.0078272335\n",
      "Iteration 05080: loss = 0.006275976,0.008337317\n",
      "Iteration 05085: loss = 0.006276544,0.00861883\n",
      "Iteration 05090: loss = 0.0062702014,0.0094035175\n",
      "Iteration 05095: loss = 0.00627118,0.009292562\n",
      "Iteration 05100: loss = 0.0062645623,0.009262306\n",
      "Iteration 05105: loss = 0.006264897,0.008562848\n",
      "Iteration 05110: loss = 0.0062597822,0.008561129\n",
      "Iteration 05115: loss = 0.0062602167,0.008513282\n",
      "Iteration 05120: loss = 0.006255165,0.009304094\n",
      "Iteration 05125: loss = 0.006256372,0.009956954\n",
      "Iteration 05130: loss = 0.0062493696,0.011180269\n",
      "Iteration 05135: loss = 0.006251154,0.010745883\n",
      "Iteration 05140: loss = 0.0062441714,0.010023978\n",
      "Iteration 05145: loss = 0.006244456,0.008930134\n",
      "Iteration 05150: loss = 0.006241156,0.0088095125\n",
      "Iteration 05155: loss = 0.006242492,0.008784624\n",
      "Iteration 05160: loss = 0.0062388307,0.0097399615\n",
      "Iteration 05165: loss = 0.006241018,0.011060726\n",
      "Iteration 05170: loss = 0.006230308,0.014031747\n",
      "Iteration 05175: loss = 0.006225617,0.015100567\n",
      "Iteration 05180: loss = 0.0062182895,0.011593514\n",
      "Iteration 05185: loss = 0.006213993,0.010111287\n",
      "Iteration 05190: loss = 0.0062171477,0.010431234\n",
      "Iteration 05195: loss = 0.0062128752,0.011558918\n",
      "Iteration 05200: loss = 0.006214589,0.011357108\n",
      "Iteration 05205: loss = 0.006212264,0.011070101\n",
      "Iteration 05210: loss = 0.006213596,0.010543219\n",
      "Iteration 05215: loss = 0.006210107,0.010777922\n",
      "Iteration 05220: loss = 0.006212337,0.010906747\n",
      "Iteration 05225: loss = 0.0062059765,0.012258124\n",
      "Iteration 05230: loss = 0.0062088934,0.013037242\n",
      "Iteration 05235: loss = 0.006201076,0.014085721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 05240: loss = 0.0062024803,0.012738553\n",
      "Iteration 05245: loss = 0.0061959424,0.01173122\n",
      "Iteration 05250: loss = 0.0061962325,0.010905197\n",
      "Iteration 05255: loss = 0.006194135,0.010921132\n",
      "Iteration 05260: loss = 0.0061966325,0.010963839\n",
      "Iteration 05265: loss = 0.006192625,0.012453656\n",
      "Iteration 05270: loss = 0.0061960393,0.01465608\n",
      "Iteration 05275: loss = 0.006172038,0.021720454\n",
      "Iteration 05280: loss = 0.00617269,0.019656006\n",
      "Iteration 05285: loss = 0.006163007,0.013494436\n",
      "Iteration 05290: loss = 0.0061554443,0.015547701\n",
      "Iteration 05295: loss = 0.0061593875,0.015570188\n",
      "Iteration 05300: loss = 0.0061534476,0.014362304\n",
      "Iteration 05305: loss = 0.006156871,0.013361455\n",
      "Iteration 05310: loss = 0.0061649713,0.012707699\n",
      "Iteration 05315: loss = 0.006165235,0.013055964\n",
      "Iteration 05320: loss = 0.0061712167,0.012996912\n",
      "Iteration 05325: loss = 0.006168755,0.0137938205\n",
      "Iteration 05330: loss = 0.0061717643,0.013961598\n",
      "Iteration 05335: loss = 0.006166452,0.014537502\n",
      "Iteration 05340: loss = 0.006167343,0.013962872\n",
      "Iteration 05345: loss = 0.0061589587,0.014153072\n",
      "Iteration 05350: loss = 0.006159645,0.013655286\n",
      "Iteration 05355: loss = 0.006152258,0.014412435\n",
      "Iteration 05360: loss = 0.006152755,0.014764988\n",
      "Iteration 05365: loss = 0.0061450363,0.016452475\n",
      "Iteration 05370: loss = 0.006146353,0.017211478\n",
      "Iteration 05375: loss = 0.006137619,0.018165788\n",
      "Iteration 05380: loss = 0.006139649,0.016481888\n",
      "Iteration 05385: loss = 0.0061328225,0.015690077\n",
      "Iteration 05390: loss = 0.006134635,0.01474695\n",
      "Iteration 05395: loss = 0.0061329114,0.014794508\n",
      "Iteration 05400: loss = 0.0061362884,0.014787135\n",
      "Iteration 05405: loss = 0.0061329654,0.016491469\n",
      "Iteration 05410: loss = 0.006137429,0.018936832\n",
      "Iteration 05415: loss = 0.0061193877,0.024889493\n",
      "Iteration 05420: loss = 0.0061208825,0.021946814\n",
      "Iteration 05425: loss = 0.0061123404,0.016492086\n",
      "Iteration 05430: loss = 0.006107513,0.01776364\n",
      "Iteration 05435: loss = 0.0061109704,0.018394262\n",
      "Iteration 05440: loss = 0.00610449,0.017929295\n",
      "Iteration 05445: loss = 0.0061075524,0.016502142\n",
      "Iteration 05450: loss = 0.0061119744,0.01568954\n",
      "Iteration 05455: loss = 0.006116417,0.015100247\n",
      "Iteration 05460: loss = 0.0061235144,0.014337887\n",
      "Iteration 05465: loss = 0.006127929,0.014147816\n",
      "Iteration 05470: loss = 0.006133543,0.015044387\n",
      "Iteration 05475: loss = 0.006128644,0.02011119\n",
      "Iteration 05480: loss = 0.0060914685,0.041513488\n",
      "Iteration 05485: loss = 0.0060387626,0.03881637\n",
      "Iteration 05490: loss = 0.006025674,0.034002446\n",
      "Iteration 05495: loss = 0.006006982,0.033961553\n",
      "Iteration 05500: loss = 0.006002868,0.035061356\n",
      "Iteration 05505: loss = 0.0059946585,0.03301365\n",
      "Iteration 05510: loss = 0.005996497,0.033273138\n",
      "Iteration 05515: loss = 0.0060146744,0.02843603\n",
      "Iteration 05520: loss = 0.0060255732,0.026667833\n",
      "Iteration 05525: loss = 0.006036177,0.024427678\n",
      "Iteration 05530: loss = 0.006053342,0.021592919\n",
      "Iteration 05535: loss = 0.0060664327,0.0197851\n",
      "Iteration 05540: loss = 0.006078962,0.017984308\n",
      "Iteration 05545: loss = 0.0060894494,0.016592339\n",
      "Iteration 05550: loss = 0.0060957973,0.015802072\n",
      "Iteration 05555: loss = 0.0061029135,0.015029971\n",
      "Iteration 05560: loss = 0.0061066714,0.014618089\n",
      "Iteration 05565: loss = 0.006107323,0.014557412\n",
      "Iteration 05570: loss = 0.006109414,0.014530004\n",
      "Iteration 05575: loss = 0.006103017,0.015744163\n",
      "Iteration 05580: loss = 0.006102147,0.017253755\n",
      "Iteration 05585: loss = 0.0060892445,0.021132385\n",
      "Iteration 05590: loss = 0.0060881153,0.023256445\n",
      "Iteration 05595: loss = 0.0060733357,0.022798633\n",
      "Iteration 05600: loss = 0.006069839,0.01917649\n",
      "Iteration 05605: loss = 0.0060640504,0.01874193\n",
      "Iteration 05610: loss = 0.006062916,0.019140868\n",
      "Iteration 05615: loss = 0.0060642846,0.01975578\n",
      "Iteration 05620: loss = 0.0060599297,0.02173282\n",
      "Iteration 05625: loss = 0.006063914,0.023172505\n",
      "Iteration 05630: loss = 0.0060549867,0.025285507\n",
      "Iteration 05635: loss = 0.0060594617,0.02274161\n",
      "Iteration 05640: loss = 0.006051272,0.020932492\n",
      "Iteration 05645: loss = 0.0060523427,0.019693408\n",
      "Iteration 05650: loss = 0.006053891,0.019298509\n",
      "Iteration 05655: loss = 0.006056285,0.019006051\n",
      "Iteration 05660: loss = 0.006061858,0.01860488\n",
      "Iteration 05665: loss = 0.0060607675,0.020082526\n",
      "Iteration 05670: loss = 0.006066194,0.02412607\n",
      "Iteration 05675: loss = 0.0060171722,0.04794942\n",
      "Iteration 05680: loss = 0.0060014804,0.04402652\n",
      "Iteration 05685: loss = 0.0059846216,0.032848448\n",
      "Iteration 05690: loss = 0.0059576514,0.039875552\n",
      "Iteration 05695: loss = 0.005955053,0.036669865\n",
      "Iteration 05700: loss = 0.005953062,0.036167007\n",
      "Iteration 05705: loss = 0.005955064,0.034675628\n",
      "Iteration 05710: loss = 0.00596693,0.03227743\n",
      "Iteration 05715: loss = 0.005974507,0.03005268\n",
      "Iteration 05720: loss = 0.0059892293,0.027787015\n",
      "Iteration 05725: loss = 0.0060015083,0.02527347\n",
      "Iteration 05730: loss = 0.006012818,0.023502607\n",
      "Iteration 05735: loss = 0.0060272496,0.021564566\n",
      "Iteration 05740: loss = 0.0060333363,0.020559879\n",
      "Iteration 05745: loss = 0.006040435,0.019547012\n",
      "Iteration 05750: loss = 0.006046687,0.018730393\n",
      "Iteration 05755: loss = 0.0060490593,0.01838425\n",
      "Iteration 05760: loss = 0.006052606,0.017933257\n",
      "Iteration 05765: loss = 0.006053474,0.01778577\n",
      "Iteration 05770: loss = 0.0060545295,0.017692553\n",
      "Iteration 05775: loss = 0.0060534417,0.01819087\n",
      "Iteration 05780: loss = 0.006052622,0.020420063\n",
      "Iteration 05785: loss = 0.0060395407,0.029703913\n",
      "Iteration 05790: loss = 0.0059779207,0.069782116\n",
      "Iteration 05795: loss = 0.00592815,0.04231615\n",
      "Iteration 05800: loss = 0.005898355,0.05931552\n",
      "Iteration 05805: loss = 0.00587624,0.05134402\n",
      "Iteration 05810: loss = 0.0058734734,0.051465623\n",
      "Iteration 05815: loss = 0.005866546,0.052865893\n",
      "Iteration 05820: loss = 0.00587602,0.047239996\n",
      "Iteration 05825: loss = 0.0058944616,0.04399944\n",
      "Iteration 05830: loss = 0.0059075044,0.039715353\n",
      "Iteration 05835: loss = 0.005931296,0.03514308\n",
      "Iteration 05840: loss = 0.0059497193,0.03125681\n",
      "Iteration 05845: loss = 0.005965904,0.028473794\n",
      "Iteration 05850: loss = 0.0059827827,0.025688507\n",
      "Iteration 05855: loss = 0.005991032,0.024422426\n",
      "Iteration 05860: loss = 0.0059985784,0.023251355\n",
      "Iteration 05865: loss = 0.006003674,0.022505302\n",
      "Iteration 05870: loss = 0.006004812,0.022270871\n",
      "Iteration 05875: loss = 0.006006501,0.021956623\n",
      "Iteration 05880: loss = 0.0060053356,0.022009706\n",
      "Iteration 05885: loss = 0.0060052346,0.021943837\n",
      "Iteration 05890: loss = 0.0060056546,0.021817176\n",
      "Iteration 05895: loss = 0.006005079,0.021818638\n",
      "Iteration 05900: loss = 0.006006364,0.021601213\n",
      "Iteration 05905: loss = 0.0060077235,0.021387646\n",
      "Iteration 05910: loss = 0.006008387,0.02132112\n",
      "Iteration 05915: loss = 0.006011378,0.021311022\n",
      "Iteration 05920: loss = 0.0060056723,0.024044245\n",
      "Iteration 05925: loss = 0.0060055293,0.0326226\n",
      "Iteration 05930: loss = 0.005917138,0.0838293\n",
      "Iteration 05935: loss = 0.0058870786,0.04718625\n",
      "Iteration 05940: loss = 0.005861342,0.06562234\n",
      "Iteration 05945: loss = 0.005829921,0.059526816\n",
      "Iteration 05950: loss = 0.00581705,0.0593151\n",
      "Iteration 05955: loss = 0.0058068037,0.063962735\n",
      "Iteration 05960: loss = 0.0058228797,0.05576319\n",
      "Iteration 05965: loss = 0.0058456813,0.050163895\n",
      "Iteration 05970: loss = 0.0058594984,0.04648296\n",
      "Iteration 05975: loss = 0.0058832406,0.04050848\n",
      "Iteration 05980: loss = 0.005903492,0.036567893\n",
      "Iteration 05985: loss = 0.0059233042,0.032560512\n",
      "Iteration 05990: loss = 0.0059414613,0.029657584\n",
      "Iteration 05995: loss = 0.005947931,0.028340768\n",
      "Iteration 06000: loss = 0.005956561,0.027046729\n",
      "Iteration 06005: loss = 0.00596044,0.026354615\n",
      "Iteration 06010: loss = 0.005960541,0.026232246\n",
      "Iteration 06015: loss = 0.005961323,0.026025575\n",
      "Iteration 06020: loss = 0.0059586167,0.026290959\n",
      "Iteration 06025: loss = 0.0059584104,0.026229678\n",
      "Iteration 06030: loss = 0.0059577473,0.026239013\n",
      "Iteration 06035: loss = 0.005956996,0.026264362\n",
      "Iteration 06040: loss = 0.005958677,0.025962193\n",
      "Iteration 06045: loss = 0.005959599,0.025766099\n",
      "Iteration 06050: loss = 0.0059619616,0.025381533\n",
      "Iteration 06055: loss = 0.0059649213,0.024924159\n",
      "Iteration 06060: loss = 0.005966741,0.024622075\n",
      "Iteration 06065: loss = 0.0059704683,0.024130765\n",
      "Iteration 06070: loss = 0.0059705977,0.02437001\n",
      "Iteration 06075: loss = 0.0059727696,0.0261957\n",
      "Iteration 06080: loss = 0.0059605767,0.03870526\n",
      "Iteration 06085: loss = 0.0058595035,0.12254931\n",
      "Iteration 06090: loss = 0.0057967263,0.06417677\n",
      "Iteration 06095: loss = 0.005751891,0.082660615\n",
      "Iteration 06100: loss = 0.005712573,0.09847275\n",
      "Iteration 06105: loss = 0.005701389,0.09499854\n",
      "Iteration 06110: loss = 0.005692826,0.09050295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 06115: loss = 0.005719367,0.07877821\n",
      "Iteration 06120: loss = 0.005751302,0.06989856\n",
      "Iteration 06125: loss = 0.005773097,0.062819354\n",
      "Iteration 06130: loss = 0.00581054,0.05247463\n",
      "Iteration 06135: loss = 0.0058384943,0.045315735\n",
      "Iteration 06140: loss = 0.005864316,0.04007901\n",
      "Iteration 06145: loss = 0.005885333,0.036112458\n",
      "Iteration 06150: loss = 0.0058923475,0.034583416\n",
      "Iteration 06155: loss = 0.00590162,0.03300284\n",
      "Iteration 06160: loss = 0.0059019215,0.032788448\n",
      "Iteration 06165: loss = 0.005898493,0.033132236\n",
      "Iteration 06170: loss = 0.0058962796,0.033336487\n",
      "Iteration 06175: loss = 0.0058899107,0.034228634\n",
      "Iteration 06180: loss = 0.0058883284,0.034359474\n",
      "Iteration 06185: loss = 0.005886098,0.03462233\n",
      "Iteration 06190: loss = 0.005884705,0.03476004\n",
      "Iteration 06195: loss = 0.0058871713,0.03427509\n",
      "Iteration 06200: loss = 0.0058885994,0.033953216\n",
      "Iteration 06205: loss = 0.00589281,0.033203553\n",
      "Iteration 06210: loss = 0.005896725,0.032507136\n",
      "Iteration 06215: loss = 0.0059002065,0.031885397\n",
      "Iteration 06220: loss = 0.0059047514,0.03112083\n",
      "Iteration 06225: loss = 0.005907308,0.030653946\n",
      "Iteration 06230: loss = 0.0059103332,0.030131035\n",
      "Iteration 06235: loss = 0.0059123714,0.029752903\n",
      "Iteration 06240: loss = 0.0059135314,0.029504998\n",
      "Iteration 06245: loss = 0.0059151077,0.029203087\n",
      "Iteration 06250: loss = 0.0059155333,0.029061846\n",
      "Iteration 06255: loss = 0.0059165116,0.028846681\n",
      "Iteration 06260: loss = 0.0059172097,0.028671142\n",
      "Iteration 06265: loss = 0.005918037,0.028483316\n",
      "Iteration 06270: loss = 0.0059190243,0.02828899\n",
      "Iteration 06275: loss = 0.0059205457,0.028130412\n",
      "Iteration 06280: loss = 0.0059185927,0.029394247\n",
      "Iteration 06285: loss = 0.0059203343,0.037248522\n",
      "Iteration 06290: loss = 0.0057863942,0.1554297\n",
      "Iteration 06295: loss = 0.0056921765,0.09436422\n",
      "Iteration 06300: loss = 0.005653205,0.11077842\n",
      "Iteration 06305: loss = 0.0055931476,0.13207062\n",
      "Iteration 06310: loss = 0.0055704997,0.13450988\n",
      "Iteration 06315: loss = 0.005555734,0.13556588\n",
      "Iteration 06320: loss = 0.005580455,0.12213461\n",
      "Iteration 06325: loss = 0.0056314333,0.10266332\n",
      "Iteration 06330: loss = 0.0056630597,0.087812364\n",
      "Iteration 06335: loss = 0.0057147685,0.070891365\n",
      "Iteration 06340: loss = 0.005760375,0.0579306\n",
      "Iteration 06345: loss = 0.0057934723,0.04970437\n",
      "Iteration 06350: loss = 0.005822407,0.043707922\n",
      "Iteration 06355: loss = 0.0058283,0.042233367\n",
      "Iteration 06360: loss = 0.005833959,0.041009042\n",
      "Iteration 06365: loss = 0.005831549,0.041179873\n",
      "Iteration 06370: loss = 0.0058209165,0.042842068\n",
      "Iteration 06375: loss = 0.0058138203,0.04386299\n",
      "Iteration 06380: loss = 0.0058015957,0.046011176\n",
      "Iteration 06385: loss = 0.0057954993,0.047009286\n",
      "Iteration 06390: loss = 0.005791843,0.047600497\n",
      "Iteration 06395: loss = 0.0057891025,0.04803767\n",
      "Iteration 06400: loss = 0.0057928846,0.047176093\n",
      "Iteration 06405: loss = 0.005796185,0.046400525\n",
      "Iteration 06410: loss = 0.0058029215,0.044998344\n",
      "Iteration 06415: loss = 0.0058101364,0.04353509\n",
      "Iteration 06420: loss = 0.005815853,0.04237372\n",
      "Iteration 06425: loss = 0.005822659,0.041065108\n",
      "Iteration 06430: loss = 0.0058265137,0.040284943\n",
      "Iteration 06435: loss = 0.0058299173,0.039596982\n",
      "Iteration 06440: loss = 0.0058319014,0.03915564\n",
      "Iteration 06445: loss = 0.0058322176,0.039000053\n",
      "Iteration 06450: loss = 0.005832834,0.03879252\n",
      "Iteration 06455: loss = 0.005832128,0.038810603\n",
      "Iteration 06460: loss = 0.0058319713,0.038737487\n",
      "Iteration 06465: loss = 0.0058319434,0.038645077\n",
      "Iteration 06470: loss = 0.0058320668,0.038530067\n",
      "Iteration 06475: loss = 0.005833199,0.03824718\n",
      "Iteration 06480: loss = 0.0058343126,0.037970867\n",
      "Iteration 06485: loss = 0.005836194,0.0375673\n",
      "Iteration 06490: loss = 0.005838252,0.037138805\n",
      "Iteration 06495: loss = 0.0058403597,0.03670522\n",
      "Iteration 06500: loss = 0.005842723,0.03623401\n",
      "Iteration 06505: loss = 0.0058447267,0.035822336\n",
      "Iteration 06510: loss = 0.005846735,0.03541495\n",
      "Iteration 06515: loss = 0.0058485083,0.035048623\n",
      "Iteration 06520: loss = 0.0058499225,0.03473946\n",
      "Iteration 06525: loss = 0.0058514695,0.034414805\n",
      "Iteration 06530: loss = 0.0058523505,0.034200884\n",
      "Iteration 06535: loss = 0.0058538946,0.033915304\n",
      "Iteration 06540: loss = 0.0058535994,0.03409375\n",
      "Iteration 06545: loss = 0.005855016,0.035234466\n",
      "Iteration 06550: loss = 0.005844304,0.045318015\n",
      "Iteration 06555: loss = 0.005752589,0.13544805\n",
      "Iteration 06560: loss = 0.0056389626,0.10383346\n",
      "Iteration 06565: loss = 0.0055822562,0.13505013\n",
      "Iteration 06570: loss = 0.00552405,0.15283963\n",
      "Iteration 06575: loss = 0.0054941517,0.15234798\n",
      "Iteration 06580: loss = 0.005479923,0.14881147\n",
      "Iteration 06585: loss = 0.005509361,0.13095593\n",
      "Iteration 06590: loss = 0.0055591036,0.11109095\n",
      "Iteration 06595: loss = 0.005603457,0.09358916\n",
      "Iteration 06600: loss = 0.0056633097,0.07490055\n",
      "Iteration 06605: loss = 0.0057114963,0.061319664\n",
      "Iteration 06610: loss = 0.005751177,0.052002676\n",
      "Iteration 06615: loss = 0.005775997,0.04694351\n",
      "Iteration 06620: loss = 0.005781499,0.045818377\n",
      "Iteration 06625: loss = 0.005783133,0.045194544\n",
      "Iteration 06630: loss = 0.0057716914,0.04689729\n",
      "Iteration 06635: loss = 0.005757384,0.04923655\n",
      "Iteration 06640: loss = 0.0057411254,0.05219422\n",
      "Iteration 06645: loss = 0.0057246927,0.05542228\n",
      "Iteration 06650: loss = 0.005715564,0.057206042\n",
      "Iteration 06655: loss = 0.0057086456,0.058608886\n",
      "Iteration 06660: loss = 0.0057091457,0.058352556\n",
      "Iteration 06665: loss = 0.0057140724,0.057102457\n",
      "Iteration 06670: loss = 0.0057220417,0.055210862\n",
      "Iteration 06675: loss = 0.005733326,0.052700635\n",
      "Iteration 06680: loss = 0.00574325,0.05053658\n",
      "Iteration 06685: loss = 0.0057533663,0.048438497\n",
      "Iteration 06690: loss = 0.0057606976,0.046925265\n",
      "Iteration 06695: loss = 0.005765476,0.04591707\n",
      "Iteration 06700: loss = 0.0057681003,0.045311794\n",
      "Iteration 06705: loss = 0.0057677836,0.04523532\n",
      "Iteration 06710: loss = 0.0057666455,0.045308277\n",
      "Iteration 06715: loss = 0.0057640634,0.045650214\n",
      "Iteration 06720: loss = 0.0057616704,0.04596531\n",
      "Iteration 06725: loss = 0.005759806,0.04619198\n",
      "Iteration 06730: loss = 0.0057585984,0.046301182\n",
      "Iteration 06735: loss = 0.005758969,0.046115946\n",
      "Iteration 06740: loss = 0.005760129,0.045788802\n",
      "Iteration 06745: loss = 0.005762601,0.0452194\n",
      "Iteration 06750: loss = 0.0057655703,0.044565585\n",
      "Iteration 06755: loss = 0.0057690428,0.043827802\n",
      "Iteration 06760: loss = 0.0057722772,0.043150824\n",
      "Iteration 06765: loss = 0.0057757418,0.04251124\n",
      "Iteration 06770: loss = 0.0057763276,0.0428987\n",
      "Iteration 06775: loss = 0.005778464,0.047257937\n",
      "Iteration 06780: loss = 0.0057115178,0.1084444\n",
      "Iteration 06785: loss = 0.0055282414,0.20248052\n",
      "Iteration 06790: loss = 0.0054459027,0.2110816\n",
      "Iteration 06795: loss = 0.0053552496,0.23179612\n",
      "Iteration 06800: loss = 0.0052923094,0.24483782\n",
      "Iteration 06805: loss = 0.005264697,0.24699539\n",
      "Iteration 06810: loss = 0.0052865664,0.22538742\n",
      "Iteration 06815: loss = 0.0053644604,0.1826873\n",
      "Iteration 06820: loss = 0.005440291,0.14104706\n",
      "Iteration 06825: loss = 0.005530206,0.104318365\n",
      "Iteration 06830: loss = 0.005610999,0.07917043\n",
      "Iteration 06835: loss = 0.005666955,0.065238625\n",
      "Iteration 06840: loss = 0.005703297,0.057637002\n",
      "Iteration 06845: loss = 0.005704488,0.056944318\n",
      "Iteration 06850: loss = 0.005693575,0.058288038\n",
      "Iteration 06855: loss = 0.005666193,0.06301848\n",
      "Iteration 06860: loss = 0.0056302617,0.07031511\n",
      "Iteration 06865: loss = 0.005595315,0.07838644\n",
      "Iteration 06870: loss = 0.005561052,0.08743107\n",
      "Iteration 06875: loss = 0.0055406005,0.09303772\n",
      "Iteration 06880: loss = 0.0055297744,0.096015126\n",
      "Iteration 06885: loss = 0.0055323094,0.09488513\n",
      "Iteration 06890: loss = 0.005546432,0.09026713\n",
      "Iteration 06895: loss = 0.0055665164,0.08414156\n",
      "Iteration 06900: loss = 0.005591817,0.07712333\n",
      "Iteration 06905: loss = 0.0056144856,0.071238294\n",
      "Iteration 06910: loss = 0.0056338985,0.06654858\n",
      "Iteration 06915: loss = 0.0056468216,0.06351506\n",
      "Iteration 06920: loss = 0.0056524756,0.06209682\n",
      "Iteration 06925: loss = 0.0056522973,0.061887816\n",
      "Iteration 06930: loss = 0.0056460276,0.06296398\n",
      "Iteration 06935: loss = 0.0056371293,0.06463722\n",
      "Iteration 06940: loss = 0.0056264526,0.06679012\n",
      "Iteration 06945: loss = 0.0056166723,0.06882976\n",
      "Iteration 06950: loss = 0.005609324,0.070357785\n",
      "Iteration 06955: loss = 0.005605152,0.071155176\n",
      "Iteration 06960: loss = 0.005605221,0.07093025\n",
      "Iteration 06965: loss = 0.005608548,0.06991229\n",
      "Iteration 06970: loss = 0.0056149415,0.06818623\n",
      "Iteration 06975: loss = 0.0056228884,0.066135325\n",
      "Iteration 06980: loss = 0.0056312494,0.0640505\n",
      "Iteration 06985: loss = 0.005638974,0.062160358\n",
      "Iteration 06990: loss = 0.0056449655,0.060680635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 06995: loss = 0.005649032,0.059629597\n",
      "Iteration 07000: loss = 0.0056507844,0.05906341\n",
      "Iteration 07005: loss = 0.00565071,0.058878295\n",
      "Iteration 07010: loss = 0.005649209,0.058990944\n",
      "Iteration 07015: loss = 0.00564692,0.059271373\n",
      "Iteration 07020: loss = 0.005644617,0.059560355\n",
      "Iteration 07025: loss = 0.005642816,0.059753306\n",
      "Iteration 07030: loss = 0.0056420234,0.059730943\n",
      "Iteration 07035: loss = 0.0056424416,0.05945454\n",
      "Iteration 07040: loss = 0.005644088,0.058918674\n",
      "Iteration 07045: loss = 0.0056467843,0.058163024\n",
      "Iteration 07050: loss = 0.005650131,0.057282776\n",
      "Iteration 07055: loss = 0.005653962,0.056316204\n",
      "Iteration 07060: loss = 0.0056572356,0.05550418\n",
      "Iteration 07065: loss = 0.0056611802,0.054911148\n",
      "Iteration 07070: loss = 0.005657792,0.060123548\n",
      "Iteration 07075: loss = 0.0055768825,0.1726018\n",
      "Iteration 07080: loss = 0.005252072,0.29952887\n",
      "Iteration 07085: loss = 0.0051054717,0.3214845\n",
      "Iteration 07090: loss = 0.004953232,0.45506504\n",
      "Iteration 07095: loss = 0.0048533557,0.5373207\n",
      "Iteration 07100: loss = 0.0048284396,0.51923275\n",
      "Iteration 07105: loss = 0.004888968,0.43712395\n",
      "Iteration 07110: loss = 0.00504314,0.31877276\n",
      "Iteration 07115: loss = 0.005205456,0.2104107\n",
      "Iteration 07120: loss = 0.005369507,0.13494338\n",
      "Iteration 07125: loss = 0.005494174,0.09744558\n",
      "Iteration 07130: loss = 0.005563537,0.083026685\n",
      "Iteration 07135: loss = 0.0055888426,0.07776366\n",
      "Iteration 07140: loss = 0.005557588,0.08233625\n",
      "Iteration 07145: loss = 0.005501084,0.09234471\n",
      "Iteration 07150: loss = 0.005418092,0.112581305\n",
      "Iteration 07155: loss = 0.0053248447,0.14173874\n",
      "Iteration 07160: loss = 0.0052362736,0.17607825\n",
      "Iteration 07165: loss = 0.0051604602,0.21074535\n",
      "Iteration 07170: loss = 0.0051174774,0.23145327\n",
      "Iteration 07175: loss = 0.005107816,0.234403\n",
      "Iteration 07180: loss = 0.005133893,0.21785645\n",
      "Iteration 07185: loss = 0.005186852,0.18943702\n",
      "Iteration 07190: loss = 0.005251859,0.15962909\n",
      "Iteration 07195: loss = 0.005318289,0.13424101\n",
      "Iteration 07200: loss = 0.0053722425,0.11663154\n",
      "Iteration 07205: loss = 0.005408766,0.105984494\n",
      "Iteration 07210: loss = 0.005423788,0.10149127\n",
      "Iteration 07215: loss = 0.005417919,0.10221797\n",
      "Iteration 07220: loss = 0.0053947936,0.107516184\n",
      "Iteration 07225: loss = 0.0053582448,0.11726421\n",
      "Iteration 07230: loss = 0.00531535,0.13018033\n",
      "Iteration 07235: loss = 0.0052719656,0.14473183\n",
      "Iteration 07240: loss = 0.0052351435,0.1580158\n",
      "Iteration 07245: loss = 0.005210255,0.16713452\n",
      "Iteration 07250: loss = 0.0052005183,0.16999872\n",
      "Iteration 07255: loss = 0.0052067325,0.16607499\n",
      "Iteration 07260: loss = 0.0052260705,0.15698528\n",
      "Iteration 07265: loss = 0.005254095,0.14522281\n",
      "Iteration 07270: loss = 0.0052848607,0.13347337\n",
      "Iteration 07275: loss = 0.005312932,0.12357636\n",
      "Iteration 07280: loss = 0.005333943,0.11648875\n",
      "Iteration 07285: loss = 0.005345172,0.112525366\n",
      "Iteration 07290: loss = 0.005345879,0.111590266\n",
      "Iteration 07295: loss = 0.005336761,0.11345855\n",
      "Iteration 07300: loss = 0.005320026,0.117672116\n",
      "Iteration 07305: loss = 0.0052986704,0.123558104\n",
      "Iteration 07310: loss = 0.005276237,0.13012475\n",
      "Iteration 07315: loss = 0.0052562044,0.13615422\n",
      "Iteration 07320: loss = 0.0052415193,0.14044747\n",
      "Iteration 07325: loss = 0.005234119,0.14211877\n",
      "Iteration 07330: loss = 0.0052345577,0.14090632\n",
      "Iteration 07335: loss = 0.005242022,0.13720354\n",
      "Iteration 07340: loss = 0.005254517,0.13190016\n",
      "Iteration 07345: loss = 0.0052693547,0.12606873\n",
      "Iteration 07350: loss = 0.0052837115,0.12066096\n",
      "Iteration 07355: loss = 0.0052951607,0.11635188\n",
      "Iteration 07360: loss = 0.0053020287,0.11351197\n",
      "Iteration 07365: loss = 0.0053035505,0.11226307\n",
      "Iteration 07370: loss = 0.0052999053,0.1125149\n",
      "Iteration 07375: loss = 0.005292019,0.11401852\n",
      "Iteration 07380: loss = 0.00528145,0.11636865\n",
      "Iteration 07385: loss = 0.0052700345,0.11904027\n",
      "Iteration 07390: loss = 0.005259564,0.12146037\n",
      "Iteration 07395: loss = 0.0052515864,0.1230981\n",
      "Iteration 07400: loss = 0.005247057,0.1236077\n",
      "Iteration 07405: loss = 0.005246382,0.122854464\n",
      "Iteration 07410: loss = 0.005249186,0.12098824\n",
      "Iteration 07415: loss = 0.005254537,0.118356355\n",
      "Iteration 07420: loss = 0.005261256,0.1153738\n",
      "Iteration 07425: loss = 0.0052679568,0.11247259\n",
      "Iteration 07430: loss = 0.005273487,0.10997997\n",
      "Iteration 07435: loss = 0.005277071,0.108088784\n",
      "Iteration 07440: loss = 0.005278127,0.106929615\n",
      "Iteration 07445: loss = 0.0052771666,0.10635497\n",
      "Iteration 07450: loss = 0.005273539,0.106587276\n",
      "Iteration 07455: loss = 0.0052706003,0.10716665\n",
      "Iteration 07460: loss = 0.005258726,0.116300896\n",
      "Iteration 07465: loss = 0.0051843445,0.24697718\n",
      "Iteration 07470: loss = 0.0047666477,0.4586715\n",
      "Iteration 07475: loss = 0.0044030217,0.7279644\n",
      "Iteration 07480: loss = 0.0041025463,1.1493983\n",
      "Iteration 07485: loss = 0.004007299,1.2801152\n",
      "Iteration 07490: loss = 0.0041154413,0.9997722\n",
      "Iteration 07495: loss = 0.004334523,0.64370537\n",
      "Iteration 07500: loss = 0.0045944625,0.38994604\n",
      "Iteration 07505: loss = 0.0048157987,0.25994852\n",
      "Iteration 07510: loss = 0.0049757874,0.198591\n",
      "Iteration 07515: loss = 0.0050457628,0.17435716\n",
      "Iteration 07520: loss = 0.005030216,0.17211293\n",
      "Iteration 07525: loss = 0.0049428893,0.19280705\n",
      "Iteration 07530: loss = 0.0047974386,0.2460039\n",
      "Iteration 07535: loss = 0.004621249,0.33567977\n",
      "Iteration 07540: loss = 0.0044410448,0.45807585\n",
      "Iteration 07545: loss = 0.004299842,0.5738861\n",
      "Iteration 07550: loss = 0.004230377,0.6291186\n",
      "Iteration 07555: loss = 0.0042417785,0.5978983\n",
      "Iteration 07560: loss = 0.0043100207,0.5149732\n",
      "Iteration 07565: loss = 0.0043932586,0.43373337\n",
      "Iteration 07570: loss = 0.004457819,0.3794836\n",
      "Iteration 07575: loss = 0.004488195,0.35315123\n",
      "Iteration 07580: loss = 0.004485797,0.34755495\n",
      "Iteration 07585: loss = 0.0044580395,0.35697705\n",
      "Iteration 07590: loss = 0.004415113,0.37628138\n",
      "Iteration 07595: loss = 0.0043684174,0.3992265\n",
      "Iteration 07600: loss = 0.004328707,0.4183691\n",
      "Iteration 07605: loss = 0.0043022623,0.42816955\n",
      "Iteration 07610: loss = 0.0042888992,0.42807752\n",
      "Iteration 07615: loss = 0.004283973,0.4215635\n",
      "Iteration 07620: loss = 0.0042819316,0.4129683\n",
      "Iteration 07625: loss = 0.0042787646,0.40528488\n",
      "Iteration 07630: loss = 0.004272438,0.39986497\n",
      "Iteration 07635: loss = 0.00426274,0.39674267\n",
      "Iteration 07640: loss = 0.0042507886,0.3951252\n",
      "Iteration 07645: loss = 0.0042380495,0.39398158\n",
      "Iteration 07650: loss = 0.004225575,0.39259967\n",
      "Iteration 07655: loss = 0.004213659,0.3907865\n",
      "Iteration 07660: loss = 0.00420211,0.3886603\n",
      "Iteration 07665: loss = 0.0041907625,0.3863598\n",
      "Iteration 07670: loss = 0.0041795066,0.38396338\n",
      "Iteration 07675: loss = 0.00416832,0.38148338\n",
      "Iteration 07680: loss = 0.004157379,0.3788394\n",
      "Iteration 07685: loss = 0.00414683,0.37592703\n",
      "Iteration 07690: loss = 0.0041367463,0.3727335\n",
      "Iteration 07695: loss = 0.0041271225,0.36927152\n",
      "Iteration 07700: loss = 0.0041179,0.36560288\n",
      "Iteration 07705: loss = 0.0041089933,0.3617963\n",
      "Iteration 07710: loss = 0.0041004648,0.35783234\n",
      "Iteration 07715: loss = 0.0040918933,0.3539743\n",
      "Iteration 07720: loss = 0.004084704,0.34948722\n",
      "Iteration 07725: loss = 0.004072085,0.35101244\n",
      "Iteration 07730: loss = 0.004086783,0.4180803\n",
      "Iteration 07735: loss = 0.003276784,1.6287497\n",
      "Iteration 07740: loss = 0.0026408716,3.5136623\n",
      "Iteration 07745: loss = 0.0025917762,3.2066007\n",
      "Iteration 07750: loss = 0.0028864462,1.8430659\n",
      "Iteration 07755: loss = 0.0030875064,1.2963277\n",
      "Iteration 07760: loss = 0.0030942096,1.1790297\n",
      "Iteration 07765: loss = 0.0031742966,0.9647328\n",
      "Iteration 07770: loss = 0.003235887,0.84034616\n",
      "Iteration 07775: loss = 0.0032415444,0.7948666\n",
      "Iteration 07780: loss = 0.0032572926,0.75077367\n",
      "Iteration 07785: loss = 0.0032548315,0.72676265\n",
      "Iteration 07790: loss = 0.0032274465,0.7260181\n",
      "Iteration 07795: loss = 0.00319538,0.7356771\n",
      "Iteration 07800: loss = 0.0031706665,0.7386425\n",
      "Iteration 07805: loss = 0.0031485606,0.73800033\n",
      "Iteration 07810: loss = 0.0031277,0.73628485\n",
      "Iteration 07815: loss = 0.0031134805,0.72761655\n",
      "Iteration 07820: loss = 0.0030979577,0.72028047\n",
      "Iteration 07825: loss = 0.0030815657,0.7139706\n",
      "Iteration 07830: loss = 0.0030677083,0.70509934\n",
      "Iteration 07835: loss = 0.0030544617,0.695699\n",
      "Iteration 07840: loss = 0.0030426297,0.685056\n",
      "Iteration 07845: loss = 0.0030321989,0.67322403\n",
      "Iteration 07850: loss = 0.0030224398,0.6610432\n",
      "Iteration 07855: loss = 0.0030132467,0.6486707\n",
      "Iteration 07860: loss = 0.0030044552,0.6362979\n",
      "Iteration 07865: loss = 0.002995845,0.624154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 07870: loss = 0.0029877536,0.6119253\n",
      "Iteration 07875: loss = 0.0029802062,0.5996103\n",
      "Iteration 07880: loss = 0.0029728934,0.58749676\n",
      "Iteration 07885: loss = 0.0029658938,0.5755382\n",
      "Iteration 07890: loss = 0.0029591369,0.5637824\n",
      "Iteration 07895: loss = 0.0029525484,0.55227906\n",
      "Iteration 07900: loss = 0.002946195,0.5409791\n",
      "Iteration 07905: loss = 0.0029400487,0.5298991\n",
      "Iteration 07910: loss = 0.0029340766,0.51905394\n",
      "Iteration 07915: loss = 0.0029282896,0.50842935\n",
      "Iteration 07920: loss = 0.0029226665,0.4980333\n",
      "Iteration 07925: loss = 0.0029171843,0.4878729\n",
      "Iteration 07930: loss = 0.002911844,0.47794098\n",
      "Iteration 07935: loss = 0.0029066352,0.4682365\n",
      "Iteration 07940: loss = 0.002901555,0.45874888\n",
      "Iteration 07945: loss = 0.002896603,0.4494768\n",
      "Iteration 07950: loss = 0.0028917592,0.4404178\n",
      "Iteration 07955: loss = 0.002887021,0.43156514\n",
      "Iteration 07960: loss = 0.0028823984,0.42291355\n",
      "Iteration 07965: loss = 0.0028778736,0.41445926\n",
      "Iteration 07970: loss = 0.0028734512,0.40619683\n",
      "Iteration 07975: loss = 0.0028691136,0.3981198\n",
      "Iteration 07980: loss = 0.0028648935,0.39022434\n",
      "Iteration 07985: loss = 0.0028607417,0.38250533\n",
      "Iteration 07990: loss = 0.0028566911,0.37495533\n",
      "Iteration 07995: loss = 0.0028527377,0.3675678\n",
      "Iteration 08000: loss = 0.002848856,0.3603474\n",
      "Iteration 08005: loss = 0.0028450612,0.35328752\n",
      "Iteration 08010: loss = 0.002841355,0.34637856\n",
      "Iteration 08015: loss = 0.0028377187,0.33962208\n",
      "Iteration 08020: loss = 0.002834168,0.33300686\n",
      "Iteration 08025: loss = 0.0028306935,0.32653877\n",
      "Iteration 08030: loss = 0.002827295,0.32021004\n",
      "Iteration 08035: loss = 0.0028239836,0.31401008\n",
      "Iteration 08040: loss = 0.0028207293,0.30794963\n",
      "Iteration 08045: loss = 0.00281755,0.30201933\n",
      "Iteration 08050: loss = 0.0028144622,0.29620543\n",
      "Iteration 08055: loss = 0.0028113548,0.2905577\n",
      "Iteration 08060: loss = 0.0028086475,0.284891\n",
      "Iteration 08065: loss = 0.002804434,0.28066182\n",
      "Iteration 08070: loss = 0.0028072102,0.29131597\n",
      "Iteration 08075: loss = 0.0019870335,3.8875494\n",
      "Iteration 08080: loss = 0.0010384354,9.098385\n",
      "Iteration 08085: loss = 0.0008965614,9.704166\n",
      "Iteration 08090: loss = 0.0010797147,5.3172097\n",
      "Iteration 08095: loss = 0.0011767925,3.7824934\n",
      "Iteration 08100: loss = 0.001349342,2.5765128\n",
      "Iteration 08105: loss = 0.0014013458,2.1077347\n",
      "Iteration 08110: loss = 0.0015062595,1.5597363\n",
      "Iteration 08115: loss = 0.0015358137,1.3503987\n",
      "Iteration 08120: loss = 0.0015655062,1.1996543\n",
      "Iteration 08125: loss = 0.001597866,1.0743494\n",
      "Iteration 08130: loss = 0.0015986172,1.025814\n",
      "Iteration 08135: loss = 0.0016199328,0.9433172\n",
      "Iteration 08140: loss = 0.0016257078,0.896789\n",
      "Iteration 08145: loss = 0.0016313298,0.8566317\n",
      "Iteration 08150: loss = 0.001641936,0.8120268\n",
      "Iteration 08155: loss = 0.0016432739,0.7846968\n",
      "Iteration 08160: loss = 0.001649918,0.7510162\n",
      "Iteration 08165: loss = 0.0016545182,0.7225307\n",
      "Iteration 08170: loss = 0.0016574055,0.69812375\n",
      "Iteration 08175: loss = 0.0016632673,0.67087495\n",
      "Iteration 08180: loss = 0.0016670342,0.64788604\n",
      "Iteration 08185: loss = 0.0016715504,0.62513083\n",
      "Iteration 08190: loss = 0.0016767628,0.60265535\n",
      "Iteration 08195: loss = 0.001680838,0.5826952\n",
      "Iteration 08200: loss = 0.0016856071,0.5628662\n",
      "Iteration 08205: loss = 0.0016902142,0.5441852\n",
      "Iteration 08210: loss = 0.0016944214,0.5268208\n",
      "Iteration 08215: loss = 0.0016990121,0.50984854\n",
      "Iteration 08220: loss = 0.0017033209,0.49393034\n",
      "Iteration 08225: loss = 0.0017075496,0.47880784\n",
      "Iteration 08230: loss = 0.0017118659,0.46424943\n",
      "Iteration 08235: loss = 0.00171598,0.45050934\n",
      "Iteration 08240: loss = 0.0017200542,0.43739414\n",
      "Iteration 08245: loss = 0.0017241025,0.4248333\n",
      "Iteration 08250: loss = 0.0017280197,0.41290274\n",
      "Iteration 08255: loss = 0.0017318893,0.40147686\n",
      "Iteration 08260: loss = 0.0017357169,0.3905403\n",
      "Iteration 08265: loss = 0.0017394569,0.3800966\n",
      "Iteration 08270: loss = 0.0017431377,0.3700861\n",
      "Iteration 08275: loss = 0.0017467636,0.3604896\n",
      "Iteration 08280: loss = 0.0017503224,0.35130066\n",
      "Iteration 08285: loss = 0.0017538174,0.34248164\n",
      "Iteration 08290: loss = 0.0017572519,0.33401263\n",
      "Iteration 08295: loss = 0.0017606289,0.3258861\n",
      "Iteration 08300: loss = 0.0017639417,0.31807134\n",
      "Iteration 08305: loss = 0.0017672073,0.31055993\n",
      "Iteration 08310: loss = 0.0017704079,0.30333424\n",
      "Iteration 08315: loss = 0.0017735633,0.2963775\n",
      "Iteration 08320: loss = 0.0017766625,0.289681\n",
      "Iteration 08325: loss = 0.0017797077,0.2832288\n",
      "Iteration 08330: loss = 0.0017827018,0.27701047\n",
      "Iteration 08335: loss = 0.0017856363,0.27101207\n",
      "Iteration 08340: loss = 0.0017885249,0.26522598\n",
      "Iteration 08345: loss = 0.0017913751,0.25964147\n",
      "Iteration 08350: loss = 0.0017941637,0.25425178\n",
      "Iteration 08355: loss = 0.0017969157,0.24904394\n",
      "Iteration 08360: loss = 0.0017996136,0.24401188\n",
      "Iteration 08365: loss = 0.0018022718,0.23914686\n",
      "Iteration 08370: loss = 0.0018048855,0.23444378\n",
      "Iteration 08375: loss = 0.001807448,0.22989447\n",
      "Iteration 08380: loss = 0.0018099807,0.22549\n",
      "Iteration 08385: loss = 0.001812465,0.2212284\n",
      "Iteration 08390: loss = 0.0018149059,0.2171034\n",
      "Iteration 08395: loss = 0.0018173134,0.21310638\n",
      "Iteration 08400: loss = 0.0018196857,0.20923182\n",
      "Iteration 08405: loss = 0.0018220086,0.20547774\n",
      "Iteration 08410: loss = 0.0018242985,0.20183815\n",
      "Iteration 08415: loss = 0.0018265523,0.19831064\n",
      "Iteration 08420: loss = 0.0018287668,0.19488737\n",
      "Iteration 08425: loss = 0.0018309443,0.19156724\n",
      "Iteration 08430: loss = 0.00183309,0.18834229\n",
      "Iteration 08435: loss = 0.0018352009,0.18521422\n",
      "Iteration 08440: loss = 0.001837275,0.18217479\n",
      "Iteration 08445: loss = 0.0018393155,0.17922476\n",
      "Iteration 08450: loss = 0.0018413244,0.17635901\n",
      "Iteration 08455: loss = 0.0018432989,0.1735742\n",
      "Iteration 08460: loss = 0.0018452415,0.1708671\n",
      "Iteration 08465: loss = 0.0018471497,0.16823645\n",
      "Iteration 08470: loss = 0.0018490326,0.16567919\n",
      "Iteration 08475: loss = 0.0018508774,0.16319071\n",
      "Iteration 08480: loss = 0.0018526964,0.16077062\n",
      "Iteration 08485: loss = 0.0018544899,0.15841639\n",
      "Iteration 08490: loss = 0.0018562429,0.15612628\n",
      "Iteration 08495: loss = 0.0018579712,0.15389718\n",
      "Iteration 08500: loss = 0.0018596757,0.15172634\n",
      "Iteration 08505: loss = 0.0018613446,0.14961442\n",
      "Iteration 08510: loss = 0.0018629925,0.14755517\n",
      "Iteration 08515: loss = 0.001864601,0.14555359\n",
      "Iteration 08520: loss = 0.0018661906,0.1436017\n",
      "Iteration 08525: loss = 0.0018677519,0.14169912\n",
      "Iteration 08530: loss = 0.0018692861,0.13984554\n",
      "Iteration 08535: loss = 0.0018707955,0.13804136\n",
      "Iteration 08540: loss = 0.0018722736,0.13627991\n",
      "Iteration 08545: loss = 0.0018737329,0.13456307\n",
      "Iteration 08550: loss = 0.0018751615,0.13289128\n",
      "Iteration 08555: loss = 0.0018765595,0.13125904\n",
      "Iteration 08560: loss = 0.0018779379,0.12966718\n",
      "Iteration 08565: loss = 0.0018792971,0.12811495\n",
      "Iteration 08570: loss = 0.0018806244,0.12659986\n",
      "Iteration 08575: loss = 0.0018819297,0.12512138\n",
      "Iteration 08580: loss = 0.0018832088,0.12367971\n",
      "Iteration 08585: loss = 0.001884466,0.12227343\n",
      "Iteration 08590: loss = 0.0018857014,0.12089835\n",
      "Iteration 08595: loss = 0.0018869145,0.119555734\n",
      "Iteration 08600: loss = 0.0018880948,0.118246526\n",
      "Iteration 08605: loss = 0.0018892592,0.1169675\n",
      "Iteration 08610: loss = 0.0018904019,0.115717895\n",
      "Iteration 08615: loss = 0.0018915292,0.11449695\n",
      "Iteration 08620: loss = 0.001892622,0.11330438\n",
      "Iteration 08625: loss = 0.0018936992,0.11213925\n",
      "Iteration 08630: loss = 0.0018947503,0.111001275\n",
      "Iteration 08635: loss = 0.0018957761,0.10988905\n",
      "Iteration 08640: loss = 0.0018967901,0.10880096\n",
      "Iteration 08645: loss = 0.0018977746,0.10773744\n",
      "Iteration 08650: loss = 0.0018987447,0.10669829\n",
      "Iteration 08655: loss = 0.0018996997,0.10568081\n",
      "Iteration 08660: loss = 0.0019006192,0.10468768\n",
      "Iteration 08665: loss = 0.0019015315,0.10371466\n",
      "Iteration 08670: loss = 0.0019024207,0.10276347\n",
      "Iteration 08675: loss = 0.0019032914,0.10183288\n",
      "Iteration 08680: loss = 0.0019041384,0.100923926\n",
      "Iteration 08685: loss = 0.001904962,0.10003262\n",
      "Iteration 08690: loss = 0.0019057769,0.09916068\n",
      "Iteration 08695: loss = 0.0019065655,0.09830808\n",
      "Iteration 08700: loss = 0.0019073398,0.09747159\n",
      "Iteration 08705: loss = 0.0019080881,0.09665521\n",
      "Iteration 08710: loss = 0.0019088265,0.09585412\n",
      "Iteration 08715: loss = 0.0019095391,0.09507267\n",
      "Iteration 08720: loss = 0.0019102432,0.094302095\n",
      "Iteration 08725: loss = 0.0019109143,0.09355417\n",
      "Iteration 08730: loss = 0.0019115978,0.0928145\n",
      "Iteration 08735: loss = 0.0019121924,0.0921084\n",
      "Iteration 08740: loss = 0.0019129174,0.09141091\n",
      "Iteration 08745: loss = 0.0019130312,0.091113165\n",
      "Iteration 08750: loss = 0.0019138457,0.093293786\n",
      "Iteration 08755: loss = 0.0018010219,0.33003685\n",
      "Iteration 08760: loss = 0.0007946624,1.9392024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 08765: loss = 0.0003614401,8.910852\n",
      "Iteration 08770: loss = 0.0002497152,13.939032\n",
      "Iteration 08775: loss = 0.00021616695,14.027046\n",
      "Iteration 08780: loss = 0.00022736563,10.908685\n",
      "Iteration 08785: loss = 0.00025284852,8.311399\n",
      "Iteration 08790: loss = 0.00029874302,5.9680786\n",
      "Iteration 08795: loss = 0.0003550171,4.2134643\n",
      "Iteration 08800: loss = 0.00040267626,3.1304057\n",
      "Iteration 08805: loss = 0.00044049058,2.4714773\n",
      "Iteration 08810: loss = 0.0004680388,2.0768354\n",
      "Iteration 08815: loss = 0.0004857135,1.8427378\n",
      "Iteration 08820: loss = 0.00049832975,1.6780307\n",
      "Iteration 08825: loss = 0.0005086697,1.5441321\n",
      "Iteration 08830: loss = 0.00051653216,1.4395593\n",
      "Iteration 08835: loss = 0.00052234676,1.3591486\n",
      "Iteration 08840: loss = 0.0005277307,1.2898765\n",
      "Iteration 08845: loss = 0.00053263624,1.2289113\n",
      "Iteration 08850: loss = 0.0005369188,1.1756907\n",
      "Iteration 08855: loss = 0.0005414299,1.125689\n",
      "Iteration 08860: loss = 0.0005460491,1.0786945\n",
      "Iteration 08865: loss = 0.0005504387,1.0356021\n",
      "Iteration 08870: loss = 0.0005547782,0.9953946\n",
      "Iteration 08875: loss = 0.00055892573,0.9582948\n",
      "Iteration 08880: loss = 0.00056268735,0.9246852\n",
      "Iteration 08885: loss = 0.000566209,0.89373326\n",
      "Iteration 08890: loss = 0.0005695882,0.8648556\n",
      "Iteration 08895: loss = 0.00057279895,0.83794695\n",
      "Iteration 08900: loss = 0.00057591364,0.8126292\n",
      "Iteration 08905: loss = 0.00057898235,0.7886012\n",
      "Iteration 08910: loss = 0.0005819755,0.76587456\n",
      "Iteration 08915: loss = 0.0005848773,0.7443944\n",
      "Iteration 08920: loss = 0.00058770104,0.72402734\n",
      "Iteration 08925: loss = 0.00059043994,0.70473397\n",
      "Iteration 08930: loss = 0.00059309165,0.68644035\n",
      "Iteration 08935: loss = 0.0005956723,0.66903687\n",
      "Iteration 08940: loss = 0.0005981955,0.65243673\n",
      "Iteration 08945: loss = 0.0006006533,0.63660896\n",
      "Iteration 08950: loss = 0.00060305855,0.6214818\n",
      "Iteration 08955: loss = 0.00060541235,0.6070047\n",
      "Iteration 08960: loss = 0.0006077198,0.5931303\n",
      "Iteration 08965: loss = 0.00060998724,0.5798164\n",
      "Iteration 08970: loss = 0.00061221514,0.5670278\n",
      "Iteration 08975: loss = 0.0006144095,0.55472696\n",
      "Iteration 08980: loss = 0.0006165711,0.54287845\n",
      "Iteration 08985: loss = 0.00061870756,0.5314572\n",
      "Iteration 08990: loss = 0.00062081317,0.5204484\n",
      "Iteration 08995: loss = 0.0006228932,0.5098183\n",
      "Iteration 09000: loss = 0.0006249483,0.49955013\n",
      "Iteration 09005: loss = 0.0006269787,0.4896279\n",
      "Iteration 09010: loss = 0.0006289895,0.4800311\n",
      "Iteration 09015: loss = 0.00063097756,0.47074154\n",
      "Iteration 09020: loss = 0.0006329477,0.46174642\n",
      "Iteration 09025: loss = 0.0006348979,0.45303217\n",
      "Iteration 09030: loss = 0.0006368297,0.44458324\n",
      "Iteration 09035: loss = 0.0006387494,0.43638778\n",
      "Iteration 09040: loss = 0.00064064754,0.42843297\n",
      "Iteration 09045: loss = 0.00064253184,0.42071474\n",
      "Iteration 09050: loss = 0.00064439926,0.41321838\n",
      "Iteration 09055: loss = 0.0006462533,0.40593415\n",
      "Iteration 09060: loss = 0.0006480917,0.39885405\n",
      "Iteration 09065: loss = 0.00064992084,0.39196903\n",
      "Iteration 09070: loss = 0.0006517341,0.38527372\n",
      "Iteration 09075: loss = 0.0006535291,0.37876117\n",
      "Iteration 09080: loss = 0.000655319,0.37241933\n",
      "Iteration 09085: loss = 0.0006570934,0.36624798\n",
      "Iteration 09090: loss = 0.0006588557,0.36023405\n",
      "Iteration 09095: loss = 0.000660606,0.35437828\n",
      "Iteration 09100: loss = 0.00066234387,0.34867465\n",
      "Iteration 09105: loss = 0.00066407025,0.34311372\n",
      "Iteration 09110: loss = 0.00066578295,0.33769307\n",
      "Iteration 09115: loss = 0.0006674884,0.33240578\n",
      "Iteration 09120: loss = 0.0006691794,0.32724884\n",
      "Iteration 09125: loss = 0.00067086244,0.32221743\n",
      "Iteration 09130: loss = 0.0006725346,0.31730762\n",
      "Iteration 09135: loss = 0.00067419326,0.3125158\n",
      "Iteration 09140: loss = 0.00067584356,0.3078381\n",
      "Iteration 09145: loss = 0.0006774846,0.30326757\n",
      "Iteration 09150: loss = 0.00067911315,0.29880682\n",
      "Iteration 09155: loss = 0.000680736,0.29444435\n",
      "Iteration 09160: loss = 0.0006823444,0.29018882\n",
      "Iteration 09165: loss = 0.00068394584,0.2860246\n",
      "Iteration 09170: loss = 0.00068553584,0.28195316\n",
      "Iteration 09175: loss = 0.0006871166,0.27797684\n",
      "Iteration 09180: loss = 0.0006886891,0.2740866\n",
      "Iteration 09185: loss = 0.0006902504,0.27028263\n",
      "Iteration 09190: loss = 0.00069180486,0.26656103\n",
      "Iteration 09195: loss = 0.00069334824,0.26292044\n",
      "Iteration 09200: loss = 0.00069488323,0.2593559\n",
      "Iteration 09205: loss = 0.00069640577,0.25587082\n",
      "Iteration 09210: loss = 0.0006979227,0.252457\n",
      "Iteration 09215: loss = 0.00069942884,0.24911566\n",
      "Iteration 09220: loss = 0.00070092926,0.24584314\n",
      "Iteration 09225: loss = 0.00070241856,0.2426384\n",
      "Iteration 09230: loss = 0.0007038976,0.23950115\n",
      "Iteration 09235: loss = 0.00070537213,0.2364255\n",
      "Iteration 09240: loss = 0.0007068348,0.23341349\n",
      "Iteration 09245: loss = 0.00070828805,0.23046269\n",
      "Iteration 09250: loss = 0.00070973556,0.22756816\n",
      "Iteration 09255: loss = 0.0007111749,0.22473289\n",
      "Iteration 09260: loss = 0.0007126033,0.2219546\n",
      "Iteration 09265: loss = 0.00071402505,0.21922678\n",
      "Iteration 09270: loss = 0.0007154389,0.21655263\n",
      "Iteration 09275: loss = 0.0007168445,0.2139321\n",
      "Iteration 09280: loss = 0.00071824115,0.2113611\n",
      "Iteration 09285: loss = 0.0007196299,0.20883952\n",
      "Iteration 09290: loss = 0.0007210116,0.20636338\n",
      "Iteration 09295: loss = 0.0007223837,0.20393434\n",
      "Iteration 09300: loss = 0.0007237464,0.2015514\n",
      "Iteration 09305: loss = 0.00072510354,0.19921257\n",
      "Iteration 09310: loss = 0.00072645344,0.19691503\n",
      "Iteration 09315: loss = 0.0007277955,0.19466095\n",
      "Iteration 09320: loss = 0.0007291315,0.19244458\n",
      "Iteration 09325: loss = 0.00073044957,0.19027938\n",
      "Iteration 09330: loss = 0.0007317868,0.18813886\n",
      "Iteration 09335: loss = 0.00073303026,0.1861973\n",
      "Iteration 09340: loss = 0.00073447527,0.18521404\n",
      "Iteration 09345: loss = 0.0007347155,0.19917507\n",
      "Iteration 09350: loss = 0.00071923697,0.3851352\n",
      "Iteration 09355: loss = 0.00048736887,0.6187506\n",
      "Iteration 09360: loss = 0.00033852225,1.1969321\n",
      "Iteration 09365: loss = 0.00027689978,1.664748\n",
      "Iteration 09370: loss = 0.0002486096,1.9779502\n",
      "Iteration 09375: loss = 0.00023431324,2.1275306\n",
      "Iteration 09380: loss = 0.00022769552,2.1449242\n",
      "Iteration 09385: loss = 0.00022636843,2.0862627\n",
      "Iteration 09390: loss = 0.00022874954,1.9879334\n",
      "Iteration 09395: loss = 0.00023365486,1.8660867\n",
      "Iteration 09400: loss = 0.00023991521,1.7370641\n",
      "Iteration 09405: loss = 0.00024652053,1.6149299\n",
      "Iteration 09410: loss = 0.00025291237,1.5059456\n",
      "Iteration 09415: loss = 0.0002590674,1.4092047\n",
      "Iteration 09420: loss = 0.00026517405,1.3215275\n",
      "Iteration 09425: loss = 0.00027126298,1.241538\n",
      "Iteration 09430: loss = 0.000277205,1.1692855\n",
      "Iteration 09435: loss = 0.0002828767,1.1046615\n",
      "Iteration 09440: loss = 0.00028826832,1.0467534\n",
      "Iteration 09445: loss = 0.00029343177,0.9943851\n",
      "Iteration 09450: loss = 0.00029841837,0.94662696\n",
      "Iteration 09455: loss = 0.00030325985,0.9027768\n",
      "Iteration 09460: loss = 0.0003079596,0.8624261\n",
      "Iteration 09465: loss = 0.00031249222,0.82531965\n",
      "Iteration 09470: loss = 0.00031685457,0.7911838\n",
      "Iteration 09475: loss = 0.00032106548,0.7596162\n",
      "Iteration 09480: loss = 0.00032516022,0.730236\n",
      "Iteration 09485: loss = 0.0003291625,0.7027578\n",
      "Iteration 09490: loss = 0.0003330782,0.6769991\n",
      "Iteration 09495: loss = 0.00033689884,0.6528399\n",
      "Iteration 09500: loss = 0.00034062876,0.63015896\n",
      "Iteration 09505: loss = 0.00034427087,0.6088198\n",
      "Iteration 09510: loss = 0.00034783478,0.5886973\n",
      "Iteration 09515: loss = 0.0003513289,0.56967723\n",
      "Iteration 09520: loss = 0.0003547571,0.5516719\n",
      "Iteration 09525: loss = 0.00035812127,0.534602\n",
      "Iteration 09530: loss = 0.00036141954,0.51840603\n",
      "Iteration 09535: loss = 0.00036466017,0.5030178\n",
      "Iteration 09540: loss = 0.00036784264,0.48838088\n",
      "Iteration 09545: loss = 0.00037097232,0.47443515\n",
      "Iteration 09550: loss = 0.00037405093,0.46114358\n",
      "Iteration 09555: loss = 0.00037707775,0.4484561\n",
      "Iteration 09560: loss = 0.00038005598,0.4363378\n",
      "Iteration 09565: loss = 0.0003829881,0.4247526\n",
      "Iteration 09570: loss = 0.0003858738,0.41366503\n",
      "Iteration 09575: loss = 0.00038871847,0.4030449\n",
      "Iteration 09580: loss = 0.00039152056,0.39286676\n",
      "Iteration 09585: loss = 0.00039428056,0.38310426\n",
      "Iteration 09590: loss = 0.00039700323,0.37373582\n",
      "Iteration 09595: loss = 0.00039968546,0.36473972\n",
      "Iteration 09600: loss = 0.00040233054,0.3561505\n",
      "Iteration 09605: loss = 0.00040493463,0.34864032\n",
      "Iteration 09610: loss = 0.00040740645,0.3535309\n",
      "Iteration 09615: loss = 0.00040935414,0.5273107\n",
      "Iteration 09620: loss = 0.0003785837,0.49112746\n",
      "Iteration 09625: loss = 0.0003447741,0.5873053\n",
      "Iteration 09630: loss = 0.00032811938,0.5990055\n",
      "Iteration 09635: loss = 0.00031984187,0.5946335\n",
      "Iteration 09640: loss = 0.0003154952,0.58794665\n",
      "Iteration 09645: loss = 0.0003135873,0.5800291\n",
      "Iteration 09650: loss = 0.00031376115,0.56898034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 09655: loss = 0.00031553433,0.5545509\n",
      "Iteration 09660: loss = 0.000318168,0.53850096\n",
      "Iteration 09665: loss = 0.0003210096,0.5227049\n",
      "Iteration 09670: loss = 0.00032382066,0.5077284\n",
      "Iteration 09675: loss = 0.000326684,0.4931923\n",
      "Iteration 09680: loss = 0.00032969072,0.47874424\n",
      "Iteration 09685: loss = 0.00033280006,0.46452093\n",
      "Iteration 09690: loss = 0.00033588402,0.45088315\n",
      "Iteration 09695: loss = 0.00033886908,0.4380125\n",
      "Iteration 09700: loss = 0.00034177574,0.42582\n",
      "Iteration 09705: loss = 0.0003446637,0.41410944\n",
      "Iteration 09710: loss = 0.00034755087,0.40281707\n",
      "Iteration 09715: loss = 0.0003504072,0.39199612\n",
      "Iteration 09720: loss = 0.00035320615,0.38168246\n",
      "Iteration 09725: loss = 0.0003559433,0.37185484\n",
      "Iteration 09730: loss = 0.00035863442,0.36244345\n",
      "Iteration 09735: loss = 0.0003612948,0.35339555\n",
      "Iteration 09740: loss = 0.000363924,0.34470066\n",
      "Iteration 09745: loss = 0.00036650978,0.33635852\n",
      "Iteration 09750: loss = 0.00036905546,0.32835054\n",
      "Iteration 09755: loss = 0.00037156485,0.32065344\n",
      "Iteration 09760: loss = 0.00037404275,0.31324193\n",
      "Iteration 09765: loss = 0.00037648543,0.30610573\n",
      "Iteration 09770: loss = 0.00037889482,0.2992332\n",
      "Iteration 09775: loss = 0.00038127098,0.29261473\n",
      "Iteration 09780: loss = 0.00038361736,0.28622866\n",
      "Iteration 09785: loss = 0.0003859316,0.28006756\n",
      "Iteration 09790: loss = 0.0003882166,0.27412057\n",
      "Iteration 09795: loss = 0.00039047538,0.26837316\n",
      "Iteration 09800: loss = 0.00039270366,0.26282167\n",
      "Iteration 09805: loss = 0.00039490312,0.25745454\n",
      "Iteration 09810: loss = 0.00039707834,0.25226182\n",
      "Iteration 09815: loss = 0.00039922772,0.24723664\n",
      "Iteration 09820: loss = 0.00040135035,0.24237253\n",
      "Iteration 09825: loss = 0.00040344917,0.23766123\n",
      "Iteration 09830: loss = 0.00040552506,0.23309599\n",
      "Iteration 09835: loss = 0.0004075767,0.22867146\n",
      "Iteration 09840: loss = 0.00040960324,0.2243794\n",
      "Iteration 09845: loss = 0.0004116081,0.22021444\n",
      "Iteration 09850: loss = 0.0004135891,0.21617767\n",
      "Iteration 09855: loss = 0.0004155543,0.21225324\n",
      "Iteration 09860: loss = 0.00041748618,0.2084571\n",
      "Iteration 09865: loss = 0.00041942555,0.20475504\n",
      "Iteration 09870: loss = 0.00042126141,0.20142606\n",
      "Iteration 09875: loss = 0.00042326152,0.19961177\n",
      "Iteration 09880: loss = 0.0004242761,0.21709524\n",
      "Iteration 09885: loss = 0.0004220244,0.39088553\n",
      "Iteration 09890: loss = 0.00034372867,0.3915572\n",
      "Iteration 09895: loss = 0.00028508983,0.58914673\n",
      "Iteration 09900: loss = 0.0002577465,0.66351175\n",
      "Iteration 09905: loss = 0.0002433282,0.6986194\n",
      "Iteration 09910: loss = 0.00023509756,0.7172144\n",
      "Iteration 09915: loss = 0.0002316677,0.718396\n",
      "Iteration 09920: loss = 0.00023202896,0.7026144\n",
      "Iteration 09925: loss = 0.00023431546,0.67928565\n",
      "Iteration 09930: loss = 0.00023697526,0.6564153\n",
      "Iteration 09935: loss = 0.00023966325,0.6349931\n",
      "Iteration 09940: loss = 0.00024272868,0.6126532\n",
      "Iteration 09945: loss = 0.00024623534,0.5888833\n",
      "Iteration 09950: loss = 0.00024982454,0.5655777\n",
      "Iteration 09955: loss = 0.00025320277,0.54428387\n",
      "Iteration 09960: loss = 0.00025643874,0.52476573\n",
      "Iteration 09965: loss = 0.00025972104,0.50605905\n",
      "Iteration 09970: loss = 0.00026307392,0.487864\n",
      "Iteration 09975: loss = 0.0002663841,0.47058725\n",
      "Iteration 09980: loss = 0.0002695812,0.45450628\n",
      "Iteration 09985: loss = 0.00027269765,0.4393828\n",
      "Iteration 09990: loss = 0.00027578173,0.42497444\n",
      "Iteration 09995: loss = 0.00027882794,0.41127738\n",
      "Iteration 10000: loss = 0.0002818132,0.39831114\n",
      "Iteration 10005: loss = 0.00028473456,0.38602906\n",
      "Iteration 10010: loss = 0.00028760612,0.37435874\n",
      "Iteration 10015: loss = 0.000290432,0.36324322\n",
      "Iteration 10020: loss = 0.00029321256,0.3526565\n",
      "Iteration 10025: loss = 0.00029594149,0.34257716\n",
      "Iteration 10030: loss = 0.0002986245,0.33296895\n",
      "Iteration 10035: loss = 0.00030126024,0.3238012\n",
      "Iteration 10040: loss = 0.00030386148,0.3150266\n",
      "Iteration 10045: loss = 0.00030642064,0.3066307\n",
      "Iteration 10050: loss = 0.0003089357,0.29860893\n",
      "Iteration 10055: loss = 0.00031141753,0.29091385\n",
      "Iteration 10060: loss = 0.000313862,0.28353897\n",
      "Iteration 10065: loss = 0.0003162731,0.2764591\n",
      "Iteration 10070: loss = 0.0003186495,0.26966348\n",
      "Iteration 10075: loss = 0.00032098984,0.26313937\n",
      "Iteration 10080: loss = 0.0003233011,0.25685844\n",
      "Iteration 10085: loss = 0.0003255756,0.25082964\n",
      "Iteration 10090: loss = 0.0003278358,0.24500299\n",
      "Iteration 10095: loss = 0.00033001797,0.23951936\n",
      "Iteration 10100: loss = 0.00033231042,0.23449159\n",
      "Iteration 10105: loss = 0.00033399943,0.23703016\n",
      "Iteration 10110: loss = 0.00033699116,0.3211446\n",
      "Iteration 10115: loss = 0.00031588136,0.540659\n",
      "Iteration 10120: loss = 0.00027242422,0.48507893\n",
      "Iteration 10125: loss = 0.0002512713,0.46982273\n",
      "Iteration 10130: loss = 0.00023986347,0.48271614\n",
      "Iteration 10135: loss = 0.00023312769,0.49699837\n",
      "Iteration 10140: loss = 0.00023025887,0.50131994\n",
      "Iteration 10145: loss = 0.0002305067,0.4941579\n",
      "Iteration 10150: loss = 0.00023214544,0.48211527\n",
      "Iteration 10155: loss = 0.00023404375,0.46972656\n",
      "Iteration 10160: loss = 0.00023624033,0.45670345\n",
      "Iteration 10165: loss = 0.0002389701,0.44208947\n",
      "Iteration 10170: loss = 0.00024195411,0.42699188\n",
      "Iteration 10175: loss = 0.00024482523,0.4127604\n",
      "Iteration 10180: loss = 0.0002475862,0.39936858\n",
      "Iteration 10185: loss = 0.00025040196,0.38629383\n",
      "Iteration 10190: loss = 0.0002532698,0.37362742\n",
      "Iteration 10195: loss = 0.00025607413,0.36173853\n",
      "Iteration 10200: loss = 0.0002588002,0.3505778\n",
      "Iteration 10205: loss = 0.00026152006,0.33984923\n",
      "Iteration 10210: loss = 0.00026424098,0.32948965\n",
      "Iteration 10215: loss = 0.00026691804,0.31963447\n",
      "Iteration 10220: loss = 0.00026952795,0.31032276\n",
      "Iteration 10225: loss = 0.00027208935,0.30145693\n",
      "Iteration 10230: loss = 0.0002746181,0.29297462\n",
      "Iteration 10235: loss = 0.00027710668,0.2848815\n",
      "Iteration 10240: loss = 0.00027955006,0.27715567\n",
      "Iteration 10245: loss = 0.000281959,0.26976013\n",
      "Iteration 10250: loss = 0.00028433316,0.26267418\n",
      "Iteration 10255: loss = 0.00028667224,0.25589216\n",
      "Iteration 10260: loss = 0.00028897062,0.24939995\n",
      "Iteration 10265: loss = 0.0002912396,0.24316312\n",
      "Iteration 10270: loss = 0.00029348076,0.23717019\n",
      "Iteration 10275: loss = 0.00029568918,0.23141539\n",
      "Iteration 10280: loss = 0.00029786534,0.22588286\n",
      "Iteration 10285: loss = 0.00030001378,0.22056065\n",
      "Iteration 10290: loss = 0.0003021372,0.21543409\n",
      "Iteration 10295: loss = 0.00030423264,0.21049544\n",
      "Iteration 10300: loss = 0.00030630195,0.205738\n",
      "Iteration 10305: loss = 0.00030834056,0.20115164\n",
      "Iteration 10310: loss = 0.0003103603,0.1967216\n",
      "Iteration 10315: loss = 0.0003123529,0.19244929\n",
      "Iteration 10320: loss = 0.0003143252,0.18831712\n",
      "Iteration 10325: loss = 0.00031627386,0.18432623\n",
      "Iteration 10330: loss = 0.00031819436,0.18047613\n",
      "Iteration 10335: loss = 0.0003201056,0.17673571\n",
      "Iteration 10340: loss = 0.0003219681,0.17315723\n",
      "Iteration 10345: loss = 0.00032387333,0.1696404\n",
      "Iteration 10350: loss = 0.00032557154,0.16678718\n",
      "Iteration 10355: loss = 0.00032774333,0.1668023\n",
      "Iteration 10360: loss = 0.00032757342,0.21005352\n",
      "Iteration 10365: loss = 0.00031630832,0.49248302\n",
      "Iteration 10370: loss = 0.00024160375,0.3447705\n",
      "Iteration 10375: loss = 0.00020351248,0.48055142\n",
      "Iteration 10380: loss = 0.0001841736,0.5867136\n",
      "Iteration 10385: loss = 0.00017319575,0.64965224\n",
      "Iteration 10390: loss = 0.00016855862,0.67199767\n",
      "Iteration 10395: loss = 0.00016825773,0.6637742\n",
      "Iteration 10400: loss = 0.0001694358,0.6456723\n",
      "Iteration 10405: loss = 0.00017107923,0.62514037\n",
      "Iteration 10410: loss = 0.00017361109,0.6000219\n",
      "Iteration 10415: loss = 0.00017686437,0.5720983\n",
      "Iteration 10420: loss = 0.0001801089,0.5462527\n",
      "Iteration 10425: loss = 0.00018320708,0.5230725\n",
      "Iteration 10430: loss = 0.0001864402,0.5006954\n",
      "Iteration 10435: loss = 0.00018976394,0.47908902\n",
      "Iteration 10440: loss = 0.00019294074,0.4591883\n",
      "Iteration 10445: loss = 0.00019599994,0.44076782\n",
      "Iteration 10450: loss = 0.00019907371,0.42322224\n",
      "Iteration 10455: loss = 0.000202137,0.40662807\n",
      "Iteration 10460: loss = 0.00020511694,0.39114755\n",
      "Iteration 10465: loss = 0.00020803818,0.3765834\n",
      "Iteration 10470: loss = 0.00021093203,0.3627859\n",
      "Iteration 10475: loss = 0.00021376803,0.349817\n",
      "Iteration 10480: loss = 0.00021651818,0.33768007\n",
      "Iteration 10485: loss = 0.00021921698,0.32622686\n",
      "Iteration 10490: loss = 0.00022187993,0.31535494\n",
      "Iteration 10495: loss = 0.00022449669,0.30505693\n",
      "Iteration 10500: loss = 0.0002270561,0.29533038\n",
      "Iteration 10505: loss = 0.0002295712,0.28610268\n",
      "Iteration 10510: loss = 0.00023205097,0.277319\n",
      "Iteration 10515: loss = 0.00023448894,0.2689694\n",
      "Iteration 10520: loss = 0.00023688634,0.26102138\n",
      "Iteration 10525: loss = 0.00023924722,0.253446\n",
      "Iteration 10530: loss = 0.00024157246,0.24621792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10535: loss = 0.00024386513,0.2393116\n",
      "Iteration 10540: loss = 0.0002461224,0.2327208\n",
      "Iteration 10545: loss = 0.00024834744,0.22641282\n",
      "Iteration 10550: loss = 0.0002505425,0.22037645\n",
      "Iteration 10555: loss = 0.00025270847,0.21458885\n",
      "Iteration 10560: loss = 0.00025483684,0.20905899\n",
      "Iteration 10565: loss = 0.00025695175,0.20372698\n",
      "Iteration 10570: loss = 0.0002590177,0.19865099\n",
      "Iteration 10575: loss = 0.00026109233,0.19371659\n",
      "Iteration 10580: loss = 0.00026305552,0.18917589\n",
      "Iteration 10585: loss = 0.0002652146,0.18489374\n",
      "Iteration 10590: loss = 0.00026656967,0.18695\n",
      "Iteration 10595: loss = 0.0002697951,0.23985918\n",
      "Iteration 10600: loss = 0.0002591861,0.46409175\n",
      "Iteration 10605: loss = 0.00022776863,0.29044974\n",
      "Iteration 10610: loss = 0.00021192302,0.30398083\n",
      "Iteration 10615: loss = 0.00020247087,0.34627387\n",
      "Iteration 10620: loss = 0.00019700445,0.36858526\n",
      "Iteration 10625: loss = 0.00019521767,0.3699278\n",
      "Iteration 10630: loss = 0.00019530114,0.361345\n",
      "Iteration 10635: loss = 0.00019592856,0.35093364\n",
      "Iteration 10640: loss = 0.00019730929,0.3396664\n",
      "Iteration 10645: loss = 0.0001993749,0.3282826\n",
      "Iteration 10650: loss = 0.0002015384,0.31836674\n",
      "Iteration 10655: loss = 0.00020372834,0.30913243\n",
      "Iteration 10660: loss = 0.00020614704,0.2992096\n",
      "Iteration 10665: loss = 0.0002086652,0.2890562\n",
      "Iteration 10670: loss = 0.00021112063,0.2795357\n",
      "Iteration 10675: loss = 0.0002135727,0.2705989\n",
      "Iteration 10680: loss = 0.00021602958,0.26203188\n",
      "Iteration 10685: loss = 0.00021840697,0.2539271\n",
      "Iteration 10690: loss = 0.00022071689,0.24630298\n",
      "Iteration 10695: loss = 0.00022302395,0.23899709\n",
      "Iteration 10700: loss = 0.00022532312,0.23195851\n",
      "Iteration 10705: loss = 0.00022758299,0.22525696\n",
      "Iteration 10710: loss = 0.00022980233,0.21889102\n",
      "Iteration 10715: loss = 0.000231981,0.21282221\n",
      "Iteration 10720: loss = 0.00023411853,0.20704697\n",
      "Iteration 10725: loss = 0.00023623335,0.20150413\n",
      "Iteration 10730: loss = 0.00023832552,0.19618991\n",
      "Iteration 10735: loss = 0.00024038118,0.19111216\n",
      "Iteration 10740: loss = 0.00024240384,0.18625487\n",
      "Iteration 10745: loss = 0.00024440893,0.18157963\n",
      "Iteration 10750: loss = 0.00024638555,0.1770959\n",
      "Iteration 10755: loss = 0.0002483329,0.17279786\n",
      "Iteration 10760: loss = 0.0002502613,0.16865616\n",
      "Iteration 10765: loss = 0.0002521618,0.16467926\n",
      "Iteration 10770: loss = 0.00025404102,0.16085291\n",
      "Iteration 10775: loss = 0.0002558989,0.15716624\n",
      "Iteration 10780: loss = 0.00025773057,0.15361951\n",
      "Iteration 10785: loss = 0.00025954566,0.15019612\n",
      "Iteration 10790: loss = 0.00026133336,0.14690624\n",
      "Iteration 10795: loss = 0.0002631138,0.1437135\n",
      "Iteration 10800: loss = 0.00026484497,0.1406766\n",
      "Iteration 10805: loss = 0.0002666239,0.13767372\n",
      "Iteration 10810: loss = 0.00026819823,0.1351739\n",
      "Iteration 10815: loss = 0.00027023113,0.13375323\n",
      "Iteration 10820: loss = 0.00027031897,0.15257731\n",
      "Iteration 10825: loss = 0.0002714733,0.3264856\n",
      "Iteration 10830: loss = 0.00021030808,0.3196774\n",
      "Iteration 10835: loss = 0.00016951341,0.49174336\n",
      "Iteration 10840: loss = 0.00014923856,0.551206\n",
      "Iteration 10845: loss = 0.0001381052,0.58481526\n",
      "Iteration 10850: loss = 0.00013387922,0.5914145\n",
      "Iteration 10855: loss = 0.00013321421,0.581657\n",
      "Iteration 10860: loss = 0.00013359392,0.5695097\n",
      "Iteration 10865: loss = 0.0001352467,0.5511171\n",
      "Iteration 10870: loss = 0.00013788299,0.52765715\n",
      "Iteration 10875: loss = 0.00014051703,0.5053857\n",
      "Iteration 10880: loss = 0.00014323302,0.4832504\n",
      "Iteration 10885: loss = 0.00014624473,0.46010184\n",
      "Iteration 10890: loss = 0.00014919198,0.43859318\n",
      "Iteration 10895: loss = 0.0001520439,0.41907176\n",
      "Iteration 10900: loss = 0.0001549959,0.40032685\n",
      "Iteration 10905: loss = 0.00015794265,0.3826274\n",
      "Iteration 10910: loss = 0.00016080313,0.3661799\n",
      "Iteration 10915: loss = 0.00016365963,0.3506275\n",
      "Iteration 10920: loss = 0.00016647279,0.33611855\n",
      "Iteration 10925: loss = 0.00016917614,0.322748\n",
      "Iteration 10930: loss = 0.0001718223,0.31023878\n",
      "Iteration 10935: loss = 0.00017443323,0.2984618\n",
      "Iteration 10940: loss = 0.00017699176,0.28740367\n",
      "Iteration 10945: loss = 0.00017950738,0.2769896\n",
      "Iteration 10950: loss = 0.00018197957,0.26717737\n",
      "Iteration 10955: loss = 0.00018439512,0.2579617\n",
      "Iteration 10960: loss = 0.0001867714,0.24924664\n",
      "Iteration 10965: loss = 0.00018912154,0.24096966\n",
      "Iteration 10970: loss = 0.00019142922,0.23314099\n",
      "Iteration 10975: loss = 0.00019369596,0.22573246\n",
      "Iteration 10980: loss = 0.0001959331,0.21868682\n",
      "Iteration 10985: loss = 0.00019813777,0.21199024\n",
      "Iteration 10990: loss = 0.00020030695,0.20562601\n",
      "Iteration 10995: loss = 0.00020244316,0.19957428\n",
      "Iteration 11000: loss = 0.0002045526,0.19379696\n",
      "Iteration 11005: loss = 0.00020662759,0.1883001\n",
      "Iteration 11010: loss = 0.00020867803,0.18304825\n",
      "Iteration 11015: loss = 0.00021069433,0.17803952\n",
      "Iteration 11020: loss = 0.00021269121,0.1732434\n",
      "Iteration 11025: loss = 0.00021464475,0.16868551\n",
      "Iteration 11030: loss = 0.00021660606,0.1642667\n",
      "Iteration 11035: loss = 0.00021845817,0.16021992\n",
      "Iteration 11040: loss = 0.0002205023,0.15626752\n",
      "Iteration 11045: loss = 0.00022178366,0.15695515\n",
      "Iteration 11050: loss = 0.00022497626,0.19274552\n",
      "Iteration 11055: loss = 0.0002192743,0.4325103\n",
      "Iteration 11060: loss = 0.00019087414,0.21789007\n",
      "Iteration 11065: loss = 0.00017574558,0.27846196\n",
      "Iteration 11070: loss = 0.00016600163,0.3241789\n",
      "Iteration 11075: loss = 0.00016076413,0.33751577\n",
      "Iteration 11080: loss = 0.00015904357,0.3327983\n",
      "Iteration 11085: loss = 0.00015848044,0.32426202\n",
      "Iteration 11090: loss = 0.00015895483,0.3139766\n",
      "Iteration 11095: loss = 0.00016043938,0.30236873\n",
      "Iteration 11100: loss = 0.0001620846,0.29249623\n",
      "Iteration 11105: loss = 0.00016396027,0.2834371\n",
      "Iteration 11110: loss = 0.00016616647,0.2740805\n",
      "Iteration 11115: loss = 0.00016837452,0.26501924\n",
      "Iteration 11120: loss = 0.00017064175,0.25585136\n",
      "Iteration 11125: loss = 0.00017302303,0.24659637\n",
      "Iteration 11130: loss = 0.0001753577,0.23799567\n",
      "Iteration 11135: loss = 0.00017765282,0.23000033\n",
      "Iteration 11140: loss = 0.0001799313,0.22237837\n",
      "Iteration 11145: loss = 0.00018212933,0.21523151\n",
      "Iteration 11150: loss = 0.00018428173,0.20849696\n",
      "Iteration 11155: loss = 0.00018642498,0.20206606\n",
      "Iteration 11160: loss = 0.00018854106,0.1959357\n",
      "Iteration 11165: loss = 0.00019063546,0.19008285\n",
      "Iteration 11170: loss = 0.00019269883,0.18452361\n",
      "Iteration 11175: loss = 0.00019471411,0.17926691\n",
      "Iteration 11180: loss = 0.00019669648,0.17426443\n",
      "Iteration 11185: loss = 0.00019866262,0.16947003\n",
      "Iteration 11190: loss = 0.00020060712,0.16488267\n",
      "Iteration 11195: loss = 0.00020252114,0.16050693\n",
      "Iteration 11200: loss = 0.00020440428,0.15633473\n",
      "Iteration 11205: loss = 0.00020626344,0.1523403\n",
      "Iteration 11210: loss = 0.00020810509,0.14850417\n",
      "Iteration 11215: loss = 0.00020991999,0.14483483\n",
      "Iteration 11220: loss = 0.00021170883,0.14132361\n",
      "Iteration 11225: loss = 0.0002134804,0.13794705\n",
      "Iteration 11230: loss = 0.0002152241,0.13471502\n",
      "Iteration 11235: loss = 0.00021695053,0.13160586\n",
      "Iteration 11240: loss = 0.00021865603,0.12861848\n",
      "Iteration 11245: loss = 0.00022033369,0.12575863\n",
      "Iteration 11250: loss = 0.00022200438,0.12298784\n",
      "Iteration 11255: loss = 0.00022362982,0.12035908\n",
      "Iteration 11260: loss = 0.00022528355,0.117769554\n",
      "Iteration 11265: loss = 0.00022681548,0.11543332\n",
      "Iteration 11270: loss = 0.00022856345,0.11309277\n",
      "Iteration 11275: loss = 0.00022956263,0.11349438\n",
      "Iteration 11280: loss = 0.0002322538,0.12950677\n",
      "Iteration 11285: loss = 0.0002276473,0.27963987\n",
      "Iteration 11290: loss = 0.00018078601,0.29342303\n",
      "Iteration 11295: loss = 0.00014675564,0.42127785\n",
      "Iteration 11300: loss = 0.00012778839,0.44530463\n",
      "Iteration 11305: loss = 0.000118052034,0.4666134\n",
      "Iteration 11310: loss = 0.00011456847,0.47747484\n",
      "Iteration 11315: loss = 0.0001131541,0.48496866\n",
      "Iteration 11320: loss = 0.00011356745,0.4805026\n",
      "Iteration 11325: loss = 0.0001153858,0.4637884\n",
      "Iteration 11330: loss = 0.00011737796,0.44440153\n",
      "Iteration 11335: loss = 0.00011989533,0.42214555\n",
      "Iteration 11340: loss = 0.00012269571,0.40049747\n",
      "Iteration 11345: loss = 0.00012536441,0.3818685\n",
      "Iteration 11350: loss = 0.0001281454,0.36369392\n",
      "Iteration 11355: loss = 0.0001308782,0.34646606\n",
      "Iteration 11360: loss = 0.00013348932,0.3308656\n",
      "Iteration 11365: loss = 0.00013615747,0.31607565\n",
      "Iteration 11370: loss = 0.00013878761,0.3022181\n",
      "Iteration 11375: loss = 0.00014138185,0.28922477\n",
      "Iteration 11380: loss = 0.00014395255,0.27706775\n",
      "Iteration 11385: loss = 0.00014642351,0.2659036\n",
      "Iteration 11390: loss = 0.00014883593,0.25550762\n",
      "Iteration 11395: loss = 0.00015122617,0.24570584\n",
      "Iteration 11400: loss = 0.00015358026,0.23648329\n",
      "Iteration 11405: loss = 0.00015589771,0.22781529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11410: loss = 0.00015816225,0.21969971\n",
      "Iteration 11415: loss = 0.00016038987,0.21205436\n",
      "Iteration 11420: loss = 0.0001625985,0.20479865\n",
      "Iteration 11425: loss = 0.0001647614,0.19797432\n",
      "Iteration 11430: loss = 0.00016688614,0.19153635\n",
      "Iteration 11435: loss = 0.00016898631,0.18542083\n",
      "Iteration 11440: loss = 0.00017105234,0.17963675\n",
      "Iteration 11445: loss = 0.00017308087,0.17416519\n",
      "Iteration 11450: loss = 0.00017508445,0.16895965\n",
      "Iteration 11455: loss = 0.00017705037,0.16403362\n",
      "Iteration 11460: loss = 0.0001789952,0.15933661\n",
      "Iteration 11465: loss = 0.00018090096,0.15488869\n",
      "Iteration 11470: loss = 0.00018279436,0.15062465\n",
      "Iteration 11475: loss = 0.000184625,0.14663365\n",
      "Iteration 11480: loss = 0.00018651302,0.14270987\n",
      "Iteration 11485: loss = 0.00018812792,0.13968627\n",
      "Iteration 11490: loss = 0.00019039784,0.13880715\n",
      "Iteration 11495: loss = 0.0001898581,0.1784389\n",
      "Iteration 11500: loss = 0.00019041135,0.431964\n",
      "Iteration 11505: loss = 0.00015344712,0.21771276\n",
      "Iteration 11510: loss = 0.00013330641,0.31707603\n",
      "Iteration 11515: loss = 0.00012196166,0.3816278\n",
      "Iteration 11520: loss = 0.00011713017,0.39906037\n",
      "Iteration 11525: loss = 0.000115425704,0.39573544\n",
      "Iteration 11530: loss = 0.000115123876,0.38565603\n",
      "Iteration 11535: loss = 0.000116400675,0.36909822\n",
      "Iteration 11540: loss = 0.00011803714,0.35358468\n",
      "Iteration 11545: loss = 0.000120064746,0.33864298\n",
      "Iteration 11550: loss = 0.00012245493,0.32383844\n",
      "Iteration 11555: loss = 0.00012476553,0.31065434\n",
      "Iteration 11560: loss = 0.00012722252,0.29733145\n",
      "Iteration 11565: loss = 0.00012964765,0.28454304\n",
      "Iteration 11570: loss = 0.00013200162,0.27263883\n",
      "Iteration 11575: loss = 0.00013439562,0.26134127\n",
      "Iteration 11580: loss = 0.00013672558,0.25099018\n",
      "Iteration 11585: loss = 0.00013905989,0.24112199\n",
      "Iteration 11590: loss = 0.00014138916,0.23171486\n",
      "Iteration 11595: loss = 0.00014366362,0.22297011\n",
      "Iteration 11600: loss = 0.00014590757,0.21476005\n",
      "Iteration 11605: loss = 0.00014808508,0.20711863\n",
      "Iteration 11610: loss = 0.00015021896,0.1999492\n",
      "Iteration 11615: loss = 0.00015233368,0.19315825\n",
      "Iteration 11620: loss = 0.00015442136,0.18672656\n",
      "Iteration 11625: loss = 0.0001564836,0.18064116\n",
      "Iteration 11630: loss = 0.00015850147,0.17491773\n",
      "Iteration 11635: loss = 0.00016048494,0.16950841\n",
      "Iteration 11640: loss = 0.00016244497,0.16437101\n",
      "Iteration 11645: loss = 0.00016437432,0.15950182\n",
      "Iteration 11650: loss = 0.00016627215,0.15488759\n",
      "Iteration 11655: loss = 0.00016814005,0.15050891\n",
      "Iteration 11660: loss = 0.00016997741,0.14635111\n",
      "Iteration 11665: loss = 0.0001717946,0.14238635\n",
      "Iteration 11670: loss = 0.00017357513,0.13862763\n",
      "Iteration 11675: loss = 0.00017533514,0.13503797\n",
      "Iteration 11680: loss = 0.00017706736,0.13161929\n",
      "Iteration 11685: loss = 0.00017877364,0.12835968\n",
      "Iteration 11690: loss = 0.00018045671,0.12524673\n",
      "Iteration 11695: loss = 0.00018211307,0.12227871\n",
      "Iteration 11700: loss = 0.00018375363,0.11942771\n",
      "Iteration 11705: loss = 0.00018535194,0.11672944\n",
      "Iteration 11710: loss = 0.00018697068,0.11409075\n",
      "Iteration 11715: loss = 0.00018845867,0.11175578\n",
      "Iteration 11720: loss = 0.00019022198,0.109518096\n",
      "Iteration 11725: loss = 0.00019086094,0.113339074\n",
      "Iteration 11730: loss = 0.00019435317,0.16838169\n",
      "Iteration 11735: loss = 0.00016963143,0.4652485\n",
      "Iteration 11740: loss = 0.000118814474,0.33682877\n",
      "Iteration 11745: loss = 9.5274714e-05,0.4416569\n",
      "Iteration 11750: loss = 8.3010564e-05,0.556261\n",
      "Iteration 11755: loss = 7.8437406e-05,0.606692\n",
      "Iteration 11760: loss = 7.6399905e-05,0.6266068\n",
      "Iteration 11765: loss = 7.665916e-05,0.6168234\n",
      "Iteration 11770: loss = 7.811797e-05,0.5922516\n",
      "Iteration 11775: loss = 7.995075e-05,0.5642409\n",
      "Iteration 11780: loss = 8.251257e-05,0.5301726\n",
      "Iteration 11785: loss = 8.504759e-05,0.49897224\n",
      "Iteration 11790: loss = 8.782075e-05,0.4682962\n",
      "Iteration 11795: loss = 9.06421e-05,0.440005\n",
      "Iteration 11800: loss = 9.340338e-05,0.41464493\n",
      "Iteration 11805: loss = 9.624756e-05,0.39063588\n",
      "Iteration 11810: loss = 9.8991266e-05,0.36896935\n",
      "Iteration 11815: loss = 0.000101749865,0.34865496\n",
      "Iteration 11820: loss = 0.000104451174,0.33010393\n",
      "Iteration 11825: loss = 0.000107082895,0.31323853\n",
      "Iteration 11830: loss = 0.0001096722,0.29769355\n",
      "Iteration 11835: loss = 0.000112169706,0.28355512\n",
      "Iteration 11840: loss = 0.00011463308,0.27043873\n",
      "Iteration 11845: loss = 0.00011704765,0.25831717\n",
      "Iteration 11850: loss = 0.00011943101,0.24702066\n",
      "Iteration 11855: loss = 0.000121776946,0.23650934\n",
      "Iteration 11860: loss = 0.00012406647,0.22677901\n",
      "Iteration 11865: loss = 0.00012631327,0.21772212\n",
      "Iteration 11870: loss = 0.00012851406,0.20928818\n",
      "Iteration 11875: loss = 0.00013067834,0.20139158\n",
      "Iteration 11880: loss = 0.00013280108,0.19401738\n",
      "Iteration 11885: loss = 0.00013487913,0.18712477\n",
      "Iteration 11890: loss = 0.00013691974,0.18065953\n",
      "Iteration 11895: loss = 0.00013892278,0.17459004\n",
      "Iteration 11900: loss = 0.0001408895,0.1688842\n",
      "Iteration 11905: loss = 0.00014281836,0.16352111\n",
      "Iteration 11910: loss = 0.00014471415,0.15846364\n",
      "Iteration 11915: loss = 0.00014657456,0.15369329\n",
      "Iteration 11920: loss = 0.00014840317,0.14919055\n",
      "Iteration 11925: loss = 0.00015020098,0.14493002\n",
      "Iteration 11930: loss = 0.00015196526,0.1409025\n",
      "Iteration 11935: loss = 0.00015370647,0.13707484\n",
      "Iteration 11940: loss = 0.00015540625,0.13346557\n",
      "Iteration 11945: loss = 0.00015710242,0.12999856\n",
      "Iteration 11950: loss = 0.00015871374,0.12681253\n",
      "Iteration 11955: loss = 0.00016044732,0.12363171\n",
      "Iteration 11960: loss = 0.00016168533,0.12232535\n",
      "Iteration 11965: loss = 0.00016418862,0.13035144\n",
      "Iteration 11970: loss = 0.00016092532,0.26222235\n",
      "Iteration 11975: loss = 0.00014268154,0.36632\n",
      "Iteration 11980: loss = 0.00011544412,0.38385966\n",
      "Iteration 11985: loss = 9.970122e-05,0.37035728\n",
      "Iteration 11990: loss = 9.2341514e-05,0.3780751\n",
      "Iteration 11995: loss = 8.899427e-05,0.387479\n",
      "Iteration 12000: loss = 8.781386e-05,0.39094433\n",
      "Iteration 12005: loss = 8.8365545e-05,0.38444984\n",
      "Iteration 12010: loss = 8.9387824e-05,0.3753588\n",
      "Iteration 12015: loss = 9.123358e-05,0.3605477\n",
      "Iteration 12020: loss = 9.3202885e-05,0.34508598\n",
      "Iteration 12025: loss = 9.5488715e-05,0.3279903\n",
      "Iteration 12030: loss = 9.785395e-05,0.31128752\n",
      "Iteration 12035: loss = 0.00010024594,0.29564247\n",
      "Iteration 12040: loss = 0.00010268298,0.28106076\n",
      "Iteration 12045: loss = 0.00010502694,0.26803204\n",
      "Iteration 12050: loss = 0.00010737021,0.25577277\n",
      "Iteration 12055: loss = 0.00010961618,0.24457967\n",
      "Iteration 12060: loss = 0.00011184185,0.23409775\n",
      "Iteration 12065: loss = 0.00011402157,0.22442956\n",
      "Iteration 12070: loss = 0.000116189265,0.21533914\n",
      "Iteration 12075: loss = 0.0001183397,0.20678021\n",
      "Iteration 12080: loss = 0.00012045424,0.1987967\n",
      "Iteration 12085: loss = 0.00012252944,0.19135408\n",
      "Iteration 12090: loss = 0.00012455026,0.18444072\n",
      "Iteration 12095: loss = 0.00012653707,0.17796396\n",
      "Iteration 12100: loss = 0.00012849056,0.1718861\n",
      "Iteration 12105: loss = 0.00013041632,0.16616261\n",
      "Iteration 12110: loss = 0.00013230137,0.16080226\n",
      "Iteration 12115: loss = 0.00013414536,0.15577537\n",
      "Iteration 12120: loss = 0.00013595661,0.15104158\n",
      "Iteration 12125: loss = 0.00013773785,0.14657313\n",
      "Iteration 12130: loss = 0.00013948586,0.1423595\n",
      "Iteration 12135: loss = 0.0001411998,0.13838574\n",
      "Iteration 12140: loss = 0.00014288806,0.13461673\n",
      "Iteration 12145: loss = 0.00014454206,0.13106042\n",
      "Iteration 12150: loss = 0.0001461733,0.12767638\n",
      "Iteration 12155: loss = 0.00014777006,0.12448003\n",
      "Iteration 12160: loss = 0.00014934716,0.121429704\n",
      "Iteration 12165: loss = 0.00015089456,0.11853819\n",
      "Iteration 12170: loss = 0.00015241998,0.11578009\n",
      "Iteration 12175: loss = 0.00015391925,0.11315639\n",
      "Iteration 12180: loss = 0.00015539257,0.11065723\n",
      "Iteration 12185: loss = 0.00015685383,0.10825873\n",
      "Iteration 12190: loss = 0.00015826643,0.10600435\n",
      "Iteration 12195: loss = 0.00015972804,0.10377357\n",
      "Iteration 12200: loss = 0.00016094344,0.102131665\n",
      "Iteration 12205: loss = 0.00016281557,0.10227733\n",
      "Iteration 12210: loss = 0.00016153157,0.14283223\n",
      "Iteration 12215: loss = 0.00015244361,0.49232116\n",
      "Iteration 12220: loss = 9.416535e-05,0.30437374\n",
      "Iteration 12225: loss = 6.793565e-05,0.5131146\n",
      "Iteration 12230: loss = 5.6184585e-05,0.67646545\n",
      "Iteration 12235: loss = 5.171451e-05,0.75009936\n",
      "Iteration 12240: loss = 4.9962746e-05,0.77678996\n",
      "Iteration 12245: loss = 5.0518458e-05,0.7572345\n",
      "Iteration 12250: loss = 5.1730178e-05,0.7258773\n",
      "Iteration 12255: loss = 5.388832e-05,0.6793608\n",
      "Iteration 12260: loss = 5.622736e-05,0.63376486\n",
      "Iteration 12265: loss = 5.8927955e-05,0.58681285\n",
      "Iteration 12270: loss = 6.1665916e-05,0.5438218\n",
      "Iteration 12275: loss = 6.4480315e-05,0.5039454\n",
      "Iteration 12280: loss = 6.729587e-05,0.46778002\n",
      "Iteration 12285: loss = 7.008114e-05,0.43522602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12290: loss = 7.286143e-05,0.4056217\n",
      "Iteration 12295: loss = 7.5577955e-05,0.37916055\n",
      "Iteration 12300: loss = 7.827088e-05,0.35514408\n",
      "Iteration 12305: loss = 8.088819e-05,0.3336767\n",
      "Iteration 12310: loss = 8.346319e-05,0.3141997\n",
      "Iteration 12315: loss = 8.596347e-05,0.2966689\n",
      "Iteration 12320: loss = 8.8411114e-05,0.28070903\n",
      "Iteration 12325: loss = 9.079295e-05,0.26623616\n",
      "Iteration 12330: loss = 9.312211e-05,0.25301054\n",
      "Iteration 12335: loss = 9.539558e-05,0.24092245\n",
      "Iteration 12340: loss = 9.762187e-05,0.2298118\n",
      "Iteration 12345: loss = 9.979218e-05,0.21961625\n",
      "Iteration 12350: loss = 0.00010191216,0.2102328\n",
      "Iteration 12355: loss = 0.00010397788,0.20158646\n",
      "Iteration 12360: loss = 0.00010599555,0.1935948\n",
      "Iteration 12365: loss = 0.00010796905,0.18618506\n",
      "Iteration 12370: loss = 0.000109902634,0.1792883\n",
      "Iteration 12375: loss = 0.0001117939,0.17287505\n",
      "Iteration 12380: loss = 0.0001136406,0.16690227\n",
      "Iteration 12385: loss = 0.00011544825,0.16132745\n",
      "Iteration 12390: loss = 0.000117219315,0.15610783\n",
      "Iteration 12395: loss = 0.00011895598,0.1512123\n",
      "Iteration 12400: loss = 0.00012065561,0.14662148\n",
      "Iteration 12405: loss = 0.00012232033,0.14230986\n",
      "Iteration 12410: loss = 0.00012395403,0.13824736\n",
      "Iteration 12415: loss = 0.00012555446,0.13441812\n",
      "Iteration 12420: loss = 0.00012712645,0.13080388\n",
      "Iteration 12425: loss = 0.00012867058,0.12738045\n",
      "Iteration 12430: loss = 0.00013017528,0.1241636\n",
      "Iteration 12435: loss = 0.00013167172,0.12107966\n",
      "Iteration 12440: loss = 0.00013311715,0.1181952\n",
      "Iteration 12445: loss = 0.0001345701,0.115402035\n",
      "Iteration 12450: loss = 0.00013593597,0.11285555\n",
      "Iteration 12455: loss = 0.00013741164,0.1102726\n",
      "Iteration 12460: loss = 0.00013849781,0.108814105\n",
      "Iteration 12465: loss = 0.00014045632,0.110187106\n",
      "Iteration 12470: loss = 0.00013888934,0.1562253\n",
      "Iteration 12475: loss = 0.00013762002,0.3823515\n",
      "Iteration 12480: loss = 0.00010495197,0.18555456\n",
      "Iteration 12485: loss = 8.746583e-05,0.29845694\n",
      "Iteration 12490: loss = 7.912703e-05,0.35715768\n",
      "Iteration 12495: loss = 7.5320735e-05,0.3642016\n",
      "Iteration 12500: loss = 7.4049494e-05,0.35350758\n",
      "Iteration 12505: loss = 7.417921e-05,0.3411129\n",
      "Iteration 12510: loss = 7.51318e-05,0.33037162\n",
      "Iteration 12515: loss = 7.657294e-05,0.31954062\n",
      "Iteration 12520: loss = 7.8284094e-05,0.306773\n",
      "Iteration 12525: loss = 8.014051e-05,0.2922503\n",
      "Iteration 12530: loss = 8.206727e-05,0.27783048\n",
      "Iteration 12535: loss = 8.405177e-05,0.26469252\n",
      "Iteration 12540: loss = 8.6098065e-05,0.25248355\n",
      "Iteration 12545: loss = 8.820574e-05,0.2405164\n",
      "Iteration 12550: loss = 9.0350346e-05,0.22898288\n",
      "Iteration 12555: loss = 9.248185e-05,0.21839088\n",
      "Iteration 12560: loss = 9.455342e-05,0.20872632\n",
      "Iteration 12565: loss = 9.655469e-05,0.19984545\n",
      "Iteration 12570: loss = 9.8511584e-05,0.1916754\n",
      "Iteration 12575: loss = 0.00010045147,0.18402064\n",
      "Iteration 12580: loss = 0.000102368045,0.17684834\n",
      "Iteration 12585: loss = 0.00010423523,0.1702256\n",
      "Iteration 12590: loss = 0.00010603737,0.16412798\n",
      "Iteration 12595: loss = 0.000107797154,0.15845834\n",
      "Iteration 12600: loss = 0.00010953378,0.15311977\n",
      "Iteration 12605: loss = 0.00011123047,0.14813498\n",
      "Iteration 12610: loss = 0.0001128769,0.14350007\n",
      "Iteration 12615: loss = 0.00011449496,0.1391343\n",
      "Iteration 12620: loss = 0.00011608404,0.13502134\n",
      "Iteration 12625: loss = 0.00011763308,0.13116646\n",
      "Iteration 12630: loss = 0.00011915151,0.12752901\n",
      "Iteration 12635: loss = 0.00012064245,0.12408881\n",
      "Iteration 12640: loss = 0.00012209652,0.120850585\n",
      "Iteration 12645: loss = 0.00012353183,0.117766716\n",
      "Iteration 12650: loss = 0.00012491875,0.114881806\n",
      "Iteration 12655: loss = 0.00012632088,0.11207458\n",
      "Iteration 12660: loss = 0.00012760868,0.109571256\n",
      "Iteration 12665: loss = 0.00012905274,0.106982574\n",
      "Iteration 12670: loss = 0.00012999086,0.10594747\n",
      "Iteration 12675: loss = 0.0001319979,0.10897233\n",
      "Iteration 12680: loss = 0.00012987758,0.16600034\n",
      "Iteration 12685: loss = 0.00012884833,0.35456324\n",
      "Iteration 12690: loss = 0.00010632177,0.1598897\n",
      "Iteration 12695: loss = 9.345788e-05,0.22536437\n",
      "Iteration 12700: loss = 8.762994e-05,0.26394102\n",
      "Iteration 12705: loss = 8.4575346e-05,0.25791982\n",
      "Iteration 12710: loss = 8.3907384e-05,0.24244703\n",
      "Iteration 12715: loss = 8.392985e-05,0.23732375\n",
      "Iteration 12720: loss = 8.4817926e-05,0.23447876\n",
      "Iteration 12725: loss = 8.58159e-05,0.22922432\n",
      "Iteration 12730: loss = 8.713875e-05,0.21988809\n",
      "Iteration 12735: loss = 8.8519475e-05,0.21137664\n",
      "Iteration 12740: loss = 9.0118694e-05,0.20387056\n",
      "Iteration 12745: loss = 9.18514e-05,0.19600064\n",
      "Iteration 12750: loss = 9.3710485e-05,0.18755773\n",
      "Iteration 12755: loss = 9.559898e-05,0.17980142\n",
      "Iteration 12760: loss = 9.741996e-05,0.1727953\n",
      "Iteration 12765: loss = 9.916772e-05,0.16623104\n",
      "Iteration 12770: loss = 0.00010087614,0.16022548\n",
      "Iteration 12775: loss = 0.000102608676,0.15442179\n",
      "Iteration 12780: loss = 0.000104329585,0.14892301\n",
      "Iteration 12785: loss = 0.00010599338,0.14385842\n",
      "Iteration 12790: loss = 0.000107584194,0.13919853\n",
      "Iteration 12795: loss = 0.00010915945,0.13478886\n",
      "Iteration 12800: loss = 0.0001107204,0.13059686\n",
      "Iteration 12805: loss = 0.0001122315,0.12669307\n",
      "Iteration 12810: loss = 0.000113697555,0.123045355\n",
      "Iteration 12815: loss = 0.000115147915,0.119569615\n",
      "Iteration 12820: loss = 0.00011656667,0.116289675\n",
      "Iteration 12825: loss = 0.00011794596,0.11320411\n",
      "Iteration 12830: loss = 0.00011930269,0.11027268\n",
      "Iteration 12835: loss = 0.000120629615,0.10749218\n",
      "Iteration 12840: loss = 0.000121924524,0.104861304\n",
      "Iteration 12845: loss = 0.00012320363,0.10234222\n",
      "Iteration 12850: loss = 0.00012443782,0.09997299\n",
      "Iteration 12855: loss = 0.0001256776,0.09766896\n",
      "Iteration 12860: loss = 0.00012683785,0.09556077\n",
      "Iteration 12865: loss = 0.00012809182,0.09340866\n",
      "Iteration 12870: loss = 0.00012900401,0.09208921\n",
      "Iteration 12875: loss = 0.00013066347,0.092323445\n",
      "Iteration 12880: loss = 0.00012934291,0.121797465\n",
      "Iteration 12885: loss = 0.00012989932,0.31720257\n",
      "Iteration 12890: loss = 9.3071234e-05,0.20548727\n",
      "Iteration 12895: loss = 7.374097e-05,0.3590446\n",
      "Iteration 12900: loss = 6.477252e-05,0.39146724\n",
      "Iteration 12905: loss = 6.039869e-05,0.39357188\n",
      "Iteration 12910: loss = 5.896439e-05,0.38876677\n",
      "Iteration 12915: loss = 5.863764e-05,0.38645464\n",
      "Iteration 12920: loss = 5.9513623e-05,0.37689483\n",
      "Iteration 12925: loss = 6.077232e-05,0.3635896\n",
      "Iteration 12930: loss = 6.254776e-05,0.34421507\n",
      "Iteration 12935: loss = 6.4442356e-05,0.32401085\n",
      "Iteration 12940: loss = 6.647315e-05,0.3045988\n",
      "Iteration 12945: loss = 6.858209e-05,0.28694525\n",
      "Iteration 12950: loss = 7.073133e-05,0.27056852\n",
      "Iteration 12955: loss = 7.297285e-05,0.25452498\n",
      "Iteration 12960: loss = 7.518789e-05,0.23975483\n",
      "Iteration 12965: loss = 7.739909e-05,0.22625782\n",
      "Iteration 12970: loss = 7.950891e-05,0.21425664\n",
      "Iteration 12975: loss = 8.155071e-05,0.20332465\n",
      "Iteration 12980: loss = 8.352822e-05,0.193425\n",
      "Iteration 12985: loss = 8.547463e-05,0.18428329\n",
      "Iteration 12990: loss = 8.73963e-05,0.17577933\n",
      "Iteration 12995: loss = 8.92556e-05,0.16802067\n",
      "Iteration 13000: loss = 9.105213e-05,0.16091399\n",
      "Iteration 13005: loss = 9.2790404e-05,0.1543914\n",
      "Iteration 13010: loss = 9.449999e-05,0.1482962\n",
      "Iteration 13015: loss = 9.6167154e-05,0.1426394\n",
      "Iteration 13020: loss = 9.7780685e-05,0.13740878\n",
      "Iteration 13025: loss = 9.935011e-05,0.13254343\n",
      "Iteration 13030: loss = 0.000100885394,0.12798685\n",
      "Iteration 13035: loss = 0.00010238314,0.12372283\n",
      "Iteration 13040: loss = 0.00010383517,0.11974661\n",
      "Iteration 13045: loss = 0.00010526215,0.115989804\n",
      "Iteration 13050: loss = 0.00010664336,0.11248028\n",
      "Iteration 13055: loss = 0.00010799974,0.109156795\n",
      "Iteration 13060: loss = 0.000109319204,0.10602942\n",
      "Iteration 13065: loss = 0.000110610585,0.103067145\n",
      "Iteration 13070: loss = 0.000111867856,0.10027262\n",
      "Iteration 13075: loss = 0.000113104856,0.097604156\n",
      "Iteration 13080: loss = 0.000114293776,0.0951111\n",
      "Iteration 13085: loss = 0.00011550501,0.09266306\n",
      "Iteration 13090: loss = 0.00011655341,0.09062123\n",
      "Iteration 13095: loss = 0.00011794418,0.08871214\n",
      "Iteration 13100: loss = 0.00011794441,0.09560378\n",
      "Iteration 13105: loss = 0.000121253484,0.16996613\n",
      "Iteration 13110: loss = 0.00010028629,0.41037256\n",
      "Iteration 13115: loss = 8.013538e-05,0.2585015\n",
      "Iteration 13120: loss = 6.830331e-05,0.25864634\n",
      "Iteration 13125: loss = 6.272665e-05,0.29969943\n",
      "Iteration 13130: loss = 5.9622544e-05,0.33108538\n",
      "Iteration 13135: loss = 5.8441277e-05,0.34193563\n",
      "Iteration 13140: loss = 5.849831e-05,0.3377207\n",
      "Iteration 13145: loss = 5.9210404e-05,0.3259742\n",
      "Iteration 13150: loss = 6.0627255e-05,0.30842325\n",
      "Iteration 13155: loss = 6.2202605e-05,0.29129428\n",
      "Iteration 13160: loss = 6.403935e-05,0.27430472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13165: loss = 6.594811e-05,0.25879994\n",
      "Iteration 13170: loss = 6.792893e-05,0.24422215\n",
      "Iteration 13175: loss = 7.000246e-05,0.23000321\n",
      "Iteration 13180: loss = 7.204862e-05,0.21677703\n",
      "Iteration 13185: loss = 7.412482e-05,0.20432657\n",
      "Iteration 13190: loss = 7.61261e-05,0.19323127\n",
      "Iteration 13195: loss = 7.8079254e-05,0.18314703\n",
      "Iteration 13200: loss = 7.9960686e-05,0.17398827\n",
      "Iteration 13205: loss = 8.177155e-05,0.16567676\n",
      "Iteration 13210: loss = 8.3546045e-05,0.1580202\n",
      "Iteration 13215: loss = 8.527171e-05,0.1509771\n",
      "Iteration 13220: loss = 8.696568e-05,0.14442617\n",
      "Iteration 13225: loss = 8.860819e-05,0.13839738\n",
      "Iteration 13230: loss = 9.0192705e-05,0.13285522\n",
      "Iteration 13235: loss = 9.1728194e-05,0.12773123\n",
      "Iteration 13240: loss = 9.32237e-05,0.12295937\n",
      "Iteration 13245: loss = 9.4685565e-05,0.11849506\n",
      "Iteration 13250: loss = 9.610611e-05,0.114335224\n",
      "Iteration 13255: loss = 9.748163e-05,0.110459894\n",
      "Iteration 13260: loss = 9.882429e-05,0.10682252\n",
      "Iteration 13265: loss = 0.000100132725,0.10339955\n",
      "Iteration 13270: loss = 0.000101409736,0.10017948\n",
      "Iteration 13275: loss = 0.000102645696,0.09716179\n",
      "Iteration 13280: loss = 0.0001038625,0.094290964\n",
      "Iteration 13285: loss = 0.00010504032,0.09159404\n",
      "Iteration 13290: loss = 0.00010619126,0.089038126\n",
      "Iteration 13295: loss = 0.00010732044,0.08660265\n",
      "Iteration 13300: loss = 0.000108409404,0.08431425\n",
      "Iteration 13305: loss = 0.00010948987,0.08211068\n",
      "Iteration 13310: loss = 0.00011052809,0.080041334\n",
      "Iteration 13315: loss = 0.000111563735,0.07803456\n",
      "Iteration 13320: loss = 0.00011254129,0.07617669\n",
      "Iteration 13325: loss = 0.00011356748,0.0743088\n",
      "Iteration 13330: loss = 0.00011439535,0.0728579\n",
      "Iteration 13335: loss = 0.00011561986,0.071540706\n",
      "Iteration 13340: loss = 0.000115351395,0.078447595\n",
      "Iteration 13345: loss = 0.00011835652,0.14048605\n",
      "Iteration 13350: loss = 9.805771e-05,0.36335027\n",
      "Iteration 13355: loss = 7.549201e-05,0.2028128\n",
      "Iteration 13360: loss = 6.2798004e-05,0.25457722\n",
      "Iteration 13365: loss = 5.657472e-05,0.32430047\n",
      "Iteration 13370: loss = 5.319924e-05,0.35546035\n",
      "Iteration 13375: loss = 5.1726674e-05,0.3578927\n",
      "Iteration 13380: loss = 5.1744322e-05,0.34515923\n",
      "Iteration 13385: loss = 5.242118e-05,0.33067802\n",
      "Iteration 13390: loss = 5.379229e-05,0.3140671\n",
      "Iteration 13395: loss = 5.5450957e-05,0.29721326\n",
      "Iteration 13400: loss = 5.73237e-05,0.27900434\n",
      "Iteration 13405: loss = 5.942451e-05,0.25950706\n",
      "Iteration 13410: loss = 6.153549e-05,0.24164352\n",
      "Iteration 13415: loss = 6.3702006e-05,0.22539547\n",
      "Iteration 13420: loss = 6.5784065e-05,0.21107496\n",
      "Iteration 13425: loss = 6.7784356e-05,0.19814128\n",
      "Iteration 13430: loss = 6.973955e-05,0.18642923\n",
      "Iteration 13435: loss = 7.163574e-05,0.17594177\n",
      "Iteration 13440: loss = 7.3523275e-05,0.16617322\n",
      "Iteration 13445: loss = 7.536492e-05,0.15725711\n",
      "Iteration 13450: loss = 7.713632e-05,0.1492287\n",
      "Iteration 13455: loss = 7.8834746e-05,0.14195654\n",
      "Iteration 13460: loss = 8.046627e-05,0.135359\n",
      "Iteration 13465: loss = 8.206704e-05,0.12923668\n",
      "Iteration 13470: loss = 8.3625455e-05,0.123580426\n",
      "Iteration 13475: loss = 8.512556e-05,0.11840076\n",
      "Iteration 13480: loss = 8.657205e-05,0.11363343\n",
      "Iteration 13485: loss = 8.7981105e-05,0.10919825\n",
      "Iteration 13490: loss = 8.935697e-05,0.10505649\n",
      "Iteration 13495: loss = 9.0683316e-05,0.10122415\n",
      "Iteration 13500: loss = 9.197111e-05,0.09764836\n",
      "Iteration 13505: loss = 9.323202e-05,0.09428306\n",
      "Iteration 13510: loss = 9.445119e-05,0.09114673\n",
      "Iteration 13515: loss = 9.563966e-05,0.08819574\n",
      "Iteration 13520: loss = 9.679963e-05,0.08541488\n",
      "Iteration 13525: loss = 9.792196e-05,0.08280875\n",
      "Iteration 13530: loss = 9.902767e-05,0.08032505\n",
      "Iteration 13535: loss = 0.00010008982,0.078008726\n",
      "Iteration 13540: loss = 0.00010115342,0.075766884\n",
      "Iteration 13545: loss = 0.00010213555,0.07374547\n",
      "Iteration 13550: loss = 0.00010321584,0.07166929\n",
      "Iteration 13555: loss = 0.00010394179,0.07056016\n",
      "Iteration 13560: loss = 0.00010542203,0.071221724\n",
      "Iteration 13565: loss = 0.00010390909,0.103577785\n",
      "Iteration 13570: loss = 0.00010586681,0.28794393\n",
      "Iteration 13575: loss = 8.100961e-05,0.14328234\n",
      "Iteration 13580: loss = 6.687137e-05,0.26341063\n",
      "Iteration 13585: loss = 6.0122853e-05,0.27164698\n",
      "Iteration 13590: loss = 5.646315e-05,0.26193807\n",
      "Iteration 13595: loss = 5.5181645e-05,0.25875893\n",
      "Iteration 13600: loss = 5.5074557e-05,0.25969985\n",
      "Iteration 13605: loss = 5.565076e-05,0.25627533\n",
      "Iteration 13610: loss = 5.678329e-05,0.24507861\n",
      "Iteration 13615: loss = 5.8064423e-05,0.23135793\n",
      "Iteration 13620: loss = 5.95734e-05,0.21782066\n",
      "Iteration 13625: loss = 6.125613e-05,0.2055321\n",
      "Iteration 13630: loss = 6.3008134e-05,0.1937982\n",
      "Iteration 13635: loss = 6.48907e-05,0.18163571\n",
      "Iteration 13640: loss = 6.6770845e-05,0.1704172\n",
      "Iteration 13645: loss = 6.8612644e-05,0.16047946\n",
      "Iteration 13650: loss = 7.0385e-05,0.15148306\n",
      "Iteration 13655: loss = 7.20609e-05,0.14343718\n",
      "Iteration 13660: loss = 7.3706884e-05,0.1361004\n",
      "Iteration 13665: loss = 7.533526e-05,0.12925524\n",
      "Iteration 13670: loss = 7.6927616e-05,0.12294589\n",
      "Iteration 13675: loss = 7.846116e-05,0.11721233\n",
      "Iteration 13680: loss = 7.992219e-05,0.11201259\n",
      "Iteration 13685: loss = 8.13435e-05,0.10721276\n",
      "Iteration 13690: loss = 8.273865e-05,0.10272503\n",
      "Iteration 13695: loss = 8.409059e-05,0.09858103\n",
      "Iteration 13700: loss = 8.539123e-05,0.09476065\n",
      "Iteration 13705: loss = 8.6657245e-05,0.091200836\n",
      "Iteration 13710: loss = 8.7895525e-05,0.0878623\n",
      "Iteration 13715: loss = 8.9093664e-05,0.084755175\n",
      "Iteration 13720: loss = 9.025974e-05,0.081846856\n",
      "Iteration 13725: loss = 9.139647e-05,0.0791143\n",
      "Iteration 13730: loss = 9.250509e-05,0.07654277\n",
      "Iteration 13735: loss = 9.358453e-05,0.07412373\n",
      "Iteration 13740: loss = 9.463619e-05,0.071845755\n",
      "Iteration 13745: loss = 9.566831e-05,0.06968234\n",
      "Iteration 13750: loss = 9.666336e-05,0.06765593\n",
      "Iteration 13755: loss = 9.765627e-05,0.06570372\n",
      "Iteration 13760: loss = 9.859086e-05,0.06391099\n",
      "Iteration 13765: loss = 9.9573044e-05,0.06211251\n",
      "Iteration 13770: loss = 0.00010036775,0.06073405\n",
      "Iteration 13775: loss = 0.00010154077,0.05952603\n",
      "Iteration 13780: loss = 0.00010124382,0.06713934\n",
      "Iteration 13785: loss = 0.0001041386,0.13631639\n",
      "Iteration 13790: loss = 8.459104e-05,0.3512296\n",
      "Iteration 13795: loss = 6.673068e-05,0.19678375\n",
      "Iteration 13800: loss = 5.643743e-05,0.21457651\n",
      "Iteration 13805: loss = 5.1003226e-05,0.27275318\n",
      "Iteration 13810: loss = 4.833649e-05,0.30245197\n",
      "Iteration 13815: loss = 4.7146124e-05,0.30743152\n",
      "Iteration 13820: loss = 4.7274272e-05,0.2958215\n",
      "Iteration 13825: loss = 4.8009435e-05,0.2802995\n",
      "Iteration 13830: loss = 4.918726e-05,0.2646163\n",
      "Iteration 13835: loss = 5.0765666e-05,0.24857195\n",
      "Iteration 13840: loss = 5.2479125e-05,0.2330329\n",
      "Iteration 13845: loss = 5.437319e-05,0.2167237\n",
      "Iteration 13850: loss = 5.634956e-05,0.20077914\n",
      "Iteration 13855: loss = 5.8312842e-05,0.18649791\n",
      "Iteration 13860: loss = 6.0277616e-05,0.17371736\n",
      "Iteration 13865: loss = 6.214272e-05,0.16248745\n",
      "Iteration 13870: loss = 6.393369e-05,0.1523893\n",
      "Iteration 13875: loss = 6.5674314e-05,0.14332135\n",
      "Iteration 13880: loss = 6.736394e-05,0.13515557\n",
      "Iteration 13885: loss = 6.9031e-05,0.12760882\n",
      "Iteration 13890: loss = 7.0652146e-05,0.12074213\n",
      "Iteration 13895: loss = 7.220394e-05,0.1145679\n",
      "Iteration 13900: loss = 7.369274e-05,0.10896623\n",
      "Iteration 13905: loss = 7.513279e-05,0.103849605\n",
      "Iteration 13910: loss = 7.653896e-05,0.09911405\n",
      "Iteration 13915: loss = 7.790936e-05,0.09473348\n",
      "Iteration 13920: loss = 7.922877e-05,0.09071664\n",
      "Iteration 13925: loss = 8.050591e-05,0.08700555\n",
      "Iteration 13930: loss = 8.175245e-05,0.08354673\n",
      "Iteration 13935: loss = 8.296862e-05,0.08031763\n",
      "Iteration 13940: loss = 8.4144536e-05,0.07732168\n",
      "Iteration 13945: loss = 8.5291016e-05,0.07451882\n",
      "Iteration 13950: loss = 8.641423e-05,0.071881846\n",
      "Iteration 13955: loss = 8.7498694e-05,0.069426104\n",
      "Iteration 13960: loss = 8.8564295e-05,0.06710368\n",
      "Iteration 13965: loss = 8.960231e-05,0.06492157\n",
      "Iteration 13970: loss = 9.0609894e-05,0.06287399\n",
      "Iteration 13975: loss = 9.16051e-05,0.06092079\n",
      "Iteration 13980: loss = 9.256359e-05,0.05909812\n",
      "Iteration 13985: loss = 9.352035e-05,0.057341136\n",
      "Iteration 13990: loss = 9.442371e-05,0.055727515\n",
      "Iteration 13995: loss = 9.53769e-05,0.054108042\n",
      "Iteration 14000: loss = 9.6121985e-05,0.052952006\n",
      "Iteration 14005: loss = 9.7311684e-05,0.052226003\n",
      "Iteration 14010: loss = 9.669424e-05,0.06403921\n",
      "Iteration 14015: loss = 9.9768076e-05,0.1660183\n",
      "Iteration 14020: loss = 7.298361e-05,0.31153107\n",
      "Iteration 14025: loss = 5.4198885e-05,0.2869983\n",
      "Iteration 14030: loss = 4.478132e-05,0.2852977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14035: loss = 4.005992e-05,0.32830358\n",
      "Iteration 14040: loss = 3.818377e-05,0.35829553\n",
      "Iteration 14045: loss = 3.767708e-05,0.36688358\n",
      "Iteration 14050: loss = 3.8217022e-05,0.35505643\n",
      "Iteration 14055: loss = 3.9359464e-05,0.3327465\n",
      "Iteration 14060: loss = 4.083237e-05,0.3077493\n",
      "Iteration 14065: loss = 4.2612548e-05,0.2827078\n",
      "Iteration 14070: loss = 4.4514793e-05,0.26011822\n",
      "Iteration 14075: loss = 4.65112e-05,0.23925377\n",
      "Iteration 14080: loss = 4.859918e-05,0.21932878\n",
      "Iteration 14085: loss = 5.0674247e-05,0.20122142\n",
      "Iteration 14090: loss = 5.2740423e-05,0.18507442\n",
      "Iteration 14095: loss = 5.4744756e-05,0.17103067\n",
      "Iteration 14100: loss = 5.665155e-05,0.15877715\n",
      "Iteration 14105: loss = 5.8482856e-05,0.14791125\n",
      "Iteration 14110: loss = 6.0239192e-05,0.13834155\n",
      "Iteration 14115: loss = 6.195179e-05,0.12971306\n",
      "Iteration 14120: loss = 6.3625404e-05,0.1218666\n",
      "Iteration 14125: loss = 6.523774e-05,0.11483234\n",
      "Iteration 14130: loss = 6.678095e-05,0.10852644\n",
      "Iteration 14135: loss = 6.826115e-05,0.102839686\n",
      "Iteration 14140: loss = 6.9700065e-05,0.09763675\n",
      "Iteration 14145: loss = 7.110625e-05,0.09284102\n",
      "Iteration 14150: loss = 7.246393e-05,0.08845827\n",
      "Iteration 14155: loss = 7.37705e-05,0.08445481\n",
      "Iteration 14160: loss = 7.5041935e-05,0.08075068\n",
      "Iteration 14165: loss = 7.6284006e-05,0.0773076\n",
      "Iteration 14170: loss = 7.74905e-05,0.074117854\n",
      "Iteration 14175: loss = 7.8656776e-05,0.07116769\n",
      "Iteration 14180: loss = 7.979862e-05,0.06840605\n",
      "Iteration 14185: loss = 8.091235e-05,0.065824315\n",
      "Iteration 14190: loss = 8.19903e-05,0.0634249\n",
      "Iteration 14195: loss = 8.3049665e-05,0.061160643\n",
      "Iteration 14200: loss = 8.407788e-05,0.059046254\n",
      "Iteration 14205: loss = 8.508377e-05,0.057053335\n",
      "Iteration 14210: loss = 8.606858e-05,0.05517375\n",
      "Iteration 14215: loss = 8.702549e-05,0.053410165\n",
      "Iteration 14220: loss = 8.797308e-05,0.051726293\n",
      "Iteration 14225: loss = 8.887288e-05,0.050177883\n",
      "Iteration 14230: loss = 8.9821384e-05,0.04862941\n",
      "Iteration 14235: loss = 9.054841e-05,0.047587834\n",
      "Iteration 14240: loss = 9.1783986e-05,0.047453336\n",
      "Iteration 14245: loss = 9.073041e-05,0.068613656\n",
      "Iteration 14250: loss = 9.1762086e-05,0.2593063\n",
      "Iteration 14255: loss = 5.8233298e-05,0.20739394\n",
      "Iteration 14260: loss = 4.12756e-05,0.37634662\n",
      "Iteration 14265: loss = 3.3744822e-05,0.43700647\n",
      "Iteration 14270: loss = 3.0138328e-05,0.46703732\n",
      "Iteration 14275: loss = 2.9024375e-05,0.46773216\n",
      "Iteration 14280: loss = 2.9176406e-05,0.45046848\n",
      "Iteration 14285: loss = 3.0118827e-05,0.42204136\n",
      "Iteration 14290: loss = 3.1665735e-05,0.38667187\n",
      "Iteration 14295: loss = 3.349304e-05,0.35141546\n",
      "Iteration 14300: loss = 3.5538444e-05,0.31744555\n",
      "Iteration 14305: loss = 3.7695034e-05,0.28617188\n",
      "Iteration 14310: loss = 3.986186e-05,0.2583588\n",
      "Iteration 14315: loss = 4.2037533e-05,0.2335134\n",
      "Iteration 14320: loss = 4.4157598e-05,0.21192558\n",
      "Iteration 14325: loss = 4.6213438e-05,0.19328573\n",
      "Iteration 14330: loss = 4.822284e-05,0.17704894\n",
      "Iteration 14335: loss = 5.015935e-05,0.16296282\n",
      "Iteration 14340: loss = 5.2044506e-05,0.15049848\n",
      "Iteration 14345: loss = 5.3871885e-05,0.13945062\n",
      "Iteration 14350: loss = 5.562675e-05,0.12973548\n",
      "Iteration 14355: loss = 5.731529e-05,0.12113621\n",
      "Iteration 14360: loss = 5.8928883e-05,0.11352019\n",
      "Iteration 14365: loss = 6.047577e-05,0.10672351\n",
      "Iteration 14370: loss = 6.197184e-05,0.10059451\n",
      "Iteration 14375: loss = 6.342206e-05,0.095031776\n",
      "Iteration 14380: loss = 6.4829415e-05,0.089965194\n",
      "Iteration 14385: loss = 6.619062e-05,0.08535196\n",
      "Iteration 14390: loss = 6.750371e-05,0.08115028\n",
      "Iteration 14395: loss = 6.8777714e-05,0.077293366\n",
      "Iteration 14400: loss = 7.0016504e-05,0.07373772\n",
      "Iteration 14405: loss = 7.122353e-05,0.07044912\n",
      "Iteration 14410: loss = 7.2393e-05,0.06741359\n",
      "Iteration 14415: loss = 7.35324e-05,0.064598136\n",
      "Iteration 14420: loss = 7.464174e-05,0.06197906\n",
      "Iteration 14425: loss = 7.5724514e-05,0.059536938\n",
      "Iteration 14430: loss = 7.677578e-05,0.05726535\n",
      "Iteration 14435: loss = 7.780336e-05,0.055137616\n",
      "Iteration 14440: loss = 7.88103e-05,0.053138163\n",
      "Iteration 14445: loss = 7.9788864e-05,0.051268972\n",
      "Iteration 14450: loss = 8.0747646e-05,0.049509756\n",
      "Iteration 14455: loss = 8.168467e-05,0.047854044\n",
      "Iteration 14460: loss = 8.260165e-05,0.046292372\n",
      "Iteration 14465: loss = 8.34988e-05,0.044819783\n",
      "Iteration 14470: loss = 8.4378116e-05,0.043426007\n",
      "Iteration 14475: loss = 8.5239415e-05,0.04210749\n",
      "Iteration 14480: loss = 8.607843e-05,0.04086542\n",
      "Iteration 14485: loss = 8.692066e-05,0.039664067\n",
      "Iteration 14490: loss = 8.768854e-05,0.038606077\n",
      "Iteration 14495: loss = 8.860956e-05,0.03753649\n",
      "Iteration 14500: loss = 8.8834444e-05,0.03891802\n",
      "Iteration 14505: loss = 9.086181e-05,0.060768746\n",
      "Iteration 14510: loss = 7.807009e-05,0.31243956\n",
      "Iteration 14515: loss = 4.7218797e-05,0.18914951\n",
      "Iteration 14520: loss = 3.1417763e-05,0.40986544\n",
      "Iteration 14525: loss = 2.4620102e-05,0.5402712\n",
      "Iteration 14530: loss = 2.1643422e-05,0.60601497\n",
      "Iteration 14535: loss = 2.0834668e-05,0.6149438\n",
      "Iteration 14540: loss = 2.1243031e-05,0.5864749\n",
      "Iteration 14545: loss = 2.2345137e-05,0.53989995\n",
      "Iteration 14550: loss = 2.4006908e-05,0.48362717\n",
      "Iteration 14555: loss = 2.5952542e-05,0.4284686\n",
      "Iteration 14560: loss = 2.8094042e-05,0.3774459\n",
      "Iteration 14565: loss = 3.0356845e-05,0.33194655\n",
      "Iteration 14570: loss = 3.2640353e-05,0.29291436\n",
      "Iteration 14575: loss = 3.4917244e-05,0.2595964\n",
      "Iteration 14580: loss = 3.7146223e-05,0.23145132\n",
      "Iteration 14585: loss = 3.9303195e-05,0.20774771\n",
      "Iteration 14590: loss = 4.1389972e-05,0.18761383\n",
      "Iteration 14595: loss = 4.3394557e-05,0.17047851\n",
      "Iteration 14600: loss = 4.531849e-05,0.15575966\n",
      "Iteration 14605: loss = 4.7171532e-05,0.14298157\n",
      "Iteration 14610: loss = 4.894755e-05,0.1318663\n",
      "Iteration 14615: loss = 5.0655115e-05,0.12212566\n",
      "Iteration 14620: loss = 5.2295305e-05,0.11355681\n",
      "Iteration 14625: loss = 5.386604e-05,0.10599384\n",
      "Iteration 14630: loss = 5.53759e-05,0.09926488\n",
      "Iteration 14635: loss = 5.6826826e-05,0.09325021\n",
      "Iteration 14640: loss = 5.8227743e-05,0.087836\n",
      "Iteration 14645: loss = 5.9586207e-05,0.082925186\n",
      "Iteration 14650: loss = 6.090367e-05,0.07845702\n",
      "Iteration 14655: loss = 6.2180065e-05,0.074386485\n",
      "Iteration 14660: loss = 6.341682e-05,0.07066705\n",
      "Iteration 14665: loss = 6.461425e-05,0.067261584\n",
      "Iteration 14670: loss = 6.577892e-05,0.06412846\n",
      "Iteration 14675: loss = 6.691235e-05,0.061233554\n",
      "Iteration 14680: loss = 6.801684e-05,0.058554735\n",
      "Iteration 14685: loss = 6.9091104e-05,0.056074314\n",
      "Iteration 14690: loss = 7.01358e-05,0.053773396\n",
      "Iteration 14695: loss = 7.115732e-05,0.051627416\n",
      "Iteration 14700: loss = 7.215086e-05,0.0496312\n",
      "Iteration 14705: loss = 7.312563e-05,0.047758114\n",
      "Iteration 14710: loss = 7.407519e-05,0.04600925\n",
      "Iteration 14715: loss = 7.500317e-05,0.04436909\n",
      "Iteration 14720: loss = 7.591233e-05,0.042827826\n",
      "Iteration 14725: loss = 7.680155e-05,0.041378703\n",
      "Iteration 14730: loss = 7.76716e-05,0.040013235\n",
      "Iteration 14735: loss = 7.8528676e-05,0.03872149\n",
      "Iteration 14740: loss = 7.9359845e-05,0.037512057\n",
      "Iteration 14745: loss = 8.018663e-05,0.0363538\n",
      "Iteration 14750: loss = 8.098207e-05,0.035276715\n",
      "Iteration 14755: loss = 8.179175e-05,0.034224477\n",
      "Iteration 14760: loss = 8.25185e-05,0.033312384\n",
      "Iteration 14765: loss = 8.338446e-05,0.03235892\n",
      "Iteration 14770: loss = 8.374015e-05,0.032573607\n",
      "Iteration 14775: loss = 8.514186e-05,0.0373552\n",
      "Iteration 14780: loss = 8.191669e-05,0.09850157\n",
      "Iteration 14785: loss = 6.818762e-05,0.3249225\n",
      "Iteration 14790: loss = 4.245882e-05,0.18167393\n",
      "Iteration 14795: loss = 3.0958403e-05,0.2954212\n",
      "Iteration 14800: loss = 2.5952046e-05,0.402714\n",
      "Iteration 14805: loss = 2.4070732e-05,0.44017524\n",
      "Iteration 14810: loss = 2.371981e-05,0.43374312\n",
      "Iteration 14815: loss = 2.4405868e-05,0.40389615\n",
      "Iteration 14820: loss = 2.5694988e-05,0.36752728\n",
      "Iteration 14825: loss = 2.7342345e-05,0.33165878\n",
      "Iteration 14830: loss = 2.922771e-05,0.29726106\n",
      "Iteration 14835: loss = 3.119962e-05,0.26534635\n",
      "Iteration 14840: loss = 3.3206663e-05,0.2363549\n",
      "Iteration 14845: loss = 3.521069e-05,0.2112602\n",
      "Iteration 14850: loss = 3.7188544e-05,0.19004488\n",
      "Iteration 14855: loss = 3.915366e-05,0.17156012\n",
      "Iteration 14860: loss = 4.1090087e-05,0.15526132\n",
      "Iteration 14865: loss = 4.297237e-05,0.14119664\n",
      "Iteration 14870: loss = 4.47748e-05,0.12920107\n",
      "Iteration 14875: loss = 4.6481684e-05,0.11888859\n",
      "Iteration 14880: loss = 4.8108788e-05,0.10994809\n",
      "Iteration 14885: loss = 4.9685896e-05,0.102047876\n",
      "Iteration 14890: loss = 5.122008e-05,0.09498043\n",
      "Iteration 14895: loss = 5.2701766e-05,0.08869781\n",
      "Iteration 14900: loss = 5.4120217e-05,0.083123036\n",
      "Iteration 14905: loss = 5.548282e-05,0.078144625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14910: loss = 5.6809266e-05,0.073626816\n",
      "Iteration 14915: loss = 5.8101166e-05,0.06951373\n",
      "Iteration 14920: loss = 5.934625e-05,0.06579524\n",
      "Iteration 14925: loss = 6.05496e-05,0.06241567\n",
      "Iteration 14930: loss = 6.172589e-05,0.059303053\n",
      "Iteration 14935: loss = 6.286703e-05,0.056451406\n",
      "Iteration 14940: loss = 6.3975785e-05,0.053833053\n",
      "Iteration 14945: loss = 6.50552e-05,0.051413257\n",
      "Iteration 14950: loss = 6.6109365e-05,0.04917178\n",
      "Iteration 14955: loss = 6.7132605e-05,0.047101308\n",
      "Iteration 14960: loss = 6.8138186e-05,0.04516637\n",
      "Iteration 14965: loss = 6.910852e-05,0.04338369\n",
      "Iteration 14970: loss = 7.0071575e-05,0.041696828\n",
      "Iteration 14975: loss = 7.099695e-05,0.040145412\n",
      "Iteration 14980: loss = 7.192198e-05,0.038665395\n",
      "Iteration 14985: loss = 7.278958e-05,0.037334714\n",
      "Iteration 14990: loss = 7.372987e-05,0.03600024\n",
      "Iteration 14995: loss = 7.434823e-05,0.035475496\n",
      "Iteration 15000: loss = 7.570555e-05,0.0378964\n",
      "Iteration 15005: loss = 7.347995e-05,0.088005714\n",
      "Iteration 15010: loss = 6.602001e-05,0.36573586\n",
      "Iteration 15015: loss = 3.9896186e-05,0.16599852\n",
      "Iteration 15020: loss = 2.828846e-05,0.29019305\n",
      "Iteration 15025: loss = 2.330724e-05,0.3984859\n",
      "Iteration 15030: loss = 2.1335412e-05,0.44617707\n",
      "Iteration 15035: loss = 2.0888476e-05,0.45052907\n",
      "Iteration 15040: loss = 2.147637e-05,0.42549822\n",
      "Iteration 15045: loss = 2.2647635e-05,0.38778514\n",
      "Iteration 15050: loss = 2.4203486e-05,0.34625825\n",
      "Iteration 15055: loss = 2.6040158e-05,0.30542663\n",
      "Iteration 15060: loss = 2.8007076e-05,0.26893708\n",
      "Iteration 15065: loss = 3.0053408e-05,0.23715805\n",
      "Iteration 15070: loss = 3.2116663e-05,0.21002348\n",
      "Iteration 15075: loss = 3.413331e-05,0.18715262\n",
      "Iteration 15080: loss = 3.609523e-05,0.1676467\n",
      "Iteration 15085: loss = 3.7983827e-05,0.15104356\n",
      "Iteration 15090: loss = 3.9791168e-05,0.13697119\n",
      "Iteration 15095: loss = 4.1533363e-05,0.124919206\n",
      "Iteration 15100: loss = 4.3215918e-05,0.114484414\n",
      "Iteration 15105: loss = 4.4842e-05,0.1053488\n",
      "Iteration 15110: loss = 4.641693e-05,0.0973045\n",
      "Iteration 15115: loss = 4.7931302e-05,0.09025172\n",
      "Iteration 15120: loss = 4.938175e-05,0.08404904\n",
      "Iteration 15125: loss = 5.077402e-05,0.07855107\n",
      "Iteration 15130: loss = 5.2116677e-05,0.07364322\n",
      "Iteration 15135: loss = 5.342125e-05,0.069213495\n",
      "Iteration 15140: loss = 5.468731e-05,0.06520631\n",
      "Iteration 15145: loss = 5.5914537e-05,0.061577603\n",
      "Iteration 15150: loss = 5.7098525e-05,0.0582939\n",
      "Iteration 15155: loss = 5.8249712e-05,0.055296358\n",
      "Iteration 15160: loss = 5.93687e-05,0.052553363\n",
      "Iteration 15165: loss = 6.045836e-05,0.050032634\n",
      "Iteration 15170: loss = 6.151777e-05,0.047716103\n",
      "Iteration 15175: loss = 6.255184e-05,0.04557574\n",
      "Iteration 15180: loss = 6.3556465e-05,0.043604363\n",
      "Iteration 15185: loss = 6.453968e-05,0.041771185\n",
      "Iteration 15190: loss = 6.5498374e-05,0.04007169\n",
      "Iteration 15195: loss = 6.6434586e-05,0.038490247\n",
      "Iteration 15200: loss = 6.735304e-05,0.037012577\n",
      "Iteration 15205: loss = 6.824821e-05,0.035636827\n",
      "Iteration 15210: loss = 6.912473e-05,0.034349456\n",
      "Iteration 15215: loss = 6.998807e-05,0.03313691\n",
      "Iteration 15220: loss = 7.082593e-05,0.032009855\n",
      "Iteration 15225: loss = 7.1661176e-05,0.030934008\n",
      "Iteration 15230: loss = 7.245579e-05,0.029950637\n",
      "Iteration 15235: loss = 7.328621e-05,0.028976358\n",
      "Iteration 15240: loss = 7.396901e-05,0.028237585\n",
      "Iteration 15245: loss = 7.492499e-05,0.027637124\n",
      "Iteration 15250: loss = 7.470743e-05,0.03183417\n",
      "Iteration 15255: loss = 7.694716e-05,0.07192635\n",
      "Iteration 15260: loss = 5.6186287e-05,0.3211713\n",
      "Iteration 15265: loss = 3.7831032e-05,0.1429921\n",
      "Iteration 15270: loss = 2.8083654e-05,0.27458847\n",
      "Iteration 15275: loss = 2.3295957e-05,0.3697333\n",
      "Iteration 15280: loss = 2.1266242e-05,0.4025527\n",
      "Iteration 15285: loss = 2.0717758e-05,0.39902598\n",
      "Iteration 15290: loss = 2.1132691e-05,0.3750838\n",
      "Iteration 15295: loss = 2.21641e-05,0.34234294\n",
      "Iteration 15300: loss = 2.3565175e-05,0.30828258\n",
      "Iteration 15305: loss = 2.5233645e-05,0.27515468\n",
      "Iteration 15310: loss = 2.7069967e-05,0.24415\n",
      "Iteration 15315: loss = 2.8992166e-05,0.21587667\n",
      "Iteration 15320: loss = 3.0960848e-05,0.19061355\n",
      "Iteration 15325: loss = 3.2922402e-05,0.16892798\n",
      "Iteration 15330: loss = 3.4836405e-05,0.15080686\n",
      "Iteration 15335: loss = 3.66831e-05,0.1356168\n",
      "Iteration 15340: loss = 3.8441613e-05,0.12275973\n",
      "Iteration 15345: loss = 4.0111598e-05,0.111828364\n",
      "Iteration 15350: loss = 4.1717594e-05,0.10243833\n",
      "Iteration 15355: loss = 4.3274184e-05,0.094234265\n",
      "Iteration 15360: loss = 4.4783446e-05,0.086995594\n",
      "Iteration 15365: loss = 4.624293e-05,0.08062205\n",
      "Iteration 15370: loss = 4.764264e-05,0.07502188\n",
      "Iteration 15375: loss = 4.898135e-05,0.070082754\n",
      "Iteration 15380: loss = 5.0275168e-05,0.06567306\n",
      "Iteration 15385: loss = 5.153391e-05,0.061691526\n",
      "Iteration 15390: loss = 5.2755e-05,0.058097538\n",
      "Iteration 15395: loss = 5.3934706e-05,0.05485685\n",
      "Iteration 15400: loss = 5.5075852e-05,0.051922925\n",
      "Iteration 15405: loss = 5.6185992e-05,0.049245495\n",
      "Iteration 15410: loss = 5.726964e-05,0.046789058\n",
      "Iteration 15415: loss = 5.8319285e-05,0.044546347\n",
      "Iteration 15420: loss = 5.9339945e-05,0.04248907\n",
      "Iteration 15425: loss = 6.0339284e-05,0.040584285\n",
      "Iteration 15430: loss = 6.131304e-05,0.03882711\n",
      "Iteration 15435: loss = 6.225914e-05,0.037207384\n",
      "Iteration 15440: loss = 6.319135e-05,0.035692394\n",
      "Iteration 15445: loss = 6.409495e-05,0.034294587\n",
      "Iteration 15450: loss = 6.498538e-05,0.032984603\n",
      "Iteration 15455: loss = 6.5853375e-05,0.03176621\n",
      "Iteration 15460: loss = 6.67063e-05,0.030624093\n",
      "Iteration 15465: loss = 6.7536246e-05,0.02956205\n",
      "Iteration 15470: loss = 6.836742e-05,0.028549414\n",
      "Iteration 15475: loss = 6.913936e-05,0.027646955\n",
      "Iteration 15480: loss = 7.000288e-05,0.026738456\n",
      "Iteration 15485: loss = 7.04651e-05,0.026694987\n",
      "Iteration 15490: loss = 7.1799645e-05,0.03068895\n",
      "Iteration 15495: loss = 6.874691e-05,0.09282053\n",
      "Iteration 15500: loss = 5.4065848e-05,0.37419105\n",
      "Iteration 15505: loss = 3.2300766e-05,0.20597179\n",
      "Iteration 15510: loss = 2.2905459e-05,0.29325595\n",
      "Iteration 15515: loss = 1.8833829e-05,0.3941683\n",
      "Iteration 15520: loss = 1.73461e-05,0.44079548\n",
      "Iteration 15525: loss = 1.7201713e-05,0.44222093\n",
      "Iteration 15530: loss = 1.7870263e-05,0.4155409\n",
      "Iteration 15535: loss = 1.909012e-05,0.37469622\n",
      "Iteration 15540: loss = 2.065073e-05,0.33031502\n",
      "Iteration 15545: loss = 2.2438517e-05,0.28772137\n",
      "Iteration 15550: loss = 2.4372821e-05,0.24944554\n",
      "Iteration 15555: loss = 2.6363794e-05,0.21678379\n",
      "Iteration 15560: loss = 2.8354836e-05,0.18955712\n",
      "Iteration 15565: loss = 3.0313886e-05,0.16695948\n",
      "Iteration 15570: loss = 3.220823e-05,0.14818883\n",
      "Iteration 15575: loss = 3.4020017e-05,0.13250594\n",
      "Iteration 15580: loss = 3.575237e-05,0.1192898\n",
      "Iteration 15585: loss = 3.740828e-05,0.108123526\n",
      "Iteration 15590: loss = 3.8994825e-05,0.09861491\n",
      "Iteration 15595: loss = 4.0529154e-05,0.09036277\n",
      "Iteration 15600: loss = 4.201483e-05,0.08313265\n",
      "Iteration 15605: loss = 4.3446824e-05,0.07680692\n",
      "Iteration 15610: loss = 4.482199e-05,0.071269125\n",
      "Iteration 15615: loss = 4.6141515e-05,0.06639005\n",
      "Iteration 15620: loss = 4.7407546e-05,0.06207577\n",
      "Iteration 15625: loss = 4.863141e-05,0.05821829\n",
      "Iteration 15630: loss = 4.982154e-05,0.054739073\n",
      "Iteration 15635: loss = 5.0976403e-05,0.051596265\n",
      "Iteration 15640: loss = 5.209265e-05,0.04876195\n",
      "Iteration 15645: loss = 5.3173466e-05,0.04619249\n",
      "Iteration 15650: loss = 5.4222528e-05,0.04385191\n",
      "Iteration 15655: loss = 5.5245633e-05,0.041709527\n",
      "Iteration 15660: loss = 5.6243527e-05,0.039741013\n",
      "Iteration 15665: loss = 5.721363e-05,0.037935566\n",
      "Iteration 15670: loss = 5.8158694e-05,0.03627186\n",
      "Iteration 15675: loss = 5.908434e-05,0.03473125\n",
      "Iteration 15680: loss = 5.9987357e-05,0.03330599\n",
      "Iteration 15685: loss = 6.0869792e-05,0.03198387\n",
      "Iteration 15690: loss = 6.17334e-05,0.030754337\n",
      "Iteration 15695: loss = 6.258133e-05,0.029605832\n",
      "Iteration 15700: loss = 6.340715e-05,0.02853987\n",
      "Iteration 15705: loss = 6.4225314e-05,0.027533779\n",
      "Iteration 15710: loss = 6.501498e-05,0.026604563\n",
      "Iteration 15715: loss = 6.5809836e-05,0.025714396\n",
      "Iteration 15720: loss = 6.655527e-05,0.02491347\n",
      "Iteration 15725: loss = 6.735355e-05,0.024109507\n",
      "Iteration 15730: loss = 6.7962705e-05,0.023570718\n",
      "Iteration 15735: loss = 6.8888505e-05,0.023270829\n",
      "Iteration 15740: loss = 6.846716e-05,0.02815965\n",
      "Iteration 15745: loss = 7.0590875e-05,0.068122916\n",
      "Iteration 15750: loss = 4.9281738e-05,0.28330702\n",
      "Iteration 15755: loss = 3.257324e-05,0.13647777\n",
      "Iteration 15760: loss = 2.3979204e-05,0.27949765\n",
      "Iteration 15765: loss = 1.9753634e-05,0.36423457\n",
      "Iteration 15770: loss = 1.7966908e-05,0.38421476\n",
      "Iteration 15775: loss = 1.7568787e-05,0.37532175\n",
      "Iteration 15780: loss = 1.803526e-05,0.35384613\n",
      "Iteration 15785: loss = 1.906914e-05,0.32523337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15790: loss = 2.0484149e-05,0.29160422\n",
      "Iteration 15795: loss = 2.2167636e-05,0.25556135\n",
      "Iteration 15800: loss = 2.4017114e-05,0.22137299\n",
      "Iteration 15805: loss = 2.5936539e-05,0.19222201\n",
      "Iteration 15810: loss = 2.785182e-05,0.16836753\n",
      "Iteration 15815: loss = 2.9704815e-05,0.1485516\n",
      "Iteration 15820: loss = 3.1473046e-05,0.13195689\n",
      "Iteration 15825: loss = 3.3166885e-05,0.11822525\n",
      "Iteration 15830: loss = 3.4812667e-05,0.10660673\n",
      "Iteration 15835: loss = 3.642317e-05,0.09647341\n",
      "Iteration 15840: loss = 3.7981004e-05,0.087773114\n",
      "Iteration 15845: loss = 3.9462033e-05,0.080383\n",
      "Iteration 15850: loss = 4.0864874e-05,0.074033566\n",
      "Iteration 15855: loss = 4.2212003e-05,0.06850733\n",
      "Iteration 15860: loss = 4.3524047e-05,0.063590325\n",
      "Iteration 15865: loss = 4.4796052e-05,0.059223343\n",
      "Iteration 15870: loss = 4.6014273e-05,0.05537431\n",
      "Iteration 15875: loss = 4.7181034e-05,0.051961083\n",
      "Iteration 15880: loss = 4.832055e-05,0.048873182\n",
      "Iteration 15885: loss = 4.942706e-05,0.046084568\n",
      "Iteration 15890: loss = 5.049104e-05,0.043581218\n",
      "Iteration 15895: loss = 5.1523282e-05,0.04130998\n",
      "Iteration 15900: loss = 5.2538628e-05,0.03921768\n",
      "Iteration 15905: loss = 5.351301e-05,0.037328947\n",
      "Iteration 15910: loss = 5.446946e-05,0.035586696\n",
      "Iteration 15915: loss = 5.540311e-05,0.03398232\n",
      "Iteration 15920: loss = 5.6306933e-05,0.03251379\n",
      "Iteration 15925: loss = 5.7197594e-05,0.031147674\n",
      "Iteration 15930: loss = 5.806502e-05,0.029885741\n",
      "Iteration 15935: loss = 5.8912414e-05,0.028716454\n",
      "Iteration 15940: loss = 5.9748007e-05,0.027622186\n",
      "Iteration 15945: loss = 6.055467e-05,0.026616972\n",
      "Iteration 15950: loss = 6.137518e-05,0.025650421\n",
      "Iteration 15955: loss = 6.209083e-05,0.024863522\n",
      "Iteration 15960: loss = 6.300957e-05,0.024189359\n",
      "Iteration 15965: loss = 6.2929736e-05,0.027585436\n",
      "Iteration 15970: loss = 6.508399e-05,0.0682522\n",
      "Iteration 15975: loss = 4.4427845e-05,0.3649992\n",
      "Iteration 15980: loss = 2.657086e-05,0.18205748\n",
      "Iteration 15985: loss = 1.8023064e-05,0.32959834\n",
      "Iteration 15990: loss = 1.4276593e-05,0.4563819\n",
      "Iteration 15995: loss = 1.29098435e-05,0.51017743\n",
      "Iteration 16000: loss = 1.2802608e-05,0.5076265\n",
      "Iteration 16005: loss = 1.3460282e-05,0.47052777\n",
      "Iteration 16010: loss = 1.4633202e-05,0.41766426\n",
      "Iteration 16015: loss = 1.615001e-05,0.3615536\n",
      "Iteration 16020: loss = 1.7895085e-05,0.30904\n",
      "Iteration 16025: loss = 1.9776213e-05,0.26302722\n",
      "Iteration 16030: loss = 2.1712147e-05,0.22442763\n",
      "Iteration 16035: loss = 2.3643237e-05,0.19281381\n",
      "Iteration 16040: loss = 2.553241e-05,0.16716051\n",
      "Iteration 16045: loss = 2.7366614e-05,0.14625806\n",
      "Iteration 16050: loss = 2.9133735e-05,0.12912503\n",
      "Iteration 16055: loss = 3.0828025e-05,0.11493586\n",
      "Iteration 16060: loss = 3.2458247e-05,0.10299822\n",
      "Iteration 16065: loss = 3.4025074e-05,0.09288871\n",
      "Iteration 16070: loss = 3.5526202e-05,0.0843075\n",
      "Iteration 16075: loss = 3.6962094e-05,0.07698832\n",
      "Iteration 16080: loss = 3.8333772e-05,0.07069845\n",
      "Iteration 16085: loss = 3.964412e-05,0.06524791\n",
      "Iteration 16090: loss = 4.089704e-05,0.06049762\n",
      "Iteration 16095: loss = 4.21013e-05,0.05632042\n",
      "Iteration 16100: loss = 4.326685e-05,0.052603398\n",
      "Iteration 16105: loss = 4.4396133e-05,0.04927911\n",
      "Iteration 16110: loss = 4.5486773e-05,0.04630841\n",
      "Iteration 16115: loss = 4.6540055e-05,0.043642953\n",
      "Iteration 16120: loss = 4.7558027e-05,0.041241586\n",
      "Iteration 16125: loss = 4.854657e-05,0.039064527\n",
      "Iteration 16130: loss = 4.9509672e-05,0.037078764\n",
      "Iteration 16135: loss = 5.0448903e-05,0.035262734\n",
      "Iteration 16140: loss = 5.136122e-05,0.033604562\n",
      "Iteration 16145: loss = 5.2250558e-05,0.03208193\n",
      "Iteration 16150: loss = 5.312143e-05,0.030675666\n",
      "Iteration 16155: loss = 5.396728e-05,0.029384872\n",
      "Iteration 16160: loss = 5.480033e-05,0.028182963\n",
      "Iteration 16165: loss = 5.5614742e-05,0.027069198\n",
      "Iteration 16170: loss = 5.6409335e-05,0.02603909\n",
      "Iteration 16175: loss = 5.7190515e-05,0.02507616\n",
      "Iteration 16180: loss = 5.7959915e-05,0.024174301\n",
      "Iteration 16185: loss = 5.8710513e-05,0.023337135\n",
      "Iteration 16190: loss = 5.9449212e-05,0.022552425\n",
      "Iteration 16195: loss = 6.0180766e-05,0.021810412\n",
      "Iteration 16200: loss = 6.0883558e-05,0.021128861\n",
      "Iteration 16205: loss = 6.1606734e-05,0.02046389\n",
      "Iteration 16210: loss = 6.225242e-05,0.019893829\n",
      "Iteration 16215: loss = 6.299868e-05,0.019304523\n",
      "Iteration 16220: loss = 6.3379994e-05,0.019195851\n",
      "Iteration 16225: loss = 6.433123e-05,0.020130035\n",
      "Iteration 16230: loss = 6.288499e-05,0.034306515\n",
      "Iteration 16235: loss = 6.0450653e-05,0.13661729\n",
      "Iteration 16240: loss = 3.5280766e-05,0.2274498\n",
      "Iteration 16245: loss = 2.3743354e-05,0.24699159\n",
      "Iteration 16250: loss = 1.8580613e-05,0.26996663\n",
      "Iteration 16255: loss = 1.6347924e-05,0.32821262\n",
      "Iteration 16260: loss = 1.5635693e-05,0.35525486\n",
      "Iteration 16265: loss = 1.5799169e-05,0.34349462\n",
      "Iteration 16270: loss = 1.6524422e-05,0.312495\n",
      "Iteration 16275: loss = 1.7648148e-05,0.27836806\n",
      "Iteration 16280: loss = 1.9062369e-05,0.24613307\n",
      "Iteration 16285: loss = 2.0672329e-05,0.215161\n",
      "Iteration 16290: loss = 2.2407163e-05,0.18584682\n",
      "Iteration 16295: loss = 2.4202973e-05,0.16050005\n",
      "Iteration 16300: loss = 2.5977659e-05,0.14008008\n",
      "Iteration 16305: loss = 2.7670298e-05,0.12340264\n",
      "Iteration 16310: loss = 2.9269739e-05,0.109589234\n",
      "Iteration 16315: loss = 3.080731e-05,0.09818572\n",
      "Iteration 16320: loss = 3.2312655e-05,0.08839043\n",
      "Iteration 16325: loss = 3.3782606e-05,0.079899564\n",
      "Iteration 16330: loss = 3.518828e-05,0.07273111\n",
      "Iteration 16335: loss = 3.6512767e-05,0.06664723\n",
      "Iteration 16340: loss = 3.777751e-05,0.06141005\n",
      "Iteration 16345: loss = 3.9008326e-05,0.056782324\n",
      "Iteration 16350: loss = 4.0204584e-05,0.05268536\n",
      "Iteration 16355: loss = 4.1342806e-05,0.049111232\n",
      "Iteration 16360: loss = 4.2434578e-05,0.045955695\n",
      "Iteration 16365: loss = 4.350165e-05,0.043108188\n",
      "Iteration 16370: loss = 4.4534838e-05,0.040554997\n",
      "Iteration 16375: loss = 4.5527002e-05,0.038272876\n",
      "Iteration 16380: loss = 4.6494668e-05,0.03619993\n",
      "Iteration 16385: loss = 4.744141e-05,0.034305707\n",
      "Iteration 16390: loss = 4.834968e-05,0.03260239\n",
      "Iteration 16395: loss = 4.924434e-05,0.03103022\n",
      "Iteration 16400: loss = 5.011501e-05,0.029590381\n",
      "Iteration 16405: loss = 5.095785e-05,0.028277146\n",
      "Iteration 16410: loss = 5.180278e-05,0.027040014\n",
      "Iteration 16415: loss = 5.2583375e-05,0.025957614\n",
      "Iteration 16420: loss = 5.3431042e-05,0.024874043\n",
      "Iteration 16425: loss = 5.404656e-05,0.024210464\n",
      "Iteration 16430: loss = 5.503072e-05,0.023946956\n",
      "Iteration 16435: loss = 5.4510707e-05,0.031228147\n",
      "Iteration 16440: loss = 5.6531164e-05,0.08535622\n",
      "Iteration 16445: loss = 4.1897325e-05,0.25000352\n",
      "Iteration 16450: loss = 3.0124473e-05,0.107342385\n",
      "Iteration 16455: loss = 2.3675959e-05,0.17775139\n",
      "Iteration 16460: loss = 2.0160433e-05,0.24320252\n",
      "Iteration 16465: loss = 1.849568e-05,0.2556456\n",
      "Iteration 16470: loss = 1.800138e-05,0.24895078\n",
      "Iteration 16475: loss = 1.8277273e-05,0.2389888\n",
      "Iteration 16480: loss = 1.9082074e-05,0.22407582\n",
      "Iteration 16485: loss = 2.0263687e-05,0.20187722\n",
      "Iteration 16490: loss = 2.1692917e-05,0.17675978\n",
      "Iteration 16495: loss = 2.3255634e-05,0.15439805\n",
      "Iteration 16500: loss = 2.485117e-05,0.13603392\n",
      "Iteration 16505: loss = 2.6402757e-05,0.12031501\n",
      "Iteration 16510: loss = 2.7890454e-05,0.10696743\n",
      "Iteration 16515: loss = 2.9339748e-05,0.09593872\n",
      "Iteration 16520: loss = 3.078212e-05,0.086293116\n",
      "Iteration 16525: loss = 3.2209446e-05,0.07779986\n",
      "Iteration 16530: loss = 3.358183e-05,0.07064996\n",
      "Iteration 16535: loss = 3.487022e-05,0.064609505\n",
      "Iteration 16540: loss = 3.6096608e-05,0.0594351\n",
      "Iteration 16545: loss = 3.729629e-05,0.05485953\n",
      "Iteration 16550: loss = 3.8470313e-05,0.050784368\n",
      "Iteration 16555: loss = 3.959059e-05,0.047237594\n",
      "Iteration 16560: loss = 4.0657465e-05,0.044130214\n",
      "Iteration 16565: loss = 4.1698233e-05,0.04133995\n",
      "Iteration 16570: loss = 4.2716118e-05,0.03881764\n",
      "Iteration 16575: loss = 4.368766e-05,0.036581695\n",
      "Iteration 16580: loss = 4.4633267e-05,0.034557804\n",
      "Iteration 16585: loss = 4.556321e-05,0.032704085\n",
      "Iteration 16590: loss = 4.6451038e-05,0.031046446\n",
      "Iteration 16595: loss = 4.733031e-05,0.02950959\n",
      "Iteration 16600: loss = 4.8179132e-05,0.028116148\n",
      "Iteration 16605: loss = 4.901006e-05,0.02683344\n",
      "Iteration 16610: loss = 4.982411e-05,0.025649054\n",
      "Iteration 16615: loss = 5.061523e-05,0.024562731\n",
      "Iteration 16620: loss = 5.1399325e-05,0.023546653\n",
      "Iteration 16625: loss = 5.2150135e-05,0.022624535\n",
      "Iteration 16630: loss = 5.2920746e-05,0.021734947\n",
      "Iteration 16635: loss = 5.3582193e-05,0.02102456\n",
      "Iteration 16640: loss = 5.443218e-05,0.020404616\n",
      "Iteration 16645: loss = 5.4378997e-05,0.022978336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16650: loss = 5.6213128e-05,0.05113201\n",
      "Iteration 16655: loss = 4.1693627e-05,0.28173032\n",
      "Iteration 16660: loss = 2.6447617e-05,0.11952713\n",
      "Iteration 16665: loss = 1.775277e-05,0.28564546\n",
      "Iteration 16670: loss = 1.3704746e-05,0.38948578\n",
      "Iteration 16675: loss = 1.2071705e-05,0.42993438\n",
      "Iteration 16680: loss = 1.172775e-05,0.4255812\n",
      "Iteration 16685: loss = 1.21634475e-05,0.39480054\n",
      "Iteration 16690: loss = 1.3092004e-05,0.35070696\n",
      "Iteration 16695: loss = 1.4355764e-05,0.30425808\n",
      "Iteration 16700: loss = 1.5849046e-05,0.26043347\n",
      "Iteration 16705: loss = 1.748022e-05,0.22215785\n",
      "Iteration 16710: loss = 1.9177385e-05,0.18976057\n",
      "Iteration 16715: loss = 2.0884589e-05,0.16286066\n",
      "Iteration 16720: loss = 2.257224e-05,0.14054966\n",
      "Iteration 16725: loss = 2.4222003e-05,0.122126326\n",
      "Iteration 16730: loss = 2.5816926e-05,0.10701504\n",
      "Iteration 16735: loss = 2.734639e-05,0.09466714\n",
      "Iteration 16740: loss = 2.8803188e-05,0.084498644\n",
      "Iteration 16745: loss = 3.01866e-05,0.076015204\n",
      "Iteration 16750: loss = 3.1499785e-05,0.068867035\n",
      "Iteration 16755: loss = 3.2753112e-05,0.06279263\n",
      "Iteration 16760: loss = 3.3957505e-05,0.057559825\n",
      "Iteration 16765: loss = 3.5121477e-05,0.052988537\n",
      "Iteration 16770: loss = 3.624616e-05,0.048977874\n",
      "Iteration 16775: loss = 3.7327434e-05,0.045465656\n",
      "Iteration 16780: loss = 3.8363174e-05,0.042378522\n",
      "Iteration 16785: loss = 3.9358423e-05,0.039647374\n",
      "Iteration 16790: loss = 4.0326686e-05,0.037195373\n",
      "Iteration 16795: loss = 4.1267765e-05,0.03498759\n",
      "Iteration 16800: loss = 4.2180556e-05,0.03299855\n",
      "Iteration 16805: loss = 4.3062533e-05,0.031207476\n",
      "Iteration 16810: loss = 4.3924112e-05,0.02957484\n",
      "Iteration 16815: loss = 4.4764456e-05,0.028084256\n",
      "Iteration 16820: loss = 4.5583365e-05,0.026722679\n",
      "Iteration 16825: loss = 4.6380315e-05,0.025478061\n",
      "Iteration 16830: loss = 4.7163645e-05,0.024327472\n",
      "Iteration 16835: loss = 4.7925732e-05,0.023271188\n",
      "Iteration 16840: loss = 4.867408e-05,0.022293635\n",
      "Iteration 16845: loss = 4.9408958e-05,0.021385517\n",
      "Iteration 16850: loss = 5.0125334e-05,0.02054758\n",
      "Iteration 16855: loss = 5.0833085e-05,0.019763332\n",
      "Iteration 16860: loss = 5.1523908e-05,0.019036047\n",
      "Iteration 16865: loss = 5.2208477e-05,0.018353667\n",
      "Iteration 16870: loss = 5.286932e-05,0.017724618\n",
      "Iteration 16875: loss = 5.354385e-05,0.01711816\n",
      "Iteration 16880: loss = 5.41469e-05,0.016600017\n",
      "Iteration 16885: loss = 5.4847274e-05,0.016077051\n",
      "Iteration 16890: loss = 5.5139666e-05,0.016116844\n",
      "Iteration 16895: loss = 5.6098117e-05,0.017936522\n",
      "Iteration 16900: loss = 5.4039494e-05,0.041607272\n",
      "Iteration 16905: loss = 4.709834e-05,0.20446424\n",
      "Iteration 16910: loss = 2.52498e-05,0.15235932\n",
      "Iteration 16915: loss = 1.6351545e-05,0.31781185\n",
      "Iteration 16920: loss = 1.267969e-05,0.3726421\n",
      "Iteration 16925: loss = 1.1280212e-05,0.3937031\n",
      "Iteration 16930: loss = 1.1065828e-05,0.39298514\n",
      "Iteration 16935: loss = 1.1575631e-05,0.37064654\n",
      "Iteration 16940: loss = 1.2531317e-05,0.3334864\n",
      "Iteration 16945: loss = 1.3777152e-05,0.28961688\n",
      "Iteration 16950: loss = 1.5214832e-05,0.24643447\n",
      "Iteration 16955: loss = 1.6766357e-05,0.20889021\n",
      "Iteration 16960: loss = 1.8373461e-05,0.17816195\n",
      "Iteration 16965: loss = 1.9996127e-05,0.15290228\n",
      "Iteration 16970: loss = 2.1615459e-05,0.1316055\n",
      "Iteration 16975: loss = 2.3209068e-05,0.1138863\n",
      "Iteration 16980: loss = 2.4747953e-05,0.099595554\n",
      "Iteration 16985: loss = 2.6207324e-05,0.088031694\n",
      "Iteration 16990: loss = 2.7580032e-05,0.0785081\n",
      "Iteration 16995: loss = 2.8880904e-05,0.07061524\n",
      "Iteration 17000: loss = 3.0139114e-05,0.06389873\n",
      "Iteration 17005: loss = 3.136228e-05,0.058068402\n",
      "Iteration 17010: loss = 3.2538985e-05,0.05306251\n",
      "Iteration 17015: loss = 3.365392e-05,0.04878649\n",
      "Iteration 17020: loss = 3.4714703e-05,0.04509253\n",
      "Iteration 17025: loss = 3.574252e-05,0.04183316\n",
      "Iteration 17030: loss = 3.6743426e-05,0.038928654\n",
      "Iteration 17035: loss = 3.7707723e-05,0.03635957\n",
      "Iteration 17040: loss = 3.863433e-05,0.03408128\n",
      "Iteration 17045: loss = 3.953536e-05,0.032032255\n",
      "Iteration 17050: loss = 4.041412e-05,0.030178335\n",
      "Iteration 17055: loss = 4.1265648e-05,0.028507702\n",
      "Iteration 17060: loss = 4.2090596e-05,0.026997015\n",
      "Iteration 17065: loss = 4.290038e-05,0.025612371\n",
      "Iteration 17070: loss = 4.3688735e-05,0.02434951\n",
      "Iteration 17075: loss = 4.445504e-05,0.023197396\n",
      "Iteration 17080: loss = 4.520921e-05,0.022132233\n",
      "Iteration 17085: loss = 4.5942765e-05,0.02115669\n",
      "Iteration 17090: loss = 4.666284e-05,0.020253161\n",
      "Iteration 17095: loss = 4.7369092e-05,0.019417303\n",
      "Iteration 17100: loss = 4.8059894e-05,0.018642986\n",
      "Iteration 17105: loss = 4.8739068e-05,0.017922353\n",
      "Iteration 17110: loss = 4.9401842e-05,0.017255848\n",
      "Iteration 17115: loss = 5.006755e-05,0.016623281\n",
      "Iteration 17120: loss = 5.067413e-05,0.01607697\n",
      "Iteration 17125: loss = 5.1392544e-05,0.015573954\n",
      "Iteration 17130: loss = 5.1441108e-05,0.016930569\n",
      "Iteration 17135: loss = 5.310459e-05,0.03839788\n",
      "Iteration 17140: loss = 3.6094978e-05,0.31255746\n",
      "Iteration 17145: loss = 1.8991735e-05,0.1666673\n",
      "Iteration 17150: loss = 1.0630493e-05,0.4115635\n",
      "Iteration 17155: loss = 7.479555e-06,0.5891814\n",
      "Iteration 17160: loss = 6.430586e-06,0.6641687\n",
      "Iteration 17165: loss = 6.377904e-06,0.6575282\n",
      "Iteration 17170: loss = 6.936698e-06,0.59577805\n",
      "Iteration 17175: loss = 7.914083e-06,0.51277715\n",
      "Iteration 17180: loss = 9.211794e-06,0.4270806\n",
      "Iteration 17185: loss = 1.07421e-05,0.34982187\n",
      "Iteration 17190: loss = 1.2413521e-05,0.28543544\n",
      "Iteration 17195: loss = 1.41399205e-05,0.23402451\n",
      "Iteration 17200: loss = 1.5857671e-05,0.19392122\n",
      "Iteration 17205: loss = 1.7533408e-05,0.1626514\n",
      "Iteration 17210: loss = 1.914611e-05,0.13816047\n",
      "Iteration 17215: loss = 2.0687266e-05,0.11880615\n",
      "Iteration 17220: loss = 2.2160413e-05,0.10326887\n",
      "Iteration 17225: loss = 2.3564839e-05,0.0906694\n",
      "Iteration 17230: loss = 2.4903513e-05,0.080328345\n",
      "Iteration 17235: loss = 2.6178377e-05,0.07175566\n",
      "Iteration 17240: loss = 2.7392476e-05,0.06457997\n",
      "Iteration 17245: loss = 2.8550081e-05,0.05851581\n",
      "Iteration 17250: loss = 2.965674e-05,0.053343184\n",
      "Iteration 17255: loss = 3.0716994e-05,0.0488873\n",
      "Iteration 17260: loss = 3.173691e-05,0.04501639\n",
      "Iteration 17265: loss = 3.2719905e-05,0.041626636\n",
      "Iteration 17270: loss = 3.366896e-05,0.038640283\n",
      "Iteration 17275: loss = 3.4589117e-05,0.03598849\n",
      "Iteration 17280: loss = 3.5478708e-05,0.03363058\n",
      "Iteration 17285: loss = 3.6339476e-05,0.031524904\n",
      "Iteration 17290: loss = 3.717496e-05,0.029633766\n",
      "Iteration 17295: loss = 3.7984875e-05,0.027932195\n",
      "Iteration 17300: loss = 3.8772607e-05,0.026391955\n",
      "Iteration 17305: loss = 3.9540402e-05,0.0249916\n",
      "Iteration 17310: loss = 4.029112e-05,0.023712313\n",
      "Iteration 17315: loss = 4.1023046e-05,0.022544334\n",
      "Iteration 17320: loss = 4.1737945e-05,0.021474151\n",
      "Iteration 17325: loss = 4.2435975e-05,0.020491537\n",
      "Iteration 17330: loss = 4.311932e-05,0.019585809\n",
      "Iteration 17335: loss = 4.378986e-05,0.01874943\n",
      "Iteration 17340: loss = 4.444714e-05,0.017974796\n",
      "Iteration 17345: loss = 4.5086956e-05,0.017260961\n",
      "Iteration 17350: loss = 4.5719262e-05,0.016593948\n",
      "Iteration 17355: loss = 4.6336325e-05,0.01597599\n",
      "Iteration 17360: loss = 4.6946323e-05,0.015398105\n",
      "Iteration 17365: loss = 4.754549e-05,0.014858184\n",
      "Iteration 17370: loss = 4.812982e-05,0.014356774\n",
      "Iteration 17375: loss = 4.871056e-05,0.013882962\n",
      "Iteration 17380: loss = 4.92788e-05,0.013441625\n",
      "Iteration 17385: loss = 4.983992e-05,0.013025633\n",
      "Iteration 17390: loss = 5.039366e-05,0.012633783\n",
      "Iteration 17395: loss = 5.0936975e-05,0.012266589\n",
      "Iteration 17400: loss = 5.1476116e-05,0.011918696\n",
      "Iteration 17405: loss = 5.2001134e-05,0.011593469\n",
      "Iteration 17410: loss = 5.253541e-05,0.011278965\n",
      "Iteration 17415: loss = 5.3024472e-05,0.011000721\n",
      "Iteration 17420: loss = 5.3567946e-05,0.010721884\n",
      "Iteration 17425: loss = 5.3839452e-05,0.010646903\n",
      "Iteration 17430: loss = 5.4470584e-05,0.011072354\n",
      "Iteration 17435: loss = 5.328318e-05,0.01829332\n",
      "Iteration 17440: loss = 4.849694e-05,0.089669205\n",
      "Iteration 17445: loss = 2.6232658e-05,0.30381888\n",
      "Iteration 17450: loss = 1.5607795e-05,0.23886223\n",
      "Iteration 17455: loss = 1.09052025e-05,0.3207899\n",
      "Iteration 17460: loss = 8.9324185e-06,0.41609332\n",
      "Iteration 17465: loss = 8.336629e-06,0.4536434\n",
      "Iteration 17470: loss = 8.5375295e-06,0.43798038\n",
      "Iteration 17475: loss = 9.225038e-06,0.39337564\n",
      "Iteration 17480: loss = 1.0247171e-05,0.33857706\n",
      "Iteration 17485: loss = 1.1515912e-05,0.28450853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17490: loss = 1.2937085e-05,0.23727976\n",
      "Iteration 17495: loss = 1.4426154e-05,0.19864424\n",
      "Iteration 17500: loss = 1.5930627e-05,0.1675558\n",
      "Iteration 17505: loss = 1.7427128e-05,0.14225166\n",
      "Iteration 17510: loss = 1.8900222e-05,0.12147145\n",
      "Iteration 17515: loss = 2.033392e-05,0.10455812\n",
      "Iteration 17520: loss = 2.1715072e-05,0.09093543\n",
      "Iteration 17525: loss = 2.3030429e-05,0.079933494\n",
      "Iteration 17530: loss = 2.4272334e-05,0.070927314\n",
      "Iteration 17535: loss = 2.5449846e-05,0.06345898\n",
      "Iteration 17540: loss = 2.657352e-05,0.05720352\n",
      "Iteration 17545: loss = 2.7657597e-05,0.051856913\n",
      "Iteration 17550: loss = 2.870821e-05,0.04722554\n",
      "Iteration 17555: loss = 2.9719135e-05,0.043234278\n",
      "Iteration 17560: loss = 3.068622e-05,0.039788436\n",
      "Iteration 17565: loss = 3.1609594e-05,0.036797915\n",
      "Iteration 17570: loss = 3.2502994e-05,0.0341614\n",
      "Iteration 17575: loss = 3.3374115e-05,0.031807132\n",
      "Iteration 17580: loss = 3.421828e-05,0.029712835\n",
      "Iteration 17585: loss = 3.503093e-05,0.027853606\n",
      "Iteration 17590: loss = 3.5815505e-05,0.026191277\n",
      "Iteration 17595: loss = 3.658514e-05,0.024680233\n",
      "Iteration 17600: loss = 3.7333837e-05,0.023313442\n",
      "Iteration 17605: loss = 3.805912e-05,0.022078868\n",
      "Iteration 17610: loss = 3.876846e-05,0.020951228\n",
      "Iteration 17615: loss = 3.9461276e-05,0.019919978\n",
      "Iteration 17620: loss = 4.0135e-05,0.01897975\n",
      "Iteration 17625: loss = 4.0794057e-05,0.018115131\n",
      "Iteration 17630: loss = 4.1443524e-05,0.017314758\n",
      "Iteration 17635: loss = 4.2069078e-05,0.016586373\n",
      "Iteration 17640: loss = 4.2696916e-05,0.015898196\n",
      "Iteration 17645: loss = 4.328732e-05,0.015284369\n",
      "Iteration 17650: loss = 4.3906664e-05,0.014684374\n",
      "Iteration 17655: loss = 4.4415745e-05,0.01421984\n",
      "Iteration 17660: loss = 4.5070712e-05,0.013779187\n",
      "Iteration 17665: loss = 4.5084395e-05,0.014607476\n",
      "Iteration 17670: loss = 4.620424e-05,0.02130636\n",
      "Iteration 17675: loss = 4.2111464e-05,0.08567144\n",
      "Iteration 17680: loss = 3.3525892e-05,0.23260495\n",
      "Iteration 17685: loss = 2.0049889e-05,0.12754275\n",
      "Iteration 17690: loss = 1.3930608e-05,0.20413448\n",
      "Iteration 17695: loss = 1.1337393e-05,0.29363897\n",
      "Iteration 17700: loss = 1.0429663e-05,0.32058212\n",
      "Iteration 17705: loss = 1.044962e-05,0.3050933\n",
      "Iteration 17710: loss = 1.1011648e-05,0.27542123\n",
      "Iteration 17715: loss = 1.1923238e-05,0.24381547\n",
      "Iteration 17720: loss = 1.30616745e-05,0.21227068\n",
      "Iteration 17725: loss = 1.4310601e-05,0.18193081\n",
      "Iteration 17730: loss = 1.5596508e-05,0.15510088\n",
      "Iteration 17735: loss = 1.6890799e-05,0.13326484\n",
      "Iteration 17740: loss = 1.8196612e-05,0.11534631\n",
      "Iteration 17745: loss = 1.9508714e-05,0.09991606\n",
      "Iteration 17750: loss = 2.0808058e-05,0.08686873\n",
      "Iteration 17755: loss = 2.2062857e-05,0.076298624\n",
      "Iteration 17760: loss = 2.3241788e-05,0.067692205\n",
      "Iteration 17765: loss = 2.4350098e-05,0.06058423\n",
      "Iteration 17770: loss = 2.5419278e-05,0.05459608\n",
      "Iteration 17775: loss = 2.6466747e-05,0.049382657\n",
      "Iteration 17780: loss = 2.7481472e-05,0.044894584\n",
      "Iteration 17785: loss = 2.8444207e-05,0.041080065\n",
      "Iteration 17790: loss = 2.9357645e-05,0.0378093\n",
      "Iteration 17795: loss = 3.0247034e-05,0.03492593\n",
      "Iteration 17800: loss = 3.111657e-05,0.032359384\n",
      "Iteration 17805: loss = 3.1950014e-05,0.030111786\n",
      "Iteration 17810: loss = 3.2748914e-05,0.028131478\n",
      "Iteration 17815: loss = 3.353113e-05,0.026347406\n",
      "Iteration 17820: loss = 3.4291472e-05,0.024745127\n",
      "Iteration 17825: loss = 3.5022735e-05,0.02331657\n",
      "Iteration 17830: loss = 3.574071e-05,0.022014309\n",
      "Iteration 17835: loss = 3.6435576e-05,0.02083988\n",
      "Iteration 17840: loss = 3.7115435e-05,0.019767739\n",
      "Iteration 17845: loss = 3.7775393e-05,0.018794464\n",
      "Iteration 17850: loss = 3.8423965e-05,0.017898057\n",
      "Iteration 17855: loss = 3.9051894e-05,0.017082695\n",
      "Iteration 17860: loss = 3.9671202e-05,0.01632731\n",
      "Iteration 17865: loss = 4.0278464e-05,0.015630182\n",
      "Iteration 17870: loss = 4.085705e-05,0.015002534\n",
      "Iteration 17875: loss = 4.147124e-05,0.014391029\n",
      "Iteration 17880: loss = 4.1891723e-05,0.014078092\n",
      "Iteration 17885: loss = 4.2678246e-05,0.014647735\n",
      "Iteration 17890: loss = 4.1422627e-05,0.03091254\n",
      "Iteration 17895: loss = 4.0079773e-05,0.19476523\n",
      "Iteration 17900: loss = 1.8806573e-05,0.18914187\n",
      "Iteration 17905: loss = 1.0283343e-05,0.39026684\n",
      "Iteration 17910: loss = 7.069628e-06,0.51198465\n",
      "Iteration 17915: loss = 5.946247e-06,0.56845886\n",
      "Iteration 17920: loss = 5.795008e-06,0.56258863\n",
      "Iteration 17925: loss = 6.2141316e-06,0.5112928\n",
      "Iteration 17930: loss = 7.0108395e-06,0.44151288\n",
      "Iteration 17935: loss = 8.100869e-06,0.36800176\n",
      "Iteration 17940: loss = 9.393137e-06,0.3014689\n",
      "Iteration 17945: loss = 1.0804996e-05,0.24617141\n",
      "Iteration 17950: loss = 1.2259766e-05,0.20202221\n",
      "Iteration 17955: loss = 1.371713e-05,0.16735375\n",
      "Iteration 17960: loss = 1.51391905e-05,0.14019246\n",
      "Iteration 17965: loss = 1.650803e-05,0.118824534\n",
      "Iteration 17970: loss = 1.7818413e-05,0.101838894\n",
      "Iteration 17975: loss = 1.9071716e-05,0.08818875\n",
      "Iteration 17980: loss = 2.0267546e-05,0.07715835\n",
      "Iteration 17985: loss = 2.1411084e-05,0.06812138\n",
      "Iteration 17990: loss = 2.2510654e-05,0.060597762\n",
      "Iteration 17995: loss = 2.3566654e-05,0.054266386\n",
      "Iteration 18000: loss = 2.458233e-05,0.048897855\n",
      "Iteration 18005: loss = 2.5557953e-05,0.044328146\n",
      "Iteration 18010: loss = 2.6491309e-05,0.04042456\n",
      "Iteration 18015: loss = 2.7383583e-05,0.037069704\n",
      "Iteration 18020: loss = 2.8240167e-05,0.0341558\n",
      "Iteration 18025: loss = 2.9066723e-05,0.03160304\n",
      "Iteration 18030: loss = 2.9866991e-05,0.029348698\n",
      "Iteration 18035: loss = 3.064172e-05,0.027349198\n",
      "Iteration 18040: loss = 3.1390216e-05,0.025574181\n",
      "Iteration 18045: loss = 3.2113523e-05,0.023992386\n",
      "Iteration 18050: loss = 3.2815118e-05,0.022572737\n",
      "Iteration 18055: loss = 3.349865e-05,0.021290759\n",
      "Iteration 18060: loss = 3.4162917e-05,0.020132376\n",
      "Iteration 18065: loss = 3.4810244e-05,0.019080065\n",
      "Iteration 18070: loss = 3.544042e-05,0.018124286\n",
      "Iteration 18075: loss = 3.605426e-05,0.017252063\n",
      "Iteration 18080: loss = 3.66555e-05,0.016452536\n",
      "Iteration 18085: loss = 3.7240166e-05,0.01572135\n",
      "Iteration 18090: loss = 3.7811635e-05,0.015049376\n",
      "Iteration 18095: loss = 3.837908e-05,0.014422419\n",
      "Iteration 18100: loss = 3.892062e-05,0.0138555225\n",
      "Iteration 18105: loss = 3.9464914e-05,0.013319301\n",
      "Iteration 18110: loss = 3.998815e-05,0.012831102\n",
      "Iteration 18115: loss = 4.0511604e-05,0.01236938\n",
      "Iteration 18120: loss = 4.101329e-05,0.011948417\n",
      "Iteration 18125: loss = 4.152807e-05,0.011543621\n",
      "Iteration 18130: loss = 4.198393e-05,0.011199627\n",
      "Iteration 18135: loss = 4.2509946e-05,0.010854365\n",
      "Iteration 18140: loss = 4.270989e-05,0.010869941\n",
      "Iteration 18145: loss = 4.339787e-05,0.011965183\n",
      "Iteration 18150: loss = 4.178463e-05,0.02654023\n",
      "Iteration 18155: loss = 3.8079314e-05,0.14307769\n",
      "Iteration 18160: loss = 1.9554516e-05,0.19331686\n",
      "Iteration 18165: loss = 1.1530737e-05,0.29763904\n",
      "Iteration 18170: loss = 8.267777e-06,0.3534445\n",
      "Iteration 18175: loss = 7.0221904e-06,0.40524808\n",
      "Iteration 18180: loss = 6.7686474e-06,0.42274952\n",
      "Iteration 18185: loss = 7.1001814e-06,0.40128618\n",
      "Iteration 18190: loss = 7.807172e-06,0.3557078\n",
      "Iteration 18195: loss = 8.78371e-06,0.30151796\n",
      "Iteration 18200: loss = 9.9296785e-06,0.25075775\n",
      "Iteration 18205: loss = 1.116615e-05,0.20851618\n",
      "Iteration 18210: loss = 1.2449549e-05,0.17444746\n",
      "Iteration 18215: loss = 1.37561865e-05,0.14647454\n",
      "Iteration 18220: loss = 1.5061788e-05,0.12334515\n",
      "Iteration 18225: loss = 1.6345377e-05,0.10464846\n",
      "Iteration 18230: loss = 1.7586131e-05,0.08984528\n",
      "Iteration 18235: loss = 1.8764667e-05,0.07801978\n",
      "Iteration 18240: loss = 1.9875471e-05,0.068435036\n",
      "Iteration 18245: loss = 2.0931884e-05,0.060601085\n",
      "Iteration 18250: loss = 2.1952155e-05,0.054042\n",
      "Iteration 18255: loss = 2.2944907e-05,0.04842886\n",
      "Iteration 18260: loss = 2.3901213e-05,0.04367748\n",
      "Iteration 18265: loss = 2.480859e-05,0.03967079\n",
      "Iteration 18270: loss = 2.5672192e-05,0.036256127\n",
      "Iteration 18275: loss = 2.6507187e-05,0.03329011\n",
      "Iteration 18280: loss = 2.7320784e-05,0.03067628\n",
      "Iteration 18285: loss = 2.810314e-05,0.028397892\n",
      "Iteration 18290: loss = 2.8849308e-05,0.026413413\n",
      "Iteration 18295: loss = 2.9574729e-05,0.02464693\n",
      "Iteration 18300: loss = 3.0281852e-05,0.023065602\n",
      "Iteration 18305: loss = 3.0962165e-05,0.021662608\n",
      "Iteration 18310: loss = 3.162162e-05,0.020405525\n",
      "Iteration 18315: loss = 3.2266355e-05,0.019265957\n",
      "Iteration 18320: loss = 3.2892913e-05,0.01823792\n",
      "Iteration 18325: loss = 3.3497836e-05,0.017311405\n",
      "Iteration 18330: loss = 3.4096465e-05,0.016457634\n",
      "Iteration 18335: loss = 3.466947e-05,0.01569143\n",
      "Iteration 18340: loss = 3.5243753e-05,0.014974394\n",
      "Iteration 18345: loss = 3.5783956e-05,0.014338839\n",
      "Iteration 18350: loss = 3.633996e-05,0.013728534\n",
      "Iteration 18355: loss = 3.6835176e-05,0.0132146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18360: loss = 3.7406015e-05,0.012703783\n",
      "Iteration 18365: loss = 3.7631882e-05,0.012770405\n",
      "Iteration 18370: loss = 3.844989e-05,0.014942945\n",
      "Iteration 18375: loss = 3.6326994e-05,0.04439001\n",
      "Iteration 18380: loss = 3.4026227e-05,0.22308564\n",
      "Iteration 18385: loss = 1.8220246e-05,0.0891679\n",
      "Iteration 18390: loss = 1.1343396e-05,0.25149032\n",
      "Iteration 18395: loss = 8.470862e-06,0.33809772\n",
      "Iteration 18400: loss = 7.3902193e-06,0.3654304\n",
      "Iteration 18405: loss = 7.2282696e-06,0.3582219\n",
      "Iteration 18410: loss = 7.5944354e-06,0.3309323\n",
      "Iteration 18415: loss = 8.30621e-06,0.29383197\n",
      "Iteration 18420: loss = 9.260494e-06,0.2528514\n",
      "Iteration 18425: loss = 1.03516395e-05,0.21341181\n",
      "Iteration 18430: loss = 1.15049725e-05,0.17888704\n",
      "Iteration 18435: loss = 1.2678308e-05,0.15037286\n",
      "Iteration 18440: loss = 1.3852987e-05,0.1275383\n",
      "Iteration 18445: loss = 1.502104e-05,0.10903329\n",
      "Iteration 18450: loss = 1.6180811e-05,0.09357625\n",
      "Iteration 18455: loss = 1.7321292e-05,0.080720276\n",
      "Iteration 18460: loss = 1.8424396e-05,0.07027869\n",
      "Iteration 18465: loss = 1.9474382e-05,0.06182089\n",
      "Iteration 18470: loss = 2.0465037e-05,0.054872755\n",
      "Iteration 18475: loss = 2.1407008e-05,0.04910822\n",
      "Iteration 18480: loss = 2.2317976e-05,0.04422009\n",
      "Iteration 18485: loss = 2.3205279e-05,0.039997227\n",
      "Iteration 18490: loss = 2.4062363e-05,0.036373463\n",
      "Iteration 18495: loss = 2.4879477e-05,0.033281203\n",
      "Iteration 18500: loss = 2.5658308e-05,0.030623773\n",
      "Iteration 18505: loss = 2.641078e-05,0.028300704\n",
      "Iteration 18510: loss = 2.7145034e-05,0.026239878\n",
      "Iteration 18515: loss = 2.7853295e-05,0.024427397\n",
      "Iteration 18520: loss = 2.8530512e-05,0.022838056\n",
      "Iteration 18525: loss = 2.9189285e-05,0.021416388\n",
      "Iteration 18530: loss = 2.9834062e-05,0.020135181\n",
      "Iteration 18535: loss = 3.0453366e-05,0.0189951\n",
      "Iteration 18540: loss = 3.1057993e-05,0.01796422\n",
      "Iteration 18545: loss = 3.1650663e-05,0.01702495\n",
      "Iteration 18550: loss = 3.2221094e-05,0.016183173\n",
      "Iteration 18555: loss = 3.278463e-05,0.015406974\n",
      "Iteration 18560: loss = 3.332952e-05,0.014705306\n",
      "Iteration 18565: loss = 3.386841e-05,0.014056044\n",
      "Iteration 18570: loss = 3.4386987e-05,0.013469063\n",
      "Iteration 18575: loss = 3.4909408e-05,0.012916095\n",
      "Iteration 18580: loss = 3.5389745e-05,0.012435837\n",
      "Iteration 18585: loss = 3.5921417e-05,0.011958361\n",
      "Iteration 18590: loss = 3.6228812e-05,0.01178138\n",
      "Iteration 18595: loss = 3.688138e-05,0.012291702\n",
      "Iteration 18600: loss = 3.581704e-05,0.023041733\n",
      "Iteration 18605: loss = 3.607664e-05,0.11670093\n",
      "Iteration 18610: loss = 2.0250092e-05,0.21536149\n",
      "Iteration 18615: loss = 1.1861095e-05,0.2395681\n",
      "Iteration 18620: loss = 8.200721e-06,0.28968006\n",
      "Iteration 18625: loss = 6.7074575e-06,0.35944104\n",
      "Iteration 18630: loss = 6.276196e-06,0.39159238\n",
      "Iteration 18635: loss = 6.4527376e-06,0.37963706\n",
      "Iteration 18640: loss = 7.0029478e-06,0.34087655\n",
      "Iteration 18645: loss = 7.820244e-06,0.2914476\n",
      "Iteration 18650: loss = 8.803152e-06,0.24403481\n",
      "Iteration 18655: loss = 9.882176e-06,0.20363554\n",
      "Iteration 18660: loss = 1.1015712e-05,0.17050925\n",
      "Iteration 18665: loss = 1.2179112e-05,0.14321658\n",
      "Iteration 18670: loss = 1.3350948e-05,0.12055287\n",
      "Iteration 18675: loss = 1.45167105e-05,0.10192022\n",
      "Iteration 18680: loss = 1.5655392e-05,0.08700576\n",
      "Iteration 18685: loss = 1.6746639e-05,0.07514167\n",
      "Iteration 18690: loss = 1.7779623e-05,0.06556474\n",
      "Iteration 18695: loss = 1.8759296e-05,0.057747994\n",
      "Iteration 18700: loss = 1.9698344e-05,0.051298685\n",
      "Iteration 18705: loss = 2.0612e-05,0.045821507\n",
      "Iteration 18710: loss = 2.1498468e-05,0.04115209\n",
      "Iteration 18715: loss = 2.2348848e-05,0.03720929\n",
      "Iteration 18720: loss = 2.31557e-05,0.03387623\n",
      "Iteration 18725: loss = 2.3924265e-05,0.031032922\n",
      "Iteration 18730: loss = 2.4670828e-05,0.028548792\n",
      "Iteration 18735: loss = 2.539441e-05,0.02637167\n",
      "Iteration 18740: loss = 2.6089672e-05,0.024473213\n",
      "Iteration 18745: loss = 2.675611e-05,0.02281145\n",
      "Iteration 18750: loss = 2.7402888e-05,0.021336004\n",
      "Iteration 18755: loss = 2.8033748e-05,0.020014508\n",
      "Iteration 18760: loss = 2.864284e-05,0.0188388\n",
      "Iteration 18765: loss = 2.923418e-05,0.017784134\n",
      "Iteration 18770: loss = 2.9810499e-05,0.016832577\n",
      "Iteration 18775: loss = 3.0373194e-05,0.015970472\n",
      "Iteration 18780: loss = 3.091923e-05,0.015191651\n",
      "Iteration 18785: loss = 3.1454776e-05,0.014480487\n",
      "Iteration 18790: loss = 3.1977488e-05,0.013831943\n",
      "Iteration 18795: loss = 3.248881e-05,0.013238728\n",
      "Iteration 18800: loss = 3.298721e-05,0.012697207\n",
      "Iteration 18805: loss = 3.3486005e-05,0.012191115\n",
      "Iteration 18810: loss = 3.394655e-05,0.011750031\n",
      "Iteration 18815: loss = 3.4455592e-05,0.011315651\n",
      "Iteration 18820: loss = 3.473098e-05,0.011199827\n",
      "Iteration 18825: loss = 3.538524e-05,0.012053004\n",
      "Iteration 18830: loss = 3.405604e-05,0.027136862\n",
      "Iteration 18835: loss = 3.3042583e-05,0.15816312\n",
      "Iteration 18840: loss = 1.542807e-05,0.18055181\n",
      "Iteration 18845: loss = 8.208564e-06,0.35122335\n",
      "Iteration 18850: loss = 5.5223645e-06,0.4508265\n",
      "Iteration 18855: loss = 4.597875e-06,0.50601774\n",
      "Iteration 18860: loss = 4.486066e-06,0.50868577\n",
      "Iteration 18865: loss = 4.8366005e-06,0.468252\n",
      "Iteration 18870: loss = 5.503273e-06,0.40660107\n",
      "Iteration 18875: loss = 6.4036444e-06,0.33927265\n",
      "Iteration 18880: loss = 7.4477757e-06,0.27782744\n",
      "Iteration 18885: loss = 8.563067e-06,0.22645748\n",
      "Iteration 18890: loss = 9.708794e-06,0.18537818\n",
      "Iteration 18895: loss = 1.0858057e-05,0.15325542\n",
      "Iteration 18900: loss = 1.2003296e-05,0.12791571\n",
      "Iteration 18905: loss = 1.3138059e-05,0.10755922\n",
      "Iteration 18910: loss = 1.4253416e-05,0.09102445\n",
      "Iteration 18915: loss = 1.5336575e-05,0.07772134\n",
      "Iteration 18920: loss = 1.6377442e-05,0.06708548\n",
      "Iteration 18925: loss = 1.7365457e-05,0.05854122\n",
      "Iteration 18930: loss = 1.8298253e-05,0.05159174\n",
      "Iteration 18935: loss = 1.918779e-05,0.0458634\n",
      "Iteration 18940: loss = 2.0045723e-05,0.04104784\n",
      "Iteration 18945: loss = 2.0876027e-05,0.03694922\n",
      "Iteration 18950: loss = 2.1674094e-05,0.033471517\n",
      "Iteration 18955: loss = 2.2431626e-05,0.030532038\n",
      "Iteration 18960: loss = 2.3152666e-05,0.02802328\n",
      "Iteration 18965: loss = 2.384618e-05,0.025850447\n",
      "Iteration 18970: loss = 2.452083e-05,0.023938186\n",
      "Iteration 18975: loss = 2.517164e-05,0.022261871\n",
      "Iteration 18980: loss = 2.5796122e-05,0.02079407\n",
      "Iteration 18985: loss = 2.6401549e-05,0.019491559\n",
      "Iteration 18990: loss = 2.6991127e-05,0.018326156\n",
      "Iteration 18995: loss = 2.7563852e-05,0.017284134\n",
      "Iteration 19000: loss = 2.8117096e-05,0.016354438\n",
      "Iteration 19005: loss = 2.8661423e-05,0.015508337\n",
      "Iteration 19010: loss = 2.9189083e-05,0.01474784\n",
      "Iteration 19015: loss = 2.9702089e-05,0.014059092\n",
      "Iteration 19020: loss = 3.0209469e-05,0.01342629\n",
      "Iteration 19025: loss = 3.069676e-05,0.01285887\n",
      "Iteration 19030: loss = 3.118771e-05,0.012326246\n",
      "Iteration 19035: loss = 3.1643725e-05,0.011861054\n",
      "Iteration 19040: loss = 3.2134347e-05,0.011406324\n",
      "Iteration 19045: loss = 3.2484273e-05,0.011121362\n",
      "Iteration 19050: loss = 3.3017222e-05,0.011002616\n",
      "Iteration 19055: loss = 3.265592e-05,0.013739053\n",
      "Iteration 19060: loss = 3.382818e-05,0.03613321\n",
      "Iteration 19065: loss = 2.5069618e-05,0.18036352\n",
      "Iteration 19070: loss = 1.5867783e-05,0.09362451\n",
      "Iteration 19075: loss = 9.695969e-06,0.2480806\n",
      "Iteration 19080: loss = 6.846322e-06,0.32157308\n",
      "Iteration 19085: loss = 5.7407924e-06,0.36245236\n",
      "Iteration 19090: loss = 5.514035e-06,0.37542218\n",
      "Iteration 19095: loss = 5.7841694e-06,0.35721374\n",
      "Iteration 19100: loss = 6.3920384e-06,0.31491414\n",
      "Iteration 19105: loss = 7.231865e-06,0.26417786\n",
      "Iteration 19110: loss = 8.2133265e-06,0.21795179\n",
      "Iteration 19115: loss = 9.265484e-06,0.18052916\n",
      "Iteration 19120: loss = 1.03389675e-05,0.15028584\n",
      "Iteration 19125: loss = 1.1393621e-05,0.12566648\n",
      "Iteration 19130: loss = 1.2426844e-05,0.10600441\n",
      "Iteration 19135: loss = 1.3444179e-05,0.09034213\n",
      "Iteration 19140: loss = 1.4455731e-05,0.0772939\n",
      "Iteration 19145: loss = 1.5449286e-05,0.0665047\n",
      "Iteration 19150: loss = 1.6401375e-05,0.057855897\n",
      "Iteration 19155: loss = 1.7296261e-05,0.05087037\n",
      "Iteration 19160: loss = 1.8143794e-05,0.045155395\n",
      "Iteration 19165: loss = 1.89686e-05,0.040327188\n",
      "Iteration 19170: loss = 1.9773364e-05,0.036195528\n",
      "Iteration 19175: loss = 2.0541533e-05,0.032730423\n",
      "Iteration 19180: loss = 2.1263128e-05,0.029833721\n",
      "Iteration 19185: loss = 2.1959022e-05,0.027341407\n",
      "Iteration 19190: loss = 2.2638693e-05,0.025156414\n",
      "Iteration 19195: loss = 2.328793e-05,0.023276208\n",
      "Iteration 19200: loss = 2.390866e-05,0.021645049\n",
      "Iteration 19205: loss = 2.4515999e-05,0.020193767\n",
      "Iteration 19210: loss = 2.510585e-05,0.018908253\n",
      "Iteration 19215: loss = 2.5670312e-05,0.017781585\n",
      "Iteration 19220: loss = 2.6228621e-05,0.016758896\n",
      "Iteration 19225: loss = 2.6765352e-05,0.015853098\n",
      "Iteration 19230: loss = 2.7290023e-05,0.015035421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19235: loss = 2.7805887e-05,0.014293432\n",
      "Iteration 19240: loss = 2.830237e-05,0.013629703\n",
      "Iteration 19245: loss = 2.880112e-05,0.013014503\n",
      "Iteration 19250: loss = 2.9261646e-05,0.012482767\n",
      "Iteration 19255: loss = 2.9767267e-05,0.011964981\n",
      "Iteration 19260: loss = 3.0062214e-05,0.011793598\n",
      "Iteration 19265: loss = 3.0719944e-05,0.012707913\n",
      "Iteration 19270: loss = 2.9426134e-05,0.030405136\n",
      "Iteration 19275: loss = 2.9751945e-05,0.1803914\n",
      "Iteration 19280: loss = 1.5024715e-05,0.12648593\n",
      "Iteration 19285: loss = 8.258299e-06,0.29550892\n",
      "Iteration 19290: loss = 5.573728e-06,0.39179346\n",
      "Iteration 19295: loss = 4.606051e-06,0.44130954\n",
      "Iteration 19300: loss = 4.4415433e-06,0.44270134\n",
      "Iteration 19305: loss = 4.719092e-06,0.41007084\n",
      "Iteration 19310: loss = 5.283396e-06,0.35939926\n",
      "Iteration 19315: loss = 6.052703e-06,0.3045213\n",
      "Iteration 19320: loss = 6.950433e-06,0.25297365\n",
      "Iteration 19325: loss = 7.9139545e-06,0.20892827\n",
      "Iteration 19330: loss = 8.914715e-06,0.17233875\n",
      "Iteration 19335: loss = 9.929795e-06,0.14283246\n",
      "Iteration 19340: loss = 1.0947929e-05,0.11930376\n",
      "Iteration 19345: loss = 1.19545375e-05,0.10052388\n",
      "Iteration 19350: loss = 1.2944398e-05,0.08531396\n",
      "Iteration 19355: loss = 1.3913715e-05,0.07286411\n",
      "Iteration 19360: loss = 1.4855028e-05,0.062762596\n",
      "Iteration 19365: loss = 1.5758667e-05,0.05460949\n",
      "Iteration 19370: loss = 1.6616572e-05,0.04799112\n",
      "Iteration 19375: loss = 1.7430744e-05,0.042556446\n",
      "Iteration 19380: loss = 1.8208155e-05,0.0380531\n",
      "Iteration 19385: loss = 1.8957913e-05,0.034247942\n",
      "Iteration 19390: loss = 1.968138e-05,0.031007469\n",
      "Iteration 19395: loss = 2.03763e-05,0.028252115\n",
      "Iteration 19400: loss = 2.1035208e-05,0.025917577\n",
      "Iteration 19405: loss = 2.1667867e-05,0.023905633\n",
      "Iteration 19410: loss = 2.2280989e-05,0.022148144\n",
      "Iteration 19415: loss = 2.287914e-05,0.020598145\n",
      "Iteration 19420: loss = 2.345407e-05,0.019243054\n",
      "Iteration 19425: loss = 2.4009909e-05,0.018048482\n",
      "Iteration 19430: loss = 2.4552399e-05,0.01698482\n",
      "Iteration 19435: loss = 2.5082809e-05,0.01602989\n",
      "Iteration 19440: loss = 2.5594649e-05,0.01518254\n",
      "Iteration 19445: loss = 2.60974e-05,0.014416955\n",
      "Iteration 19450: loss = 2.6590424e-05,0.0137238875\n",
      "Iteration 19455: loss = 2.7067463e-05,0.013103356\n",
      "Iteration 19460: loss = 2.753819e-05,0.012536472\n",
      "Iteration 19465: loss = 2.8002527e-05,0.012018015\n",
      "Iteration 19470: loss = 2.8449866e-05,0.0115531515\n",
      "Iteration 19475: loss = 2.8901895e-05,0.011117623\n",
      "Iteration 19480: loss = 2.9329492e-05,0.0107315825\n",
      "Iteration 19485: loss = 2.9779208e-05,0.0103613585\n",
      "Iteration 19490: loss = 3.0131205e-05,0.010098597\n",
      "Iteration 19495: loss = 3.0613406e-05,0.00997666\n",
      "Iteration 19500: loss = 3.0320409e-05,0.012288813\n",
      "Iteration 19505: loss = 3.1490534e-05,0.036351413\n",
      "Iteration 19510: loss = 2.1172318e-05,0.21612358\n",
      "Iteration 19515: loss = 1.1922498e-05,0.105186366\n",
      "Iteration 19520: loss = 6.6904868e-06,0.31077546\n",
      "Iteration 19525: loss = 4.5604925e-06,0.44689065\n",
      "Iteration 19530: loss = 3.8352914e-06,0.49827316\n",
      "Iteration 19535: loss = 3.7729703e-06,0.4876828\n",
      "Iteration 19540: loss = 4.08635e-06,0.44155836\n",
      "Iteration 19545: loss = 4.658304e-06,0.38057524\n",
      "Iteration 19550: loss = 5.404678e-06,0.31920663\n",
      "Iteration 19555: loss = 6.266209e-06,0.26379952\n",
      "Iteration 19560: loss = 7.2100665e-06,0.21589015\n",
      "Iteration 19565: loss = 8.211165e-06,0.17583925\n",
      "Iteration 19570: loss = 9.24043e-06,0.14361228\n",
      "Iteration 19575: loss = 1.027391e-05,0.11830816\n",
      "Iteration 19580: loss = 1.128258e-05,0.098618805\n",
      "Iteration 19585: loss = 1.2249001e-05,0.083151646\n",
      "Iteration 19590: loss = 1.31692e-05,0.07090452\n",
      "Iteration 19595: loss = 1.4052467e-05,0.06111655\n",
      "Iteration 19600: loss = 1.4910026e-05,0.053117305\n",
      "Iteration 19605: loss = 1.5744019e-05,0.046469543\n",
      "Iteration 19610: loss = 1.654797e-05,0.040982604\n",
      "Iteration 19615: loss = 1.7309538e-05,0.03649901\n",
      "Iteration 19620: loss = 1.8026276e-05,0.032807432\n",
      "Iteration 19625: loss = 1.8707256e-05,0.029724859\n",
      "Iteration 19630: loss = 1.9367451e-05,0.02707711\n",
      "Iteration 19635: loss = 2.0006248e-05,0.024800248\n",
      "Iteration 19640: loss = 2.0618476e-05,0.022849062\n",
      "Iteration 19645: loss = 2.1202759e-05,0.021174228\n",
      "Iteration 19650: loss = 2.177011e-05,0.019706301\n",
      "Iteration 19655: loss = 2.2325463e-05,0.018405905\n",
      "Iteration 19660: loss = 2.285974e-05,0.017269121\n",
      "Iteration 19665: loss = 2.3377454e-05,0.016264614\n",
      "Iteration 19670: loss = 2.3887029e-05,0.015360553\n",
      "Iteration 19675: loss = 2.4379297e-05,0.014559862\n",
      "Iteration 19680: loss = 2.4860617e-05,0.013841231\n",
      "Iteration 19685: loss = 2.5335708e-05,0.013189997\n",
      "Iteration 19690: loss = 2.579196e-05,0.012610652\n",
      "Iteration 19695: loss = 2.625357e-05,0.012072548\n",
      "Iteration 19700: loss = 2.6681482e-05,0.011608625\n",
      "Iteration 19705: loss = 2.7140879e-05,0.01115777\n",
      "Iteration 19710: loss = 2.7486152e-05,0.010852862\n",
      "Iteration 19715: loss = 2.796711e-05,0.010642936\n",
      "Iteration 19720: loss = 2.779249e-05,0.012185856\n",
      "Iteration 19725: loss = 2.8743516e-05,0.024573851\n",
      "Iteration 19730: loss = 2.3627586e-05,0.12094289\n",
      "Iteration 19735: loss = 1.7373603e-05,0.12167781\n",
      "Iteration 19740: loss = 9.979035e-06,0.1833751\n",
      "Iteration 19745: loss = 6.55723e-06,0.23834741\n",
      "Iteration 19750: loss = 5.2289197e-06,0.31923383\n",
      "Iteration 19755: loss = 4.867537e-06,0.35267666\n",
      "Iteration 19760: loss = 5.029448e-06,0.33244726\n",
      "Iteration 19765: loss = 5.519043e-06,0.28943497\n",
      "Iteration 19770: loss = 6.204749e-06,0.24651575\n",
      "Iteration 19775: loss = 6.990604e-06,0.20846008\n",
      "Iteration 19780: loss = 7.818238e-06,0.17445338\n",
      "Iteration 19785: loss = 8.671292e-06,0.14569002\n",
      "Iteration 19790: loss = 9.548107e-06,0.12253555\n",
      "Iteration 19795: loss = 1.0458995e-05,0.10288038\n",
      "Iteration 19800: loss = 1.1386015e-05,0.08626683\n",
      "Iteration 19805: loss = 1.2294881e-05,0.073097266\n",
      "Iteration 19810: loss = 1.3152363e-05,0.06272886\n",
      "Iteration 19815: loss = 1.3959667e-05,0.05444766\n",
      "Iteration 19820: loss = 1.4746948e-05,0.04763697\n",
      "Iteration 19825: loss = 1.552501e-05,0.041844923\n",
      "Iteration 19830: loss = 1.627507e-05,0.037069105\n",
      "Iteration 19835: loss = 1.6973188e-05,0.0331958\n",
      "Iteration 19840: loss = 1.7637403e-05,0.02997564\n",
      "Iteration 19845: loss = 1.8289304e-05,0.027190348\n",
      "Iteration 19850: loss = 1.8914929e-05,0.024828179\n",
      "Iteration 19855: loss = 1.9506795e-05,0.022836456\n",
      "Iteration 19860: loss = 2.0083835e-05,0.02110039\n",
      "Iteration 19865: loss = 2.064702e-05,0.019581132\n",
      "Iteration 19870: loss = 2.1184904e-05,0.018271143\n",
      "Iteration 19875: loss = 2.1710925e-05,0.017112792\n",
      "Iteration 19880: loss = 2.2226062e-05,0.01608407\n",
      "Iteration 19885: loss = 2.2719503e-05,0.015186234\n",
      "Iteration 19890: loss = 2.3215134e-05,0.01436632\n",
      "Iteration 19895: loss = 2.367523e-05,0.013668348\n",
      "Iteration 19900: loss = 2.416484e-05,0.012999617\n",
      "Iteration 19905: loss = 2.4546893e-05,0.012536581\n",
      "Iteration 19910: loss = 2.5075939e-05,0.012177166\n",
      "Iteration 19915: loss = 2.4941799e-05,0.014183948\n",
      "Iteration 19920: loss = 2.5935535e-05,0.030780034\n",
      "Iteration 19925: loss = 2.0416664e-05,0.15450853\n",
      "Iteration 19930: loss = 1.3769408e-05,0.10896475\n",
      "Iteration 19935: loss = 7.898987e-06,0.23998201\n",
      "Iteration 19940: loss = 5.2827636e-06,0.30835488\n",
      "Iteration 19945: loss = 4.3105074e-06,0.36531827\n",
      "Iteration 19950: loss = 4.1014614e-06,0.38770425\n",
      "Iteration 19955: loss = 4.307045e-06,0.36890125\n",
      "Iteration 19960: loss = 4.7901253e-06,0.3228916\n",
      "Iteration 19965: loss = 5.456281e-06,0.2703934\n",
      "Iteration 19970: loss = 6.2482322e-06,0.22302331\n",
      "Iteration 19975: loss = 7.1203526e-06,0.18350644\n",
      "Iteration 19980: loss = 8.0245745e-06,0.15093811\n",
      "Iteration 19985: loss = 8.926826e-06,0.12452939\n",
      "Iteration 19990: loss = 9.816426e-06,0.10377366\n",
      "Iteration 19995: loss = 1.0700554e-05,0.08720912\n",
      "Iteration 20000: loss = 1.15833e-05,0.07349418\n",
      "Iteration 20005: loss = 1.2454907e-05,0.0623523\n",
      "Iteration 20010: loss = 1.32898895e-05,0.053533074\n",
      "Iteration 20015: loss = 1.4075598e-05,0.046495583\n",
      "Iteration 20020: loss = 1.4822278e-05,0.040828228\n",
      "Iteration 20025: loss = 1.5550051e-05,0.036085706\n",
      "Iteration 20030: loss = 1.6253294e-05,0.032135617\n",
      "Iteration 20035: loss = 1.6913455e-05,0.028910328\n",
      "Iteration 20040: loss = 1.7538294e-05,0.026232425\n",
      "Iteration 20045: loss = 1.8147703e-05,0.02393086\n",
      "Iteration 20050: loss = 1.8739747e-05,0.02195158\n",
      "Iteration 20055: loss = 1.9299936e-05,0.020280475\n",
      "Iteration 20060: loss = 1.9846431e-05,0.018824007\n",
      "Iteration 20065: loss = 2.0378799e-05,0.017551132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20070: loss = 2.0891639e-05,0.016445387\n",
      "Iteration 20075: loss = 2.1394315e-05,0.0154670505\n",
      "Iteration 20080: loss = 2.1884154e-05,0.014602867\n",
      "Iteration 20085: loss = 2.2358729e-05,0.013842089\n",
      "Iteration 20090: loss = 2.2831482e-05,0.013153793\n",
      "Iteration 20095: loss = 2.3277333e-05,0.012559343\n",
      "Iteration 20100: loss = 2.3745813e-05,0.011998701\n",
      "Iteration 20105: loss = 2.4118272e-05,0.0116031235\n",
      "Iteration 20110: loss = 2.4622894e-05,0.011343871\n",
      "Iteration 20115: loss = 2.4456385e-05,0.013577886\n",
      "Iteration 20120: loss = 2.5472016e-05,0.034686476\n",
      "Iteration 20125: loss = 1.9087498e-05,0.1757187\n",
      "Iteration 20130: loss = 1.1837445e-05,0.08703506\n",
      "Iteration 20135: loss = 6.8195245e-06,0.2549067\n",
      "Iteration 20140: loss = 4.6329947e-06,0.3422674\n",
      "Iteration 20145: loss = 3.843235e-06,0.38911697\n",
      "Iteration 20150: loss = 3.7226694e-06,0.40029415\n",
      "Iteration 20155: loss = 3.9632832e-06,0.37459323\n",
      "Iteration 20160: loss = 4.4426038e-06,0.32805008\n",
      "Iteration 20165: loss = 5.077151e-06,0.2747495\n",
      "Iteration 20170: loss = 5.841934e-06,0.22565286\n",
      "Iteration 20175: loss = 6.701586e-06,0.18386802\n",
      "Iteration 20180: loss = 7.592053e-06,0.15028706\n",
      "Iteration 20185: loss = 8.4711155e-06,0.123383835\n",
      "Iteration 20190: loss = 9.330167e-06,0.10246274\n",
      "Iteration 20195: loss = 1.0187676e-05,0.085743874\n",
      "Iteration 20200: loss = 1.1043179e-05,0.07202201\n",
      "Iteration 20205: loss = 1.1888959e-05,0.06086481\n",
      "Iteration 20210: loss = 1.2700698e-05,0.05206403\n",
      "Iteration 20215: loss = 1.3462042e-05,0.045095656\n",
      "Iteration 20220: loss = 1.4184654e-05,0.039505236\n",
      "Iteration 20225: loss = 1.4885961e-05,0.034868795\n",
      "Iteration 20230: loss = 1.556553e-05,0.031004619\n",
      "Iteration 20235: loss = 1.6206885e-05,0.027849833\n",
      "Iteration 20240: loss = 1.6810956e-05,0.025249183\n",
      "Iteration 20245: loss = 1.7400673e-05,0.023019113\n",
      "Iteration 20250: loss = 1.797368e-05,0.021110833\n",
      "Iteration 20255: loss = 1.851443e-05,0.01950854\n",
      "Iteration 20260: loss = 1.9042325e-05,0.018115457\n",
      "Iteration 20265: loss = 1.9561414e-05,0.01689265\n",
      "Iteration 20270: loss = 2.0053063e-05,0.015850734\n",
      "Iteration 20275: loss = 2.0545316e-05,0.0149127515\n",
      "Iteration 20280: loss = 2.1014499e-05,0.014104077\n",
      "Iteration 20285: loss = 2.1481957e-05,0.013377918\n",
      "Iteration 20290: loss = 2.1931903e-05,0.0127424905\n",
      "Iteration 20295: loss = 2.238415e-05,0.012164211\n",
      "Iteration 20300: loss = 2.280763e-05,0.011669089\n",
      "Iteration 20305: loss = 2.3263201e-05,0.011201103\n",
      "Iteration 20310: loss = 2.3570637e-05,0.010962854\n",
      "Iteration 20315: loss = 2.410674e-05,0.011263961\n",
      "Iteration 20320: loss = 2.3408966e-05,0.019633858\n",
      "Iteration 20325: loss = 2.4273957e-05,0.10011283\n",
      "Iteration 20330: loss = 1.283305e-05,0.20553915\n",
      "Iteration 20335: loss = 6.3319535e-06,0.26323304\n",
      "Iteration 20340: loss = 3.832204e-06,0.37051192\n",
      "Iteration 20345: loss = 2.9524115e-06,0.4744393\n",
      "Iteration 20350: loss = 2.753363e-06,0.50984305\n",
      "Iteration 20355: loss = 2.9137634e-06,0.4835321\n",
      "Iteration 20360: loss = 3.3014242e-06,0.424436\n",
      "Iteration 20365: loss = 3.8398653e-06,0.35722047\n",
      "Iteration 20370: loss = 4.490534e-06,0.29419792\n",
      "Iteration 20375: loss = 5.2261726e-06,0.24025522\n",
      "Iteration 20380: loss = 6.030869e-06,0.19552836\n",
      "Iteration 20385: loss = 6.8790905e-06,0.15900227\n",
      "Iteration 20390: loss = 7.754782e-06,0.12918185\n",
      "Iteration 20395: loss = 8.639795e-06,0.105306335\n",
      "Iteration 20400: loss = 9.513179e-06,0.08671366\n",
      "Iteration 20405: loss = 1.036064e-05,0.07225714\n",
      "Iteration 20410: loss = 1.1167688e-05,0.060938004\n",
      "Iteration 20415: loss = 1.1936033e-05,0.052019645\n",
      "Iteration 20420: loss = 1.2673074e-05,0.044907197\n",
      "Iteration 20425: loss = 1.3385746e-05,0.039092235\n",
      "Iteration 20430: loss = 1.4070636e-05,0.0343504\n",
      "Iteration 20435: loss = 1.4716615e-05,0.030527191\n",
      "Iteration 20440: loss = 1.532279e-05,0.027417526\n",
      "Iteration 20445: loss = 1.5903874e-05,0.024824763\n",
      "Iteration 20450: loss = 1.6470098e-05,0.022609504\n",
      "Iteration 20455: loss = 1.7015036e-05,0.020736955\n",
      "Iteration 20460: loss = 1.7534521e-05,0.019155221\n",
      "Iteration 20465: loss = 1.8036148e-05,0.01779923\n",
      "Iteration 20470: loss = 1.8531095e-05,0.016606098\n",
      "Iteration 20475: loss = 1.900633e-05,0.015581073\n",
      "Iteration 20480: loss = 1.9469187e-05,0.014684288\n",
      "Iteration 20485: loss = 1.9925019e-05,0.013890849\n",
      "Iteration 20490: loss = 2.036704e-05,0.013196467\n",
      "Iteration 20495: loss = 2.0797519e-05,0.012584649\n",
      "Iteration 20500: loss = 2.122989e-05,0.0120299645\n",
      "Iteration 20505: loss = 2.1640657e-05,0.011550439\n",
      "Iteration 20510: loss = 2.2064845e-05,0.011105516\n",
      "Iteration 20515: loss = 2.2440448e-05,0.010743888\n",
      "Iteration 20520: loss = 2.2872113e-05,0.010413009\n",
      "Iteration 20525: loss = 2.3008586e-05,0.010562395\n",
      "Iteration 20530: loss = 2.362564e-05,0.0128492415\n",
      "Iteration 20535: loss = 2.1843181e-05,0.03968124\n",
      "Iteration 20540: loss = 2.05905e-05,0.17527981\n",
      "Iteration 20545: loss = 9.562865e-06,0.08698773\n",
      "Iteration 20550: loss = 5.2147648e-06,0.28099024\n",
      "Iteration 20555: loss = 3.6414392e-06,0.387219\n",
      "Iteration 20560: loss = 3.140647e-06,0.42033684\n",
      "Iteration 20565: loss = 3.1494922e-06,0.41348538\n",
      "Iteration 20570: loss = 3.4316297e-06,0.38035107\n",
      "Iteration 20575: loss = 3.880478e-06,0.33069506\n",
      "Iteration 20580: loss = 4.44147e-06,0.27755255\n",
      "Iteration 20585: loss = 5.0927433e-06,0.23000892\n",
      "Iteration 20590: loss = 5.8281053e-06,0.18923955\n",
      "Iteration 20595: loss = 6.6219022e-06,0.15429148\n",
      "Iteration 20600: loss = 7.4527493e-06,0.12513474\n",
      "Iteration 20605: loss = 8.288925e-06,0.10233334\n",
      "Iteration 20610: loss = 9.100587e-06,0.08468159\n",
      "Iteration 20615: loss = 9.877135e-06,0.07082211\n",
      "Iteration 20620: loss = 1.0631171e-05,0.059889473\n",
      "Iteration 20625: loss = 1.1376425e-05,0.05094025\n",
      "Iteration 20630: loss = 1.2102185e-05,0.04368838\n",
      "Iteration 20635: loss = 1.2785206e-05,0.037982892\n",
      "Iteration 20640: loss = 1.3419115e-05,0.033466823\n",
      "Iteration 20645: loss = 1.4028119e-05,0.029760838\n",
      "Iteration 20650: loss = 1.4623517e-05,0.026633633\n",
      "Iteration 20655: loss = 1.5187197e-05,0.024071313\n",
      "Iteration 20660: loss = 1.5719015e-05,0.021957293\n",
      "Iteration 20665: loss = 1.6240227e-05,0.020139115\n",
      "Iteration 20670: loss = 1.6745658e-05,0.018586244\n",
      "Iteration 20675: loss = 1.7227094e-05,0.01727612\n",
      "Iteration 20680: loss = 1.7701193e-05,0.016130209\n",
      "Iteration 20685: loss = 1.8163024e-05,0.0151362065\n",
      "Iteration 20690: loss = 1.8606095e-05,0.0142825525\n",
      "Iteration 20695: loss = 1.9050269e-05,0.013518194\n",
      "Iteration 20700: loss = 1.9469599e-05,0.012867763\n",
      "Iteration 20705: loss = 1.9904031e-05,0.012267466\n",
      "Iteration 20710: loss = 2.0278689e-05,0.011802513\n",
      "Iteration 20715: loss = 2.0729794e-05,0.0113765\n",
      "Iteration 20720: loss = 2.08259e-05,0.011785892\n",
      "Iteration 20725: loss = 2.1530846e-05,0.016253097\n",
      "Iteration 20730: loss = 1.905441e-05,0.065144055\n",
      "Iteration 20735: loss = 1.5956553e-05,0.20650427\n",
      "Iteration 20740: loss = 7.0261267e-06,0.14802435\n",
      "Iteration 20745: loss = 3.8501703e-06,0.32564533\n",
      "Iteration 20750: loss = 2.792198e-06,0.46677595\n",
      "Iteration 20755: loss = 2.514942e-06,0.5017919\n",
      "Iteration 20760: loss = 2.6182843e-06,0.4694507\n",
      "Iteration 20765: loss = 2.9494915e-06,0.41168097\n",
      "Iteration 20770: loss = 3.4285495e-06,0.3503716\n",
      "Iteration 20775: loss = 4.018381e-06,0.29136306\n",
      "Iteration 20780: loss = 4.688851e-06,0.23786844\n",
      "Iteration 20785: loss = 5.4187526e-06,0.19239241\n",
      "Iteration 20790: loss = 6.1857368e-06,0.15595213\n",
      "Iteration 20795: loss = 6.9784405e-06,0.12677868\n",
      "Iteration 20800: loss = 7.790999e-06,0.10296685\n",
      "Iteration 20805: loss = 8.606017e-06,0.084039286\n",
      "Iteration 20810: loss = 9.400425e-06,0.06942384\n",
      "Iteration 20815: loss = 1.0153603e-05,0.058137815\n",
      "Iteration 20820: loss = 1.0866545e-05,0.049369436\n",
      "Iteration 20825: loss = 1.1554658e-05,0.04236764\n",
      "Iteration 20830: loss = 1.2220022e-05,0.036686063\n",
      "Iteration 20835: loss = 1.2851936e-05,0.03215171\n",
      "Iteration 20840: loss = 1.3439225e-05,0.028552273\n",
      "Iteration 20845: loss = 1.3998651e-05,0.025609957\n",
      "Iteration 20850: loss = 1.454498e-05,0.023123419\n",
      "Iteration 20855: loss = 1.5070022e-05,0.021050768\n",
      "Iteration 20860: loss = 1.5564192e-05,0.019342475\n",
      "Iteration 20865: loss = 1.6046275e-05,0.017879678\n",
      "Iteration 20870: loss = 1.6517663e-05,0.016618645\n",
      "Iteration 20875: loss = 1.6968115e-05,0.015551962\n",
      "Iteration 20880: loss = 1.7412303e-05,0.014618696\n",
      "Iteration 20885: loss = 1.78434e-05,0.013811967\n",
      "Iteration 20890: loss = 1.8262772e-05,0.013112179\n",
      "Iteration 20895: loss = 1.8682227e-05,0.012487893\n",
      "Iteration 20900: loss = 1.9073974e-05,0.011963882\n",
      "Iteration 20905: loss = 1.9498437e-05,0.01147371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20910: loss = 1.9783012e-05,0.011219994\n",
      "Iteration 20915: loss = 2.0261583e-05,0.011348904\n",
      "Iteration 20920: loss = 1.9801044e-05,0.016721476\n",
      "Iteration 20925: loss = 2.0650697e-05,0.06332962\n",
      "Iteration 20930: loss = 1.2365224e-05,0.19169943\n",
      "Iteration 20935: loss = 6.472272e-06,0.15391046\n",
      "Iteration 20940: loss = 3.8995026e-06,0.3004996\n",
      "Iteration 20945: loss = 2.9022056e-06,0.4239699\n",
      "Iteration 20950: loss = 2.6066602e-06,0.4539711\n",
      "Iteration 20955: loss = 2.6753412e-06,0.4295256\n",
      "Iteration 20960: loss = 2.955292e-06,0.3856353\n",
      "Iteration 20965: loss = 3.374398e-06,0.3349097\n",
      "Iteration 20970: loss = 3.9203305e-06,0.27961254\n",
      "Iteration 20975: loss = 4.5766487e-06,0.2265149\n",
      "Iteration 20980: loss = 5.3148146e-06,0.18171147\n",
      "Iteration 20985: loss = 6.0897087e-06,0.14609602\n",
      "Iteration 20990: loss = 6.857042e-06,0.11812428\n",
      "Iteration 20995: loss = 7.609456e-06,0.09647351\n",
      "Iteration 21000: loss = 8.360112e-06,0.07944614\n",
      "Iteration 21005: loss = 9.1140655e-06,0.06556999\n",
      "Iteration 21010: loss = 9.854187e-06,0.054621197\n",
      "Iteration 21015: loss = 1.055013e-05,0.046225317\n",
      "Iteration 21020: loss = 1.1197614e-05,0.03972742\n",
      "Iteration 21025: loss = 1.1818207e-05,0.034535788\n",
      "Iteration 21030: loss = 1.2423038e-05,0.030258428\n",
      "Iteration 21035: loss = 1.2993983e-05,0.0268439\n",
      "Iteration 21040: loss = 1.3524157e-05,0.024117231\n",
      "Iteration 21045: loss = 1.4041122e-05,0.021827238\n",
      "Iteration 21050: loss = 1.4542721e-05,0.019903492\n",
      "Iteration 21055: loss = 1.5015869e-05,0.018319577\n",
      "Iteration 21060: loss = 1.5475662e-05,0.0169732\n",
      "Iteration 21065: loss = 1.5926498e-05,0.015813742\n",
      "Iteration 21070: loss = 1.6358403e-05,0.014833418\n",
      "Iteration 21075: loss = 1.6784752e-05,0.013979262\n",
      "Iteration 21080: loss = 1.7198347e-05,0.013244126\n",
      "Iteration 21085: loss = 1.7604796e-05,0.012602831\n",
      "Iteration 21090: loss = 1.8003126e-05,0.012044238\n",
      "Iteration 21095: loss = 1.839979e-05,0.011549683\n",
      "Iteration 21100: loss = 1.8779441e-05,0.011126854\n",
      "Iteration 21105: loss = 1.918361e-05,0.010740592\n",
      "Iteration 21110: loss = 1.9444187e-05,0.010589284\n",
      "Iteration 21115: loss = 1.9963609e-05,0.011484826\n",
      "Iteration 21120: loss = 1.8875407e-05,0.029625854\n",
      "Iteration 21125: loss = 1.8465353e-05,0.20701642\n",
      "Iteration 21130: loss = 6.4090686e-06,0.14675291\n",
      "Iteration 21135: loss = 2.5039838e-06,0.49662694\n",
      "Iteration 21140: loss = 1.4540146e-06,0.7302938\n",
      "Iteration 21145: loss = 1.1832375e-06,0.81388634\n",
      "Iteration 21150: loss = 1.2158333e-06,0.7837517\n",
      "Iteration 21155: loss = 1.4144379e-06,0.6958927\n",
      "Iteration 21160: loss = 1.7310653e-06,0.5906996\n",
      "Iteration 21165: loss = 2.155715e-06,0.4865916\n",
      "Iteration 21170: loss = 2.6860478e-06,0.39191994\n",
      "Iteration 21175: loss = 3.316208e-06,0.31042945\n",
      "Iteration 21180: loss = 4.0136415e-06,0.24464375\n",
      "Iteration 21185: loss = 4.7580884e-06,0.19267488\n",
      "Iteration 21190: loss = 5.522294e-06,0.15247355\n",
      "Iteration 21195: loss = 6.292488e-06,0.121378824\n",
      "Iteration 21200: loss = 7.057237e-06,0.09739951\n",
      "Iteration 21205: loss = 7.801212e-06,0.07911012\n",
      "Iteration 21210: loss = 8.523762e-06,0.06502292\n",
      "Iteration 21215: loss = 9.217459e-06,0.05412149\n",
      "Iteration 21220: loss = 9.880834e-06,0.045607097\n",
      "Iteration 21225: loss = 1.0507913e-05,0.03898149\n",
      "Iteration 21230: loss = 1.1094521e-05,0.033823762\n",
      "Iteration 21235: loss = 1.16426045e-05,0.029752238\n",
      "Iteration 21240: loss = 1.2158608e-05,0.026487667\n",
      "Iteration 21245: loss = 1.2651816e-05,0.023813061\n",
      "Iteration 21250: loss = 1.3127607e-05,0.02158378\n",
      "Iteration 21255: loss = 1.3582776e-05,0.019736212\n",
      "Iteration 21260: loss = 1.401504e-05,0.018203825\n",
      "Iteration 21265: loss = 1.4430363e-05,0.016911369\n",
      "Iteration 21270: loss = 1.4837368e-05,0.015797397\n",
      "Iteration 21275: loss = 1.5231265e-05,0.0148454495\n",
      "Iteration 21280: loss = 1.5610522e-05,0.014033648\n",
      "Iteration 21285: loss = 1.5983223e-05,0.013324741\n",
      "Iteration 21290: loss = 1.6349595e-05,0.012705508\n",
      "Iteration 21295: loss = 1.670695e-05,0.012167414\n",
      "Iteration 21300: loss = 1.7059776e-05,0.011692803\n",
      "Iteration 21305: loss = 1.7411583e-05,0.011272653\n",
      "Iteration 21310: loss = 1.7752927e-05,0.010907222\n",
      "Iteration 21315: loss = 1.8099532e-05,0.010577714\n",
      "Iteration 21320: loss = 1.8436083e-05,0.010291256\n",
      "Iteration 21325: loss = 1.8781442e-05,0.010032449\n",
      "Iteration 21330: loss = 1.9098332e-05,0.0098174\n",
      "Iteration 21335: loss = 1.9440331e-05,0.009639505\n",
      "Iteration 21340: loss = 1.9536214e-05,0.009764172\n",
      "Iteration 21345: loss = 2.005587e-05,0.012032523\n",
      "Iteration 21350: loss = 1.7407207e-05,0.04264423\n",
      "Iteration 21355: loss = 1.4281475e-05,0.19579256\n",
      "Iteration 21360: loss = 5.494226e-06,0.15259445\n",
      "Iteration 21365: loss = 2.6337345e-06,0.40783477\n",
      "Iteration 21370: loss = 1.7690395e-06,0.58233094\n",
      "Iteration 21375: loss = 1.5464931e-06,0.6263684\n",
      "Iteration 21380: loss = 1.6195771e-06,0.58963203\n",
      "Iteration 21385: loss = 1.8652316e-06,0.52056634\n",
      "Iteration 21390: loss = 2.2324275e-06,0.4427941\n",
      "Iteration 21395: loss = 2.7109515e-06,0.36469582\n",
      "Iteration 21400: loss = 3.2692162e-06,0.29460466\n",
      "Iteration 21405: loss = 3.884939e-06,0.23555326\n",
      "Iteration 21410: loss = 4.541771e-06,0.188259\n",
      "Iteration 21415: loss = 5.2314704e-06,0.15064274\n",
      "Iteration 21420: loss = 5.955944e-06,0.11997115\n",
      "Iteration 21425: loss = 6.6934476e-06,0.09570303\n",
      "Iteration 21430: loss = 7.419391e-06,0.07718922\n",
      "Iteration 21435: loss = 8.112424e-06,0.06317562\n",
      "Iteration 21440: loss = 8.764812e-06,0.052514352\n",
      "Iteration 21445: loss = 9.384657e-06,0.044292197\n",
      "Iteration 21450: loss = 9.978095e-06,0.037800748\n",
      "Iteration 21455: loss = 1.0543131e-05,0.032662246\n",
      "Iteration 21460: loss = 1.10705905e-05,0.028646402\n",
      "Iteration 21465: loss = 1.1561114e-05,0.025471926\n",
      "Iteration 21470: loss = 1.2030289e-05,0.022880567\n",
      "Iteration 21475: loss = 1.2484149e-05,0.02072448\n",
      "Iteration 21480: loss = 1.2914752e-05,0.018958312\n",
      "Iteration 21485: loss = 1.33226395e-05,0.017502006\n",
      "Iteration 21490: loss = 1.372349e-05,0.016253036\n",
      "Iteration 21495: loss = 1.4110284e-05,0.015198395\n",
      "Iteration 21500: loss = 1.4480828e-05,0.014309443\n",
      "Iteration 21505: loss = 1.4847678e-05,0.013533971\n",
      "Iteration 21510: loss = 1.5203267e-05,0.012870478\n",
      "Iteration 21515: loss = 1.5551735e-05,0.012296598\n",
      "Iteration 21520: loss = 1.5895263e-05,0.0117943045\n",
      "Iteration 21525: loss = 1.6236389e-05,0.01135455\n",
      "Iteration 21530: loss = 1.6567743e-05,0.01097361\n",
      "Iteration 21535: loss = 1.6908101e-05,0.010631477\n",
      "Iteration 21540: loss = 1.7212527e-05,0.01035783\n",
      "Iteration 21545: loss = 1.75603e-05,0.010127248\n",
      "Iteration 21550: loss = 1.7629542e-05,0.010399083\n",
      "Iteration 21555: loss = 1.8210045e-05,0.01415307\n",
      "Iteration 21560: loss = 1.587211e-05,0.061606787\n",
      "Iteration 21565: loss = 1.2248234e-05,0.24747756\n",
      "Iteration 21570: loss = 3.839086e-06,0.26045716\n",
      "Iteration 21575: loss = 1.6586299e-06,0.55016375\n",
      "Iteration 21580: loss = 1.090087e-06,0.7552611\n",
      "Iteration 21585: loss = 9.744646e-07,0.81191564\n",
      "Iteration 21590: loss = 1.0553033e-06,0.7664151\n",
      "Iteration 21595: loss = 1.2563306e-06,0.6748776\n",
      "Iteration 21600: loss = 1.5536934e-06,0.57071865\n",
      "Iteration 21605: loss = 1.9555239e-06,0.46651784\n",
      "Iteration 21610: loss = 2.4544008e-06,0.37277386\n",
      "Iteration 21615: loss = 3.032499e-06,0.29434448\n",
      "Iteration 21620: loss = 3.6657812e-06,0.23157646\n",
      "Iteration 21625: loss = 4.333611e-06,0.18218568\n",
      "Iteration 21630: loss = 5.0269186e-06,0.14329462\n",
      "Iteration 21635: loss = 5.7280654e-06,0.11326572\n",
      "Iteration 21640: loss = 6.420503e-06,0.09047803\n",
      "Iteration 21645: loss = 7.0978444e-06,0.07305668\n",
      "Iteration 21650: loss = 7.755679e-06,0.059602555\n",
      "Iteration 21655: loss = 8.384389e-06,0.049307458\n",
      "Iteration 21660: loss = 8.97349e-06,0.04150122\n",
      "Iteration 21665: loss = 9.515142e-06,0.035569675\n",
      "Iteration 21670: loss = 1.0018665e-05,0.03096791\n",
      "Iteration 21675: loss = 1.0497747e-05,0.027280807\n",
      "Iteration 21680: loss = 1.0956383e-05,0.024281759\n",
      "Iteration 21685: loss = 1.138946e-05,0.021866385\n",
      "Iteration 21690: loss = 1.179566e-05,0.01991287\n",
      "Iteration 21695: loss = 1.21835e-05,0.01829709\n",
      "Iteration 21700: loss = 1.255978e-05,0.01693212\n",
      "Iteration 21705: loss = 1.2921941e-05,0.015785234\n",
      "Iteration 21710: loss = 1.3267984e-05,0.0148236165\n",
      "Iteration 21715: loss = 1.3605139e-05,0.013997827\n",
      "Iteration 21720: loss = 1.39364665e-05,0.013282951\n",
      "Iteration 21725: loss = 1.4257009e-05,0.012671578\n",
      "Iteration 21730: loss = 1.4572438e-05,0.012139371\n",
      "Iteration 21735: loss = 1.4884787e-05,0.011672816\n",
      "Iteration 21740: loss = 1.5192646e-05,0.011265589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21745: loss = 1.5496002e-05,0.010911033\n",
      "Iteration 21750: loss = 1.580233e-05,0.010595296\n",
      "Iteration 21755: loss = 1.609509e-05,0.010327093\n",
      "Iteration 21760: loss = 1.6406757e-05,0.0100851245\n",
      "Iteration 21765: loss = 1.661163e-05,0.0099648535\n",
      "Iteration 21770: loss = 1.696281e-05,0.010256771\n",
      "Iteration 21775: loss = 1.6437107e-05,0.016241772\n",
      "Iteration 21780: loss = 1.7077427e-05,0.0839084\n",
      "Iteration 21785: loss = 7.779819e-06,0.20842701\n",
      "Iteration 21790: loss = 2.9247778e-06,0.3628874\n",
      "Iteration 21795: loss = 1.4728079e-06,0.57577926\n",
      "Iteration 21800: loss = 1.0626484e-06,0.718664\n",
      "Iteration 21805: loss = 1.0129547e-06,0.74420464\n",
      "Iteration 21810: loss = 1.1242977e-06,0.6927737\n",
      "Iteration 21815: loss = 1.3280254e-06,0.61145353\n",
      "Iteration 21820: loss = 1.6193734e-06,0.51920974\n",
      "Iteration 21825: loss = 2.0004675e-06,0.42753288\n",
      "Iteration 21830: loss = 2.4753815e-06,0.34355336\n",
      "Iteration 21835: loss = 3.0165172e-06,0.27345687\n",
      "Iteration 21840: loss = 3.6058818e-06,0.21647334\n",
      "Iteration 21845: loss = 4.246152e-06,0.16969076\n",
      "Iteration 21850: loss = 4.9216164e-06,0.1324841\n",
      "Iteration 21855: loss = 5.610377e-06,0.10389382\n",
      "Iteration 21860: loss = 6.280401e-06,0.08255401\n",
      "Iteration 21865: loss = 6.915143e-06,0.066657715\n",
      "Iteration 21870: loss = 7.5191797e-06,0.05464244\n",
      "Iteration 21875: loss = 8.0922055e-06,0.045487072\n",
      "Iteration 21880: loss = 8.636329e-06,0.038380805\n",
      "Iteration 21885: loss = 9.147716e-06,0.032892786\n",
      "Iteration 21890: loss = 9.621303e-06,0.028659118\n",
      "Iteration 21895: loss = 1.0062843e-05,0.02533061\n",
      "Iteration 21900: loss = 1.047938e-05,0.022666859\n",
      "Iteration 21905: loss = 1.0879093e-05,0.020479467\n",
      "Iteration 21910: loss = 1.1258801e-05,0.018695826\n",
      "Iteration 21915: loss = 1.1615398e-05,0.017243987\n",
      "Iteration 21920: loss = 1.1959783e-05,0.016026119\n",
      "Iteration 21925: loss = 1.229741e-05,0.014985232\n",
      "Iteration 21930: loss = 1.2618032e-05,0.014119261\n",
      "Iteration 21935: loss = 1.2929057e-05,0.013381888\n",
      "Iteration 21940: loss = 1.3238e-05,0.01273935\n",
      "Iteration 21945: loss = 1.3534479e-05,0.012194919\n",
      "Iteration 21950: loss = 1.3830994e-05,0.01171657\n",
      "Iteration 21955: loss = 1.4121301e-05,0.011303529\n",
      "Iteration 21960: loss = 1.4409235e-05,0.0109434845\n",
      "Iteration 21965: loss = 1.46966095e-05,0.010628211\n",
      "Iteration 21970: loss = 1.4978745e-05,0.010355507\n",
      "Iteration 21975: loss = 1.5268093e-05,0.010113968\n",
      "Iteration 21980: loss = 1.5528538e-05,0.00992066\n",
      "Iteration 21985: loss = 1.5818523e-05,0.009784305\n",
      "Iteration 21990: loss = 1.5814958e-05,0.010271781\n",
      "Iteration 21995: loss = 1.6429129e-05,0.017794423\n",
      "Iteration 22000: loss = 1.2350521e-05,0.12192321\n",
      "Iteration 22005: loss = 6.161213e-06,0.2205544\n",
      "Iteration 22010: loss = 1.7238567e-06,0.58405507\n",
      "Iteration 22015: loss = 7.814663e-07,0.88418925\n",
      "Iteration 22020: loss = 5.631006e-07,1.0161761\n",
      "Iteration 22025: loss = 5.6643034e-07,0.9948702\n",
      "Iteration 22030: loss = 6.734631e-07,0.8974152\n",
      "Iteration 22035: loss = 8.443247e-07,0.7806\n",
      "Iteration 22040: loss = 1.0947803e-06,0.6548541\n",
      "Iteration 22045: loss = 1.421288e-06,0.5369671\n",
      "Iteration 22050: loss = 1.8359739e-06,0.42951152\n",
      "Iteration 22055: loss = 2.3172072e-06,0.33966717\n",
      "Iteration 22060: loss = 2.8518537e-06,0.26697162\n",
      "Iteration 22065: loss = 3.4472741e-06,0.20741577\n",
      "Iteration 22070: loss = 4.0840555e-06,0.16027418\n",
      "Iteration 22075: loss = 4.74499e-06,0.123821676\n",
      "Iteration 22080: loss = 5.4021916e-06,0.096533954\n",
      "Iteration 22085: loss = 6.039368e-06,0.07631248\n",
      "Iteration 22090: loss = 6.6486587e-06,0.061266854\n",
      "Iteration 22095: loss = 7.2150992e-06,0.050156903\n",
      "Iteration 22100: loss = 7.737888e-06,0.041878007\n",
      "Iteration 22105: loss = 8.222655e-06,0.035601847\n",
      "Iteration 22110: loss = 8.679076e-06,0.030704273\n",
      "Iteration 22115: loss = 9.110708e-06,0.026831629\n",
      "Iteration 22120: loss = 9.513703e-06,0.023787344\n",
      "Iteration 22125: loss = 9.887682e-06,0.021383425\n",
      "Iteration 22130: loss = 1.0237312e-05,0.01945292\n",
      "Iteration 22135: loss = 1.057062e-05,0.017861221\n",
      "Iteration 22140: loss = 1.0890115e-05,0.016540393\n",
      "Iteration 22145: loss = 1.1196081e-05,0.0154366335\n",
      "Iteration 22150: loss = 1.1488731e-05,0.0145129515\n",
      "Iteration 22155: loss = 1.1770873e-05,0.01373101\n",
      "Iteration 22160: loss = 1.2046757e-05,0.013056742\n",
      "Iteration 22165: loss = 1.2317884e-05,0.012474501\n",
      "Iteration 22170: loss = 1.2578638e-05,0.011979082\n",
      "Iteration 22175: loss = 1.2838675e-05,0.011544528\n",
      "Iteration 22180: loss = 1.3095704e-05,0.011164168\n",
      "Iteration 22185: loss = 1.3349615e-05,0.010834265\n",
      "Iteration 22190: loss = 1.3599652e-05,0.01054741\n",
      "Iteration 22195: loss = 1.3853736e-05,0.0102931075\n",
      "Iteration 22200: loss = 1.4099062e-05,0.010077267\n",
      "Iteration 22205: loss = 1.4355843e-05,0.009883119\n",
      "Iteration 22210: loss = 1.4564321e-05,0.009739232\n",
      "Iteration 22215: loss = 1.4801961e-05,0.00964362\n",
      "Iteration 22220: loss = 1.47790015e-05,0.009959807\n",
      "Iteration 22225: loss = 1.5224358e-05,0.013380384\n",
      "Iteration 22230: loss = 1.3013768e-05,0.050899677\n",
      "Iteration 22235: loss = 1.0000502e-05,0.18333207\n",
      "Iteration 22240: loss = 3.3443132e-06,0.22132744\n",
      "Iteration 22245: loss = 1.4884131e-06,0.5072197\n",
      "Iteration 22250: loss = 9.892467e-07,0.6982347\n",
      "Iteration 22255: loss = 8.9389096e-07,0.73615617\n",
      "Iteration 22260: loss = 9.70307e-07,0.6852615\n",
      "Iteration 22265: loss = 1.1356245e-06,0.60801196\n",
      "Iteration 22270: loss = 1.3815171e-06,0.52240366\n",
      "Iteration 22275: loss = 1.7097392e-06,0.43477154\n",
      "Iteration 22280: loss = 2.1217222e-06,0.35040405\n",
      "Iteration 22285: loss = 2.591235e-06,0.27834257\n",
      "Iteration 22290: loss = 3.1065836e-06,0.21995096\n",
      "Iteration 22295: loss = 3.6776735e-06,0.17171015\n",
      "Iteration 22300: loss = 4.291255e-06,0.13262114\n",
      "Iteration 22305: loss = 4.927461e-06,0.10233344\n",
      "Iteration 22310: loss = 5.5449814e-06,0.08013582\n",
      "Iteration 22315: loss = 6.1271408e-06,0.06385408\n",
      "Iteration 22320: loss = 6.6788125e-06,0.051750682\n",
      "Iteration 22325: loss = 7.204386e-06,0.042558253\n",
      "Iteration 22330: loss = 7.700823e-06,0.035551887\n",
      "Iteration 22335: loss = 8.155543e-06,0.030324034\n",
      "Iteration 22340: loss = 8.570824e-06,0.026359107\n",
      "Iteration 22345: loss = 8.96304e-06,0.0232327\n",
      "Iteration 22350: loss = 9.337044e-06,0.020719483\n",
      "Iteration 22355: loss = 9.685399e-06,0.018736312\n",
      "Iteration 22360: loss = 1.0009325e-05,0.017158894\n",
      "Iteration 22365: loss = 1.0324949e-05,0.015839344\n",
      "Iteration 22370: loss = 1.0626513e-05,0.014752578\n",
      "Iteration 22375: loss = 1.0911178e-05,0.013863991\n",
      "Iteration 22380: loss = 1.1192397e-05,0.013102391\n",
      "Iteration 22385: loss = 1.14601535e-05,0.012470914\n",
      "Iteration 22390: loss = 1.17255595e-05,0.011926237\n",
      "Iteration 22395: loss = 1.1982337e-05,0.011466533\n",
      "Iteration 22400: loss = 1.2241031e-05,0.011064477\n",
      "Iteration 22405: loss = 1.2483616e-05,0.010734445\n",
      "Iteration 22410: loss = 1.2748176e-05,0.01043569\n",
      "Iteration 22415: loss = 1.2910569e-05,0.010317608\n",
      "Iteration 22420: loss = 1.323181e-05,0.010705814\n",
      "Iteration 22425: loss = 1.2690423e-05,0.018176362\n",
      "Iteration 22430: loss = 1.3048694e-05,0.095483236\n",
      "Iteration 22435: loss = 4.9899404e-06,0.2485554\n",
      "Iteration 22440: loss = 1.5765014e-06,0.5282736\n",
      "Iteration 22445: loss = 7.5289114e-07,0.7868075\n",
      "Iteration 22450: loss = 5.459874e-07,0.92012\n",
      "Iteration 22455: loss = 5.407871e-07,0.92175156\n",
      "Iteration 22460: loss = 6.275572e-07,0.85022515\n",
      "Iteration 22465: loss = 7.725084e-07,0.7521441\n",
      "Iteration 22470: loss = 9.846525e-07,0.6404796\n",
      "Iteration 22475: loss = 1.2643271e-06,0.5298575\n",
      "Iteration 22480: loss = 1.6215818e-06,0.42616042\n",
      "Iteration 22485: loss = 2.0420462e-06,0.3375215\n",
      "Iteration 22490: loss = 2.524614e-06,0.26368728\n",
      "Iteration 22495: loss = 3.0709805e-06,0.20281209\n",
      "Iteration 22500: loss = 3.6631945e-06,0.1544336\n",
      "Iteration 22505: loss = 4.2784054e-06,0.11740113\n",
      "Iteration 22510: loss = 4.8911256e-06,0.09006576\n",
      "Iteration 22515: loss = 5.4846264e-06,0.07009663\n",
      "Iteration 22520: loss = 6.0405596e-06,0.055641536\n",
      "Iteration 22525: loss = 6.5555487e-06,0.04513259\n",
      "Iteration 22530: loss = 7.033587e-06,0.03735743\n",
      "Iteration 22535: loss = 7.482424e-06,0.03143891\n",
      "Iteration 22540: loss = 7.901973e-06,0.026927471\n",
      "Iteration 22545: loss = 8.283845e-06,0.023527704\n",
      "Iteration 22550: loss = 8.634502e-06,0.020913636\n",
      "Iteration 22555: loss = 8.964223e-06,0.018837037\n",
      "Iteration 22560: loss = 9.279763e-06,0.01715064\n",
      "Iteration 22565: loss = 9.574615e-06,0.015805148\n",
      "Iteration 22570: loss = 9.8522e-06,0.014714551\n",
      "Iteration 22575: loss = 1.0119758e-05,0.013807206\n",
      "Iteration 22580: loss = 1.03801285e-05,0.013043856\n",
      "Iteration 22585: loss = 1.0627363e-05,0.012414689\n",
      "Iteration 22590: loss = 1.087191e-05,0.011873715\n",
      "Iteration 22595: loss = 1.1109245e-05,0.01141792\n",
      "Iteration 22600: loss = 1.1342603e-05,0.011028887\n",
      "Iteration 22605: loss = 1.1574601e-05,0.010693019\n",
      "Iteration 22610: loss = 1.179957e-05,0.0104108015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22615: loss = 1.2031167e-05,0.010162008\n",
      "Iteration 22620: loss = 1.2241547e-05,0.00996519\n",
      "Iteration 22625: loss = 1.2473061e-05,0.009803048\n",
      "Iteration 22630: loss = 1.2537378e-05,0.0098899435\n",
      "Iteration 22635: loss = 1.2887743e-05,0.011652529\n",
      "Iteration 22640: loss = 1.16492565e-05,0.035774738\n",
      "Iteration 22645: loss = 1.0106261e-05,0.20035829\n",
      "Iteration 22650: loss = 2.6773812e-06,0.23735452\n",
      "Iteration 22655: loss = 9.203606e-07,0.66899467\n",
      "Iteration 22660: loss = 5.345362e-07,0.92078865\n",
      "Iteration 22665: loss = 4.603994e-07,0.9795285\n",
      "Iteration 22670: loss = 5.0514996e-07,0.9222409\n",
      "Iteration 22675: loss = 6.096879e-07,0.8254855\n",
      "Iteration 22680: loss = 7.677615e-07,0.7159442\n",
      "Iteration 22685: loss = 9.894942e-07,0.6026063\n",
      "Iteration 22690: loss = 1.2904279e-06,0.49051338\n",
      "Iteration 22695: loss = 1.6570602e-06,0.39114907\n",
      "Iteration 22700: loss = 2.0836571e-06,0.30657178\n",
      "Iteration 22705: loss = 2.5778174e-06,0.23530394\n",
      "Iteration 22710: loss = 3.1204347e-06,0.1788001\n",
      "Iteration 22715: loss = 3.6993752e-06,0.13510963\n",
      "Iteration 22720: loss = 4.2850675e-06,0.10254946\n",
      "Iteration 22725: loss = 4.868513e-06,0.078329116\n",
      "Iteration 22730: loss = 5.434025e-06,0.060682107\n",
      "Iteration 22735: loss = 5.958804e-06,0.048099846\n",
      "Iteration 22740: loss = 6.432099e-06,0.039157055\n",
      "Iteration 22745: loss = 6.8667377e-06,0.032595422\n",
      "Iteration 22750: loss = 7.27579e-06,0.027586173\n",
      "Iteration 22755: loss = 7.657001e-06,0.023767632\n",
      "Iteration 22760: loss = 8.004638e-06,0.020889692\n",
      "Iteration 22765: loss = 8.321341e-06,0.018690275\n",
      "Iteration 22770: loss = 8.62021e-06,0.01693901\n",
      "Iteration 22775: loss = 8.904831e-06,0.015522291\n",
      "Iteration 22780: loss = 9.172322e-06,0.014387098\n",
      "Iteration 22785: loss = 9.425287e-06,0.013464897\n",
      "Iteration 22790: loss = 9.6689155e-06,0.012701571\n",
      "Iteration 22795: loss = 9.9062945e-06,0.012062305\n",
      "Iteration 22800: loss = 1.013263e-05,0.011537073\n",
      "Iteration 22805: loss = 1.035717e-05,0.011090246\n",
      "Iteration 22810: loss = 1.0574302e-05,0.0107173715\n",
      "Iteration 22815: loss = 1.079002e-05,0.010399062\n",
      "Iteration 22820: loss = 1.1001525e-05,0.010131119\n",
      "Iteration 22825: loss = 1.1213652e-05,0.009902084\n",
      "Iteration 22830: loss = 1.1420903e-05,0.009710425\n",
      "Iteration 22835: loss = 1.1632054e-05,0.009548088\n",
      "Iteration 22840: loss = 1.1819174e-05,0.00942389\n",
      "Iteration 22845: loss = 1.20162e-05,0.009354048\n",
      "Iteration 22850: loss = 1.1977555e-05,0.009789942\n",
      "Iteration 22855: loss = 1.2374229e-05,0.016528174\n",
      "Iteration 22860: loss = 9.270854e-06,0.11056183\n",
      "Iteration 22865: loss = 4.209512e-06,0.24767113\n",
      "Iteration 22870: loss = 9.274078e-07,0.72044635\n",
      "Iteration 22875: loss = 3.8186042e-07,1.0621127\n",
      "Iteration 22880: loss = 2.6539635e-07,1.2043908\n",
      "Iteration 22885: loss = 2.7443238e-07,1.170666\n",
      "Iteration 22890: loss = 3.3754193e-07,1.0618473\n",
      "Iteration 22895: loss = 4.342467e-07,0.9386359\n",
      "Iteration 22900: loss = 5.8084555e-07,0.8035161\n",
      "Iteration 22905: loss = 7.7862677e-07,0.6722797\n",
      "Iteration 22910: loss = 1.0399095e-06,0.5476665\n",
      "Iteration 22915: loss = 1.3587833e-06,0.43818793\n",
      "Iteration 22920: loss = 1.748067e-06,0.34215036\n",
      "Iteration 22925: loss = 2.2112408e-06,0.26053286\n",
      "Iteration 22930: loss = 2.738341e-06,0.19454692\n",
      "Iteration 22935: loss = 3.293458e-06,0.14490889\n",
      "Iteration 22940: loss = 3.861838e-06,0.108204775\n",
      "Iteration 22945: loss = 4.425843e-06,0.08154593\n",
      "Iteration 22950: loss = 4.9618047e-06,0.0626104\n",
      "Iteration 22955: loss = 5.458492e-06,0.049183566\n",
      "Iteration 22960: loss = 5.9086797e-06,0.03967556\n",
      "Iteration 22965: loss = 6.3259226e-06,0.032678474\n",
      "Iteration 22970: loss = 6.712175e-06,0.027457114\n",
      "Iteration 22975: loss = 7.0679357e-06,0.023536177\n",
      "Iteration 22980: loss = 7.3923948e-06,0.020589935\n",
      "Iteration 22985: loss = 7.687974e-06,0.01834941\n",
      "Iteration 22990: loss = 7.960591e-06,0.01660803\n",
      "Iteration 22995: loss = 8.218303e-06,0.0152136525\n",
      "Iteration 23000: loss = 8.462196e-06,0.01409031\n",
      "Iteration 23005: loss = 8.69266e-06,0.0131829465\n",
      "Iteration 23010: loss = 8.91157e-06,0.012441922\n",
      "Iteration 23015: loss = 9.122397e-06,0.011828311\n",
      "Iteration 23020: loss = 9.327269e-06,0.011314684\n",
      "Iteration 23025: loss = 9.526559e-06,0.0108838985\n",
      "Iteration 23030: loss = 9.720901e-06,0.010521768\n",
      "Iteration 23035: loss = 9.91178e-06,0.010217337\n",
      "Iteration 23040: loss = 1.010103e-05,0.009958564\n",
      "Iteration 23045: loss = 1.0285791e-05,0.009742071\n",
      "Iteration 23050: loss = 1.047149e-05,0.0095578935\n",
      "Iteration 23055: loss = 1.0652319e-05,0.0094067\n",
      "Iteration 23060: loss = 1.0837648e-05,0.009279005\n",
      "Iteration 23065: loss = 1.0988851e-05,0.009181985\n",
      "Iteration 23070: loss = 1.1135032e-05,0.009111015\n",
      "Iteration 23075: loss = 1.1149331e-05,0.0091442\n",
      "Iteration 23080: loss = 1.1367366e-05,0.009932845\n",
      "Iteration 23085: loss = 1.0425454e-05,0.01938246\n",
      "Iteration 23090: loss = 1.0161922e-05,0.10767342\n",
      "Iteration 23095: loss = 3.3445212e-06,0.21432064\n",
      "Iteration 23100: loss = 1.0297894e-06,0.6060052\n",
      "Iteration 23105: loss = 5.171617e-07,0.8538282\n",
      "Iteration 23110: loss = 3.944405e-07,0.952044\n",
      "Iteration 23115: loss = 4.049194e-07,0.93614143\n",
      "Iteration 23120: loss = 4.7069918e-07,0.8696313\n",
      "Iteration 23125: loss = 5.747245e-07,0.78077507\n",
      "Iteration 23130: loss = 7.287528e-07,0.6745564\n",
      "Iteration 23135: loss = 9.450144e-07,0.56098974\n",
      "Iteration 23140: loss = 1.2232258e-06,0.45393205\n",
      "Iteration 23145: loss = 1.5606553e-06,0.3598735\n",
      "Iteration 23150: loss = 1.9736515e-06,0.2766849\n",
      "Iteration 23155: loss = 2.4573076e-06,0.20686524\n",
      "Iteration 23160: loss = 2.99532e-06,0.15197586\n",
      "Iteration 23165: loss = 3.5426067e-06,0.112323664\n",
      "Iteration 23170: loss = 4.0811783e-06,0.08387484\n",
      "Iteration 23175: loss = 4.5963357e-06,0.0636803\n",
      "Iteration 23180: loss = 5.0827657e-06,0.049312264\n",
      "Iteration 23185: loss = 5.5374185e-06,0.03895841\n",
      "Iteration 23190: loss = 5.9561353e-06,0.031525265\n",
      "Iteration 23195: loss = 6.332486e-06,0.02621268\n",
      "Iteration 23200: loss = 6.6706757e-06,0.022353346\n",
      "Iteration 23205: loss = 6.9842554e-06,0.01942935\n",
      "Iteration 23210: loss = 7.2754083e-06,0.017193124\n",
      "Iteration 23215: loss = 7.5411867e-06,0.015496941\n",
      "Iteration 23220: loss = 7.786325e-06,0.014186029\n",
      "Iteration 23225: loss = 8.0220825e-06,0.013124961\n",
      "Iteration 23230: loss = 8.243383e-06,0.012285331\n",
      "Iteration 23235: loss = 8.450601e-06,0.011616966\n",
      "Iteration 23240: loss = 8.655905e-06,0.011057348\n",
      "Iteration 23245: loss = 8.848542e-06,0.010610218\n",
      "Iteration 23250: loss = 9.041945e-06,0.010231973\n",
      "Iteration 23255: loss = 9.220517e-06,0.009932824\n",
      "Iteration 23260: loss = 9.409879e-06,0.009671103\n",
      "Iteration 23265: loss = 9.562443e-06,0.009489849\n",
      "Iteration 23270: loss = 9.7475895e-06,0.009348102\n",
      "Iteration 23275: loss = 9.750521e-06,0.009576092\n",
      "Iteration 23280: loss = 1.0051553e-05,0.0118247885\n",
      "Iteration 23285: loss = 8.873536e-06,0.036442384\n",
      "Iteration 23290: loss = 7.4643026e-06,0.1763541\n",
      "Iteration 23295: loss = 1.929375e-06,0.2772657\n",
      "Iteration 23300: loss = 6.726248e-07,0.7240701\n",
      "Iteration 23305: loss = 3.9880865e-07,0.94572073\n",
      "Iteration 23310: loss = 3.4649426e-07,0.986762\n",
      "Iteration 23315: loss = 3.7728395e-07,0.93560797\n",
      "Iteration 23320: loss = 4.486492e-07,0.8553708\n",
      "Iteration 23325: loss = 5.632816e-07,0.7563598\n",
      "Iteration 23330: loss = 7.249542e-07,0.64743686\n",
      "Iteration 23335: loss = 9.506736e-07,0.5316816\n",
      "Iteration 23340: loss = 1.2309928e-06,0.425965\n",
      "Iteration 23345: loss = 1.576338e-06,0.33235887\n",
      "Iteration 23350: loss = 1.999639e-06,0.25074998\n",
      "Iteration 23355: loss = 2.491022e-06,0.18383697\n",
      "Iteration 23360: loss = 3.0206747e-06,0.13339286\n",
      "Iteration 23365: loss = 3.554774e-06,0.09745839\n",
      "Iteration 23370: loss = 4.070263e-06,0.07224228\n",
      "Iteration 23375: loss = 4.5579272e-06,0.05465598\n",
      "Iteration 23380: loss = 5.018351e-06,0.042135056\n",
      "Iteration 23385: loss = 5.443695e-06,0.033281874\n",
      "Iteration 23390: loss = 5.825448e-06,0.027087476\n",
      "Iteration 23395: loss = 6.1644873e-06,0.022706918\n",
      "Iteration 23400: loss = 6.476885e-06,0.019458693\n",
      "Iteration 23405: loss = 6.7666447e-06,0.017011484\n",
      "Iteration 23410: loss = 7.024782e-06,0.015218863\n",
      "Iteration 23415: loss = 7.264129e-06,0.013843721\n",
      "Iteration 23420: loss = 7.4933073e-06,0.012748988\n",
      "Iteration 23425: loss = 7.705402e-06,0.011903673\n",
      "Iteration 23430: loss = 7.904938e-06,0.011237407\n",
      "Iteration 23435: loss = 8.100592e-06,0.010690553\n",
      "Iteration 23440: loss = 8.284522e-06,0.010258847\n",
      "Iteration 23445: loss = 8.467004e-06,0.009901544\n",
      "Iteration 23450: loss = 8.636739e-06,0.009623686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23455: loss = 8.817701e-06,0.009385623\n",
      "Iteration 23460: loss = 8.941304e-06,0.0092588365\n",
      "Iteration 23465: loss = 9.132445e-06,0.009305293\n",
      "Iteration 23470: loss = 8.945025e-06,0.011384692\n",
      "Iteration 23475: loss = 9.378132e-06,0.03284781\n",
      "Iteration 23480: loss = 5.30472e-06,0.19435908\n",
      "Iteration 23485: loss = 1.7058625e-06,0.29778177\n",
      "Iteration 23490: loss = 5.701498e-07,0.7670537\n",
      "Iteration 23495: loss = 3.2081175e-07,1.0113682\n",
      "Iteration 23500: loss = 2.7079585e-07,1.0660686\n",
      "Iteration 23505: loss = 2.917457e-07,1.019029\n",
      "Iteration 23510: loss = 3.4158006e-07,0.9433696\n",
      "Iteration 23515: loss = 4.280225e-07,0.84379554\n",
      "Iteration 23520: loss = 5.5413744e-07,0.73252517\n",
      "Iteration 23525: loss = 7.3770747e-07,0.6102741\n",
      "Iteration 23530: loss = 9.759494e-07,0.49305856\n",
      "Iteration 23535: loss = 1.2836135e-06,0.38408205\n",
      "Iteration 23540: loss = 1.672594e-06,0.28786492\n",
      "Iteration 23545: loss = 2.1332396e-06,0.20949714\n",
      "Iteration 23550: loss = 2.6283624e-06,0.15112562\n",
      "Iteration 23555: loss = 3.1342115e-06,0.109295644\n",
      "Iteration 23560: loss = 3.6422273e-06,0.07949112\n",
      "Iteration 23565: loss = 4.1386725e-06,0.058397807\n",
      "Iteration 23570: loss = 4.6048135e-06,0.04385216\n",
      "Iteration 23575: loss = 5.021417e-06,0.03408131\n",
      "Iteration 23580: loss = 5.392381e-06,0.02735089\n",
      "Iteration 23585: loss = 5.7292477e-06,0.022564156\n",
      "Iteration 23590: loss = 6.0386533e-06,0.019064533\n",
      "Iteration 23595: loss = 6.316352e-06,0.016545657\n",
      "Iteration 23600: loss = 6.563168e-06,0.014719684\n",
      "Iteration 23605: loss = 6.7930646e-06,0.013326614\n",
      "Iteration 23610: loss = 7.0105493e-06,0.0122416355\n",
      "Iteration 23615: loss = 7.210036e-06,0.01141961\n",
      "Iteration 23620: loss = 7.3979513e-06,0.010776935\n",
      "Iteration 23625: loss = 7.5811845e-06,0.010258458\n",
      "Iteration 23630: loss = 7.754341e-06,0.009853826\n",
      "Iteration 23635: loss = 7.921431e-06,0.009530517\n",
      "Iteration 23640: loss = 8.087859e-06,0.009268925\n",
      "Iteration 23645: loss = 8.24112e-06,0.009070621\n",
      "Iteration 23650: loss = 8.402742e-06,0.008906685\n",
      "Iteration 23655: loss = 8.51411e-06,0.008812874\n",
      "Iteration 23660: loss = 8.66002e-06,0.008799396\n",
      "Iteration 23665: loss = 8.568472e-06,0.009506421\n",
      "Iteration 23670: loss = 8.911538e-06,0.016682342\n",
      "Iteration 23675: loss = 6.6647867e-06,0.08973126\n",
      "Iteration 23680: loss = 3.3188364e-06,0.22116947\n",
      "Iteration 23685: loss = 7.9422836e-07,0.6263743\n",
      "Iteration 23690: loss = 3.4471347e-07,0.924086\n",
      "Iteration 23695: loss = 2.3854335e-07,1.0678979\n",
      "Iteration 23700: loss = 2.342362e-07,1.0773624\n",
      "Iteration 23705: loss = 2.685384e-07,1.0205561\n",
      "Iteration 23710: loss = 3.262544e-07,0.9356888\n",
      "Iteration 23715: loss = 4.2093384e-07,0.82333386\n",
      "Iteration 23720: loss = 5.630614e-07,0.6962865\n",
      "Iteration 23725: loss = 7.550598e-07,0.5714344\n",
      "Iteration 23730: loss = 1.0053413e-06,0.45435423\n",
      "Iteration 23735: loss = 1.3300856e-06,0.3464764\n",
      "Iteration 23740: loss = 1.7250248e-06,0.255254\n",
      "Iteration 23745: loss = 2.1804537e-06,0.18342176\n",
      "Iteration 23750: loss = 2.6641658e-06,0.13096339\n",
      "Iteration 23755: loss = 3.170094e-06,0.092727326\n",
      "Iteration 23760: loss = 3.6711328e-06,0.066217646\n",
      "Iteration 23765: loss = 4.1334597e-06,0.048622306\n",
      "Iteration 23770: loss = 4.5432644e-06,0.037036773\n",
      "Iteration 23775: loss = 4.9196856e-06,0.028975872\n",
      "Iteration 23780: loss = 5.2661817e-06,0.023253234\n",
      "Iteration 23785: loss = 5.5740006e-06,0.019289775\n",
      "Iteration 23790: loss = 5.8396813e-06,0.016557341\n",
      "Iteration 23795: loss = 6.083527e-06,0.014538599\n",
      "Iteration 23800: loss = 6.31149e-06,0.013011967\n",
      "Iteration 23805: loss = 6.5169043e-06,0.011891178\n",
      "Iteration 23810: loss = 6.7059905e-06,0.011046008\n",
      "Iteration 23815: loss = 6.8903137e-06,0.010372953\n",
      "Iteration 23820: loss = 7.058093e-06,0.009869989\n",
      "Iteration 23825: loss = 7.2257135e-06,0.009462696\n",
      "Iteration 23830: loss = 7.3807673e-06,0.0091550825\n",
      "Iteration 23835: loss = 7.536233e-06,0.0089084525\n",
      "Iteration 23840: loss = 7.682276e-06,0.008722885\n",
      "Iteration 23845: loss = 7.831331e-06,0.00857626\n",
      "Iteration 23850: loss = 7.955504e-06,0.008478136\n",
      "Iteration 23855: loss = 8.083023e-06,0.008421715\n",
      "Iteration 23860: loss = 8.074198e-06,0.008585317\n",
      "Iteration 23865: loss = 8.3140685e-06,0.010908555\n",
      "Iteration 23870: loss = 7.168694e-06,0.04121407\n",
      "Iteration 23875: loss = 5.261943e-06,0.23466137\n",
      "Iteration 23880: loss = 8.873553e-07,0.5005238\n",
      "Iteration 23885: loss = 2.5560735e-07,1.0147289\n",
      "Iteration 23890: loss = 1.424397e-07,1.2599326\n",
      "Iteration 23895: loss = 1.2631938e-07,1.3076341\n",
      "Iteration 23900: loss = 1.4432612e-07,1.2477932\n",
      "Iteration 23905: loss = 1.7715278e-07,1.1591636\n",
      "Iteration 23910: loss = 2.3284099e-07,1.0431776\n",
      "Iteration 23915: loss = 3.1327752e-07,0.91687995\n",
      "Iteration 23920: loss = 4.3024286e-07,0.78073525\n",
      "Iteration 23925: loss = 5.834191e-07,0.6500731\n",
      "Iteration 23930: loss = 7.942981e-07,0.520498\n",
      "Iteration 23935: loss = 1.0779727e-06,0.39842352\n",
      "Iteration 23940: loss = 1.4396629e-06,0.29206488\n",
      "Iteration 23945: loss = 1.853969e-06,0.20953178\n",
      "Iteration 23950: loss = 2.3113232e-06,0.14773722\n",
      "Iteration 23955: loss = 2.7902408e-06,0.1035683\n",
      "Iteration 23960: loss = 3.2687212e-06,0.07296095\n",
      "Iteration 23965: loss = 3.7212587e-06,0.05244065\n",
      "Iteration 23970: loss = 4.136013e-06,0.038811456\n",
      "Iteration 23975: loss = 4.5076927e-06,0.029707419\n",
      "Iteration 23980: loss = 4.8335028e-06,0.023607785\n",
      "Iteration 23985: loss = 5.122782e-06,0.019391833\n",
      "Iteration 23990: loss = 5.3829913e-06,0.016391218\n",
      "Iteration 23995: loss = 5.6181602e-06,0.014229949\n",
      "Iteration 24000: loss = 5.8271885e-06,0.0126787815\n",
      "Iteration 24005: loss = 6.0179555e-06,0.011529465\n",
      "Iteration 24010: loss = 6.1968726e-06,0.010653645\n",
      "Iteration 24015: loss = 6.366075e-06,0.009981466\n",
      "Iteration 24020: loss = 6.5239014e-06,0.009472268\n",
      "Iteration 24025: loss = 6.675551e-06,0.009077553\n",
      "Iteration 24030: loss = 6.8206587e-06,0.008774617\n",
      "Iteration 24035: loss = 6.9618636e-06,0.008542184\n",
      "Iteration 24040: loss = 7.098777e-06,0.0083665885\n",
      "Iteration 24045: loss = 7.2310454e-06,0.0082373265\n",
      "Iteration 24050: loss = 7.3624656e-06,0.008143348\n",
      "Iteration 24055: loss = 7.4696723e-06,0.008081617\n",
      "Iteration 24060: loss = 7.4762743e-06,0.008034461\n",
      "Iteration 24065: loss = 7.4212167e-06,0.008008975\n",
      "Iteration 24070: loss = 7.4512177e-06,0.008053212\n",
      "Iteration 24075: loss = 7.298946e-06,0.008831399\n",
      "Iteration 24080: loss = 7.5652474e-06,0.016427452\n",
      "Iteration 24085: loss = 5.4684383e-06,0.09477299\n",
      "Iteration 24090: loss = 2.4841604e-06,0.24004297\n",
      "Iteration 24095: loss = 5.6088584e-07,0.7190318\n",
      "Iteration 24100: loss = 2.4489768e-07,1.0102054\n",
      "Iteration 24105: loss = 1.690809e-07,1.1431259\n",
      "Iteration 24110: loss = 1.6565222e-07,1.1511422\n",
      "Iteration 24115: loss = 1.8699865e-07,1.1041362\n",
      "Iteration 24120: loss = 2.2453366e-07,1.0293633\n",
      "Iteration 24125: loss = 2.892816e-07,0.9224522\n",
      "Iteration 24130: loss = 3.8860708e-07,0.79581714\n",
      "Iteration 24135: loss = 5.2515e-07,0.66673803\n",
      "Iteration 24140: loss = 7.136291e-07,0.5377887\n",
      "Iteration 24145: loss = 9.699037e-07,0.41390583\n",
      "Iteration 24150: loss = 1.3001804e-06,0.30419403\n",
      "Iteration 24155: loss = 1.6940421e-06,0.21637502\n",
      "Iteration 24160: loss = 2.1264714e-06,0.15170547\n",
      "Iteration 24165: loss = 2.5941924e-06,0.1043284\n",
      "Iteration 24170: loss = 3.0636875e-06,0.07198859\n",
      "Iteration 24175: loss = 3.4995107e-06,0.051038396\n",
      "Iteration 24180: loss = 3.8859685e-06,0.037580747\n",
      "Iteration 24185: loss = 4.2356965e-06,0.028543599\n",
      "Iteration 24190: loss = 4.5526235e-06,0.022333065\n",
      "Iteration 24195: loss = 4.8312013e-06,0.01813615\n",
      "Iteration 24200: loss = 5.0670747e-06,0.015326566\n",
      "Iteration 24205: loss = 5.2804676e-06,0.013302238\n",
      "Iteration 24210: loss = 5.4801053e-06,0.011785751\n",
      "Iteration 24215: loss = 5.65606e-06,0.010708151\n",
      "Iteration 24220: loss = 5.818531e-06,0.009904481\n",
      "Iteration 24225: loss = 5.9746612e-06,0.009284306\n",
      "Iteration 24230: loss = 6.1204223e-06,0.008820578\n",
      "Iteration 24235: loss = 6.259321e-06,0.00846925\n",
      "Iteration 24240: loss = 6.3927514e-06,0.008205261\n",
      "Iteration 24245: loss = 6.5227255e-06,0.008007321\n",
      "Iteration 24250: loss = 6.647287e-06,0.0078656\n",
      "Iteration 24255: loss = 6.7712845e-06,0.007764715\n",
      "Iteration 24260: loss = 6.871695e-06,0.007703186\n",
      "Iteration 24265: loss = 6.946472e-06,0.0076754754\n",
      "Iteration 24270: loss = 6.8755794e-06,0.0077915234\n",
      "Iteration 24275: loss = 7.01644e-06,0.009347896\n",
      "Iteration 24280: loss = 6.2018794e-06,0.029394468\n",
      "Iteration 24285: loss = 5.1464135e-06,0.19379818\n",
      "Iteration 24290: loss = 8.565676e-07,0.45540842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24295: loss = 2.0699639e-07,1.0701166\n",
      "Iteration 24300: loss = 1.0675495e-07,1.3230395\n",
      "Iteration 24305: loss = 8.818409e-08,1.3832903\n",
      "Iteration 24310: loss = 9.62292e-08,1.338767\n",
      "Iteration 24315: loss = 1.1583432e-07,1.2606062\n",
      "Iteration 24320: loss = 1.489089e-07,1.1597189\n",
      "Iteration 24325: loss = 1.9750992e-07,1.0461259\n",
      "Iteration 24330: loss = 2.7125614e-07,0.9159752\n",
      "Iteration 24335: loss = 3.6681217e-07,0.7890721\n",
      "Iteration 24340: loss = 5.063885e-07,0.6520289\n",
      "Iteration 24345: loss = 7.0320675e-07,0.5141942\n",
      "Iteration 24350: loss = 9.703724e-07,0.38542166\n",
      "Iteration 24355: loss = 1.3025706e-06,0.27793178\n",
      "Iteration 24360: loss = 1.6918883e-06,0.19437353\n",
      "Iteration 24365: loss = 2.127849e-06,0.13248405\n",
      "Iteration 24370: loss = 2.5794773e-06,0.08972369\n",
      "Iteration 24375: loss = 3.0140045e-06,0.061749812\n",
      "Iteration 24380: loss = 3.415725e-06,0.04366451\n",
      "Iteration 24385: loss = 3.7715974e-06,0.03206771\n",
      "Iteration 24390: loss = 4.0825926e-06,0.024520934\n",
      "Iteration 24395: loss = 4.35611e-06,0.01945214\n",
      "Iteration 24400: loss = 4.5970587e-06,0.01598013\n",
      "Iteration 24405: loss = 4.808856e-06,0.013573882\n",
      "Iteration 24410: loss = 4.9926934e-06,0.011899325\n",
      "Iteration 24415: loss = 5.1611714e-06,0.01066564\n",
      "Iteration 24420: loss = 5.3185013e-06,0.009740034\n",
      "Iteration 24425: loss = 5.4618595e-06,0.009060912\n",
      "Iteration 24430: loss = 5.5969963e-06,0.008549605\n",
      "Iteration 24435: loss = 5.727398e-06,0.008158766\n",
      "Iteration 24440: loss = 5.85011e-06,0.007871829\n",
      "Iteration 24445: loss = 5.9693502e-06,0.0076581873\n",
      "Iteration 24450: loss = 6.0842926e-06,0.0075056218\n",
      "Iteration 24455: loss = 6.1959595e-06,0.007400511\n",
      "Iteration 24460: loss = 6.3049097e-06,0.007332582\n",
      "Iteration 24465: loss = 6.2783897e-06,0.007301146\n",
      "Iteration 24470: loss = 6.1784413e-06,0.0073075183\n",
      "Iteration 24475: loss = 6.1422634e-06,0.0073034326\n",
      "Iteration 24480: loss = 6.1723445e-06,0.007273\n",
      "Iteration 24485: loss = 6.139737e-06,0.0074280323\n",
      "Iteration 24490: loss = 6.287977e-06,0.008892768\n",
      "Iteration 24495: loss = 5.5795663e-06,0.027993843\n",
      "Iteration 24500: loss = 4.7089165e-06,0.17683649\n",
      "Iteration 24505: loss = 8.395545e-07,0.43841156\n",
      "Iteration 24510: loss = 2.0607126e-07,1.0507717\n",
      "Iteration 24515: loss = 1.082436e-07,1.2891061\n",
      "Iteration 24520: loss = 8.849414e-08,1.3461784\n",
      "Iteration 24525: loss = 9.369993e-08,1.3123484\n",
      "Iteration 24530: loss = 1.09461126e-07,1.247507\n",
      "Iteration 24535: loss = 1.3650173e-07,1.1612655\n",
      "Iteration 24540: loss = 1.7843367e-07,1.0567598\n",
      "Iteration 24545: loss = 2.426668e-07,0.93321234\n",
      "Iteration 24550: loss = 3.261805e-07,0.8100568\n",
      "Iteration 24555: loss = 4.5222168e-07,0.6709518\n",
      "Iteration 24560: loss = 6.3089334e-07,0.52996284\n",
      "Iteration 24565: loss = 8.7844336e-07,0.39631295\n",
      "Iteration 24570: loss = 1.1911596e-06,0.28418902\n",
      "Iteration 24575: loss = 1.5656669e-06,0.19604328\n",
      "Iteration 24580: loss = 1.991604e-06,0.13075966\n",
      "Iteration 24585: loss = 2.4279484e-06,0.08704618\n",
      "Iteration 24590: loss = 2.8405084e-06,0.059219547\n",
      "Iteration 24595: loss = 3.215034e-06,0.041584853\n",
      "Iteration 24600: loss = 3.5484688e-06,0.030288832\n",
      "Iteration 24605: loss = 3.8448634e-06,0.022848612\n",
      "Iteration 24610: loss = 4.098421e-06,0.01801972\n",
      "Iteration 24615: loss = 4.31299e-06,0.014828355\n",
      "Iteration 24620: loss = 4.5004713e-06,0.012617288\n",
      "Iteration 24625: loss = 4.6689674e-06,0.011024732\n",
      "Iteration 24630: loss = 4.8213806e-06,0.009868167\n",
      "Iteration 24635: loss = 4.957789e-06,0.009032488\n",
      "Iteration 24640: loss = 5.0860594e-06,0.008399719\n",
      "Iteration 24645: loss = 5.2080754e-06,0.007921031\n",
      "Iteration 24650: loss = 5.322199e-06,0.0075679296\n",
      "Iteration 24655: loss = 5.4308766e-06,0.0073066736\n",
      "Iteration 24660: loss = 5.5384935e-06,0.007111917\n",
      "Iteration 24665: loss = 5.6373447e-06,0.006979836\n",
      "Iteration 24670: loss = 5.7374964e-06,0.0068887076\n",
      "Iteration 24675: loss = 5.8219935e-06,0.006835979\n",
      "Iteration 24680: loss = 5.767263e-06,0.0068186233\n",
      "Iteration 24685: loss = 5.6113e-06,0.006903652\n",
      "Iteration 24690: loss = 5.5693486e-06,0.006997062\n",
      "Iteration 24695: loss = 5.4413426e-06,0.0077586947\n",
      "Iteration 24700: loss = 5.6223676e-06,0.013273777\n",
      "Iteration 24705: loss = 4.2211354e-06,0.077495806\n",
      "Iteration 24710: loss = 2.0941413e-06,0.2697186\n",
      "Iteration 24715: loss = 3.895099e-07,0.76998854\n",
      "Iteration 24720: loss = 1.5753092e-07,1.0909638\n",
      "Iteration 24725: loss = 1.0161566e-07,1.2481036\n",
      "Iteration 24730: loss = 9.6957706e-08,1.2677428\n",
      "Iteration 24735: loss = 1.0539941e-07,1.237955\n",
      "Iteration 24740: loss = 1.227091e-07,1.17979\n",
      "Iteration 24745: loss = 1.5691923e-07,1.0833006\n",
      "Iteration 24750: loss = 2.0739651e-07,0.9703219\n",
      "Iteration 24755: loss = 2.823341e-07,0.84218603\n",
      "Iteration 24760: loss = 3.8953635e-07,0.70578885\n",
      "Iteration 24765: loss = 5.474442e-07,0.5611477\n",
      "Iteration 24770: loss = 7.7318026e-07,0.41952696\n",
      "Iteration 24775: loss = 1.0636428e-06,0.29957274\n",
      "Iteration 24780: loss = 1.4172755e-06,0.20491832\n",
      "Iteration 24785: loss = 1.824364e-06,0.1348647\n",
      "Iteration 24790: loss = 2.2464346e-06,0.08813817\n",
      "Iteration 24795: loss = 2.6473554e-06,0.058745123\n",
      "Iteration 24800: loss = 3.0018898e-06,0.040800378\n",
      "Iteration 24805: loss = 3.3234098e-06,0.029200675\n",
      "Iteration 24810: loss = 3.6034883e-06,0.021774456\n",
      "Iteration 24815: loss = 3.8377716e-06,0.017072378\n",
      "Iteration 24820: loss = 4.030433e-06,0.014037993\n",
      "Iteration 24825: loss = 4.2023075e-06,0.011885006\n",
      "Iteration 24830: loss = 4.355821e-06,0.010345318\n",
      "Iteration 24835: loss = 4.4878466e-06,0.009269599\n",
      "Iteration 24840: loss = 4.6094688e-06,0.008465398\n",
      "Iteration 24845: loss = 4.726034e-06,0.007846118\n",
      "Iteration 24850: loss = 4.8299335e-06,0.0074004265\n",
      "Iteration 24855: loss = 4.932944e-06,0.0070518404\n",
      "Iteration 24860: loss = 5.027287e-06,0.006801946\n",
      "Iteration 24865: loss = 5.1228617e-06,0.006611562\n",
      "Iteration 24870: loss = 5.208005e-06,0.0064855944\n",
      "Iteration 24875: loss = 5.2970117e-06,0.0063961823\n",
      "Iteration 24880: loss = 5.353843e-06,0.0063552405\n",
      "Iteration 24885: loss = 5.4206594e-06,0.0063605155\n",
      "Iteration 24890: loss = 5.142185e-06,0.006810857\n",
      "Iteration 24895: loss = 5.068247e-06,0.009820844\n",
      "Iteration 24900: loss = 4.115744e-06,0.045497965\n",
      "Iteration 24905: loss = 2.8034644e-06,0.22879861\n",
      "Iteration 24910: loss = 4.802391e-07,0.6073264\n",
      "Iteration 24915: loss = 1.5398385e-07,1.0831369\n",
      "Iteration 24920: loss = 9.138183e-08,1.2810825\n",
      "Iteration 24925: loss = 8.199689e-08,1.3130574\n",
      "Iteration 24930: loss = 8.6508486e-08,1.2857127\n",
      "Iteration 24935: loss = 9.942071e-08,1.229589\n",
      "Iteration 24940: loss = 1.2420027e-07,1.1446352\n",
      "Iteration 24945: loss = 1.6223531e-07,1.0413389\n",
      "Iteration 24950: loss = 2.2117193e-07,0.91677773\n",
      "Iteration 24955: loss = 3.0115882e-07,0.7872384\n",
      "Iteration 24960: loss = 4.2831564e-07,0.6362277\n",
      "Iteration 24965: loss = 6.108343e-07,0.48674116\n",
      "Iteration 24970: loss = 8.632687e-07,0.35048622\n",
      "Iteration 24975: loss = 1.181373e-06,0.2401425\n",
      "Iteration 24980: loss = 1.5620204e-06,0.15664956\n",
      "Iteration 24985: loss = 1.9706695e-06,0.10030754\n",
      "Iteration 24990: loss = 2.3624834e-06,0.06534972\n",
      "Iteration 24995: loss = 2.715465e-06,0.044126093\n",
      "Iteration 25000: loss = 3.0340552e-06,0.0307509\n",
      "Iteration 25005: loss = 3.3099022e-06,0.022381363\n",
      "Iteration 25010: loss = 3.5375278e-06,0.017211286\n",
      "Iteration 25015: loss = 3.719869e-06,0.013971811\n",
      "Iteration 25020: loss = 3.8806907e-06,0.0116984\n",
      "Iteration 25025: loss = 4.0221125e-06,0.010090989\n",
      "Iteration 25030: loss = 4.1421563e-06,0.008972661\n",
      "Iteration 25035: loss = 4.2519096e-06,0.008136623\n",
      "Iteration 25040: loss = 4.3557907e-06,0.0074926876\n",
      "Iteration 25045: loss = 4.449955e-06,0.007018025\n",
      "Iteration 25050: loss = 4.5387787e-06,0.006656963\n",
      "Iteration 25055: loss = 4.6249393e-06,0.006380172\n",
      "Iteration 25060: loss = 4.70532e-06,0.00617897\n",
      "Iteration 25065: loss = 4.7856843e-06,0.006028462\n",
      "Iteration 25070: loss = 4.859577e-06,0.00592738\n",
      "Iteration 25075: loss = 4.9346236e-06,0.005860409\n",
      "Iteration 25080: loss = 4.9853843e-06,0.0058259727\n",
      "Iteration 25085: loss = 4.828056e-06,0.005899523\n",
      "Iteration 25090: loss = 4.5202646e-06,0.006604506\n",
      "Iteration 25095: loss = 4.43462e-06,0.007845279\n",
      "Iteration 25100: loss = 3.98384e-06,0.021623058\n",
      "Iteration 25105: loss = 3.69997e-06,0.1317307\n",
      "Iteration 25110: loss = 7.497919e-07,0.46030506\n",
      "Iteration 25115: loss = 1.5459975e-07,1.1227212\n",
      "Iteration 25120: loss = 7.967532e-08,1.3230336\n",
      "Iteration 25125: loss = 5.981405e-08,1.3898816\n",
      "Iteration 25130: loss = 6.115286e-08,1.3701671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25135: loss = 6.7961075e-08,1.3309705\n",
      "Iteration 25140: loss = 8.0616225e-08,1.2726657\n",
      "Iteration 25145: loss = 1.0357494e-07,1.1852615\n",
      "Iteration 25150: loss = 1.3676572e-07,1.082011\n",
      "Iteration 25155: loss = 1.8316814e-07,0.9660967\n",
      "Iteration 25160: loss = 2.5630405e-07,0.82573\n",
      "Iteration 25165: loss = 3.6599855e-07,0.6729374\n",
      "Iteration 25170: loss = 5.333867e-07,0.51336414\n",
      "Iteration 25175: loss = 7.597327e-07,0.37170902\n",
      "Iteration 25180: loss = 1.0644865e-06,0.25048372\n",
      "Iteration 25185: loss = 1.4292009e-06,0.16076933\n",
      "Iteration 25190: loss = 1.8195669e-06,0.10133587\n",
      "Iteration 25195: loss = 2.1949354e-06,0.06486722\n",
      "Iteration 25200: loss = 2.5368238e-06,0.04283649\n",
      "Iteration 25205: loss = 2.8378793e-06,0.029440839\n",
      "Iteration 25210: loss = 3.0929048e-06,0.021270467\n",
      "Iteration 25215: loss = 3.3007211e-06,0.016280007\n",
      "Iteration 25220: loss = 3.4668901e-06,0.013148885\n",
      "Iteration 25225: loss = 3.6076708e-06,0.011020213\n",
      "Iteration 25230: loss = 3.7302088e-06,0.009506596\n",
      "Iteration 25235: loss = 3.837209e-06,0.008419466\n",
      "Iteration 25240: loss = 3.930686e-06,0.0076277303\n",
      "Iteration 25245: loss = 4.019627e-06,0.0070052533\n",
      "Iteration 25250: loss = 4.102267e-06,0.006529876\n",
      "Iteration 25255: loss = 4.177651e-06,0.006171846\n",
      "Iteration 25260: loss = 4.2504716e-06,0.0058931783\n",
      "Iteration 25265: loss = 4.321304e-06,0.0056777247\n",
      "Iteration 25270: loss = 4.3869363e-06,0.005520477\n",
      "Iteration 25275: loss = 4.4533085e-06,0.0054017035\n",
      "Iteration 25280: loss = 4.5147835e-06,0.005322627\n",
      "Iteration 25285: loss = 4.5769716e-06,0.0052711926\n",
      "Iteration 25290: loss = 4.6140244e-06,0.0052432343\n",
      "Iteration 25295: loss = 4.6079e-06,0.005223999\n",
      "Iteration 25300: loss = 4.569925e-06,0.005245216\n",
      "Iteration 25305: loss = 4.6126847e-06,0.0055371164\n",
      "Iteration 25310: loss = 4.377158e-06,0.0089210365\n",
      "Iteration 25315: loss = 4.5123757e-06,0.043888718\n",
      "Iteration 25320: loss = 1.6018602e-06,0.30426553\n",
      "Iteration 25325: loss = 2.758309e-07,0.7922466\n",
      "Iteration 25330: loss = 8.873923e-08,1.228459\n",
      "Iteration 25335: loss = 5.3144138e-08,1.4025747\n",
      "Iteration 25340: loss = 4.8018624e-08,1.4326254\n",
      "Iteration 25345: loss = 4.8281834e-08,1.425066\n",
      "Iteration 25350: loss = 5.5563888e-08,1.3762354\n",
      "Iteration 25355: loss = 6.6400474e-08,1.31491\n",
      "Iteration 25360: loss = 8.546787e-08,1.2276232\n",
      "Iteration 25365: loss = 1.11551856e-07,1.1309966\n",
      "Iteration 25370: loss = 1.5002162e-07,1.0168973\n",
      "Iteration 25375: loss = 2.1358885e-07,0.8722196\n",
      "Iteration 25380: loss = 3.0977694e-07,0.7136017\n",
      "Iteration 25385: loss = 4.5887964e-07,0.54603344\n",
      "Iteration 25390: loss = 6.6892363e-07,0.39288664\n",
      "Iteration 25395: loss = 9.575914e-07,0.26148915\n",
      "Iteration 25400: loss = 1.2975875e-06,0.16707581\n",
      "Iteration 25405: loss = 1.6697569e-06,0.10371343\n",
      "Iteration 25410: loss = 2.039803e-06,0.064365104\n",
      "Iteration 25415: loss = 2.3776383e-06,0.04116984\n",
      "Iteration 25420: loss = 2.657511e-06,0.028045591\n",
      "Iteration 25425: loss = 2.888163e-06,0.020250121\n",
      "Iteration 25430: loss = 3.080346e-06,0.015359898\n",
      "Iteration 25435: loss = 3.2307798e-06,0.012350818\n",
      "Iteration 25440: loss = 3.3519646e-06,0.010373679\n",
      "Iteration 25445: loss = 3.4586462e-06,0.008934514\n",
      "Iteration 25450: loss = 3.5504302e-06,0.007897222\n",
      "Iteration 25455: loss = 3.631898e-06,0.00711981\n",
      "Iteration 25460: loss = 3.708585e-06,0.0065061883\n",
      "Iteration 25465: loss = 3.7751206e-06,0.0060517387\n",
      "Iteration 25470: loss = 3.84195e-06,0.0056733917\n",
      "Iteration 25475: loss = 3.9010197e-06,0.005391204\n",
      "Iteration 25480: loss = 3.9617516e-06,0.005155832\n",
      "Iteration 25485: loss = 4.0136565e-06,0.0049889875\n",
      "Iteration 25490: loss = 4.074622e-06,0.00484421\n",
      "Iteration 25495: loss = 4.101207e-06,0.0048033553\n",
      "Iteration 25500: loss = 4.1688363e-06,0.0048756143\n",
      "Iteration 25505: loss = 4.051085e-06,0.006250697\n",
      "Iteration 25510: loss = 4.2146244e-06,0.016862364\n",
      "Iteration 25515: loss = 2.6043801e-06,0.113908134\n",
      "Iteration 25520: loss = 9.1455144e-07,0.33904594\n",
      "Iteration 25525: loss = 1.853095e-07,0.9715905\n",
      "Iteration 25530: loss = 9.8603586e-08,1.168885\n",
      "Iteration 25535: loss = 6.7032374e-08,1.2940029\n",
      "Iteration 25540: loss = 6.954292e-08,1.2864926\n",
      "Iteration 25545: loss = 7.2586246e-08,1.2703235\n",
      "Iteration 25550: loss = 9.161895e-08,1.1832604\n",
      "Iteration 25555: loss = 1.1782175e-07,1.0863509\n",
      "Iteration 25560: loss = 1.6396359e-07,0.9555165\n",
      "Iteration 25565: loss = 2.3064392e-07,0.81226724\n",
      "Iteration 25570: loss = 3.379163e-07,0.6473981\n",
      "Iteration 25575: loss = 5.009247e-07,0.4815394\n",
      "Iteration 25580: loss = 7.349134e-07,0.33082294\n",
      "Iteration 25585: loss = 1.0442749e-06,0.210346\n",
      "Iteration 25590: loss = 1.3954492e-06,0.12884659\n",
      "Iteration 25595: loss = 1.7516772e-06,0.07849729\n",
      "Iteration 25600: loss = 2.0833315e-06,0.048928823\n",
      "Iteration 25605: loss = 2.3846173e-06,0.03132183\n",
      "Iteration 25610: loss = 2.6190876e-06,0.021774586\n",
      "Iteration 25615: loss = 2.804951e-06,0.016180798\n",
      "Iteration 25620: loss = 2.9591968e-06,0.012582615\n",
      "Iteration 25625: loss = 3.0762112e-06,0.010373119\n",
      "Iteration 25630: loss = 3.1724264e-06,0.0088644875\n",
      "Iteration 25635: loss = 3.2619155e-06,0.007696188\n",
      "Iteration 25640: loss = 3.3308968e-06,0.0069151856\n",
      "Iteration 25645: loss = 3.4031546e-06,0.0062334044\n",
      "Iteration 25650: loss = 3.4586053e-06,0.0057732966\n",
      "Iteration 25655: loss = 3.5204323e-06,0.0053504948\n",
      "Iteration 25660: loss = 3.561002e-06,0.0051053623\n",
      "Iteration 25665: loss = 3.6289578e-06,0.004829194\n",
      "Iteration 25670: loss = 3.6120593e-06,0.005125954\n",
      "Iteration 25675: loss = 3.728767e-06,0.0066780355\n",
      "Iteration 25680: loss = 3.2343914e-06,0.024482487\n",
      "Iteration 25685: loss = 2.722689e-06,0.118412696\n",
      "Iteration 25690: loss = 6.1507814e-07,0.422914\n",
      "Iteration 25695: loss = 1.5327352e-07,1.0169404\n",
      "Iteration 25700: loss = 1.0143654e-07,1.1361268\n",
      "Iteration 25705: loss = 7.715503e-08,1.2257649\n",
      "Iteration 25710: loss = 8.5727095e-08,1.193414\n",
      "Iteration 25715: loss = 9.303968e-08,1.1596454\n",
      "Iteration 25720: loss = 1.2166576e-07,1.0509204\n",
      "Iteration 25725: loss = 1.6060505e-07,0.9380056\n",
      "Iteration 25730: loss = 2.3276938e-07,0.781447\n",
      "Iteration 25735: loss = 3.4385877e-07,0.61187804\n",
      "Iteration 25740: loss = 5.1997955e-07,0.43917635\n",
      "Iteration 25745: loss = 7.658946e-07,0.2909072\n",
      "Iteration 25750: loss = 1.0622833e-06,0.18372448\n",
      "Iteration 25755: loss = 1.4118806e-06,0.108547576\n",
      "Iteration 25760: loss = 1.7578153e-06,0.064126484\n",
      "Iteration 25765: loss = 2.0619798e-06,0.03954368\n",
      "Iteration 25770: loss = 2.3087116e-06,0.026229238\n",
      "Iteration 25775: loss = 2.5216534e-06,0.018073771\n",
      "Iteration 25780: loss = 2.680974e-06,0.013536174\n",
      "Iteration 25785: loss = 2.7959475e-06,0.010927873\n",
      "Iteration 25790: loss = 2.9027944e-06,0.008960944\n",
      "Iteration 25795: loss = 2.9787273e-06,0.0077685667\n",
      "Iteration 25800: loss = 3.0527342e-06,0.0067976494\n",
      "Iteration 25805: loss = 3.1145046e-06,0.006100814\n",
      "Iteration 25810: loss = 3.1700395e-06,0.00556197\n",
      "Iteration 25815: loss = 3.2225198e-06,0.005127222\n",
      "Iteration 25820: loss = 3.2658475e-06,0.004812734\n",
      "Iteration 25825: loss = 3.318367e-06,0.004509052\n",
      "Iteration 25830: loss = 3.3390743e-06,0.004433645\n",
      "Iteration 25835: loss = 3.4166026e-06,0.0045369533\n",
      "Iteration 25840: loss = 3.2491864e-06,0.008688711\n",
      "Iteration 25845: loss = 3.3340184e-06,0.040181026\n",
      "Iteration 25850: loss = 1.277875e-06,0.2750748\n",
      "Iteration 25855: loss = 2.6441327e-07,0.7127278\n",
      "Iteration 25860: loss = 8.1330995e-08,1.2075351\n",
      "Iteration 25865: loss = 5.8600875e-08,1.3256582\n",
      "Iteration 25870: loss = 4.5280174e-08,1.3915389\n",
      "Iteration 25875: loss = 5.0498784e-08,1.344981\n",
      "Iteration 25880: loss = 5.3078846e-08,1.3244009\n",
      "Iteration 25885: loss = 7.059496e-08,1.2299552\n",
      "Iteration 25890: loss = 8.973469e-08,1.1425408\n",
      "Iteration 25895: loss = 1.3160549e-07,0.99089\n",
      "Iteration 25900: loss = 1.8941016e-07,0.83844036\n",
      "Iteration 25905: loss = 2.9337585e-07,0.6509927\n",
      "Iteration 25910: loss = 4.45783e-07,0.47369274\n",
      "Iteration 25915: loss = 6.6472074e-07,0.31729984\n",
      "Iteration 25920: loss = 9.6446e-07,0.19198327\n",
      "Iteration 25925: loss = 1.2974469e-06,0.11196544\n",
      "Iteration 25930: loss = 1.633099e-06,0.06458269\n",
      "Iteration 25935: loss = 1.9240715e-06,0.03923083\n",
      "Iteration 25940: loss = 2.1676294e-06,0.025219915\n",
      "Iteration 25945: loss = 2.366912e-06,0.01725327\n",
      "Iteration 25950: loss = 2.500653e-06,0.013175804\n",
      "Iteration 25955: loss = 2.615325e-06,0.010404255\n",
      "Iteration 25960: loss = 2.703376e-06,0.00865043\n",
      "Iteration 25965: loss = 2.7730464e-06,0.00746233\n",
      "Iteration 25970: loss = 2.8399502e-06,0.006497143\n",
      "Iteration 25975: loss = 2.8916302e-06,0.0058389856\n",
      "Iteration 25980: loss = 2.9421847e-06,0.005285793\n",
      "Iteration 25985: loss = 2.98205e-06,0.0048929695\n",
      "Iteration 25990: loss = 3.0273088e-06,0.0045190537\n",
      "Iteration 25995: loss = 3.0576502e-06,0.0042898725\n",
      "Iteration 26000: loss = 3.1098778e-06,0.0040189535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26005: loss = 3.0963472e-06,0.00429798\n",
      "Iteration 26010: loss = 3.1941208e-06,0.0058684745\n",
      "Iteration 26015: loss = 2.7550504e-06,0.025592837\n",
      "Iteration 26020: loss = 2.2296551e-06,0.13002615\n",
      "Iteration 26025: loss = 4.7095637e-07,0.46094665\n",
      "Iteration 26030: loss = 1.0442864e-07,1.1176947\n",
      "Iteration 26035: loss = 7.437019e-08,1.2086071\n",
      "Iteration 26040: loss = 5.4843884e-08,1.297245\n",
      "Iteration 26045: loss = 6.017515e-08,1.2693163\n",
      "Iteration 26050: loss = 6.858088e-08,1.2233037\n",
      "Iteration 26055: loss = 8.7318085e-08,1.1287664\n",
      "Iteration 26060: loss = 1.2317454e-07,0.99148494\n",
      "Iteration 26065: loss = 1.73435e-07,0.84948975\n",
      "Iteration 26070: loss = 2.7632845e-07,0.64810675\n",
      "Iteration 26075: loss = 4.1869214e-07,0.47197556\n",
      "Iteration 26080: loss = 6.385101e-07,0.30712423\n",
      "Iteration 26085: loss = 9.2345346e-07,0.18381469\n",
      "Iteration 26090: loss = 1.2288906e-06,0.10768628\n",
      "Iteration 26095: loss = 1.5615533e-06,0.05936893\n",
      "Iteration 26100: loss = 1.830707e-06,0.035662975\n",
      "Iteration 26105: loss = 2.040937e-06,0.023339225\n",
      "Iteration 26110: loss = 2.2194497e-06,0.015907928\n",
      "Iteration 26115: loss = 2.344185e-06,0.012005997\n",
      "Iteration 26120: loss = 2.436003e-06,0.009677329\n",
      "Iteration 26125: loss = 2.5168997e-06,0.0079749925\n",
      "Iteration 26130: loss = 2.578612e-06,0.006862562\n",
      "Iteration 26135: loss = 2.6332852e-06,0.0060167904\n",
      "Iteration 26140: loss = 2.6828711e-06,0.005354681\n",
      "Iteration 26145: loss = 2.7192425e-06,0.004914588\n",
      "Iteration 26150: loss = 2.7645935e-06,0.0044648307\n",
      "Iteration 26155: loss = 2.7823337e-06,0.0043103932\n",
      "Iteration 26160: loss = 2.8423956e-06,0.004067342\n",
      "Iteration 26165: loss = 2.769012e-06,0.005743246\n",
      "Iteration 26170: loss = 2.8768852e-06,0.013643824\n",
      "Iteration 26175: loss = 1.9316e-06,0.087031364\n",
      "Iteration 26180: loss = 9.0589737e-07,0.2646001\n",
      "Iteration 26185: loss = 1.9746754e-07,0.79794264\n",
      "Iteration 26190: loss = 9.110686e-08,1.0995084\n",
      "Iteration 26195: loss = 7.774354e-08,1.1703106\n",
      "Iteration 26200: loss = 6.543737e-08,1.2211188\n",
      "Iteration 26205: loss = 8.8418794e-08,1.1013741\n",
      "Iteration 26210: loss = 1.0703599e-07,1.0276123\n",
      "Iteration 26215: loss = 1.5806862e-07,0.86294466\n",
      "Iteration 26220: loss = 2.329308e-07,0.6944436\n",
      "Iteration 26225: loss = 3.52979e-07,0.5164541\n",
      "Iteration 26230: loss = 5.602587e-07,0.3301807\n",
      "Iteration 26235: loss = 8.084343e-07,0.20325987\n",
      "Iteration 26240: loss = 1.1005578e-06,0.11640539\n",
      "Iteration 26245: loss = 1.4119846e-06,0.06364741\n",
      "Iteration 26250: loss = 1.6681506e-06,0.03749823\n",
      "Iteration 26255: loss = 1.8818085e-06,0.023278125\n",
      "Iteration 26260: loss = 2.037659e-06,0.016065134\n",
      "Iteration 26265: loss = 2.160642e-06,0.011769731\n",
      "Iteration 26270: loss = 2.24699e-06,0.009336205\n",
      "Iteration 26275: loss = 2.3181299e-06,0.007678783\n",
      "Iteration 26280: loss = 2.3792734e-06,0.006488631\n",
      "Iteration 26285: loss = 2.4238052e-06,0.005726491\n",
      "Iteration 26290: loss = 2.4724216e-06,0.005027837\n",
      "Iteration 26295: loss = 2.4965236e-06,0.004709131\n",
      "Iteration 26300: loss = 2.5486825e-06,0.0042481497\n",
      "Iteration 26305: loss = 2.5182371e-06,0.0050446074\n",
      "Iteration 26310: loss = 2.6134474e-06,0.008006295\n",
      "Iteration 26315: loss = 2.0974765e-06,0.042169727\n",
      "Iteration 26320: loss = 1.4499104e-06,0.16347405\n",
      "Iteration 26325: loss = 3.356623e-07,0.5254557\n",
      "Iteration 26330: loss = 9.671484e-08,1.0839114\n",
      "Iteration 26335: loss = 8.2614946e-08,1.1216618\n",
      "Iteration 26340: loss = 6.608711e-08,1.1910564\n",
      "Iteration 26345: loss = 7.399468e-08,1.1533258\n",
      "Iteration 26350: loss = 9.9522374e-08,1.0329878\n",
      "Iteration 26355: loss = 1.267377e-07,0.92904615\n",
      "Iteration 26360: loss = 2.0043626e-07,0.73401046\n",
      "Iteration 26365: loss = 3.0499618e-07,0.5504269\n",
      "Iteration 26370: loss = 4.7660274e-07,0.3673505\n",
      "Iteration 26375: loss = 7.221203e-07,0.21720377\n",
      "Iteration 26380: loss = 9.835506e-07,0.12703103\n",
      "Iteration 26385: loss = 1.281007e-06,0.06786526\n",
      "Iteration 26390: loss = 1.5508462e-06,0.037166893\n",
      "Iteration 26395: loss = 1.7276922e-06,0.024074482\n",
      "Iteration 26400: loss = 1.886619e-06,0.015818143\n",
      "Iteration 26405: loss = 2.0048199e-06,0.011362496\n",
      "Iteration 26410: loss = 2.0719217e-06,0.009274995\n",
      "Iteration 26415: loss = 2.1507656e-06,0.00730095\n",
      "Iteration 26420: loss = 2.1932362e-06,0.0063600857\n",
      "Iteration 26425: loss = 2.2442885e-06,0.0054271203\n",
      "Iteration 26430: loss = 2.271305e-06,0.00496801\n",
      "Iteration 26435: loss = 2.314526e-06,0.004402932\n",
      "Iteration 26440: loss = 2.3121688e-06,0.0045572305\n",
      "Iteration 26445: loss = 2.3812242e-06,0.0048831995\n",
      "Iteration 26450: loss = 2.1839544e-06,0.014081486\n",
      "Iteration 26455: loss = 2.112463e-06,0.056531116\n",
      "Iteration 26460: loss = 7.799904e-07,0.27290782\n",
      "Iteration 26465: loss = 2.2182542e-07,0.6796734\n",
      "Iteration 26470: loss = 9.143457e-08,1.0624919\n",
      "Iteration 26475: loss = 8.825595e-08,1.075904\n",
      "Iteration 26480: loss = 6.802974e-08,1.1587554\n",
      "Iteration 26485: loss = 8.3710916e-08,1.0793419\n",
      "Iteration 26490: loss = 1.17007794e-07,0.94134384\n",
      "Iteration 26495: loss = 1.574594e-07,0.8104071\n",
      "Iteration 26500: loss = 2.596893e-07,0.594157\n",
      "Iteration 26505: loss = 3.996721e-07,0.4110408\n",
      "Iteration 26510: loss = 5.8674067e-07,0.2652698\n",
      "Iteration 26515: loss = 8.626889e-07,0.14303\n",
      "Iteration 26520: loss = 1.1420912e-06,0.07630399\n",
      "Iteration 26525: loss = 1.3625178e-06,0.044769183\n",
      "Iteration 26530: loss = 1.5775017e-06,0.02549085\n",
      "Iteration 26535: loss = 1.729888e-06,0.016490031\n",
      "Iteration 26540: loss = 1.8157749e-06,0.012560213\n",
      "Iteration 26545: loss = 1.9133738e-06,0.009123979\n",
      "Iteration 26550: loss = 1.9644624e-06,0.0076160384\n",
      "Iteration 26555: loss = 2.0161192e-06,0.0063477093\n",
      "Iteration 26560: loss = 2.0631207e-06,0.005380638\n",
      "Iteration 26565: loss = 2.0834455e-06,0.0049887113\n",
      "Iteration 26570: loss = 2.1308472e-06,0.004339248\n",
      "Iteration 26575: loss = 2.1088426e-06,0.0050163367\n",
      "Iteration 26580: loss = 2.1853994e-06,0.0066737365\n",
      "Iteration 26585: loss = 1.8557944e-06,0.029604245\n",
      "Iteration 26590: loss = 1.5056424e-06,0.115128204\n",
      "Iteration 26595: loss = 4.2992284e-07,0.39395407\n",
      "Iteration 26600: loss = 1.1928174e-07,0.94725144\n",
      "Iteration 26605: loss = 8.184685e-08,1.0675035\n",
      "Iteration 26610: loss = 7.921984e-08,1.0862715\n",
      "Iteration 26615: loss = 6.784034e-08,1.1428167\n",
      "Iteration 26620: loss = 1.0033093e-07,0.9780514\n",
      "Iteration 26625: loss = 1.3408594e-07,0.8569238\n",
      "Iteration 26630: loss = 1.9303883e-07,0.6953377\n",
      "Iteration 26635: loss = 3.2927412e-07,0.46563148\n",
      "Iteration 26640: loss = 4.970672e-07,0.30130184\n",
      "Iteration 26645: loss = 6.987371e-07,0.1849321\n",
      "Iteration 26650: loss = 9.854651e-07,0.092317596\n",
      "Iteration 26655: loss = 1.2353988e-06,0.048793577\n",
      "Iteration 26660: loss = 1.3941379e-06,0.031121481\n",
      "Iteration 26665: loss = 1.5735541e-06,0.017924488\n",
      "Iteration 26670: loss = 1.6713058e-06,0.012885322\n",
      "Iteration 26675: loss = 1.7308221e-06,0.010360648\n",
      "Iteration 26680: loss = 1.8146761e-06,0.0075860666\n",
      "Iteration 26685: loss = 1.8373185e-06,0.0068644853\n",
      "Iteration 26690: loss = 1.8924896e-06,0.0055515356\n",
      "Iteration 26695: loss = 1.9004348e-06,0.005379761\n",
      "Iteration 26700: loss = 1.94778e-06,0.0047920737\n",
      "Iteration 26705: loss = 1.891095e-06,0.0071809357\n",
      "Iteration 26710: loss = 1.9544293e-06,0.014325617\n",
      "Iteration 26715: loss = 1.3708495e-06,0.07931395\n",
      "Iteration 26720: loss = 7.840874e-07,0.21016276\n",
      "Iteration 26725: loss = 2.3631047e-07,0.59729\n",
      "Iteration 26730: loss = 1.033348e-07,0.963563\n",
      "Iteration 26735: loss = 1.03254415e-07,0.95647955\n",
      "Iteration 26740: loss = 9.166769e-08,0.99445873\n",
      "Iteration 26745: loss = 1.0730167e-07,0.93180186\n",
      "Iteration 26750: loss = 1.6739727e-07,0.7332103\n",
      "Iteration 26755: loss = 2.2060958e-07,0.6127103\n",
      "Iteration 26760: loss = 3.3576842e-07,0.43090492\n",
      "Iteration 26765: loss = 5.48567e-07,0.2433276\n",
      "Iteration 26770: loss = 7.3677955e-07,0.14865725\n",
      "Iteration 26775: loss = 9.416702e-07,0.086533286\n",
      "Iteration 26780: loss = 1.2063512e-06,0.040770844\n",
      "Iteration 26785: loss = 1.3378112e-06,0.026580898\n",
      "Iteration 26790: loss = 1.4439441e-06,0.018368231\n",
      "Iteration 26795: loss = 1.5686552e-06,0.011553178\n",
      "Iteration 26800: loss = 1.590704e-06,0.010431181\n",
      "Iteration 26805: loss = 1.6667719e-06,0.007637066\n",
      "Iteration 26810: loss = 1.6815355e-06,0.007189817\n",
      "Iteration 26815: loss = 1.7316273e-06,0.0062539265\n",
      "Iteration 26820: loss = 1.6711874e-06,0.009687072\n",
      "Iteration 26825: loss = 1.7134154e-06,0.017896416\n",
      "Iteration 26830: loss = 1.185177e-06,0.086275354\n",
      "Iteration 26835: loss = 7.1715476e-07,0.1984377\n",
      "Iteration 26840: loss = 2.670356e-07,0.51619095\n",
      "Iteration 26845: loss = 1.4006457e-07,0.8089344\n",
      "Iteration 26850: loss = 1.4130973e-07,0.79121757\n",
      "Iteration 26855: loss = 1.3641784e-07,0.8051209\n",
      "Iteration 26860: loss = 1.5563488e-07,0.7436937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26865: loss = 2.366216e-07,0.557803\n",
      "Iteration 26870: loss = 3.2186463e-07,0.4245746\n",
      "Iteration 26875: loss = 4.5689364e-07,0.28641146\n",
      "Iteration 26880: loss = 6.76554e-07,0.15400375\n",
      "Iteration 26885: loss = 8.5968776e-07,0.09123957\n",
      "Iteration 26890: loss = 1.0249792e-06,0.055134054\n",
      "Iteration 26895: loss = 1.2040919e-06,0.030077666\n",
      "Iteration 26900: loss = 1.3095764e-06,0.020046135\n",
      "Iteration 26905: loss = 1.3796213e-06,0.014962652\n",
      "Iteration 26910: loss = 1.4581041e-06,0.010619006\n",
      "Iteration 26915: loss = 1.5015409e-06,0.008703791\n",
      "Iteration 26920: loss = 1.5211472e-06,0.008179712\n",
      "Iteration 26925: loss = 1.5863952e-06,0.0076761786\n",
      "Iteration 26930: loss = 1.4332645e-06,0.021662645\n",
      "Iteration 26935: loss = 1.3560276e-06,0.065020435\n",
      "Iteration 26940: loss = 5.6933874e-07,0.26322392\n",
      "Iteration 26945: loss = 2.1566797e-07,0.5857077\n",
      "Iteration 26950: loss = 1.0888491e-07,0.8970993\n",
      "Iteration 26955: loss = 1.0627698e-07,0.89675426\n",
      "Iteration 26960: loss = 9.452314e-08,0.9381853\n",
      "Iteration 26965: loss = 1.0631433e-07,0.8894355\n",
      "Iteration 26970: loss = 1.7148409e-07,0.6728738\n",
      "Iteration 26975: loss = 2.3661585e-07,0.5319001\n",
      "Iteration 26980: loss = 3.380006e-07,0.37860817\n",
      "Iteration 26985: loss = 5.338229e-07,0.20857517\n",
      "Iteration 26990: loss = 7.2528246e-07,0.11637479\n",
      "Iteration 26995: loss = 8.869333e-07,0.06913148\n",
      "Iteration 27000: loss = 1.0862703e-06,0.03441318\n",
      "Iteration 27005: loss = 1.2125257e-06,0.020609733\n",
      "Iteration 27010: loss = 1.2759333e-06,0.015357938\n",
      "Iteration 27015: loss = 1.3639602e-06,0.010025129\n",
      "Iteration 27020: loss = 1.3956959e-06,0.008428257\n",
      "Iteration 27025: loss = 1.4333846e-06,0.0068697166\n",
      "Iteration 27030: loss = 1.4774287e-06,0.005471012\n",
      "Iteration 27035: loss = 1.4644107e-06,0.006178606\n",
      "Iteration 27040: loss = 1.520264e-06,0.006835511\n",
      "Iteration 27045: loss = 1.3306504e-06,0.025939684\n",
      "Iteration 27050: loss = 1.1754781e-06,0.08864116\n",
      "Iteration 27055: loss = 4.3455645e-07,0.31138855\n",
      "Iteration 27060: loss = 1.5273777e-07,0.7263508\n",
      "Iteration 27065: loss = 8.839954e-08,0.9504271\n",
      "Iteration 27070: loss = 9.26723e-08,0.9357095\n",
      "Iteration 27075: loss = 7.965618e-08,0.98572993\n",
      "Iteration 27080: loss = 9.850752e-08,0.8968212\n",
      "Iteration 27085: loss = 1.5836123e-07,0.6833079\n",
      "Iteration 27090: loss = 2.1562839e-07,0.5455687\n",
      "Iteration 27095: loss = 3.225962e-07,0.37313327\n",
      "Iteration 27100: loss = 5.0848723e-07,0.20351596\n",
      "Iteration 27105: loss = 6.7352534e-07,0.11816589\n",
      "Iteration 27110: loss = 8.4356003e-07,0.065725066\n",
      "Iteration 27115: loss = 1.0281834e-06,0.032135237\n",
      "Iteration 27120: loss = 1.1241249e-06,0.02080702\n",
      "Iteration 27125: loss = 1.2030372e-06,0.014137279\n",
      "Iteration 27130: loss = 1.264034e-06,0.0102303205\n",
      "Iteration 27135: loss = 1.2982127e-06,0.008403802\n",
      "Iteration 27140: loss = 1.3206208e-06,0.007405369\n",
      "Iteration 27145: loss = 1.3612697e-06,0.0061793337\n",
      "Iteration 27150: loss = 1.3141167e-06,0.009252379\n",
      "Iteration 27155: loss = 1.3572526e-06,0.0139444135\n",
      "Iteration 27160: loss = 1.0313365e-06,0.061555184\n",
      "Iteration 27165: loss = 7.4169515e-07,0.14311083\n",
      "Iteration 27170: loss = 3.152617e-07,0.37055492\n",
      "Iteration 27175: loss = 1.6210193e-07,0.67229956\n",
      "Iteration 27180: loss = 1.4555768e-07,0.7013221\n",
      "Iteration 27185: loss = 1.5141474e-07,0.68842536\n",
      "Iteration 27190: loss = 1.5115187e-07,0.6805166\n",
      "Iteration 27195: loss = 2.0836774e-07,0.53882843\n",
      "Iteration 27200: loss = 3.0182915e-07,0.3773588\n",
      "Iteration 27205: loss = 3.9341424e-07,0.2724518\n",
      "Iteration 27210: loss = 5.371524e-07,0.1653952\n",
      "Iteration 27215: loss = 7.13731e-07,0.08695082\n",
      "Iteration 27220: loss = 8.205387e-07,0.056956045\n",
      "Iteration 27225: loss = 9.364839e-07,0.034663543\n",
      "Iteration 27230: loss = 1.0509234e-06,0.02004851\n",
      "Iteration 27235: loss = 1.0726453e-06,0.017807232\n",
      "Iteration 27240: loss = 1.1469012e-06,0.012403516\n",
      "Iteration 27245: loss = 1.1105493e-06,0.01697676\n",
      "Iteration 27250: loss = 1.14465e-06,0.023016496\n",
      "Iteration 27255: loss = 8.434543e-07,0.086145304\n",
      "Iteration 27260: loss = 5.8698663e-07,0.17169866\n",
      "Iteration 27265: loss = 2.8247035e-07,0.39070928\n",
      "Iteration 27270: loss = 1.7585374e-07,0.6088271\n",
      "Iteration 27275: loss = 1.7181019e-07,0.6031577\n",
      "Iteration 27280: loss = 1.7690564e-07,0.5935742\n",
      "Iteration 27285: loss = 1.8980161e-07,0.5555869\n",
      "Iteration 27290: loss = 2.6009567e-07,0.41774783\n",
      "Iteration 27295: loss = 3.555449e-07,0.28982982\n",
      "Iteration 27300: loss = 4.4976187e-07,0.20253101\n",
      "Iteration 27305: loss = 5.917949e-07,0.11811591\n",
      "Iteration 27310: loss = 7.350704e-07,0.06593241\n",
      "Iteration 27315: loss = 8.141175e-07,0.045988195\n",
      "Iteration 27320: loss = 9.244562e-07,0.026533678\n",
      "Iteration 27325: loss = 9.767258e-07,0.019752227\n",
      "Iteration 27330: loss = 1.0308295e-06,0.014531273\n",
      "Iteration 27335: loss = 1.0340469e-06,0.015470519\n",
      "Iteration 27340: loss = 1.0798277e-06,0.018723318\n",
      "Iteration 27345: loss = 8.388938e-07,0.07508821\n",
      "Iteration 27350: loss = 5.9760595e-07,0.17310786\n",
      "Iteration 27355: loss = 2.4611674e-07,0.42614844\n",
      "Iteration 27360: loss = 1.278691e-07,0.7357736\n",
      "Iteration 27365: loss = 1.17293204e-07,0.7528153\n",
      "Iteration 27370: loss = 1.1835535e-07,0.7528873\n",
      "Iteration 27375: loss = 1.2161611e-07,0.73259914\n",
      "Iteration 27380: loss = 1.7100524e-07,0.57893485\n",
      "Iteration 27385: loss = 2.3927603e-07,0.42725915\n",
      "Iteration 27390: loss = 3.209335e-07,0.30634063\n",
      "Iteration 27395: loss = 4.5241276e-07,0.18007642\n",
      "Iteration 27400: loss = 5.9228086e-07,0.10077044\n",
      "Iteration 27405: loss = 7.0482287e-07,0.0608128\n",
      "Iteration 27410: loss = 8.167364e-07,0.03438534\n",
      "Iteration 27415: loss = 9.0746124e-07,0.020129876\n",
      "Iteration 27420: loss = 9.510349e-07,0.01499033\n",
      "Iteration 27425: loss = 1.001845e-06,0.010388114\n",
      "Iteration 27430: loss = 1.0302768e-06,0.008316542\n",
      "Iteration 27435: loss = 1.043651e-06,0.0075602066\n",
      "Iteration 27440: loss = 1.0828775e-06,0.0063886736\n",
      "Iteration 27445: loss = 1.0155278e-06,0.015171171\n",
      "Iteration 27450: loss = 1.0140932e-06,0.045204252\n",
      "Iteration 27455: loss = 4.9792715e-07,0.22759345\n",
      "Iteration 27460: loss = 2.0943277e-07,0.4718979\n",
      "Iteration 27465: loss = 9.26099e-08,0.8628181\n",
      "Iteration 27470: loss = 7.639546e-08,0.92534775\n",
      "Iteration 27475: loss = 7.165284e-08,0.9474269\n",
      "Iteration 27480: loss = 6.8916144e-08,0.9652481\n",
      "Iteration 27485: loss = 1.040681e-07,0.775479\n",
      "Iteration 27490: loss = 1.495703e-07,0.6119374\n",
      "Iteration 27495: loss = 1.9937033e-07,0.47997028\n",
      "Iteration 27500: loss = 3.073619e-07,0.29882213\n",
      "Iteration 27505: loss = 4.3983516e-07,0.16855636\n",
      "Iteration 27510: loss = 5.536158e-07,0.10160378\n",
      "Iteration 27515: loss = 6.8936805e-07,0.051987447\n",
      "Iteration 27520: loss = 8.038253e-07,0.026901226\n",
      "Iteration 27525: loss = 8.6571754e-07,0.017886236\n",
      "Iteration 27530: loss = 9.20982e-07,0.011885916\n",
      "Iteration 27535: loss = 9.5211635e-07,0.009162683\n",
      "Iteration 27540: loss = 9.629941e-07,0.008241183\n",
      "Iteration 27545: loss = 9.958832e-07,0.006250645\n",
      "Iteration 27550: loss = 9.786396e-07,0.0075284904\n",
      "Iteration 27555: loss = 1.0113864e-06,0.007459202\n",
      "Iteration 27560: loss = 9.124203e-07,0.02188969\n",
      "Iteration 27565: loss = 8.5966457e-07,0.05708367\n",
      "Iteration 27570: loss = 4.3837792e-07,0.20775412\n",
      "Iteration 27575: loss = 2.2744308e-07,0.40984923\n",
      "Iteration 27580: loss = 1.4039703e-07,0.63753206\n",
      "Iteration 27585: loss = 1.2591916e-07,0.66938704\n",
      "Iteration 27590: loss = 1.2246105e-07,0.6852914\n",
      "Iteration 27595: loss = 1.2898704e-07,0.6540974\n",
      "Iteration 27600: loss = 1.7686331e-07,0.512409\n",
      "Iteration 27605: loss = 2.3398701e-07,0.3865074\n",
      "Iteration 27610: loss = 2.9725803e-07,0.28843945\n",
      "Iteration 27615: loss = 4.0699794e-07,0.17396876\n",
      "Iteration 27620: loss = 5.097335e-07,0.10557278\n",
      "Iteration 27625: loss = 5.924452e-07,0.068683416\n",
      "Iteration 27630: loss = 6.940266e-07,0.03831302\n",
      "Iteration 27635: loss = 7.4777773e-07,0.026937325\n",
      "Iteration 27640: loss = 8.0018134e-07,0.018636169\n",
      "Iteration 27645: loss = 8.106974e-07,0.017826378\n",
      "Iteration 27650: loss = 8.44037e-07,0.017108636\n",
      "Iteration 27655: loss = 7.3309246e-07,0.044966143\n",
      "Iteration 27660: loss = 6.524003e-07,0.09177314\n",
      "Iteration 27665: loss = 3.3775254e-07,0.2514285\n",
      "Iteration 27670: loss = 1.9752854e-07,0.44894668\n",
      "Iteration 27675: loss = 1.5532135e-07,0.56058013\n",
      "Iteration 27680: loss = 1.4947504e-07,0.5663991\n",
      "Iteration 27685: loss = 1.4595774e-07,0.57924205\n",
      "Iteration 27690: loss = 1.6901738e-07,0.5067394\n",
      "Iteration 27695: loss = 2.2288744e-07,0.3854686\n",
      "Iteration 27700: loss = 2.7601843e-07,0.29525915\n",
      "Iteration 27705: loss = 3.4987406e-07,0.20433597\n",
      "Iteration 27710: loss = 4.4603067e-07,0.12563486\n",
      "Iteration 27715: loss = 5.2170134e-07,0.08331798\n",
      "Iteration 27720: loss = 6.032861e-07,0.0513256\n",
      "Iteration 27725: loss = 6.586957e-07,0.03564724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27730: loss = 7.1381055e-07,0.024477808\n",
      "Iteration 27735: loss = 7.0785535e-07,0.027873334\n",
      "Iteration 27740: loss = 7.384339e-07,0.033830635\n",
      "Iteration 27745: loss = 5.300803e-07,0.119793534\n",
      "Iteration 27750: loss = 3.640496e-07,0.212577\n",
      "Iteration 27755: loss = 1.9731026e-07,0.4247638\n",
      "Iteration 27760: loss = 1.3890079e-07,0.589502\n",
      "Iteration 27765: loss = 1.3590987e-07,0.58614296\n",
      "Iteration 27770: loss = 1.3630832e-07,0.58739895\n",
      "Iteration 27775: loss = 1.4902163e-07,0.5397963\n",
      "Iteration 27780: loss = 1.9126662e-07,0.4284576\n",
      "Iteration 27785: loss = 2.436141e-07,0.32273635\n",
      "Iteration 27790: loss = 3.0563922e-07,0.23227541\n",
      "Iteration 27795: loss = 3.87378e-07,0.15021601\n",
      "Iteration 27800: loss = 4.6913132e-07,0.09422999\n",
      "Iteration 27805: loss = 5.3270384e-07,0.06323478\n",
      "Iteration 27810: loss = 6.0698284e-07,0.03772598\n",
      "Iteration 27815: loss = 6.469072e-07,0.027501892\n",
      "Iteration 27820: loss = 6.9197654e-07,0.01882294\n",
      "Iteration 27825: loss = 7.0055734e-07,0.018338751\n",
      "Iteration 27830: loss = 7.3408205e-07,0.020108666\n",
      "Iteration 27835: loss = 5.8393624e-07,0.08309201\n",
      "Iteration 27840: loss = 4.0958946e-07,0.20191933\n",
      "Iteration 27845: loss = 1.6601604e-07,0.47560766\n",
      "Iteration 27850: loss = 8.7250555e-08,0.78827447\n",
      "Iteration 27855: loss = 7.69023e-08,0.8289044\n",
      "Iteration 27860: loss = 7.2616274e-08,0.8489834\n",
      "Iteration 27865: loss = 7.363877e-08,0.8432778\n",
      "Iteration 27870: loss = 9.766493e-08,0.70751786\n",
      "Iteration 27875: loss = 1.254662e-07,0.59239393\n",
      "Iteration 27880: loss = 1.6560794e-07,0.4626392\n",
      "Iteration 27885: loss = 2.3792136e-07,0.3070313\n",
      "Iteration 27890: loss = 3.0716342e-07,0.20704783\n",
      "Iteration 27895: loss = 3.8021474e-07,0.13519214\n",
      "Iteration 27900: loss = 4.7802786e-07,0.073090866\n",
      "Iteration 27905: loss = 5.4374567e-07,0.04546935\n",
      "Iteration 27910: loss = 5.988156e-07,0.028951786\n",
      "Iteration 27915: loss = 6.45758e-07,0.018686853\n",
      "Iteration 27920: loss = 6.6510574e-07,0.015191271\n",
      "Iteration 27925: loss = 6.896678e-07,0.011554468\n",
      "Iteration 27930: loss = 6.9724683e-07,0.010648817\n",
      "Iteration 27935: loss = 7.154251e-07,0.009589591\n",
      "Iteration 27940: loss = 6.739677e-07,0.020985367\n",
      "Iteration 27945: loss = 6.60034e-07,0.058979362\n",
      "Iteration 27950: loss = 3.2145545e-07,0.2531565\n",
      "Iteration 27955: loss = 1.4901644e-07,0.49906006\n",
      "Iteration 27960: loss = 8.425775e-08,0.7733986\n",
      "Iteration 27965: loss = 7.26014e-08,0.8307504\n",
      "Iteration 27970: loss = 6.466484e-08,0.8726496\n",
      "Iteration 27975: loss = 6.3948654e-08,0.88000715\n",
      "Iteration 27980: loss = 8.603505e-08,0.73854053\n",
      "Iteration 27985: loss = 1.1131198e-07,0.61942524\n",
      "Iteration 27990: loss = 1.42319e-07,0.50359863\n",
      "Iteration 27995: loss = 1.9809625e-07,0.35719737\n",
      "Iteration 28000: loss = 2.5852077e-07,0.24711351\n",
      "Iteration 28005: loss = 3.2301844e-07,0.16648103\n",
      "Iteration 28010: loss = 4.00959e-07,0.10018587\n",
      "Iteration 28015: loss = 4.678313e-07,0.061696764\n",
      "Iteration 28020: loss = 5.214586e-07,0.039984256\n",
      "Iteration 28025: loss = 5.6502137e-07,0.026891548\n",
      "Iteration 28030: loss = 5.919526e-07,0.020469688\n",
      "Iteration 28035: loss = 6.0257526e-07,0.018336454\n",
      "Iteration 28040: loss = 6.2729356e-07,0.014935593\n",
      "Iteration 28045: loss = 5.888376e-07,0.026415136\n",
      "Iteration 28050: loss = 5.886768e-07,0.046754118\n",
      "Iteration 28055: loss = 3.6051802e-07,0.17782855\n",
      "Iteration 28060: loss = 2.1653425e-07,0.3097901\n",
      "Iteration 28065: loss = 1.3468527e-07,0.5278866\n",
      "Iteration 28070: loss = 1.07252454e-07,0.6158545\n",
      "Iteration 28075: loss = 1.0136023e-07,0.64543164\n",
      "Iteration 28080: loss = 1.013132e-07,0.63783836\n",
      "Iteration 28085: loss = 1.1896899e-07,0.5637444\n",
      "Iteration 28090: loss = 1.4432781e-07,0.47194323\n",
      "Iteration 28095: loss = 1.7477579e-07,0.38527238\n",
      "Iteration 28100: loss = 2.2208097e-07,0.28378624\n",
      "Iteration 28105: loss = 2.6874167e-07,0.20855542\n",
      "Iteration 28110: loss = 3.169929e-07,0.15037087\n",
      "Iteration 28115: loss = 3.7406767e-07,0.10014718\n",
      "Iteration 28120: loss = 4.1897647e-07,0.07085379\n",
      "Iteration 28125: loss = 4.6323825e-07,0.049199544\n",
      "Iteration 28130: loss = 4.787326e-07,0.04440127\n",
      "Iteration 28135: loss = 5.0799497e-07,0.042808678\n",
      "Iteration 28140: loss = 3.970312e-07,0.12306481\n",
      "Iteration 28145: loss = 2.8931353e-07,0.21989992\n",
      "Iteration 28150: loss = 1.5182098e-07,0.43451446\n",
      "Iteration 28155: loss = 1.0168666e-07,0.6295424\n",
      "Iteration 28160: loss = 9.29231e-08,0.6561413\n",
      "Iteration 28165: loss = 8.903748e-08,0.6775886\n",
      "Iteration 28170: loss = 9.157856e-08,0.6582463\n",
      "Iteration 28175: loss = 1.088135e-07,0.57786185\n",
      "Iteration 28180: loss = 1.3148956e-07,0.48699126\n",
      "Iteration 28185: loss = 1.6298897e-07,0.39021719\n",
      "Iteration 28190: loss = 2.0137786e-07,0.29805812\n",
      "Iteration 28195: loss = 2.420254e-07,0.22385114\n",
      "Iteration 28200: loss = 2.884293e-07,0.1605571\n",
      "Iteration 28205: loss = 3.3357256e-07,0.114223585\n",
      "Iteration 28210: loss = 3.7741881e-07,0.0802431\n",
      "Iteration 28215: loss = 4.0085715e-07,0.06610025\n",
      "Iteration 28220: loss = 4.3637422e-07,0.0516743\n",
      "Iteration 28225: loss = 3.8981378e-07,0.09090079\n",
      "Iteration 28230: loss = 3.4716848e-07,0.15193114\n",
      "Iteration 28235: loss = 1.8844122e-07,0.325982\n",
      "Iteration 28240: loss = 1.2080066e-07,0.5208631\n",
      "Iteration 28245: loss = 1.0373365e-07,0.5821191\n",
      "Iteration 28250: loss = 9.65501e-08,0.6140635\n",
      "Iteration 28255: loss = 9.07987e-08,0.6378673\n",
      "Iteration 28260: loss = 1.0154994e-07,0.5834093\n",
      "Iteration 28265: loss = 1.2180845e-07,0.4974987\n",
      "Iteration 28270: loss = 1.4204394e-07,0.42478606\n",
      "Iteration 28275: loss = 1.7104263e-07,0.3423741\n",
      "Iteration 28280: loss = 2.0688822e-07,0.26195747\n",
      "Iteration 28285: loss = 2.3796997e-07,0.20644553\n",
      "Iteration 28290: loss = 2.7674741e-07,0.15234466\n",
      "Iteration 28295: loss = 3.141101e-07,0.111974336\n",
      "Iteration 28300: loss = 3.4347218e-07,0.08652008\n",
      "Iteration 28305: loss = 3.7726485e-07,0.06366063\n",
      "Iteration 28310: loss = 3.8091375e-07,0.06520973\n",
      "Iteration 28315: loss = 3.992036e-07,0.084088005\n",
      "Iteration 28320: loss = 2.3434961e-07,0.3036434\n",
      "Iteration 28325: loss = 1.16122074e-07,0.50249016\n",
      "Iteration 28330: loss = 6.571733e-08,0.79123086\n",
      "Iteration 28335: loss = 5.1275407e-08,0.889767\n",
      "Iteration 28340: loss = 4.2749907e-08,0.9548871\n",
      "Iteration 28345: loss = 4.0084057e-08,0.9858027\n",
      "Iteration 28350: loss = 4.8458592e-08,0.89869046\n",
      "Iteration 28355: loss = 5.832368e-08,0.8099571\n",
      "Iteration 28360: loss = 7.348217e-08,0.7033292\n",
      "Iteration 28365: loss = 9.653141e-08,0.5731566\n",
      "Iteration 28370: loss = 1.1901756e-07,0.4750732\n",
      "Iteration 28375: loss = 1.4824342e-07,0.37453446\n",
      "Iteration 28380: loss = 1.8767035e-07,0.27374128\n",
      "Iteration 28385: loss = 2.2191011e-07,0.20700912\n",
      "Iteration 28390: loss = 2.5689846e-07,0.15426159\n",
      "Iteration 28395: loss = 2.9287213e-07,0.1122031\n",
      "Iteration 28400: loss = 3.1504763e-07,0.09078205\n",
      "Iteration 28405: loss = 3.3882998e-07,0.071482755\n",
      "Iteration 28410: loss = 3.5305197e-07,0.061289914\n",
      "Iteration 28415: loss = 3.6689698e-07,0.05268559\n",
      "Iteration 28420: loss = 3.671921e-07,0.053664897\n",
      "Iteration 28425: loss = 3.8320115e-07,0.055744242\n",
      "Iteration 28430: loss = 2.8803694e-07,0.17328683\n",
      "Iteration 28435: loss = 1.8271318e-07,0.3096778\n",
      "Iteration 28440: loss = 9.2590675e-08,0.5944344\n",
      "Iteration 28445: loss = 6.172739e-08,0.7621445\n",
      "Iteration 28450: loss = 5.3488268e-08,0.8355636\n",
      "Iteration 28455: loss = 4.8273822e-08,0.8710656\n",
      "Iteration 28460: loss = 5.2874388e-08,0.8278351\n",
      "Iteration 28465: loss = 6.298853e-08,0.74617887\n",
      "Iteration 28470: loss = 7.236691e-08,0.67756504\n",
      "Iteration 28475: loss = 8.8721606e-08,0.5816779\n",
      "Iteration 28480: loss = 1.09487445e-07,0.48159245\n",
      "Iteration 28485: loss = 1.3025307e-07,0.40148947\n",
      "Iteration 28490: loss = 1.5460647e-07,0.3248056\n",
      "Iteration 28495: loss = 1.8230843e-07,0.2551197\n",
      "Iteration 28500: loss = 2.0796055e-07,0.20315008\n",
      "Iteration 28505: loss = 2.3115642e-07,0.16412741\n",
      "Iteration 28510: loss = 2.5396773e-07,0.13201584\n",
      "Iteration 28515: loss = 2.6676508e-07,0.11604267\n",
      "Iteration 28520: loss = 2.8471064e-07,0.09737826\n",
      "Iteration 28525: loss = 2.7901874e-07,0.107848585\n",
      "Iteration 28530: loss = 2.8487966e-07,0.13348503\n",
      "Iteration 28535: loss = 1.7112787e-07,0.34898514\n",
      "Iteration 28540: loss = 9.4763664e-08,0.5337348\n",
      "Iteration 28545: loss = 6.4115504e-08,0.7315126\n",
      "Iteration 28550: loss = 5.2830217e-08,0.81018054\n",
      "Iteration 28555: loss = 4.4936915e-08,0.8721089\n",
      "Iteration 28560: loss = 4.4087898e-08,0.8837397\n",
      "Iteration 28565: loss = 5.10969e-08,0.8110858\n",
      "Iteration 28570: loss = 5.950123e-08,0.739222\n",
      "Iteration 28575: loss = 7.095575e-08,0.65538645\n",
      "Iteration 28580: loss = 8.494027e-08,0.56904566\n",
      "Iteration 28585: loss = 1.0000366e-07,0.49143568\n",
      "Iteration 28590: loss = 1.1915072e-07,0.40987608\n",
      "Iteration 28595: loss = 1.3925737e-07,0.33915198\n",
      "Iteration 28600: loss = 1.5782298e-07,0.2847406\n",
      "Iteration 28605: loss = 1.7772221e-07,0.23548397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28610: loss = 1.9306168e-07,0.20252587\n",
      "Iteration 28615: loss = 2.0766412e-07,0.17492066\n",
      "Iteration 28620: loss = 2.1481567e-07,0.16277663\n",
      "Iteration 28625: loss = 2.2761219e-07,0.14759912\n",
      "Iteration 28630: loss = 2.0067021e-07,0.2227389\n",
      "Iteration 28635: loss = 1.6165025e-07,0.35906628\n",
      "Iteration 28640: loss = 7.6908655e-08,0.60160637\n",
      "Iteration 28645: loss = 4.798905e-08,0.83170146\n",
      "Iteration 28650: loss = 3.9886363e-08,0.9119869\n",
      "Iteration 28655: loss = 3.545625e-08,0.9478293\n",
      "Iteration 28660: loss = 3.651695e-08,0.9350213\n",
      "Iteration 28665: loss = 4.1789217e-08,0.8744655\n",
      "Iteration 28670: loss = 4.7081045e-08,0.81522876\n",
      "Iteration 28675: loss = 5.559433e-08,0.7382746\n",
      "Iteration 28680: loss = 6.6720006e-08,0.65099394\n",
      "Iteration 28685: loss = 7.895417e-08,0.5703521\n",
      "Iteration 28690: loss = 9.294323e-08,0.49263158\n",
      "Iteration 28695: loss = 1.079323e-07,0.42253888\n",
      "Iteration 28700: loss = 1.22689e-07,0.3633873\n",
      "Iteration 28705: loss = 1.3724124e-07,0.31332713\n",
      "Iteration 28710: loss = 1.4903587e-07,0.27718574\n",
      "Iteration 28715: loss = 1.5949372e-07,0.24817237\n",
      "Iteration 28720: loss = 1.6726294e-07,0.22797924\n",
      "Iteration 28725: loss = 1.7505369e-07,0.20924512\n",
      "Iteration 28730: loss = 1.7774835e-07,0.20313801\n",
      "Iteration 28735: loss = 1.8634451e-07,0.19288646\n",
      "Iteration 28740: loss = 1.5452726e-07,0.3293938\n",
      "Iteration 28745: loss = 1.01253356e-07,0.51812273\n",
      "Iteration 28750: loss = 4.5364654e-08,0.8623505\n",
      "Iteration 28755: loss = 2.932172e-08,1.0072997\n",
      "Iteration 28760: loss = 2.4696853e-08,1.0702621\n",
      "Iteration 28765: loss = 2.1254108e-08,1.1343807\n",
      "Iteration 28770: loss = 2.2781236e-08,1.1061561\n",
      "Iteration 28775: loss = 2.6876966e-08,1.0336968\n",
      "Iteration 28780: loss = 3.1728046e-08,0.9587448\n",
      "Iteration 28785: loss = 3.9117843e-08,0.86478734\n",
      "Iteration 28790: loss = 4.705944e-08,0.78034097\n",
      "Iteration 28795: loss = 5.648835e-08,0.6937688\n",
      "Iteration 28800: loss = 6.7394275e-08,0.60937977\n",
      "Iteration 28805: loss = 7.917125e-08,0.5326967\n",
      "Iteration 28810: loss = 9.090988e-08,0.4667643\n",
      "Iteration 28815: loss = 1.02367856e-07,0.4110159\n",
      "Iteration 28820: loss = 1.11638705e-07,0.37046316\n",
      "Iteration 28825: loss = 1.188428e-07,0.34137028\n",
      "Iteration 28830: loss = 1.2567683e-07,0.31557593\n",
      "Iteration 28835: loss = 1.3098318e-07,0.29650626\n",
      "Iteration 28840: loss = 1.3522188e-07,0.28165972\n",
      "Iteration 28845: loss = 1.3895426e-07,0.2689013\n",
      "Iteration 28850: loss = 1.4097783e-07,0.26156917\n",
      "Iteration 28855: loss = 1.4380744e-07,0.2522975\n",
      "Iteration 28860: loss = 1.4246544e-07,0.25620377\n",
      "Iteration 28865: loss = 1.4584231e-07,0.26002747\n",
      "Iteration 28870: loss = 1.0957604e-07,0.4845354\n",
      "Iteration 28875: loss = 5.9274438e-08,0.6643253\n",
      "Iteration 28880: loss = 3.035326e-08,0.99914867\n",
      "Iteration 28885: loss = 2.258006e-08,1.1058064\n",
      "Iteration 28890: loss = 1.9960828e-08,1.1338472\n",
      "Iteration 28895: loss = 1.913986e-08,1.1383183\n",
      "Iteration 28900: loss = 2.1168786e-08,1.0943662\n",
      "Iteration 28905: loss = 2.3972644e-08,1.0430942\n",
      "Iteration 28910: loss = 2.8316208e-08,0.9729949\n",
      "Iteration 28915: loss = 3.3830492e-08,0.89377767\n",
      "Iteration 28920: loss = 3.9899486e-08,0.81744856\n",
      "Iteration 28925: loss = 4.797508e-08,0.7311023\n",
      "Iteration 28930: loss = 5.6536464e-08,0.65343755\n",
      "Iteration 28935: loss = 6.493812e-08,0.58691144\n",
      "Iteration 28940: loss = 7.2032314e-08,0.5366017\n",
      "Iteration 28945: loss = 7.850456e-08,0.49498713\n",
      "Iteration 28950: loss = 8.422836e-08,0.46078423\n",
      "Iteration 28955: loss = 8.901207e-08,0.43378285\n",
      "Iteration 28960: loss = 9.2913716e-08,0.41258097\n",
      "Iteration 28965: loss = 9.5767234e-08,0.3973472\n",
      "Iteration 28970: loss = 9.823276e-08,0.38431224\n",
      "Iteration 28975: loss = 9.9590046e-08,0.37655368\n",
      "Iteration 28980: loss = 1.00421175e-07,0.3712828\n",
      "Iteration 28985: loss = 1.0094703e-07,0.367437\n",
      "Iteration 28990: loss = 1.0087776e-07,0.3661704\n",
      "Iteration 28995: loss = 1.0110538e-07,0.36371934\n",
      "Iteration 29000: loss = 9.961375e-08,0.36987504\n",
      "Iteration 29005: loss = 1.0102827e-07,0.3771128\n",
      "Iteration 29010: loss = 7.498251e-08,0.6776219\n",
      "Iteration 29015: loss = 3.4100285e-08,0.8672944\n",
      "Iteration 29020: loss = 1.5593391e-08,1.1982298\n",
      "Iteration 29025: loss = 1.2020952e-08,1.2869577\n",
      "Iteration 29030: loss = 1.1278871e-08,1.3005108\n",
      "Iteration 29035: loss = 1.2253667e-08,1.2682933\n",
      "Iteration 29040: loss = 1.39179654e-08,1.2200398\n",
      "Iteration 29045: loss = 1.587752e-08,1.1696619\n",
      "Iteration 29050: loss = 1.9465737e-08,1.0894995\n",
      "Iteration 29055: loss = 2.3441833e-08,1.01246\n",
      "Iteration 29060: loss = 2.812998e-08,0.9337822\n",
      "Iteration 29065: loss = 3.4460793e-08,0.8431507\n",
      "Iteration 29070: loss = 4.0933486e-08,0.7637921\n",
      "Iteration 29075: loss = 4.7470824e-08,0.69385695\n",
      "Iteration 29080: loss = 5.2934585e-08,0.64160943\n",
      "Iteration 29085: loss = 5.7589375e-08,0.60050833\n",
      "Iteration 29090: loss = 6.180551e-08,0.5657694\n",
      "Iteration 29095: loss = 6.514036e-08,0.5396593\n",
      "Iteration 29100: loss = 6.79555e-08,0.5182993\n",
      "Iteration 29105: loss = 6.977911e-08,0.50443256\n",
      "Iteration 29110: loss = 7.0897585e-08,0.49546176\n",
      "Iteration 29115: loss = 7.134519e-08,0.49096957\n",
      "Iteration 29120: loss = 7.1367545e-08,0.48927996\n",
      "Iteration 29125: loss = 7.125701e-08,0.4884983\n",
      "Iteration 29130: loss = 7.104682e-08,0.48830932\n",
      "Iteration 29135: loss = 7.070727e-08,0.48900402\n",
      "Iteration 29140: loss = 7.003313e-08,0.49192417\n",
      "Iteration 29145: loss = 6.894803e-08,0.49773622\n",
      "Iteration 29150: loss = 6.7619816e-08,0.50527865\n",
      "Iteration 29155: loss = 6.632102e-08,0.5128433\n",
      "Iteration 29160: loss = 6.520525e-08,0.51918703\n",
      "Iteration 29165: loss = 6.435099e-08,0.52387005\n",
      "Iteration 29170: loss = 6.34057e-08,0.52928835\n",
      "Iteration 29175: loss = 6.2553916e-08,0.53428113\n",
      "Iteration 29180: loss = 6.0594466e-08,0.55014247\n",
      "Iteration 29185: loss = 6.065866e-08,0.60001326\n",
      "Iteration 29190: loss = 3.3438962e-08,1.0897062\n",
      "Iteration 29195: loss = 1.06130535e-08,1.426346\n",
      "Iteration 29200: loss = 6.3587002e-09,1.519547\n",
      "Iteration 29205: loss = 6.805282e-09,1.4603071\n",
      "Iteration 29210: loss = 8.087905e-09,1.38018\n",
      "Iteration 29215: loss = 1.070391e-08,1.2751505\n",
      "Iteration 29220: loss = 1.265967e-08,1.2138915\n",
      "Iteration 29225: loss = 1.4997147e-08,1.1517849\n",
      "Iteration 29230: loss = 1.8575543e-08,1.0674413\n",
      "Iteration 29235: loss = 2.2465272e-08,0.98739624\n",
      "Iteration 29240: loss = 2.71182e-08,0.90461993\n",
      "Iteration 29245: loss = 3.1662832e-08,0.83420783\n",
      "Iteration 29250: loss = 3.619134e-08,0.7720497\n",
      "Iteration 29255: loss = 3.9860854e-08,0.72621226\n",
      "Iteration 29260: loss = 4.2336552e-08,0.6967673\n",
      "Iteration 29265: loss = 4.4484818e-08,0.6723057\n",
      "Iteration 29270: loss = 4.6379014e-08,0.65149033\n",
      "Iteration 29275: loss = 4.7990756e-08,0.63399637\n",
      "Iteration 29280: loss = 4.919751e-08,0.62096256\n",
      "Iteration 29285: loss = 4.9518366e-08,0.6164287\n",
      "Iteration 29290: loss = 4.9201958e-08,0.6179736\n",
      "Iteration 29295: loss = 4.858278e-08,0.62252855\n",
      "Iteration 29300: loss = 4.802156e-08,0.62655824\n",
      "Iteration 29305: loss = 4.7640743e-08,0.62880766\n",
      "Iteration 29310: loss = 4.731177e-08,0.6305943\n",
      "Iteration 29315: loss = 4.6827058e-08,0.63399833\n",
      "Iteration 29320: loss = 4.6005585e-08,0.640841\n",
      "Iteration 29325: loss = 4.4984507e-08,0.6499723\n",
      "Iteration 29330: loss = 4.395834e-08,0.6593871\n",
      "Iteration 29335: loss = 4.3074607e-08,0.6673784\n",
      "Iteration 29340: loss = 4.236453e-08,0.6737516\n",
      "Iteration 29345: loss = 4.1743178e-08,0.67923826\n",
      "Iteration 29350: loss = 4.1046732e-08,0.6856653\n",
      "Iteration 29355: loss = 4.01521e-08,0.6944564\n",
      "Iteration 29360: loss = 3.915351e-08,0.7047604\n",
      "Iteration 29365: loss = 3.8119566e-08,0.71579957\n",
      "Iteration 29370: loss = 3.7117204e-08,0.7267096\n",
      "Iteration 29375: loss = 3.6205922e-08,0.736873\n",
      "Iteration 29380: loss = 3.5276628e-08,0.7473821\n",
      "Iteration 29385: loss = 3.4378886e-08,0.75798416\n",
      "Iteration 29390: loss = 3.3230158e-08,0.77230275\n",
      "Iteration 29395: loss = 3.2666247e-08,0.78549755\n",
      "Iteration 29400: loss = 2.7301203e-08,1.0247921\n",
      "Iteration 29405: loss = 1.3961188e-08,1.1741254\n",
      "Iteration 29410: loss = 5.0948894e-09,1.473465\n",
      "Iteration 29415: loss = 3.920958e-09,1.5380048\n",
      "Iteration 29420: loss = 4.837919e-09,1.5102563\n",
      "Iteration 29425: loss = 7.593516e-09,1.3706669\n",
      "Iteration 29430: loss = 9.955588e-09,1.2580209\n",
      "Iteration 29435: loss = 1.1632335e-08,1.1996827\n",
      "Iteration 29440: loss = 1.4840925e-08,1.1108929\n",
      "Iteration 29445: loss = 1.8013585e-08,1.0303884\n",
      "Iteration 29450: loss = 2.2548521e-08,0.9328443\n",
      "Iteration 29455: loss = 2.6093375e-08,0.8679266\n",
      "Iteration 29460: loss = 2.8158931e-08,0.8333571\n",
      "Iteration 29465: loss = 3.0112407e-08,0.80184525\n",
      "Iteration 29470: loss = 3.1259876e-08,0.7833478\n",
      "Iteration 29475: loss = 3.2208884e-08,0.76834154\n",
      "Iteration 29480: loss = 3.2971652e-08,0.75628483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29485: loss = 3.3465152e-08,0.748168\n",
      "Iteration 29490: loss = 3.3580104e-08,0.74533147\n",
      "Iteration 29495: loss = 3.3246796e-08,0.74858403\n",
      "Iteration 29500: loss = 3.249621e-08,0.7577603\n",
      "Iteration 29505: loss = 3.1483903e-08,0.7709711\n",
      "Iteration 29510: loss = 3.0714133e-08,0.78089494\n",
      "Iteration 29515: loss = 3.0183838e-08,0.7875561\n",
      "Iteration 29520: loss = 2.974935e-08,0.79283965\n",
      "Iteration 29525: loss = 2.9187467e-08,0.8002233\n",
      "Iteration 29530: loss = 2.8336089e-08,0.812333\n",
      "Iteration 29535: loss = 2.732707e-08,0.82735413\n",
      "Iteration 29540: loss = 2.6361311e-08,0.8421882\n",
      "Iteration 29545: loss = 2.556905e-08,0.85441095\n",
      "Iteration 29550: loss = 2.4984857e-08,0.8634383\n",
      "Iteration 29555: loss = 2.4448227e-08,0.8717279\n",
      "Iteration 29560: loss = 2.38277e-08,0.8818522\n",
      "Iteration 29565: loss = 2.3102757e-08,0.89412475\n",
      "Iteration 29570: loss = 2.2358243e-08,0.90717036\n",
      "Iteration 29575: loss = 2.1686416e-08,0.9191068\n",
      "Iteration 29580: loss = 2.1160941e-08,0.92849195\n",
      "Iteration 29585: loss = 2.072427e-08,0.93619597\n",
      "Iteration 29590: loss = 2.0300115e-08,0.94391257\n",
      "Iteration 29595: loss = 1.983901e-08,0.95246184\n",
      "Iteration 29600: loss = 1.9371724e-08,0.9613496\n",
      "Iteration 29605: loss = 1.8928512e-08,0.9700247\n",
      "Iteration 29610: loss = 1.8481638e-08,0.9789001\n",
      "Iteration 29615: loss = 1.8044e-08,0.987623\n",
      "Iteration 29620: loss = 1.7639982e-08,0.9958809\n",
      "Iteration 29625: loss = 1.725922e-08,1.0038625\n",
      "Iteration 29630: loss = 1.6740602e-08,1.0155399\n",
      "Iteration 29635: loss = 1.643766e-08,1.0271937\n",
      "Iteration 29640: loss = 1.4346042e-08,1.187565\n",
      "Iteration 29645: loss = 9.5960155e-09,1.3752997\n",
      "Iteration 29650: loss = 3.8597703e-09,1.5870955\n",
      "Iteration 29655: loss = 3.0413254e-09,1.5768842\n",
      "Iteration 29660: loss = 3.4868488e-09,1.5236151\n",
      "Iteration 29665: loss = 5.029247e-09,1.4368035\n",
      "Iteration 29670: loss = 6.755434e-09,1.3527693\n",
      "Iteration 29675: loss = 7.891715e-09,1.2942938\n",
      "Iteration 29680: loss = 9.909805e-09,1.2090933\n",
      "Iteration 29685: loss = 1.239744e-08,1.1251379\n",
      "Iteration 29690: loss = 1.6002666e-08,1.0249747\n",
      "Iteration 29695: loss = 1.8312582e-08,0.9684999\n",
      "Iteration 29700: loss = 1.934878e-08,0.94385296\n",
      "Iteration 29705: loss = 1.986078e-08,0.9313186\n",
      "Iteration 29710: loss = 2.0053745e-08,0.92587626\n",
      "Iteration 29715: loss = 2.0552676e-08,0.9141712\n",
      "Iteration 29720: loss = 2.0782247e-08,0.90823203\n",
      "Iteration 29725: loss = 2.0801465e-08,0.9068304\n",
      "Iteration 29730: loss = 2.0370672e-08,0.9146094\n",
      "Iteration 29735: loss = 1.9692365e-08,0.9280735\n",
      "Iteration 29740: loss = 1.8919547e-08,0.9439355\n",
      "Iteration 29745: loss = 1.8239355e-08,0.9581464\n",
      "Iteration 29750: loss = 1.7885275e-08,0.96528524\n",
      "Iteration 29755: loss = 1.7620012e-08,0.97042584\n",
      "Iteration 29760: loss = 1.7270777e-08,0.9777595\n",
      "Iteration 29765: loss = 1.665009e-08,0.9917877\n",
      "Iteration 29770: loss = 1.5862529e-08,1.0106455\n",
      "Iteration 29775: loss = 1.513135e-08,1.0286499\n",
      "Iteration 29780: loss = 1.4634598e-08,1.041071\n",
      "Iteration 29785: loss = 1.4289938e-08,1.0495404\n",
      "Iteration 29790: loss = 1.3977645e-08,1.0572823\n",
      "Iteration 29795: loss = 1.358454e-08,1.0675573\n",
      "Iteration 29800: loss = 1.3114483e-08,1.0803947\n",
      "Iteration 29805: loss = 1.2671459e-08,1.0927528\n",
      "Iteration 29810: loss = 1.2334186e-08,1.1021123\n",
      "Iteration 29815: loss = 1.21041195e-08,1.1084108\n",
      "Iteration 29820: loss = 1.1863116e-08,1.114955\n",
      "Iteration 29825: loss = 1.1585887e-08,1.1230308\n",
      "Iteration 29830: loss = 1.1254684e-08,1.1329703\n",
      "Iteration 29835: loss = 1.091982e-08,1.1432871\n",
      "Iteration 29840: loss = 1.0638502e-08,1.1520422\n",
      "Iteration 29845: loss = 1.0410019e-08,1.1590985\n",
      "Iteration 29850: loss = 1.0201028e-08,1.1655399\n",
      "Iteration 29855: loss = 9.983277e-09,1.1725923\n",
      "Iteration 29860: loss = 9.758729e-09,1.1799088\n",
      "Iteration 29865: loss = 9.468419e-09,1.1906308\n",
      "Iteration 29870: loss = 9.405942e-09,1.2083298\n",
      "Iteration 29875: loss = 7.587331e-09,1.5772977\n",
      "Iteration 29880: loss = 3.6356305e-09,1.5109242\n",
      "Iteration 29885: loss = 2.0172866e-09,1.6656928\n",
      "Iteration 29890: loss = 2.114228e-09,1.6776538\n",
      "Iteration 29895: loss = 2.7834828e-09,1.5762354\n",
      "Iteration 29900: loss = 5.1151052e-09,1.3866779\n",
      "Iteration 29905: loss = 6.061752e-09,1.3448777\n",
      "Iteration 29910: loss = 7.758807e-09,1.2623248\n",
      "Iteration 29915: loss = 1.0065968e-08,1.1622872\n",
      "Iteration 29920: loss = 1.3078037e-08,1.0630971\n",
      "Iteration 29925: loss = 1.6979536e-08,0.95736015\n",
      "Iteration 29930: loss = 1.9092326e-08,0.90536964\n",
      "Iteration 29935: loss = 1.9569855e-08,0.8928101\n",
      "Iteration 29940: loss = 1.8922611e-08,0.9063325\n",
      "Iteration 29945: loss = 1.81115e-08,0.92416674\n",
      "Iteration 29950: loss = 1.7327116e-08,0.94172764\n",
      "Iteration 29955: loss = 1.6870631e-08,0.9515943\n",
      "Iteration 29960: loss = 1.652481e-08,0.95912576\n",
      "Iteration 29965: loss = 1.6043233e-08,0.9705178\n",
      "Iteration 29970: loss = 1.5495315e-08,0.98376715\n",
      "Iteration 29975: loss = 1.4829305e-08,1.000728\n",
      "Iteration 29980: loss = 1.4346955e-08,1.01322\n",
      "Iteration 29985: loss = 1.4072309e-08,1.019991\n",
      "Iteration 29990: loss = 1.3790547e-08,1.0271138\n",
      "Iteration 29995: loss = 1.3299154e-08,1.0406334\n",
      "Iteration 30000: loss = 1.2539586e-08,1.0628586\n",
      "Iteration 30005: loss = 1.1739967e-08,1.0875771\n",
      "Iteration 30010: loss = 1.1111577e-08,1.1075847\n",
      "Iteration 30015: loss = 1.0786231e-08,1.1178937\n",
      "Iteration 30020: loss = 1.0623395e-08,1.1225829\n",
      "Iteration 30025: loss = 1.0466279e-08,1.1273633\n",
      "Iteration 30030: loss = 1.0185199e-08,1.1364971\n",
      "Iteration 30035: loss = 9.821285e-09,1.1491505\n",
      "Iteration 30040: loss = 9.469622e-09,1.1616012\n",
      "Iteration 30045: loss = 9.20127e-09,1.1710981\n",
      "Iteration 30050: loss = 9.017439e-09,1.177591\n",
      "Iteration 30055: loss = 8.836476e-09,1.184021\n",
      "Iteration 30060: loss = 8.590959e-09,1.1932156\n",
      "Iteration 30065: loss = 8.298727e-09,1.2045479\n",
      "Iteration 30070: loss = 8.024814e-09,1.2154727\n",
      "Iteration 30075: loss = 7.82535e-09,1.223431\n",
      "Iteration 30080: loss = 7.672401e-09,1.2293171\n",
      "Iteration 30085: loss = 7.526494e-09,1.2350892\n",
      "Iteration 30090: loss = 7.3427913e-09,1.2425638\n",
      "Iteration 30095: loss = 7.1506694e-09,1.2508856\n",
      "Iteration 30100: loss = 6.9519666e-09,1.25938\n",
      "Iteration 30105: loss = 6.7985186e-09,1.266134\n",
      "Iteration 30110: loss = 6.6687655e-09,1.2716442\n",
      "Iteration 30115: loss = 6.541267e-09,1.2770542\n",
      "Iteration 30120: loss = 6.413463e-09,1.282703\n",
      "Iteration 30125: loss = 6.2857928e-09,1.2886167\n",
      "Iteration 30130: loss = 6.205228e-09,1.2937729\n",
      "Iteration 30135: loss = 5.8153375e-09,1.3618737\n",
      "Iteration 30140: loss = 5.2244764e-09,1.8135813\n",
      "Iteration 30145: loss = 1.9895614e-09,1.8182391\n",
      "Iteration 30150: loss = 1.7151721e-09,1.7258557\n",
      "Iteration 30155: loss = 1.8381555e-09,1.6153026\n",
      "Iteration 30160: loss = 2.3672735e-09,1.563837\n",
      "Iteration 30165: loss = 3.7424086e-09,1.4614115\n",
      "Iteration 30170: loss = 4.5836157e-09,1.3846523\n",
      "Iteration 30175: loss = 6.3307684e-09,1.2834791\n",
      "Iteration 30180: loss = 8.986087e-09,1.1662843\n",
      "Iteration 30185: loss = 1.3251086e-08,1.0141432\n",
      "Iteration 30190: loss = 1.7871681e-08,0.8876524\n",
      "Iteration 30195: loss = 2.242634e-08,0.7870035\n",
      "Iteration 30200: loss = 2.4807031e-08,0.73914653\n",
      "Iteration 30205: loss = 2.3964096e-08,0.7534305\n",
      "Iteration 30210: loss = 1.9355136e-08,0.84895754\n",
      "Iteration 30215: loss = 1.3279636e-08,1.0083904\n",
      "Iteration 30220: loss = 9.221508e-09,1.14736\n",
      "Iteration 30225: loss = 7.682846e-09,1.2095917\n",
      "Iteration 30230: loss = 8.415172e-09,1.1761705\n",
      "Iteration 30235: loss = 1.0842612e-08,1.082863\n",
      "Iteration 30240: loss = 1.3646343e-08,0.99196815\n",
      "Iteration 30245: loss = 1.5500918e-08,0.9373582\n",
      "Iteration 30250: loss = 1.6806501e-08,0.90086037\n",
      "Iteration 30255: loss = 1.8160534e-08,0.8662706\n",
      "Iteration 30260: loss = 1.8237552e-08,0.8634612\n",
      "Iteration 30265: loss = 1.5231501e-08,0.94011825\n",
      "Iteration 30270: loss = 1.0068782e-08,1.1056552\n",
      "Iteration 30275: loss = 6.0756924e-09,1.2822007\n",
      "Iteration 30280: loss = 4.4305017e-09,1.3764309\n",
      "Iteration 30285: loss = 4.6053947e-09,1.3633466\n",
      "Iteration 30290: loss = 5.977699e-09,1.2831199\n",
      "Iteration 30295: loss = 7.379646e-09,1.2128491\n",
      "Iteration 30300: loss = 7.952253e-09,1.1854506\n",
      "Iteration 30305: loss = 8.401316e-09,1.1644887\n",
      "Iteration 30310: loss = 9.5616866e-09,1.1162951\n",
      "Iteration 30315: loss = 1.1091539e-08,1.0590252\n",
      "Iteration 30320: loss = 1.1670335e-08,1.0381393\n",
      "Iteration 30325: loss = 1.0215877e-08,1.0889825\n",
      "Iteration 30330: loss = 7.52144e-09,1.2005363\n",
      "Iteration 30335: loss = 5.295505e-09,1.3154192\n",
      "Iteration 30340: loss = 4.297371e-09,1.3766869\n",
      "Iteration 30345: loss = 4.3027417e-09,1.374997\n",
      "Iteration 30350: loss = 4.8668656e-09,1.3381696\n",
      "Iteration 30355: loss = 5.3913376e-09,1.3061917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30360: loss = 5.6530554e-09,1.290327\n",
      "Iteration 30365: loss = 5.9986363e-09,1.2705313\n",
      "Iteration 30370: loss = 6.6637296e-09,1.2350491\n",
      "Iteration 30375: loss = 7.4161055e-09,1.1979202\n",
      "Iteration 30380: loss = 7.679939e-09,1.1848756\n",
      "Iteration 30385: loss = 7.119757e-09,1.2104415\n",
      "Iteration 30390: loss = 6.037336e-09,1.2650511\n",
      "Iteration 30395: loss = 5.0248468e-09,1.3227957\n",
      "Iteration 30400: loss = 4.460262e-09,1.3600192\n",
      "Iteration 30405: loss = 4.119933e-09,1.4204649\n",
      "Iteration 30410: loss = 3.9289385e-09,1.7376058\n",
      "Iteration 30415: loss = 1.808269e-09,1.6723236\n",
      "Iteration 30420: loss = 1.383215e-09,1.6886264\n",
      "Iteration 30425: loss = 1.2952223e-09,1.6935992\n",
      "Iteration 30430: loss = 1.4760846e-09,1.6543726\n",
      "Iteration 30435: loss = 1.9567692e-09,1.5777916\n",
      "Iteration 30440: loss = 2.241384e-09,1.5362703\n",
      "Iteration 30445: loss = 2.8886635e-09,1.472643\n",
      "Iteration 30450: loss = 3.899222e-09,1.3929671\n",
      "Iteration 30455: loss = 5.4567018e-09,1.2926811\n",
      "Iteration 30460: loss = 7.368301e-09,1.1924758\n",
      "Iteration 30465: loss = 1.0360829e-08,1.0666504\n",
      "Iteration 30470: loss = 1.5611038e-08,0.89980817\n",
      "Iteration 30475: loss = 2.8458336e-08,0.63127685\n",
      "Iteration 30480: loss = 6.957476e-08,0.22935511\n",
      "Iteration 30485: loss = 1.8575487e-07,0.0096029155\n",
      "Iteration 30490: loss = 2.1656882e-12,10.808587\n",
      "Iteration 30495: loss = 1.0820359e-12,18.799084\n",
      "Iteration 30500: loss = 7.7670016e-13,16.497856\n",
      "Iteration 30505: loss = 6.246082e-13,12.274344\n",
      "Iteration 30510: loss = 5.7816675e-13,7.579452\n",
      "Iteration 30515: loss = 8.3341363e-13,5.7001867\n",
      "Iteration 30520: loss = 1.7164778e-12,3.4646747\n",
      "Iteration 30525: loss = 2.8084887e-11,2.7665315\n",
      "Iteration 30530: loss = 1.1428876e-09,2.1012945\n",
      "Iteration 30535: loss = 1.3045482e-08,1.193927\n",
      "Iteration 30540: loss = 4.3247397e-08,0.66291004\n",
      "Iteration 30545: loss = 8.196256e-08,0.37073967\n",
      "Iteration 30550: loss = 1.16588694e-07,0.21858919\n",
      "Iteration 30555: loss = 1.3716095e-07,0.15215337\n",
      "Iteration 30560: loss = 1.4969362e-07,0.10192018\n",
      "Iteration 30565: loss = 1.5261249e-07,0.08012815\n",
      "Iteration 30570: loss = 1.5007015e-07,0.07030471\n",
      "Iteration 30575: loss = 1.4724358e-07,0.062630065\n",
      "Iteration 30580: loss = 1.425863e-07,0.05985282\n",
      "Iteration 30585: loss = 1.3806188e-07,0.058480415\n",
      "Iteration 30590: loss = 1.3367016e-07,0.058236815\n",
      "Iteration 30595: loss = 1.2830071e-07,0.060904905\n",
      "Iteration 30600: loss = 1.2298352e-07,0.064823695\n",
      "Iteration 30605: loss = 1.1764468e-07,0.07026786\n",
      "Iteration 30610: loss = 1.12420686e-07,0.076937295\n",
      "Iteration 30615: loss = 1.0769463e-07,0.08387114\n",
      "Iteration 30620: loss = 1.0330002e-07,0.09141752\n",
      "Iteration 30625: loss = 9.922408e-08,0.099355206\n",
      "Iteration 30630: loss = 9.559282e-08,0.1072324\n",
      "Iteration 30635: loss = 9.223428e-08,0.115337335\n",
      "Iteration 30640: loss = 8.922854e-08,0.12327373\n",
      "Iteration 30645: loss = 8.647961e-08,0.13112895\n",
      "Iteration 30650: loss = 8.399818e-08,0.1387832\n",
      "Iteration 30655: loss = 8.181118e-08,0.14596884\n",
      "Iteration 30660: loss = 7.9870006e-08,0.15267949\n",
      "Iteration 30665: loss = 7.8141234e-08,0.15888053\n",
      "Iteration 30670: loss = 7.667814e-08,0.16438732\n",
      "Iteration 30675: loss = 7.541524e-08,0.1692688\n",
      "Iteration 30680: loss = 7.4331005e-08,0.17351101\n",
      "Iteration 30685: loss = 7.344395e-08,0.17701344\n",
      "Iteration 30690: loss = 7.266048e-08,0.18005909\n",
      "Iteration 30695: loss = 7.205906e-08,0.1823996\n",
      "Iteration 30700: loss = 7.157029e-08,0.18422131\n",
      "Iteration 30705: loss = 7.120471e-08,0.18550879\n",
      "Iteration 30710: loss = 7.0911945e-08,0.18640308\n",
      "Iteration 30715: loss = 7.07194e-08,0.18685225\n",
      "Iteration 30720: loss = 7.05564e-08,0.18711536\n",
      "Iteration 30725: loss = 7.047358e-08,0.18701069\n",
      "Iteration 30730: loss = 7.050991e-08,0.18635164\n",
      "Iteration 30735: loss = 7.053888e-08,0.18564491\n",
      "Iteration 30740: loss = 7.066166e-08,0.1845483\n",
      "Iteration 30745: loss = 7.0852565e-08,0.18307544\n",
      "Iteration 30750: loss = 7.1045214e-08,0.18159917\n",
      "Iteration 30755: loss = 7.127884e-08,0.17993805\n",
      "Iteration 30760: loss = 7.151714e-08,0.17824043\n",
      "Iteration 30765: loss = 7.181231e-08,0.17632161\n",
      "Iteration 30770: loss = 7.209624e-08,0.17440987\n",
      "Iteration 30775: loss = 7.2452046e-08,0.17225832\n",
      "Iteration 30780: loss = 7.2801555e-08,0.1700583\n",
      "Iteration 30785: loss = 7.322541e-08,0.16760065\n",
      "Iteration 30790: loss = 7.361593e-08,0.16527721\n",
      "Iteration 30795: loss = 7.400962e-08,0.16294758\n",
      "Iteration 30800: loss = 7.4440926e-08,0.16050781\n",
      "Iteration 30805: loss = 7.489436e-08,0.15799867\n",
      "Iteration 30810: loss = 7.533111e-08,0.1555737\n",
      "Iteration 30815: loss = 7.581013e-08,0.15299764\n",
      "Iteration 30820: loss = 7.626341e-08,0.15053216\n",
      "Iteration 30825: loss = 7.6746e-08,0.14799735\n",
      "Iteration 30830: loss = 7.723359e-08,0.1454494\n",
      "Iteration 30835: loss = 7.773953e-08,0.14288624\n",
      "Iteration 30840: loss = 7.820905e-08,0.1404758\n",
      "Iteration 30845: loss = 7.869712e-08,0.13804583\n",
      "Iteration 30850: loss = 7.919022e-08,0.1356034\n",
      "Iteration 30855: loss = 7.968607e-08,0.13317297\n",
      "Iteration 30860: loss = 8.018494e-08,0.1307526\n",
      "Iteration 30865: loss = 8.071868e-08,0.12829037\n",
      "Iteration 30870: loss = 8.1219504e-08,0.12593292\n",
      "Iteration 30875: loss = 8.175842e-08,0.12350557\n",
      "Iteration 30880: loss = 8.23021e-08,0.12107386\n",
      "Iteration 30885: loss = 8.2823114e-08,0.11871191\n",
      "Iteration 30890: loss = 8.3373834e-08,0.116302185\n",
      "Iteration 30895: loss = 8.3925386e-08,0.11392863\n",
      "Iteration 30900: loss = 8.4497316e-08,0.11153824\n",
      "Iteration 30905: loss = 8.503347e-08,0.10927022\n",
      "Iteration 30910: loss = 8.5568196e-08,0.10705494\n",
      "Iteration 30915: loss = 8.6104826e-08,0.10486177\n",
      "Iteration 30920: loss = 8.664398e-08,0.102687106\n",
      "Iteration 30925: loss = 8.718257e-08,0.10055737\n",
      "Iteration 30930: loss = 8.772421e-08,0.09844174\n",
      "Iteration 30935: loss = 8.830048e-08,0.09630875\n",
      "Iteration 30940: loss = 8.8824e-08,0.09430511\n",
      "Iteration 30945: loss = 8.931962e-08,0.09239357\n",
      "Iteration 30950: loss = 8.984117e-08,0.09047459\n",
      "Iteration 30955: loss = 9.03862e-08,0.08851862\n",
      "Iteration 30960: loss = 9.091651e-08,0.08660355\n",
      "Iteration 30965: loss = 9.146789e-08,0.084671386\n",
      "Iteration 30970: loss = 9.2020684e-08,0.0827665\n",
      "Iteration 30975: loss = 9.255371e-08,0.0809382\n",
      "Iteration 30980: loss = 9.311088e-08,0.07907485\n",
      "Iteration 30985: loss = 9.3668184e-08,0.07724452\n",
      "Iteration 30990: loss = 9.42544e-08,0.075351514\n",
      "Iteration 30995: loss = 9.483407e-08,0.073542066\n",
      "Iteration 31000: loss = 9.539835e-08,0.07177375\n",
      "Iteration 31005: loss = 9.597349e-08,0.07002019\n",
      "Iteration 31010: loss = 9.6501346e-08,0.06837002\n",
      "Iteration 31015: loss = 9.708999e-08,0.06661991\n",
      "Iteration 31020: loss = 9.7679454e-08,0.064951904\n",
      "Iteration 31025: loss = 9.820985e-08,0.06338699\n",
      "Iteration 31030: loss = 9.875506e-08,0.061853398\n",
      "Iteration 31035: loss = 9.930623e-08,0.060313575\n",
      "Iteration 31040: loss = 9.9860436e-08,0.05878694\n",
      "Iteration 31045: loss = 1.0041227e-07,0.057308313\n",
      "Iteration 31050: loss = 1.0096471e-07,0.055855155\n",
      "Iteration 31055: loss = 1.0149537e-07,0.054472502\n",
      "Iteration 31060: loss = 1.0205085e-07,0.053065304\n",
      "Iteration 31065: loss = 1.0259421e-07,0.051726382\n",
      "Iteration 31070: loss = 1.03132685e-07,0.050482083\n",
      "Iteration 31075: loss = 1.0359751e-07,0.050040703\n",
      "Iteration 31080: loss = 1.04311e-07,0.05487485\n",
      "Iteration 31085: loss = 1.04307055e-07,0.113147646\n",
      "Iteration 31090: loss = 1.0611646e-07,0.29075962\n",
      "Iteration 31095: loss = 1.06197035e-07,0.061075322\n",
      "Iteration 31100: loss = 1.063762e-07,0.053272434\n",
      "Iteration 31105: loss = 1.06927835e-07,0.071680605\n",
      "Iteration 31110: loss = 1.078063e-07,0.06478943\n",
      "Iteration 31115: loss = 1.0864554e-07,0.049687795\n",
      "Iteration 31120: loss = 1.0942079e-07,0.039105993\n",
      "Iteration 31125: loss = 1.1010831e-07,0.035402223\n",
      "Iteration 31130: loss = 1.1070893e-07,0.035270777\n",
      "Iteration 31135: loss = 1.1119067e-07,0.03505564\n",
      "Iteration 31140: loss = 1.1166153e-07,0.033574797\n",
      "Iteration 31145: loss = 1.1205969e-07,0.031973183\n",
      "Iteration 31150: loss = 1.1243525e-07,0.031088255\n",
      "Iteration 31155: loss = 1.128399e-07,0.030515973\n",
      "Iteration 31160: loss = 1.1331463e-07,0.029671114\n",
      "Iteration 31165: loss = 1.13767214e-07,0.028774245\n",
      "Iteration 31170: loss = 1.14204575e-07,0.028026316\n",
      "Iteration 31175: loss = 1.1463569e-07,0.027327783\n",
      "Iteration 31180: loss = 1.1506555e-07,0.026602218\n",
      "Iteration 31185: loss = 1.15469504e-07,0.025956364\n",
      "Iteration 31190: loss = 1.1589026e-07,0.025293492\n",
      "Iteration 31195: loss = 1.16348986e-07,0.02458998\n",
      "Iteration 31200: loss = 1.16782694e-07,0.02394201\n",
      "Iteration 31205: loss = 1.1721112e-07,0.02331843\n",
      "Iteration 31210: loss = 1.1764434e-07,0.022696957\n",
      "Iteration 31215: loss = 1.1808598e-07,0.022062462\n",
      "Iteration 31220: loss = 1.1852185e-07,0.021458894\n",
      "Iteration 31225: loss = 1.1895671e-07,0.020873848\n",
      "Iteration 31230: loss = 1.1937011e-07,0.020315958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31235: loss = 1.1980495e-07,0.019756138\n",
      "Iteration 31240: loss = 1.2024928e-07,0.019183993\n",
      "Iteration 31245: loss = 1.2066386e-07,0.018660318\n",
      "Iteration 31250: loss = 1.210837e-07,0.018134952\n",
      "Iteration 31255: loss = 1.215278e-07,0.017602812\n",
      "Iteration 31260: loss = 1.2201873e-07,0.017048819\n",
      "Iteration 31265: loss = 1.2246004e-07,0.016529929\n",
      "Iteration 31270: loss = 1.229311e-07,0.016013179\n",
      "Iteration 31275: loss = 1.2334814e-07,0.015559069\n",
      "Iteration 31280: loss = 1.2376665e-07,0.015112932\n",
      "Iteration 31285: loss = 1.2415934e-07,0.014698456\n",
      "Iteration 31290: loss = 1.2459938e-07,0.014263646\n",
      "Iteration 31295: loss = 1.2499608e-07,0.013863087\n",
      "Iteration 31300: loss = 1.2541692e-07,0.013472391\n",
      "Iteration 31305: loss = 1.2575681e-07,0.01330172\n",
      "Iteration 31310: loss = 1.2629106e-07,0.014682355\n",
      "Iteration 31315: loss = 1.2620389e-07,0.03862892\n",
      "Iteration 31320: loss = 1.281603e-07,0.2711708\n",
      "Iteration 31325: loss = 1.2708658e-07,0.03445926\n",
      "Iteration 31330: loss = 1.2720457e-07,0.082638\n",
      "Iteration 31335: loss = 1.2793993e-07,0.0701768\n",
      "Iteration 31340: loss = 1.2882633e-07,0.049543533\n",
      "Iteration 31345: loss = 1.2969976e-07,0.034139194\n",
      "Iteration 31350: loss = 1.3056409e-07,0.024021393\n",
      "Iteration 31355: loss = 1.3125445e-07,0.017661158\n",
      "Iteration 31360: loss = 1.3173944e-07,0.013667161\n",
      "Iteration 31365: loss = 1.3217051e-07,0.011121649\n",
      "Iteration 31370: loss = 1.3255375e-07,0.009490088\n",
      "Iteration 31375: loss = 1.3289734e-07,0.008482795\n",
      "Iteration 31380: loss = 1.332163e-07,0.007882226\n",
      "Iteration 31385: loss = 1.3356508e-07,0.0075280103\n",
      "Iteration 31390: loss = 1.3388605e-07,0.007333029\n",
      "Iteration 31395: loss = 1.3422961e-07,0.0071875164\n",
      "Iteration 31400: loss = 1.345508e-07,0.0070427707\n",
      "Iteration 31405: loss = 1.3485045e-07,0.006873451\n",
      "Iteration 31410: loss = 1.3517239e-07,0.0066883736\n",
      "Iteration 31415: loss = 1.3549091e-07,0.006517181\n",
      "Iteration 31420: loss = 1.3577835e-07,0.0063692867\n",
      "Iteration 31425: loss = 1.3614788e-07,0.006221486\n",
      "Iteration 31430: loss = 1.3652755e-07,0.0060586855\n",
      "Iteration 31435: loss = 1.3687821e-07,0.0059124827\n",
      "Iteration 31440: loss = 1.3723248e-07,0.0057706255\n",
      "Iteration 31445: loss = 1.375558e-07,0.005646635\n",
      "Iteration 31450: loss = 1.3785393e-07,0.0055311983\n",
      "Iteration 31455: loss = 1.3812424e-07,0.005426439\n",
      "Iteration 31460: loss = 1.3839046e-07,0.0053287284\n",
      "Iteration 31465: loss = 1.3870779e-07,0.0052243583\n",
      "Iteration 31470: loss = 1.3898075e-07,0.005128721\n",
      "Iteration 31475: loss = 1.3924686e-07,0.0050410284\n",
      "Iteration 31480: loss = 1.395151e-07,0.0049550873\n",
      "Iteration 31485: loss = 1.3980546e-07,0.0048694313\n",
      "Iteration 31490: loss = 1.4007763e-07,0.0047871573\n",
      "Iteration 31495: loss = 1.4037902e-07,0.004701721\n",
      "Iteration 31500: loss = 1.4073304e-07,0.0046126647\n",
      "Iteration 31505: loss = 1.4106138e-07,0.0045314436\n",
      "Iteration 31510: loss = 1.4136862e-07,0.0044551366\n",
      "Iteration 31515: loss = 1.4172585e-07,0.0043774284\n",
      "Iteration 31520: loss = 1.420797e-07,0.004305309\n",
      "Iteration 31525: loss = 1.4237663e-07,0.0042406176\n",
      "Iteration 31530: loss = 1.4269837e-07,0.0041811536\n",
      "Iteration 31535: loss = 1.4295019e-07,0.00412842\n",
      "Iteration 31540: loss = 1.4330182e-07,0.004072561\n",
      "Iteration 31545: loss = 1.4363597e-07,0.0040185773\n",
      "Iteration 31550: loss = 1.4396812e-07,0.003969773\n",
      "Iteration 31555: loss = 1.4427015e-07,0.003927316\n",
      "Iteration 31560: loss = 1.4460635e-07,0.0038881353\n",
      "Iteration 31565: loss = 1.4491155e-07,0.0038694756\n",
      "Iteration 31570: loss = 1.4530296e-07,0.0039520646\n",
      "Iteration 31575: loss = 1.4548245e-07,0.004779785\n",
      "Iteration 31580: loss = 1.4618017e-07,0.012084699\n",
      "Iteration 31585: loss = 1.414949e-07,0.07643567\n",
      "Iteration 31590: loss = 1.3054978e-07,0.25113675\n",
      "Iteration 31595: loss = 1.2271586e-07,0.039546702\n",
      "Iteration 31600: loss = 1.185313e-07,0.025381463\n",
      "Iteration 31605: loss = 1.1673662e-07,0.04737976\n",
      "Iteration 31610: loss = 1.1646387e-07,0.045484714\n",
      "Iteration 31615: loss = 1.1681453e-07,0.03275407\n",
      "Iteration 31620: loss = 1.1733602e-07,0.022828663\n",
      "Iteration 31625: loss = 1.1796167e-07,0.019262543\n",
      "Iteration 31630: loss = 1.1867235e-07,0.019240541\n",
      "Iteration 31635: loss = 1.1935457e-07,0.019145353\n",
      "Iteration 31640: loss = 1.1992745e-07,0.017842183\n",
      "Iteration 31645: loss = 1.2042464e-07,0.016385183\n",
      "Iteration 31650: loss = 1.2092204e-07,0.015653824\n",
      "Iteration 31655: loss = 1.2142246e-07,0.015253115\n",
      "Iteration 31660: loss = 1.219965e-07,0.014572758\n",
      "Iteration 31665: loss = 1.2254382e-07,0.013865252\n",
      "Iteration 31670: loss = 1.2312142e-07,0.013310708\n",
      "Iteration 31675: loss = 1.2364966e-07,0.012793249\n",
      "Iteration 31680: loss = 1.2414857e-07,0.0122874295\n",
      "Iteration 31685: loss = 1.2462539e-07,0.011846143\n",
      "Iteration 31690: loss = 1.2513071e-07,0.011385766\n",
      "Iteration 31695: loss = 1.2561041e-07,0.010956628\n",
      "Iteration 31700: loss = 1.26089e-07,0.010551692\n",
      "Iteration 31705: loss = 1.2659619e-07,0.010133714\n",
      "Iteration 31710: loss = 1.271024e-07,0.009737543\n",
      "Iteration 31715: loss = 1.2761173e-07,0.009347858\n",
      "Iteration 31720: loss = 1.281008e-07,0.008979933\n",
      "Iteration 31725: loss = 1.2860923e-07,0.008622179\n",
      "Iteration 31730: loss = 1.290869e-07,0.008286708\n",
      "Iteration 31735: loss = 1.2959366e-07,0.007959667\n",
      "Iteration 31740: loss = 1.3007913e-07,0.007651654\n",
      "Iteration 31745: loss = 1.305102e-07,0.007383653\n",
      "Iteration 31750: loss = 1.3093927e-07,0.007128866\n",
      "Iteration 31755: loss = 1.3137101e-07,0.006879625\n",
      "Iteration 31760: loss = 1.3177934e-07,0.0066482886\n",
      "Iteration 31765: loss = 1.3220748e-07,0.006423171\n",
      "Iteration 31770: loss = 1.3261521e-07,0.006210251\n",
      "Iteration 31775: loss = 1.3307219e-07,0.0059910845\n",
      "Iteration 31780: loss = 1.3345189e-07,0.005809722\n",
      "Iteration 31785: loss = 1.3388575e-07,0.0056277853\n",
      "Iteration 31790: loss = 1.3421446e-07,0.0055703176\n",
      "Iteration 31795: loss = 1.347666e-07,0.0061358437\n",
      "Iteration 31800: loss = 1.347695e-07,0.013701866\n",
      "Iteration 31805: loss = 1.3641909e-07,0.089447215\n",
      "Iteration 31810: loss = 1.3401109e-07,0.2561978\n",
      "Iteration 31815: loss = 1.3532313e-07,0.059236504\n",
      "Iteration 31820: loss = 1.3654795e-07,0.0048759333\n",
      "Iteration 31825: loss = 1.3757757e-07,0.012119875\n",
      "Iteration 31830: loss = 1.3864455e-07,0.018907398\n",
      "Iteration 31835: loss = 1.3953216e-07,0.018006882\n",
      "Iteration 31840: loss = 1.4006926e-07,0.013195531\n",
      "Iteration 31845: loss = 1.4037012e-07,0.008159884\n",
      "Iteration 31850: loss = 1.4063343e-07,0.004878377\n",
      "Iteration 31855: loss = 1.4089852e-07,0.0036509093\n",
      "Iteration 31860: loss = 1.4116523e-07,0.0037106192\n",
      "Iteration 31865: loss = 1.4143278e-07,0.0039891857\n",
      "Iteration 31870: loss = 1.4175608e-07,0.003904074\n",
      "Iteration 31875: loss = 1.4212264e-07,0.0035703927\n",
      "Iteration 31880: loss = 1.4249794e-07,0.0033645541\n",
      "Iteration 31885: loss = 1.4285811e-07,0.0033506947\n",
      "Iteration 31890: loss = 1.4321769e-07,0.0033424236\n",
      "Iteration 31895: loss = 1.4352621e-07,0.0032722119\n",
      "Iteration 31900: loss = 1.4385482e-07,0.0032257354\n",
      "Iteration 31905: loss = 1.4416513e-07,0.0032110608\n",
      "Iteration 31910: loss = 1.4452625e-07,0.0031831264\n",
      "Iteration 31915: loss = 1.4229674e-07,0.0032576106\n",
      "Iteration 31920: loss = 1.4067409e-07,0.0034298948\n",
      "Iteration 31925: loss = 1.3995401e-07,0.0035299927\n",
      "Iteration 31930: loss = 1.3963401e-07,0.0035723504\n",
      "Iteration 31935: loss = 1.3960089e-07,0.0035662663\n",
      "Iteration 31940: loss = 1.3971989e-07,0.00353247\n",
      "Iteration 31945: loss = 1.3993316e-07,0.0034808991\n",
      "Iteration 31950: loss = 1.4019983e-07,0.0034246421\n",
      "Iteration 31955: loss = 1.4052512e-07,0.0033610007\n",
      "Iteration 31960: loss = 1.408563e-07,0.003302384\n",
      "Iteration 31965: loss = 1.4115149e-07,0.003252439\n",
      "Iteration 31970: loss = 1.4142393e-07,0.0032056957\n",
      "Iteration 31975: loss = 1.4169397e-07,0.0031633233\n",
      "Iteration 31980: loss = 1.419591e-07,0.0031245677\n",
      "Iteration 31985: loss = 1.4220302e-07,0.0030886063\n",
      "Iteration 31990: loss = 1.4245843e-07,0.0030542542\n",
      "Iteration 31995: loss = 1.427269e-07,0.0030214817\n",
      "Iteration 32000: loss = 1.4297233e-07,0.002990874\n",
      "Iteration 32005: loss = 1.4323807e-07,0.0029636212\n",
      "Iteration 32010: loss = 1.4351112e-07,0.002938693\n",
      "Iteration 32015: loss = 1.4375021e-07,0.002926932\n",
      "Iteration 32020: loss = 1.4411074e-07,0.0029692762\n",
      "Iteration 32025: loss = 1.4426506e-07,0.0034378478\n",
      "Iteration 32030: loss = 1.4492095e-07,0.007606356\n",
      "Iteration 32035: loss = 1.365657e-07,0.0483972\n",
      "Iteration 32040: loss = 1.3168909e-07,0.2547382\n",
      "Iteration 32045: loss = 1.257289e-07,0.01007749\n",
      "Iteration 32050: loss = 1.2289887e-07,0.05486953\n",
      "Iteration 32055: loss = 1.2218538e-07,0.05952127\n",
      "Iteration 32060: loss = 1.2256514e-07,0.03842793\n",
      "Iteration 32065: loss = 1.2331513e-07,0.020436633\n",
      "Iteration 32070: loss = 1.240147e-07,0.012128771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32075: loss = 1.246594e-07,0.010628068\n",
      "Iteration 32080: loss = 1.2527788e-07,0.011369952\n",
      "Iteration 32085: loss = 1.2578317e-07,0.011461899\n",
      "Iteration 32090: loss = 1.2614352e-07,0.010419531\n",
      "Iteration 32095: loss = 1.2642691e-07,0.009241474\n",
      "Iteration 32100: loss = 1.267818e-07,0.008666441\n",
      "Iteration 32105: loss = 1.2717756e-07,0.008498992\n",
      "Iteration 32110: loss = 1.2762804e-07,0.008195598\n",
      "Iteration 32115: loss = 1.2808366e-07,0.007738781\n",
      "Iteration 32120: loss = 1.2854024e-07,0.0073823514\n",
      "Iteration 32125: loss = 1.2899926e-07,0.0071056876\n",
      "Iteration 32130: loss = 1.2941628e-07,0.0068166777\n",
      "Iteration 32135: loss = 1.2986882e-07,0.006529204\n",
      "Iteration 32140: loss = 1.3030353e-07,0.006275348\n",
      "Iteration 32145: loss = 1.3078593e-07,0.0060032327\n",
      "Iteration 32150: loss = 1.3124351e-07,0.0057575703\n",
      "Iteration 32155: loss = 1.317033e-07,0.0055215643\n",
      "Iteration 32160: loss = 1.3213932e-07,0.005302487\n",
      "Iteration 32165: loss = 1.3254814e-07,0.0051054778\n",
      "Iteration 32170: loss = 1.3300088e-07,0.004907115\n",
      "Iteration 32175: loss = 1.3343116e-07,0.0047247885\n",
      "Iteration 32180: loss = 1.3381505e-07,0.0045605088\n",
      "Iteration 32185: loss = 1.3422247e-07,0.004399829\n",
      "Iteration 32190: loss = 1.3465277e-07,0.004242126\n",
      "Iteration 32195: loss = 1.3503447e-07,0.004103794\n",
      "Iteration 32200: loss = 1.354123e-07,0.003974923\n",
      "Iteration 32205: loss = 1.3578597e-07,0.0038495683\n",
      "Iteration 32210: loss = 1.361431e-07,0.0037355693\n",
      "Iteration 32215: loss = 1.3652024e-07,0.0036266148\n",
      "Iteration 32220: loss = 1.3693672e-07,0.0035106346\n",
      "Iteration 32225: loss = 1.3737308e-07,0.003401462\n",
      "Iteration 32230: loss = 1.3778624e-07,0.0033019623\n",
      "Iteration 32235: loss = 1.3817102e-07,0.0032149921\n",
      "Iteration 32240: loss = 1.3852453e-07,0.0031393596\n",
      "Iteration 32245: loss = 1.388836e-07,0.0030667204\n",
      "Iteration 32250: loss = 1.3921259e-07,0.003004234\n",
      "Iteration 32255: loss = 1.3959057e-07,0.0029430343\n",
      "Iteration 32260: loss = 1.3995869e-07,0.0029059462\n",
      "Iteration 32265: loss = 1.402188e-07,0.0031891465\n",
      "Iteration 32270: loss = 1.4096689e-07,0.008039277\n",
      "Iteration 32275: loss = 1.3968477e-07,0.08665844\n",
      "Iteration 32280: loss = 1.3915646e-07,0.41860378\n",
      "Iteration 32285: loss = 1.3001478e-07,0.16665997\n",
      "Iteration 32290: loss = 1.2535564e-07,0.08260708\n",
      "Iteration 32295: loss = 1.235639e-07,0.056317598\n",
      "Iteration 32300: loss = 1.2333919e-07,0.042757697\n",
      "Iteration 32305: loss = 1.2368156e-07,0.03222518\n",
      "Iteration 32310: loss = 1.2415266e-07,0.023726404\n",
      "Iteration 32315: loss = 1.2465574e-07,0.017482297\n",
      "Iteration 32320: loss = 1.252147e-07,0.013333008\n",
      "Iteration 32325: loss = 1.2574735e-07,0.010789242\n",
      "Iteration 32330: loss = 1.2625729e-07,0.009266119\n",
      "Iteration 32335: loss = 1.2680925e-07,0.008297856\n",
      "Iteration 32340: loss = 1.2731834e-07,0.0076468447\n",
      "Iteration 32345: loss = 1.2788665e-07,0.0071204114\n",
      "Iteration 32350: loss = 1.2845405e-07,0.006681079\n",
      "Iteration 32355: loss = 1.2899675e-07,0.006306908\n",
      "Iteration 32360: loss = 1.2955122e-07,0.005962679\n",
      "Iteration 32365: loss = 1.3004437e-07,0.005667529\n",
      "Iteration 32370: loss = 1.3055954e-07,0.005384726\n",
      "Iteration 32375: loss = 1.3107851e-07,0.005115591\n",
      "Iteration 32380: loss = 1.3159381e-07,0.004867029\n",
      "Iteration 32385: loss = 1.3210764e-07,0.004637005\n",
      "Iteration 32390: loss = 1.3262837e-07,0.004416275\n",
      "Iteration 32395: loss = 1.3317212e-07,0.004205739\n",
      "Iteration 32400: loss = 1.3372063e-07,0.004006138\n",
      "Iteration 32405: loss = 1.3422111e-07,0.0038306213\n",
      "Iteration 32410: loss = 1.3471971e-07,0.0036679744\n",
      "Iteration 32415: loss = 1.3517904e-07,0.003532499\n",
      "Iteration 32420: loss = 1.3560098e-07,0.0034057025\n",
      "Iteration 32425: loss = 1.3605455e-07,0.0032850313\n",
      "Iteration 32430: loss = 1.3651734e-07,0.0031739264\n",
      "Iteration 32435: loss = 1.3693476e-07,0.0030751573\n",
      "Iteration 32440: loss = 1.373747e-07,0.0029824642\n",
      "Iteration 32445: loss = 1.3782362e-07,0.0028934535\n",
      "Iteration 32450: loss = 1.382882e-07,0.0028140582\n",
      "Iteration 32455: loss = 1.3867734e-07,0.0027489455\n",
      "Iteration 32460: loss = 1.3906616e-07,0.0026897101\n",
      "Iteration 32465: loss = 1.3948006e-07,0.0026346967\n",
      "Iteration 32470: loss = 1.3986634e-07,0.0025874027\n",
      "Iteration 32475: loss = 1.402538e-07,0.0025454625\n",
      "Iteration 32480: loss = 1.4061675e-07,0.0025094163\n",
      "Iteration 32485: loss = 1.410297e-07,0.0024764196\n",
      "Iteration 32490: loss = 1.4138897e-07,0.0024503493\n",
      "Iteration 32495: loss = 1.4175502e-07,0.00242699\n",
      "Iteration 32500: loss = 1.4216586e-07,0.0024088577\n",
      "Iteration 32505: loss = 1.4201977e-07,0.0023955295\n",
      "Iteration 32510: loss = 1.4181505e-07,0.0023845874\n",
      "Iteration 32515: loss = 1.4189857e-07,0.0023701661\n",
      "Iteration 32520: loss = 1.4205769e-07,0.0023537916\n",
      "Iteration 32525: loss = 1.4231179e-07,0.002338788\n",
      "Iteration 32530: loss = 1.413977e-07,0.0023472751\n",
      "Iteration 32535: loss = 1.3747582e-07,0.002792238\n",
      "Iteration 32540: loss = 1.3543685e-07,0.0032549403\n",
      "Iteration 32545: loss = 1.3444988e-07,0.003528529\n",
      "Iteration 32550: loss = 1.3398852e-07,0.0036603971\n",
      "Iteration 32555: loss = 1.3392663e-07,0.0036691786\n",
      "Iteration 32560: loss = 1.3398888e-07,0.0036383236\n",
      "Iteration 32565: loss = 1.3425328e-07,0.003573446\n",
      "Iteration 32570: loss = 1.344251e-07,0.003742297\n",
      "Iteration 32575: loss = 1.3505762e-07,0.0054863705\n",
      "Iteration 32580: loss = 1.3436637e-07,0.024139693\n",
      "Iteration 32585: loss = 1.3741733e-07,0.17131823\n",
      "Iteration 32590: loss = 1.3357736e-07,0.08965985\n",
      "Iteration 32595: loss = 1.3331685e-07,0.104245014\n",
      "Iteration 32600: loss = 1.344211e-07,0.041564636\n",
      "Iteration 32605: loss = 1.3590345e-07,0.009490924\n",
      "Iteration 32610: loss = 1.3714426e-07,0.0027585474\n",
      "Iteration 32615: loss = 1.3801404e-07,0.004853858\n",
      "Iteration 32620: loss = 1.3852338e-07,0.006989462\n",
      "Iteration 32625: loss = 1.3890502e-07,0.006659744\n",
      "Iteration 32630: loss = 1.3911078e-07,0.0047520553\n",
      "Iteration 32635: loss = 1.3919956e-07,0.0030153007\n",
      "Iteration 32640: loss = 1.3929262e-07,0.0023889723\n",
      "Iteration 32645: loss = 1.3944118e-07,0.0025296188\n",
      "Iteration 32650: loss = 1.3970273e-07,0.002641864\n",
      "Iteration 32655: loss = 1.4003103e-07,0.0024541626\n",
      "Iteration 32660: loss = 1.4036071e-07,0.0022664743\n",
      "Iteration 32665: loss = 1.4063083e-07,0.0022496467\n",
      "Iteration 32670: loss = 1.409283e-07,0.0022498078\n",
      "Iteration 32675: loss = 1.4116563e-07,0.0021954635\n",
      "Iteration 32680: loss = 1.413768e-07,0.0021679373\n",
      "Iteration 32685: loss = 1.4158908e-07,0.002159854\n",
      "Iteration 32690: loss = 1.4183028e-07,0.002135779\n",
      "Iteration 32695: loss = 1.420969e-07,0.0021212243\n",
      "Iteration 32700: loss = 1.4230203e-07,0.002109449\n",
      "Iteration 32705: loss = 1.4096173e-07,0.002137029\n",
      "Iteration 32710: loss = 1.3846933e-07,0.0023824407\n",
      "Iteration 32715: loss = 1.3725351e-07,0.0025856188\n",
      "Iteration 32720: loss = 1.3659432e-07,0.0027084327\n",
      "Iteration 32725: loss = 1.3638812e-07,0.002744781\n",
      "Iteration 32730: loss = 1.3632733e-07,0.0027494929\n",
      "Iteration 32735: loss = 1.3641426e-07,0.002720125\n",
      "Iteration 32740: loss = 1.3659023e-07,0.0026720949\n",
      "Iteration 32745: loss = 1.3682498e-07,0.0026136746\n",
      "Iteration 32750: loss = 1.3705107e-07,0.0025622544\n",
      "Iteration 32755: loss = 1.372326e-07,0.0025173044\n",
      "Iteration 32760: loss = 1.3746013e-07,0.00246975\n",
      "Iteration 32765: loss = 1.376949e-07,0.0024215328\n",
      "Iteration 32770: loss = 1.3788066e-07,0.0023809718\n",
      "Iteration 32775: loss = 1.3814072e-07,0.0023362401\n",
      "Iteration 32780: loss = 1.3837747e-07,0.0022936717\n",
      "Iteration 32785: loss = 1.3866976e-07,0.002252272\n",
      "Iteration 32790: loss = 1.3884933e-07,0.002248989\n",
      "Iteration 32795: loss = 1.3917982e-07,0.0023886454\n",
      "Iteration 32800: loss = 1.3908866e-07,0.0038694355\n",
      "Iteration 32805: loss = 1.4024955e-07,0.017611824\n",
      "Iteration 32810: loss = 1.3755633e-07,0.13050953\n",
      "Iteration 32815: loss = 1.3440423e-07,0.15222085\n",
      "Iteration 32820: loss = 1.21976e-07,0.10541682\n",
      "Iteration 32825: loss = 1.1536702e-07,0.034513664\n",
      "Iteration 32830: loss = 1.12416124e-07,0.020794757\n",
      "Iteration 32835: loss = 1.11413755e-07,0.0257976\n",
      "Iteration 32840: loss = 1.11493286e-07,0.029379437\n",
      "Iteration 32845: loss = 1.1233288e-07,0.027600253\n",
      "Iteration 32850: loss = 1.1361045e-07,0.022687556\n",
      "Iteration 32855: loss = 1.1497972e-07,0.017988585\n",
      "Iteration 32860: loss = 1.16288106e-07,0.015158989\n",
      "Iteration 32865: loss = 1.17508925e-07,0.013853989\n",
      "Iteration 32870: loss = 1.18732636e-07,0.012811172\n",
      "Iteration 32875: loss = 1.1988254e-07,0.011506451\n",
      "Iteration 32880: loss = 1.2098593e-07,0.010179406\n",
      "Iteration 32885: loss = 1.220115e-07,0.009205716\n",
      "Iteration 32890: loss = 1.2302927e-07,0.008415553\n",
      "Iteration 32895: loss = 1.2410891e-07,0.00757228\n",
      "Iteration 32900: loss = 1.2514202e-07,0.0068073045\n",
      "Iteration 32905: loss = 1.2612536e-07,0.0061786617\n",
      "Iteration 32910: loss = 1.2703865e-07,0.0056172423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32915: loss = 1.279369e-07,0.0051166364\n",
      "Iteration 32920: loss = 1.2874469e-07,0.0046975594\n",
      "Iteration 32925: loss = 1.2958323e-07,0.004303236\n",
      "Iteration 32930: loss = 1.3038533e-07,0.0039586658\n",
      "Iteration 32935: loss = 1.3116157e-07,0.0036534872\n",
      "Iteration 32940: loss = 1.3187945e-07,0.0033972294\n",
      "Iteration 32945: loss = 1.325726e-07,0.003171235\n",
      "Iteration 32950: loss = 1.3326827e-07,0.0029667001\n",
      "Iteration 32955: loss = 1.3390971e-07,0.0027946767\n",
      "Iteration 32960: loss = 1.3454998e-07,0.0026427438\n",
      "Iteration 32965: loss = 1.3513356e-07,0.0025178518\n",
      "Iteration 32970: loss = 1.3568253e-07,0.0024102018\n",
      "Iteration 32975: loss = 1.3626814e-07,0.002312771\n",
      "Iteration 32980: loss = 1.3679708e-07,0.0022346724\n",
      "Iteration 32985: loss = 1.3727328e-07,0.0021710924\n",
      "Iteration 32990: loss = 1.3780391e-07,0.0021128312\n",
      "Iteration 32995: loss = 1.3830426e-07,0.0020669848\n",
      "Iteration 33000: loss = 1.3874995e-07,0.0020316641\n",
      "Iteration 33005: loss = 1.3919379e-07,0.0020033622\n",
      "Iteration 33010: loss = 1.3964141e-07,0.0019828966\n",
      "Iteration 33015: loss = 1.378683e-07,0.002059165\n",
      "Iteration 33020: loss = 1.308363e-07,0.0037030727\n",
      "Iteration 33025: loss = 1.2702476e-07,0.006014504\n",
      "Iteration 33030: loss = 1.2591838e-07,0.012209008\n",
      "Iteration 33035: loss = 1.2326151e-07,0.07083795\n",
      "Iteration 33040: loss = 1.275634e-07,0.28152773\n",
      "Iteration 33045: loss = 1.2500028e-07,0.033113547\n",
      "Iteration 33050: loss = 1.240504e-07,0.009707009\n",
      "Iteration 33055: loss = 1.2474868e-07,0.02288684\n",
      "Iteration 33060: loss = 1.2595846e-07,0.025319098\n",
      "Iteration 33065: loss = 1.2710896e-07,0.0205855\n",
      "Iteration 33070: loss = 1.2813445e-07,0.014131967\n",
      "Iteration 33075: loss = 1.2914224e-07,0.008636734\n",
      "Iteration 33080: loss = 1.3000486e-07,0.005148413\n",
      "Iteration 33085: loss = 1.3072324e-07,0.0035923496\n",
      "Iteration 33090: loss = 1.3141153e-07,0.0032540066\n",
      "Iteration 33095: loss = 1.3209866e-07,0.003310529\n",
      "Iteration 33100: loss = 1.3276167e-07,0.003211826\n",
      "Iteration 33105: loss = 1.3333992e-07,0.0028928402\n",
      "Iteration 33110: loss = 1.3385991e-07,0.002571378\n",
      "Iteration 33115: loss = 1.343307e-07,0.0024121706\n",
      "Iteration 33120: loss = 1.3482433e-07,0.002347389\n",
      "Iteration 33125: loss = 1.3529852e-07,0.0022618026\n",
      "Iteration 33130: loss = 1.3575537e-07,0.0021576136\n",
      "Iteration 33135: loss = 1.3624786e-07,0.002083683\n",
      "Iteration 33140: loss = 1.3669141e-07,0.0020335256\n",
      "Iteration 33145: loss = 1.3710441e-07,0.001982349\n",
      "Iteration 33150: loss = 1.3749128e-07,0.0019410832\n",
      "Iteration 33155: loss = 1.3787376e-07,0.001910449\n",
      "Iteration 33160: loss = 1.382345e-07,0.0018811569\n",
      "Iteration 33165: loss = 1.3864111e-07,0.001858711\n",
      "Iteration 33170: loss = 1.3902654e-07,0.0018403006\n",
      "Iteration 33175: loss = 1.3791238e-07,0.0018610954\n",
      "Iteration 33180: loss = 1.3324166e-07,0.0025479272\n",
      "Iteration 33185: loss = 1.3087374e-07,0.003238477\n",
      "Iteration 33190: loss = 1.2975705e-07,0.003635748\n",
      "Iteration 33195: loss = 1.2935406e-07,0.0037787103\n",
      "Iteration 33200: loss = 1.2933232e-07,0.0037626626\n",
      "Iteration 33205: loss = 1.2964368e-07,0.0036203682\n",
      "Iteration 33210: loss = 1.3007522e-07,0.0034357216\n",
      "Iteration 33215: loss = 1.3053476e-07,0.0032525174\n",
      "Iteration 33220: loss = 1.3099223e-07,0.003083319\n",
      "Iteration 33225: loss = 1.3145215e-07,0.0029235783\n",
      "Iteration 33230: loss = 1.3190608e-07,0.0027796107\n",
      "Iteration 33235: loss = 1.3237232e-07,0.002638314\n",
      "Iteration 33240: loss = 1.328582e-07,0.0025071595\n",
      "Iteration 33245: loss = 1.333156e-07,0.0023936757\n",
      "Iteration 33250: loss = 1.337551e-07,0.0022899276\n",
      "Iteration 33255: loss = 1.3419351e-07,0.002196211\n",
      "Iteration 33260: loss = 1.3466332e-07,0.0021066403\n",
      "Iteration 33265: loss = 1.3509741e-07,0.0020450829\n",
      "Iteration 33270: loss = 1.3560087e-07,0.0020814994\n",
      "Iteration 33275: loss = 1.3564599e-07,0.0034275972\n",
      "Iteration 33280: loss = 1.3728749e-07,0.021889498\n",
      "Iteration 33285: loss = 1.2114792e-07,0.22785851\n",
      "Iteration 33290: loss = 8.896861e-08,0.11961081\n",
      "Iteration 33295: loss = 7.417372e-08,0.21440597\n",
      "Iteration 33300: loss = 6.889295e-08,0.2139432\n",
      "Iteration 33305: loss = 6.893779e-08,0.18758829\n",
      "Iteration 33310: loss = 7.196979e-08,0.15475759\n",
      "Iteration 33315: loss = 7.661334e-08,0.12280818\n",
      "Iteration 33320: loss = 8.188741e-08,0.095438175\n",
      "Iteration 33325: loss = 8.708842e-08,0.07389429\n",
      "Iteration 33330: loss = 9.195369e-08,0.05735096\n",
      "Iteration 33335: loss = 9.652388e-08,0.044499684\n",
      "Iteration 33340: loss = 1.0071756e-07,0.03459479\n",
      "Iteration 33345: loss = 1.0444224e-07,0.027135298\n",
      "Iteration 33350: loss = 1.0754578e-07,0.021755522\n",
      "Iteration 33355: loss = 1.10172834e-07,0.017789377\n",
      "Iteration 33360: loss = 1.124028e-07,0.014804364\n",
      "Iteration 33365: loss = 1.14352524e-07,0.012505448\n",
      "Iteration 33370: loss = 1.1606007e-07,0.010718666\n",
      "Iteration 33375: loss = 1.1757597e-07,0.009301532\n",
      "Iteration 33380: loss = 1.1894429e-07,0.008139725\n",
      "Iteration 33385: loss = 1.2023234e-07,0.0071505317\n",
      "Iteration 33390: loss = 1.214765e-07,0.0062786504\n",
      "Iteration 33395: loss = 1.226103e-07,0.005547982\n",
      "Iteration 33400: loss = 1.2371949e-07,0.004918839\n",
      "Iteration 33405: loss = 1.2472938e-07,0.004397864\n",
      "Iteration 33410: loss = 1.2568981e-07,0.003950225\n",
      "Iteration 33415: loss = 1.2661367e-07,0.003570366\n",
      "Iteration 33420: loss = 1.2744178e-07,0.0032570101\n",
      "Iteration 33425: loss = 1.2821359e-07,0.0029957315\n",
      "Iteration 33430: loss = 1.2898145e-07,0.0027669065\n",
      "Iteration 33435: loss = 1.2968555e-07,0.0025776683\n",
      "Iteration 33440: loss = 1.3037253e-07,0.002416505\n",
      "Iteration 33445: loss = 1.309776e-07,0.0022886754\n",
      "Iteration 33450: loss = 1.3163034e-07,0.0021730068\n",
      "Iteration 33455: loss = 1.3218582e-07,0.0020819705\n",
      "Iteration 33460: loss = 1.3276059e-07,0.002004094\n",
      "Iteration 33465: loss = 1.3323245e-07,0.0019447415\n",
      "Iteration 33470: loss = 1.3369838e-07,0.0018950511\n",
      "Iteration 33475: loss = 1.3416907e-07,0.0018525892\n",
      "Iteration 33480: loss = 1.3462999e-07,0.0018202065\n",
      "Iteration 33485: loss = 1.3507488e-07,0.0017936181\n",
      "Iteration 33490: loss = 1.2953001e-07,0.0024755162\n",
      "Iteration 33495: loss = 1.2042558e-07,0.0066721174\n",
      "Iteration 33500: loss = 1.1594907e-07,0.010219835\n",
      "Iteration 33505: loss = 1.1389127e-07,0.012154696\n",
      "Iteration 33510: loss = 1.1329535e-07,0.012698715\n",
      "Iteration 33515: loss = 1.1361748e-07,0.012262928\n",
      "Iteration 33520: loss = 1.14527076e-07,0.011251627\n",
      "Iteration 33525: loss = 1.15649186e-07,0.010111469\n",
      "Iteration 33530: loss = 1.1675289e-07,0.009047955\n",
      "Iteration 33535: loss = 1.1786377e-07,0.00807498\n",
      "Iteration 33540: loss = 1.1900611e-07,0.007160905\n",
      "Iteration 33545: loss = 1.200858e-07,0.0063574924\n",
      "Iteration 33550: loss = 1.2123606e-07,0.005606766\n",
      "Iteration 33555: loss = 1.2223315e-07,0.005053153\n",
      "Iteration 33560: loss = 1.2347171e-07,0.004855794\n",
      "Iteration 33565: loss = 1.2374939e-07,0.00975035\n",
      "Iteration 33570: loss = 1.2707459e-07,0.06772828\n",
      "Iteration 33575: loss = 1.212726e-07,0.29225466\n",
      "Iteration 33580: loss = 1.2307028e-07,0.046988938\n",
      "Iteration 33585: loss = 1.2485098e-07,0.0039344593\n",
      "Iteration 33590: loss = 1.2663152e-07,0.006557663\n",
      "Iteration 33595: loss = 1.279936e-07,0.00948248\n",
      "Iteration 33600: loss = 1.2896479e-07,0.009688145\n",
      "Iteration 33605: loss = 1.2978693e-07,0.00859029\n",
      "Iteration 33610: loss = 1.305258e-07,0.00707591\n",
      "Iteration 33615: loss = 1.3101607e-07,0.005546899\n",
      "Iteration 33620: loss = 1.3130904e-07,0.0041765985\n",
      "Iteration 33625: loss = 1.3165702e-07,0.0030705796\n",
      "Iteration 33630: loss = 1.3206778e-07,0.0023060292\n",
      "Iteration 33635: loss = 1.3244394e-07,0.0018960886\n",
      "Iteration 33640: loss = 1.3281628e-07,0.0017598617\n",
      "Iteration 33645: loss = 1.3318592e-07,0.0017627402\n",
      "Iteration 33650: loss = 1.335349e-07,0.0017779418\n",
      "Iteration 33655: loss = 1.3385873e-07,0.0017470638\n",
      "Iteration 33660: loss = 1.20381e-07,0.0059050946\n",
      "Iteration 33665: loss = 1.1146835e-07,0.013721433\n",
      "Iteration 33670: loss = 1.07346985e-07,0.018857002\n",
      "Iteration 33675: loss = 1.05771676e-07,0.020979602\n",
      "Iteration 33680: loss = 1.05929075e-07,0.020577688\n",
      "Iteration 33685: loss = 1.0718895e-07,0.018629573\n",
      "Iteration 33690: loss = 1.08933115e-07,0.016169807\n",
      "Iteration 33695: loss = 1.107874e-07,0.01379652\n",
      "Iteration 33700: loss = 1.1259275e-07,0.011711206\n",
      "Iteration 33705: loss = 1.1437615e-07,0.009885824\n",
      "Iteration 33710: loss = 1.1609012e-07,0.008312945\n",
      "Iteration 33715: loss = 1.1769967e-07,0.007000058\n",
      "Iteration 33720: loss = 1.1925614e-07,0.0058933054\n",
      "Iteration 33725: loss = 1.2073271e-07,0.0049817986\n",
      "Iteration 33730: loss = 1.2209217e-07,0.0042416323\n",
      "Iteration 33735: loss = 1.233638e-07,0.0036411234\n",
      "Iteration 33740: loss = 1.245123e-07,0.0031746754\n",
      "Iteration 33745: loss = 1.2555762e-07,0.0028067087\n",
      "Iteration 33750: loss = 1.2651512e-07,0.0025204765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33755: loss = 1.2739646e-07,0.0022938733\n",
      "Iteration 33760: loss = 1.28212e-07,0.0021202317\n",
      "Iteration 33765: loss = 1.2895195e-07,0.0019848826\n",
      "Iteration 33770: loss = 1.29604e-07,0.0018816895\n",
      "Iteration 33775: loss = 1.3025851e-07,0.0017996979\n",
      "Iteration 33780: loss = 1.3083913e-07,0.0017379029\n",
      "Iteration 33785: loss = 1.3135906e-07,0.0016931815\n",
      "Iteration 33790: loss = 1.3184778e-07,0.0016596993\n",
      "Iteration 33795: loss = 1.322892e-07,0.0016349066\n",
      "Iteration 33800: loss = 1.2608247e-07,0.002528627\n",
      "Iteration 33805: loss = 1.2006946e-07,0.005177383\n",
      "Iteration 33810: loss = 1.17068645e-07,0.0072202077\n",
      "Iteration 33815: loss = 1.1625062e-07,0.008333896\n",
      "Iteration 33820: loss = 1.1545361e-07,0.0137279425\n",
      "Iteration 33825: loss = 1.1847623e-07,0.059143107\n",
      "Iteration 33830: loss = 1.13193614e-07,0.25054762\n",
      "Iteration 33835: loss = 1.1597635e-07,0.015436202\n",
      "Iteration 33840: loss = 1.18262285e-07,0.021150015\n",
      "Iteration 33845: loss = 1.201737e-07,0.035476748\n",
      "Iteration 33850: loss = 1.2147449e-07,0.029802993\n",
      "Iteration 33855: loss = 1.2245785e-07,0.018349618\n",
      "Iteration 33860: loss = 1.2335109e-07,0.00919503\n",
      "Iteration 33865: loss = 1.2413442e-07,0.0042421087\n",
      "Iteration 33870: loss = 1.2475438e-07,0.0027373557\n",
      "Iteration 33875: loss = 1.2536879e-07,0.0028873342\n",
      "Iteration 33880: loss = 1.2607765e-07,0.003144817\n",
      "Iteration 33885: loss = 1.2691942e-07,0.002848378\n",
      "Iteration 33890: loss = 1.2768866e-07,0.0022681812\n",
      "Iteration 33895: loss = 1.284586e-07,0.0018615575\n",
      "Iteration 33900: loss = 1.291609e-07,0.0017696256\n",
      "Iteration 33905: loss = 1.2973635e-07,0.0017618083\n",
      "Iteration 33910: loss = 1.301986e-07,0.0016744903\n",
      "Iteration 33915: loss = 1.3061992e-07,0.0015962756\n",
      "Iteration 33920: loss = 1.3099176e-07,0.0015786015\n",
      "Iteration 33925: loss = 1.3134438e-07,0.0015578258\n",
      "Iteration 33930: loss = 1.3052109e-07,0.0015550384\n",
      "Iteration 33935: loss = 1.2684754e-07,0.002042467\n",
      "Iteration 33940: loss = 1.2505046e-07,0.0025032777\n",
      "Iteration 33945: loss = 1.2429085e-07,0.0027345004\n",
      "Iteration 33950: loss = 1.2414911e-07,0.0027701717\n",
      "Iteration 33955: loss = 1.2438186e-07,0.0026707062\n",
      "Iteration 33960: loss = 1.2488013e-07,0.0025011753\n",
      "Iteration 33965: loss = 1.254124e-07,0.002332461\n",
      "Iteration 33970: loss = 1.258868e-07,0.0021951697\n",
      "Iteration 33975: loss = 1.2636703e-07,0.002066785\n",
      "Iteration 33980: loss = 1.2682341e-07,0.001954785\n",
      "Iteration 33985: loss = 1.273012e-07,0.0018531407\n",
      "Iteration 33990: loss = 1.277615e-07,0.001762206\n",
      "Iteration 33995: loss = 1.2824717e-07,0.0016797035\n",
      "Iteration 34000: loss = 1.2869909e-07,0.0016138465\n",
      "Iteration 34005: loss = 1.2912416e-07,0.0015567439\n",
      "Iteration 34010: loss = 1.2960129e-07,0.0015073097\n",
      "Iteration 34015: loss = 1.2994907e-07,0.0014732552\n",
      "Iteration 34020: loss = 1.3029397e-07,0.0014450065\n",
      "Iteration 34025: loss = 1.3058256e-07,0.0014240054\n",
      "Iteration 34030: loss = 1.3089873e-07,0.0014071527\n",
      "Iteration 34035: loss = 1.3110134e-07,0.0014063686\n",
      "Iteration 34040: loss = 1.3147279e-07,0.0014727069\n",
      "Iteration 34045: loss = 1.300443e-07,0.0021223587\n",
      "Iteration 34050: loss = 1.2992793e-07,0.0076182657\n",
      "Iteration 34055: loss = 1.262217e-07,0.060216974\n",
      "Iteration 34060: loss = 9.146467e-08,0.29381076\n",
      "Iteration 34065: loss = 4.702606e-08,0.3026888\n",
      "Iteration 34070: loss = 3.4397e-08,0.44322285\n",
      "Iteration 34075: loss = 3.245066e-08,0.48511553\n",
      "Iteration 34080: loss = 3.562137e-08,0.4338987\n",
      "Iteration 34085: loss = 4.167777e-08,0.34642765\n",
      "Iteration 34090: loss = 4.912441e-08,0.26168126\n",
      "Iteration 34095: loss = 5.7115727e-08,0.19321056\n",
      "Iteration 34100: loss = 6.5127e-08,0.14210384\n",
      "Iteration 34105: loss = 7.216916e-08,0.107478164\n",
      "Iteration 34110: loss = 7.747579e-08,0.085555024\n",
      "Iteration 34115: loss = 8.111281e-08,0.07162248\n",
      "Iteration 34120: loss = 8.357401e-08,0.06256617\n",
      "Iteration 34125: loss = 8.536667e-08,0.056461275\n",
      "Iteration 34130: loss = 8.675445e-08,0.05210355\n",
      "Iteration 34135: loss = 8.80874e-08,0.048056025\n",
      "Iteration 34140: loss = 8.968207e-08,0.04351634\n",
      "Iteration 34145: loss = 9.148163e-08,0.038825627\n",
      "Iteration 34150: loss = 9.341289e-08,0.034261808\n",
      "Iteration 34155: loss = 9.538645e-08,0.029968439\n",
      "Iteration 34160: loss = 9.734788e-08,0.02605338\n",
      "Iteration 34165: loss = 9.9247124e-08,0.022587357\n",
      "Iteration 34170: loss = 1.01121366e-07,0.019505618\n",
      "Iteration 34175: loss = 1.0286244e-07,0.016865486\n",
      "Iteration 34180: loss = 1.04578e-07,0.014509592\n",
      "Iteration 34185: loss = 1.0619927e-07,0.01249672\n",
      "Iteration 34190: loss = 1.07736014e-07,0.010759342\n",
      "Iteration 34195: loss = 1.0923922e-07,0.009234691\n",
      "Iteration 34200: loss = 1.106672e-07,0.0079245735\n",
      "Iteration 34205: loss = 1.1203584e-07,0.0068170987\n",
      "Iteration 34210: loss = 1.1328263e-07,0.005908346\n",
      "Iteration 34215: loss = 1.1441302e-07,0.005157389\n",
      "Iteration 34220: loss = 1.1546604e-07,0.004531305\n",
      "Iteration 34225: loss = 1.1648235e-07,0.003992425\n",
      "Iteration 34230: loss = 1.1740945e-07,0.003551297\n",
      "Iteration 34235: loss = 1.18258335e-07,0.0031892743\n",
      "Iteration 34240: loss = 1.1907638e-07,0.0028825118\n",
      "Iteration 34245: loss = 1.1984604e-07,0.0026238416\n",
      "Iteration 34250: loss = 1.205375e-07,0.002415894\n",
      "Iteration 34255: loss = 1.2121914e-07,0.002239379\n",
      "Iteration 34260: loss = 1.218019e-07,0.0020995135\n",
      "Iteration 34265: loss = 1.2233596e-07,0.0019836961\n",
      "Iteration 34270: loss = 1.227707e-07,0.0019038629\n",
      "Iteration 34275: loss = 1.2333457e-07,0.001879866\n",
      "Iteration 34280: loss = 1.2339352e-07,0.002500143\n",
      "Iteration 34285: loss = 1.2493228e-07,0.009857135\n",
      "Iteration 34290: loss = 1.1097847e-07,0.10002411\n",
      "Iteration 34295: loss = 1.0743915e-07,0.2706316\n",
      "Iteration 34300: loss = 9.9350814e-08,0.1250584\n",
      "Iteration 34305: loss = 9.617855e-08,0.052412782\n",
      "Iteration 34310: loss = 9.627374e-08,0.03159491\n",
      "Iteration 34315: loss = 9.788229e-08,0.023623006\n",
      "Iteration 34320: loss = 1.00105375e-07,0.018920835\n",
      "Iteration 34325: loss = 1.0247427e-07,0.015460282\n",
      "Iteration 34330: loss = 1.04695665e-07,0.012816166\n",
      "Iteration 34335: loss = 1.0680042e-07,0.0107135475\n",
      "Iteration 34340: loss = 1.08804635e-07,0.008908685\n",
      "Iteration 34345: loss = 1.10646056e-07,0.007359969\n",
      "Iteration 34350: loss = 1.1234539e-07,0.0060301884\n",
      "Iteration 34355: loss = 1.1404641e-07,0.004828978\n",
      "Iteration 34360: loss = 1.155586e-07,0.003860401\n",
      "Iteration 34365: loss = 1.168992e-07,0.0031410684\n",
      "Iteration 34370: loss = 1.1799725e-07,0.002662829\n",
      "Iteration 34375: loss = 1.1898402e-07,0.0023513858\n",
      "Iteration 34380: loss = 1.1986113e-07,0.0021414869\n",
      "Iteration 34385: loss = 1.2064525e-07,0.0019821085\n",
      "Iteration 34390: loss = 1.2130785e-07,0.0018515615\n",
      "Iteration 34395: loss = 1.2184148e-07,0.0017518925\n",
      "Iteration 34400: loss = 1.2237247e-07,0.0016770478\n",
      "Iteration 34405: loss = 1.2280788e-07,0.001623285\n",
      "Iteration 34410: loss = 1.2325837e-07,0.0015739822\n",
      "Iteration 34415: loss = 1.2367276e-07,0.0015354496\n",
      "Iteration 34420: loss = 1.2401256e-07,0.0015049854\n",
      "Iteration 34425: loss = 1.2122413e-07,0.0017007858\n",
      "Iteration 34430: loss = 1.16568685e-07,0.0029699972\n",
      "Iteration 34435: loss = 1.14415364e-07,0.003954081\n",
      "Iteration 34440: loss = 1.1351756e-07,0.0044056494\n",
      "Iteration 34445: loss = 1.1354053e-07,0.004365074\n",
      "Iteration 34450: loss = 1.14067596e-07,0.0040502883\n",
      "Iteration 34455: loss = 1.14734604e-07,0.0036849582\n",
      "Iteration 34460: loss = 1.1540914e-07,0.003332995\n",
      "Iteration 34465: loss = 1.16096565e-07,0.003013439\n",
      "Iteration 34470: loss = 1.16831494e-07,0.0027053892\n",
      "Iteration 34475: loss = 1.1750941e-07,0.0024543747\n",
      "Iteration 34480: loss = 1.1818938e-07,0.0022306112\n",
      "Iteration 34485: loss = 1.1887653e-07,0.0020306865\n",
      "Iteration 34490: loss = 1.195393e-07,0.0018627212\n",
      "Iteration 34495: loss = 1.2016967e-07,0.0017272894\n",
      "Iteration 34500: loss = 1.2074032e-07,0.0016236731\n",
      "Iteration 34505: loss = 1.212407e-07,0.0015419292\n",
      "Iteration 34510: loss = 1.2168617e-07,0.0014786755\n",
      "Iteration 34515: loss = 1.2207643e-07,0.0014298985\n",
      "Iteration 34520: loss = 1.2243886e-07,0.0013902446\n",
      "Iteration 34525: loss = 1.2273743e-07,0.0013593293\n",
      "Iteration 34530: loss = 1.2304362e-07,0.0013336563\n",
      "Iteration 34535: loss = 1.232929e-07,0.0013165071\n",
      "Iteration 34540: loss = 1.2360137e-07,0.0013204562\n",
      "Iteration 34545: loss = 1.2350141e-07,0.0015255951\n",
      "Iteration 34550: loss = 1.0445576e-07,0.014558793\n",
      "Iteration 34555: loss = 7.678114e-08,0.10868779\n",
      "Iteration 34560: loss = 7.1341134e-08,0.37163213\n",
      "Iteration 34565: loss = 6.508346e-08,0.12881438\n",
      "Iteration 34570: loss = 6.6861254e-08,0.14402442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34575: loss = 7.283207e-08,0.11849255\n",
      "Iteration 34580: loss = 7.964539e-08,0.08507841\n",
      "Iteration 34585: loss = 8.641968e-08,0.057368442\n",
      "Iteration 34590: loss = 9.284653e-08,0.03737011\n",
      "Iteration 34595: loss = 9.819016e-08,0.02449605\n",
      "Iteration 34600: loss = 1.0223121e-07,0.016711948\n",
      "Iteration 34605: loss = 1.0534421e-07,0.011796468\n",
      "Iteration 34610: loss = 1.07903126e-07,0.008507153\n",
      "Iteration 34615: loss = 1.0983616e-07,0.0063704723\n",
      "Iteration 34620: loss = 1.1150132e-07,0.0048687337\n",
      "Iteration 34625: loss = 1.12992524e-07,0.0037983914\n",
      "Iteration 34630: loss = 1.1425613e-07,0.0030804323\n",
      "Iteration 34635: loss = 1.15360905e-07,0.0026076725\n",
      "Iteration 34640: loss = 1.16294636e-07,0.0023030809\n",
      "Iteration 34645: loss = 1.17132934e-07,0.0020835144\n",
      "Iteration 34650: loss = 1.1777052e-07,0.0019246067\n",
      "Iteration 34655: loss = 1.1834453e-07,0.0017897887\n",
      "Iteration 34660: loss = 1.1884104e-07,0.0016794058\n",
      "Iteration 34665: loss = 1.1932757e-07,0.0015922948\n",
      "Iteration 34670: loss = 1.1971753e-07,0.0015294631\n",
      "Iteration 34675: loss = 1.2007695e-07,0.0014770709\n",
      "Iteration 34680: loss = 1.2040852e-07,0.0014313982\n",
      "Iteration 34685: loss = 1.2071439e-07,0.0013931678\n",
      "Iteration 34690: loss = 1.2094308e-07,0.0013626202\n",
      "Iteration 34695: loss = 1.2118664e-07,0.0013361492\n",
      "Iteration 34700: loss = 1.21359e-07,0.0013127452\n",
      "Iteration 34705: loss = 1.2152385e-07,0.0012927225\n",
      "Iteration 34710: loss = 1.2166477e-07,0.0012749868\n",
      "Iteration 34715: loss = 1.2180139e-07,0.0012589487\n",
      "Iteration 34720: loss = 1.2191721e-07,0.0012441289\n",
      "Iteration 34725: loss = 1.18719846e-07,0.0015069351\n",
      "Iteration 34730: loss = 1.1558066e-07,0.0023002622\n",
      "Iteration 34735: loss = 1.140472e-07,0.0028686475\n",
      "Iteration 34740: loss = 1.13381674e-07,0.0031341221\n",
      "Iteration 34745: loss = 1.13373744e-07,0.0031140596\n",
      "Iteration 34750: loss = 1.1366319e-07,0.0029585464\n",
      "Iteration 34755: loss = 1.1400939e-07,0.002784514\n",
      "Iteration 34760: loss = 1.1437982e-07,0.0026132928\n",
      "Iteration 34765: loss = 1.1481263e-07,0.0024275708\n",
      "Iteration 34770: loss = 1.15294775e-07,0.002244443\n",
      "Iteration 34775: loss = 1.1583314e-07,0.0020605773\n",
      "Iteration 34780: loss = 1.163379e-07,0.0019030772\n",
      "Iteration 34785: loss = 1.1690823e-07,0.0017481886\n",
      "Iteration 34790: loss = 1.17428186e-07,0.0016234033\n",
      "Iteration 34795: loss = 1.1791946e-07,0.0015207897\n",
      "Iteration 34800: loss = 1.18328984e-07,0.0014457792\n",
      "Iteration 34805: loss = 1.1879151e-07,0.0013795249\n",
      "Iteration 34810: loss = 1.1899244e-07,0.0013821479\n",
      "Iteration 34815: loss = 1.1962068e-07,0.0015900251\n",
      "Iteration 34820: loss = 1.1899147e-07,0.004174453\n",
      "Iteration 34825: loss = 1.218584e-07,0.03004041\n",
      "Iteration 34830: loss = 1.6843883e-08,0.95840436\n",
      "Iteration 34835: loss = 6.899942e-09,1.1651555\n",
      "Iteration 34840: loss = 5.1707296e-09,1.3112601\n",
      "Iteration 34845: loss = 5.3304685e-09,1.2619635\n",
      "Iteration 34850: loss = 6.198812e-09,1.157926\n",
      "Iteration 34855: loss = 7.581074e-09,1.0510445\n",
      "Iteration 34860: loss = 9.45817e-09,0.9539641\n",
      "Iteration 34865: loss = 1.106822e-08,0.8900365\n",
      "Iteration 34870: loss = 1.2013666e-08,0.8568725\n",
      "Iteration 34875: loss = 1.2477352e-08,0.83928496\n",
      "Iteration 34880: loss = 1.2722534e-08,0.8278043\n",
      "Iteration 34885: loss = 1.3055647e-08,0.8137776\n",
      "Iteration 34890: loss = 1.3680218e-08,0.79177386\n",
      "Iteration 34895: loss = 1.4646506e-08,0.7607194\n",
      "Iteration 34900: loss = 1.5970295e-08,0.7214903\n",
      "Iteration 34905: loss = 1.7596006e-08,0.67664\n",
      "Iteration 34910: loss = 1.9435127e-08,0.6303003\n",
      "Iteration 34915: loss = 2.1364675e-08,0.5858358\n",
      "Iteration 34920: loss = 2.3320256e-08,0.5442817\n",
      "Iteration 34925: loss = 2.5329973e-08,0.5047932\n",
      "Iteration 34930: loss = 2.7480432e-08,0.46605518\n",
      "Iteration 34935: loss = 2.9766356e-08,0.42804825\n",
      "Iteration 34940: loss = 3.2264552e-08,0.39003387\n",
      "Iteration 34945: loss = 3.5013397e-08,0.3520683\n",
      "Iteration 34950: loss = 3.798341e-08,0.31474742\n",
      "Iteration 34955: loss = 4.1159314e-08,0.27877948\n",
      "Iteration 34960: loss = 4.4516653e-08,0.24464971\n",
      "Iteration 34965: loss = 4.7971312e-08,0.21302043\n",
      "Iteration 34970: loss = 5.1537583e-08,0.18389706\n",
      "Iteration 34975: loss = 5.5203966e-08,0.15719685\n",
      "Iteration 34980: loss = 5.8895353e-08,0.13324581\n",
      "Iteration 34985: loss = 6.2595504e-08,0.11196266\n",
      "Iteration 34990: loss = 6.6292245e-08,0.09319307\n",
      "Iteration 34995: loss = 6.994749e-08,0.07684998\n",
      "Iteration 35000: loss = 7.3496125e-08,0.062909484\n",
      "Iteration 35005: loss = 7.687087e-08,0.051212694\n",
      "Iteration 35010: loss = 8.015158e-08,0.04136656\n",
      "Iteration 35015: loss = 8.3167784e-08,0.033373345\n",
      "Iteration 35020: loss = 8.605294e-08,0.026747422\n",
      "Iteration 35025: loss = 8.8704915e-08,0.021438735\n",
      "Iteration 35030: loss = 9.116612e-08,0.017162561\n",
      "Iteration 35035: loss = 9.336455e-08,0.013822042\n",
      "Iteration 35040: loss = 9.541123e-08,0.011131883\n",
      "Iteration 35045: loss = 9.718061e-08,0.009086883\n",
      "Iteration 35050: loss = 9.8846094e-08,0.00743361\n",
      "Iteration 35055: loss = 1.0012209e-07,0.006397932\n",
      "Iteration 35060: loss = 1.0188486e-07,0.0060419487\n",
      "Iteration 35065: loss = 1.0139902e-07,0.017078405\n",
      "Iteration 35070: loss = 1.0734603e-07,0.13247424\n",
      "Iteration 35075: loss = 9.6461434e-08,0.17060192\n",
      "Iteration 35080: loss = 9.4536425e-08,0.12853286\n",
      "Iteration 35085: loss = 9.544456e-08,0.061443824\n",
      "Iteration 35090: loss = 9.645203e-08,0.030035056\n",
      "Iteration 35095: loss = 9.80091e-08,0.015896032\n",
      "Iteration 35100: loss = 9.965206e-08,0.0092346035\n",
      "Iteration 35105: loss = 1.0116915e-07,0.00591696\n",
      "Iteration 35110: loss = 1.02481096e-07,0.004306532\n",
      "Iteration 35115: loss = 1.0367157e-07,0.0035075548\n",
      "Iteration 35120: loss = 1.0471636e-07,0.00312817\n",
      "Iteration 35125: loss = 1.0559362e-07,0.002913037\n",
      "Iteration 35130: loss = 1.0647779e-07,0.0026720194\n",
      "Iteration 35135: loss = 1.0725726e-07,0.0023974336\n",
      "Iteration 35140: loss = 1.0782881e-07,0.002130075\n",
      "Iteration 35145: loss = 1.082522e-07,0.001896581\n",
      "Iteration 35150: loss = 1.08617066e-07,0.0017245638\n",
      "Iteration 35155: loss = 1.0885369e-07,0.001635539\n",
      "Iteration 35160: loss = 1.0907061e-07,0.0015852064\n",
      "Iteration 35165: loss = 1.0932263e-07,0.0015344151\n",
      "Iteration 35170: loss = 1.09537574e-07,0.0014788862\n",
      "Iteration 35175: loss = 1.0977207e-07,0.0014242845\n",
      "Iteration 35180: loss = 1.0998472e-07,0.0013832189\n",
      "Iteration 35185: loss = 1.10094504e-07,0.001354652\n",
      "Iteration 35190: loss = 1.1017585e-07,0.0013284598\n",
      "Iteration 35195: loss = 1.1025792e-07,0.0013036086\n",
      "Iteration 35200: loss = 1.10337886e-07,0.0012825492\n",
      "Iteration 35205: loss = 1.1041533e-07,0.0012631447\n",
      "Iteration 35210: loss = 1.10494064e-07,0.0012444913\n",
      "Iteration 35215: loss = 1.10546885e-07,0.001228961\n",
      "Iteration 35220: loss = 1.1057415e-07,0.0012158247\n",
      "Iteration 35225: loss = 1.10607566e-07,0.0012022057\n",
      "Iteration 35230: loss = 1.1068625e-07,0.0011882678\n",
      "Iteration 35235: loss = 1.10744615e-07,0.0011753467\n",
      "Iteration 35240: loss = 1.1084523e-07,0.0011624695\n",
      "Iteration 35245: loss = 1.1087517e-07,0.0011525385\n",
      "Iteration 35250: loss = 1.1092902e-07,0.0011424082\n",
      "Iteration 35255: loss = 1.1095657e-07,0.0011343227\n",
      "Iteration 35260: loss = 1.1098127e-07,0.0011271227\n",
      "Iteration 35265: loss = 1.1103774e-07,0.0011177259\n",
      "Iteration 35270: loss = 1.11088184e-07,0.0011101987\n",
      "Iteration 35275: loss = 1.1111505e-07,0.0011037483\n",
      "Iteration 35280: loss = 1.1114409e-07,0.0010970217\n",
      "Iteration 35285: loss = 1.1114733e-07,0.0010917941\n",
      "Iteration 35290: loss = 1.1122554e-07,0.0010845442\n",
      "Iteration 35295: loss = 1.112046e-07,0.0010818895\n",
      "Iteration 35300: loss = 1.1130452e-07,0.0010794494\n",
      "Iteration 35305: loss = 1.11178764e-07,0.0011024474\n",
      "Iteration 35310: loss = 1.1146324e-07,0.0012296287\n",
      "Iteration 35315: loss = 1.10748736e-07,0.0022748192\n",
      "Iteration 35320: loss = 6.882117e-08,0.08856803\n",
      "Iteration 35325: loss = 2.9729476e-08,0.5133293\n",
      "Iteration 35330: loss = 2.441597e-08,0.71778876\n",
      "Iteration 35335: loss = 2.5829026e-08,0.5378262\n",
      "Iteration 35340: loss = 3.2475082e-08,0.37165177\n",
      "Iteration 35345: loss = 4.1270074e-08,0.27087495\n",
      "Iteration 35350: loss = 5.0074934e-08,0.19876695\n",
      "Iteration 35355: loss = 5.6608886e-08,0.151065\n",
      "Iteration 35360: loss = 5.926843e-08,0.12785889\n",
      "Iteration 35365: loss = 5.9639866e-08,0.11937081\n",
      "Iteration 35370: loss = 5.984078e-08,0.11519572\n",
      "Iteration 35375: loss = 6.106902e-08,0.107693285\n",
      "Iteration 35380: loss = 6.36764e-08,0.09417649\n",
      "Iteration 35385: loss = 6.737579e-08,0.076817915\n",
      "Iteration 35390: loss = 7.147772e-08,0.05994965\n",
      "Iteration 35395: loss = 7.57996e-08,0.045223355\n",
      "Iteration 35400: loss = 7.995922e-08,0.03356982\n",
      "Iteration 35405: loss = 8.376688e-08,0.02474179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35410: loss = 8.7132186e-08,0.018222302\n",
      "Iteration 35415: loss = 9.0049376e-08,0.0135567505\n",
      "Iteration 35420: loss = 9.2499214e-08,0.010302693\n",
      "Iteration 35425: loss = 9.451785e-08,0.0080190245\n",
      "Iteration 35430: loss = 9.6220674e-08,0.0063816374\n",
      "Iteration 35435: loss = 9.7679624e-08,0.005183154\n",
      "Iteration 35440: loss = 9.8898994e-08,0.0043088817\n",
      "Iteration 35445: loss = 9.992696e-08,0.0036612279\n",
      "Iteration 35450: loss = 1.00737985e-07,0.00319697\n",
      "Iteration 35455: loss = 1.01417605e-07,0.0028475071\n",
      "Iteration 35460: loss = 1.02022746e-07,0.0025638114\n",
      "Iteration 35465: loss = 1.0252745e-07,0.0023434346\n",
      "Iteration 35470: loss = 1.0290844e-07,0.0021739816\n",
      "Iteration 35475: loss = 1.0328974e-07,0.0020251665\n",
      "Iteration 35480: loss = 1.0362351e-07,0.0019001199\n",
      "Iteration 35485: loss = 1.03907475e-07,0.0017966938\n",
      "Iteration 35490: loss = 1.04117625e-07,0.0017169458\n",
      "Iteration 35495: loss = 1.0432374e-07,0.0016449825\n",
      "Iteration 35500: loss = 1.0454958e-07,0.0015766263\n",
      "Iteration 35505: loss = 1.0473491e-07,0.0015172489\n",
      "Iteration 35510: loss = 1.0491508e-07,0.0014647932\n",
      "Iteration 35515: loss = 1.0502196e-07,0.0014267445\n",
      "Iteration 35520: loss = 1.0517629e-07,0.0013853495\n",
      "Iteration 35525: loss = 1.0528331e-07,0.0013535841\n",
      "Iteration 35530: loss = 1.0548437e-07,0.001317929\n",
      "Iteration 35535: loss = 1.0546088e-07,0.0013505514\n",
      "Iteration 35540: loss = 1.0595386e-07,0.0016149904\n",
      "Iteration 35545: loss = 1.0489773e-07,0.0050198208\n",
      "Iteration 35550: loss = 1.0845613e-07,0.040280566\n",
      "Iteration 35555: loss = 6.056407e-10,2.0286353\n",
      "Iteration 35560: loss = 1.0936312e-10,1.9275275\n",
      "Iteration 35565: loss = 5.846693e-11,1.9863219\n",
      "Iteration 35570: loss = 4.3203024e-11,1.9925134\n",
      "Iteration 35575: loss = 3.5301363e-11,1.9651158\n",
      "Iteration 35580: loss = 3.5318234e-11,1.944682\n",
      "Iteration 35585: loss = 3.7687534e-11,1.9286089\n",
      "Iteration 35590: loss = 3.8182738e-11,1.9193239\n",
      "Iteration 35595: loss = 3.8764138e-11,1.9138335\n",
      "Iteration 35600: loss = 4.10897e-11,1.9095273\n",
      "Iteration 35605: loss = 4.3163098e-11,1.9064724\n",
      "Iteration 35610: loss = 4.4725983e-11,1.9042937\n",
      "Iteration 35615: loss = 4.5338937e-11,1.9026998\n",
      "Iteration 35620: loss = 4.6399686e-11,1.900944\n",
      "Iteration 35625: loss = 4.753195e-11,1.8992245\n",
      "Iteration 35630: loss = 4.8596766e-11,1.8980112\n",
      "Iteration 35635: loss = 4.9613938e-11,1.8970767\n",
      "Iteration 35640: loss = 4.978709e-11,1.8964487\n",
      "Iteration 35645: loss = 5.085388e-11,1.895452\n",
      "Iteration 35650: loss = 5.1958073e-11,1.8944339\n",
      "Iteration 35655: loss = 5.307275e-11,1.8934393\n",
      "Iteration 35660: loss = 5.421776e-11,1.8923898\n",
      "Iteration 35665: loss = 5.496283e-11,1.8913943\n",
      "Iteration 35670: loss = 5.6609606e-11,1.8901361\n",
      "Iteration 35675: loss = 5.7820543e-11,1.8890185\n",
      "Iteration 35680: loss = 5.903948e-11,1.8879117\n",
      "Iteration 35685: loss = 6.0273044e-11,1.8867997\n",
      "Iteration 35690: loss = 6.152814e-11,1.8856646\n",
      "Iteration 35695: loss = 6.279985e-11,1.8845179\n",
      "Iteration 35700: loss = 6.408601e-11,1.8833661\n",
      "Iteration 35705: loss = 6.587967e-11,1.882049\n",
      "Iteration 35710: loss = 6.722196e-11,1.8808335\n",
      "Iteration 35715: loss = 6.859072e-11,1.8795861\n",
      "Iteration 35720: loss = 6.997468e-11,1.8783329\n",
      "Iteration 35725: loss = 7.189296e-11,1.8769026\n",
      "Iteration 35730: loss = 7.3339994e-11,1.8755761\n",
      "Iteration 35735: loss = 7.532615e-11,1.8740902\n",
      "Iteration 35740: loss = 7.683069e-11,1.8727083\n",
      "Iteration 35745: loss = 7.889218e-11,1.8711524\n",
      "Iteration 35750: loss = 8.100368e-11,1.8695472\n",
      "Iteration 35755: loss = 8.3149355e-11,1.8679264\n",
      "Iteration 35760: loss = 8.536181e-11,1.8662301\n",
      "Iteration 35765: loss = 8.757569e-11,1.8646266\n",
      "Iteration 35770: loss = 9.0509766e-11,1.8628789\n",
      "Iteration 35775: loss = 9.208839e-11,1.8639276\n",
      "Iteration 35780: loss = 9.7477755e-11,1.8807987\n",
      "Iteration 35785: loss = 9.5233675e-11,2.014102\n",
      "Iteration 35790: loss = 1.1461787e-10,1.8925756\n",
      "Iteration 35795: loss = 1.296827e-10,1.9172691\n",
      "Iteration 35800: loss = 1.4361176e-10,1.8629731\n",
      "Iteration 35805: loss = 1.5534651e-10,1.8282146\n",
      "Iteration 35810: loss = 1.6648011e-10,1.8165421\n",
      "Iteration 35815: loss = 1.7703393e-10,1.8133982\n",
      "Iteration 35820: loss = 1.8864678e-10,1.8095651\n",
      "Iteration 35825: loss = 1.9880848e-10,1.8034714\n",
      "Iteration 35830: loss = 2.1000714e-10,1.7961785\n",
      "Iteration 35835: loss = 2.215006e-10,1.7897938\n",
      "Iteration 35840: loss = 2.3334082e-10,1.7845982\n",
      "Iteration 35845: loss = 2.4358723e-10,1.7799925\n",
      "Iteration 35850: loss = 2.559348e-10,1.7746822\n",
      "Iteration 35855: loss = 2.676783e-10,1.7694056\n",
      "Iteration 35860: loss = 2.8083605e-10,1.7640446\n",
      "Iteration 35865: loss = 2.955232e-10,1.7583262\n",
      "Iteration 35870: loss = 3.1080316e-10,1.7523135\n",
      "Iteration 35875: loss = 3.2663214e-10,1.7462072\n",
      "Iteration 35880: loss = 3.4408312e-10,1.7398255\n",
      "Iteration 35885: loss = 3.6326978e-10,1.7331403\n",
      "Iteration 35890: loss = 3.8202022e-10,1.7264652\n",
      "Iteration 35895: loss = 4.026817e-10,1.7194163\n",
      "Iteration 35900: loss = 4.265809e-10,1.7118584\n",
      "Iteration 35905: loss = 4.5011983e-10,1.7043068\n",
      "Iteration 35910: loss = 4.7584064e-10,1.6964152\n",
      "Iteration 35915: loss = 5.039023e-10,1.6881578\n",
      "Iteration 35920: loss = 5.3302446e-10,1.6797379\n",
      "Iteration 35925: loss = 5.646264e-10,1.6710026\n",
      "Iteration 35930: loss = 5.974668e-10,1.6620728\n",
      "Iteration 35935: loss = 6.33001e-10,1.6528296\n",
      "Iteration 35940: loss = 6.71406e-10,1.6432577\n",
      "Iteration 35945: loss = 7.112713e-10,1.6335187\n",
      "Iteration 35950: loss = 7.542069e-10,1.6234784\n",
      "Iteration 35955: loss = 7.970737e-10,1.6134396\n",
      "Iteration 35960: loss = 8.466174e-10,1.6027668\n",
      "Iteration 35965: loss = 8.9449e-10,1.5922573\n",
      "Iteration 35970: loss = 9.475742e-10,1.5813339\n",
      "Iteration 35975: loss = 1.0046601e-09,1.5700302\n",
      "Iteration 35980: loss = 1.0608244e-09,1.5596031\n",
      "Iteration 35985: loss = 1.1343807e-09,1.5504569\n",
      "Iteration 35990: loss = 1.1638465e-09,1.5981619\n",
      "Iteration 35995: loss = 1.3330443e-09,1.8075063\n",
      "Iteration 36000: loss = 1.3504952e-09,1.5551976\n",
      "Iteration 36005: loss = 1.4214917e-09,1.498739\n",
      "Iteration 36010: loss = 1.5073951e-09,1.4830962\n",
      "Iteration 36015: loss = 1.610049e-09,1.4687414\n",
      "Iteration 36020: loss = 1.7139897e-09,1.4538401\n",
      "Iteration 36025: loss = 1.8343744e-09,1.4370078\n",
      "Iteration 36030: loss = 1.9592676e-09,1.4200435\n",
      "Iteration 36035: loss = 2.092266e-09,1.4025862\n",
      "Iteration 36040: loss = 2.23999e-09,1.3841372\n",
      "Iteration 36045: loss = 2.4057456e-09,1.3640643\n",
      "Iteration 36050: loss = 2.596386e-09,1.3422499\n",
      "Iteration 36055: loss = 2.8036642e-09,1.3194832\n",
      "Iteration 36060: loss = 3.035704e-09,1.2953904\n",
      "Iteration 36065: loss = 3.295443e-09,1.2698305\n",
      "Iteration 36070: loss = 3.591112e-09,1.2425071\n",
      "Iteration 36075: loss = 3.9180934e-09,1.2138624\n",
      "Iteration 36080: loss = 4.2807464e-09,1.183847\n",
      "Iteration 36085: loss = 4.691961e-09,1.1518474\n",
      "Iteration 36090: loss = 5.1653335e-09,1.1175575\n",
      "Iteration 36095: loss = 5.69365e-09,1.0815382\n",
      "Iteration 36100: loss = 6.30182e-09,1.0431459\n",
      "Iteration 36105: loss = 6.994958e-09,1.0024135\n",
      "Iteration 36110: loss = 7.775172e-09,0.9597923\n",
      "Iteration 36115: loss = 8.674079e-09,0.91477084\n",
      "Iteration 36120: loss = 9.703103e-09,0.86722237\n",
      "Iteration 36125: loss = 1.08853335e-08,0.81723106\n",
      "Iteration 36130: loss = 1.2227676e-08,0.7655064\n",
      "Iteration 36135: loss = 1.3759235e-08,0.7119812\n",
      "Iteration 36140: loss = 1.552634e-08,0.6563828\n",
      "Iteration 36145: loss = 1.7533377e-08,0.5996053\n",
      "Iteration 36150: loss = 1.982564e-08,0.5416747\n",
      "Iteration 36155: loss = 2.2420304e-08,0.48363668\n",
      "Iteration 36160: loss = 2.5295973e-08,0.426792\n",
      "Iteration 36165: loss = 2.8523978e-08,0.3708733\n",
      "Iteration 36170: loss = 3.2072034e-08,0.31721058\n",
      "Iteration 36175: loss = 3.593866e-08,0.2666568\n",
      "Iteration 36180: loss = 4.012627e-08,0.21970828\n",
      "Iteration 36185: loss = 4.4577785e-08,0.17723769\n",
      "Iteration 36190: loss = 4.915132e-08,0.14032936\n",
      "Iteration 36195: loss = 5.3805824e-08,0.10884456\n",
      "Iteration 36200: loss = 5.8426963e-08,0.0827586\n",
      "Iteration 36205: loss = 6.293323e-08,0.061757263\n",
      "Iteration 36210: loss = 6.72064e-08,0.04538523\n",
      "Iteration 36215: loss = 7.11606e-08,0.033014115\n",
      "Iteration 36220: loss = 7.476029e-08,0.023852691\n",
      "Iteration 36225: loss = 7.789287e-08,0.017344806\n",
      "Iteration 36230: loss = 8.063597e-08,0.012742087\n",
      "Iteration 36235: loss = 8.2948766e-08,0.009550919\n",
      "Iteration 36240: loss = 8.487802e-08,0.0073791263\n",
      "Iteration 36245: loss = 8.6569365e-08,0.005824535\n",
      "Iteration 36250: loss = 8.763075e-08,0.0050568325\n",
      "Iteration 36255: loss = 8.9328076e-08,0.005198387\n",
      "Iteration 36260: loss = 8.757588e-08,0.022529567\n",
      "Iteration 36265: loss = 9.5937416e-08,0.2069413\n",
      "Iteration 36270: loss = 2.2444636e-11,2.6313663\n",
      "Iteration 36275: loss = 2.0341855e-12,2.666551\n",
      "Iteration 36280: loss = 4.706127e-13,2.6828468\n",
      "Iteration 36285: loss = 9.174122e-13,2.6542573\n",
      "Iteration 36290: loss = 3.2544202e-12,2.3151343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36295: loss = 1.0740978e-11,2.065216\n",
      "Iteration 36300: loss = 3.8764096e-11,1.9727242\n",
      "Iteration 36305: loss = 1.09976785e-10,1.8860968\n",
      "Iteration 36310: loss = 1.605649e-10,1.8423474\n",
      "Iteration 36315: loss = 1.9229433e-10,1.8111653\n",
      "Iteration 36320: loss = 2.668615e-10,1.7653574\n",
      "Iteration 36325: loss = 3.1283254e-10,1.7443136\n",
      "Iteration 36330: loss = 3.1585387e-10,1.7398257\n",
      "Iteration 36335: loss = 3.5045347e-10,1.7234051\n",
      "Iteration 36340: loss = 3.7786033e-10,1.7128145\n",
      "Iteration 36345: loss = 4.0444292e-10,1.7028586\n",
      "Iteration 36350: loss = 4.5740756e-10,1.6844206\n",
      "Iteration 36355: loss = 5.030587e-10,1.6697652\n",
      "Iteration 36360: loss = 5.320407e-10,1.6607629\n",
      "Iteration 36365: loss = 5.69412e-10,1.6496513\n",
      "Iteration 36370: loss = 6.200081e-10,1.6354946\n",
      "Iteration 36375: loss = 6.741913e-10,1.6213639\n",
      "Iteration 36380: loss = 7.358793e-10,1.6057824\n",
      "Iteration 36385: loss = 8.1516954e-10,1.5873082\n",
      "Iteration 36390: loss = 8.9250735e-10,1.5699246\n",
      "Iteration 36395: loss = 9.823934e-10,1.5511765\n",
      "Iteration 36400: loss = 1.0950558e-09,1.5291005\n",
      "Iteration 36405: loss = 1.2307401e-09,1.5042562\n",
      "Iteration 36410: loss = 1.3958631e-09,1.4760463\n",
      "Iteration 36415: loss = 1.6081269e-09,1.4430863\n",
      "Iteration 36420: loss = 1.8796091e-09,1.4048085\n",
      "Iteration 36425: loss = 2.2268294e-09,1.3602792\n",
      "Iteration 36430: loss = 2.7035807e-09,1.3064917\n",
      "Iteration 36435: loss = 3.3585117e-09,1.2418332\n",
      "Iteration 36440: loss = 4.298188e-09,1.1628053\n",
      "Iteration 36445: loss = 5.7124967e-09,1.0646267\n",
      "Iteration 36450: loss = 7.927603e-09,0.94201475\n",
      "Iteration 36455: loss = 1.1603631e-08,0.7884067\n",
      "Iteration 36460: loss = 1.8064428e-08,0.5991287\n",
      "Iteration 36465: loss = 2.97709e-08,0.38282186\n",
      "Iteration 36470: loss = 5.1104905e-08,0.17274182\n",
      "Iteration 36475: loss = 8.2341955e-08,0.055837367\n",
      "Iteration 36480: loss = 1.1995193e-07,0.114302956\n",
      "Iteration 36485: loss = 2.4090849e-12,2.9464524\n",
      "Iteration 36490: loss = 1.872079e-13,4.7609267\n",
      "Iteration 36495: loss = 5.534555e-14,9.1459\n",
      "Iteration 36500: loss = 5.8516e-14,5.7249126\n",
      "Iteration 36505: loss = 7.026702e-07,1.4339625\n",
      "Iteration 36510: loss = 1.743534e-05,0.42051864\n",
      "Iteration 36515: loss = 2.7275739e-05,0.33541992\n",
      "Iteration 36520: loss = 1.4157221e-05,0.09636305\n",
      "Iteration 36525: loss = 5.1572592e-06,0.14843212\n",
      "Iteration 36530: loss = 2.0376401e-06,0.25736752\n",
      "Iteration 36535: loss = 1.0848584e-06,0.41566333\n",
      "Iteration 36540: loss = 7.483597e-07,0.49864233\n",
      "Iteration 36545: loss = 6.0223755e-07,0.4939929\n",
      "Iteration 36550: loss = 5.292366e-07,0.34002686\n",
      "Iteration 36555: loss = 4.5551758e-07,0.12750103\n",
      "Iteration 36560: loss = 3.2731566e-07,0.032292318\n",
      "Iteration 36565: loss = 2.0186627e-07,0.023862287\n",
      "Iteration 36570: loss = 1.2863653e-07,0.036874525\n",
      "Iteration 36575: loss = 9.210218e-08,0.065931015\n",
      "Iteration 36580: loss = 7.1452824e-08,0.10161973\n",
      "Iteration 36585: loss = 5.7916083e-08,0.14072947\n",
      "Iteration 36590: loss = 4.863149e-08,0.18061365\n",
      "Iteration 36595: loss = 4.1679893e-08,0.22166142\n",
      "Iteration 36600: loss = 3.5927428e-08,0.26745108\n",
      "Iteration 36605: loss = 3.1332874e-08,0.31408638\n",
      "Iteration 36610: loss = 2.7980983e-08,0.35533676\n",
      "Iteration 36615: loss = 2.5633986e-08,0.38852897\n",
      "Iteration 36620: loss = 2.3955293e-08,0.4148509\n",
      "Iteration 36625: loss = 2.281626e-08,0.4339554\n",
      "Iteration 36630: loss = 2.2171356e-08,0.44487068\n",
      "Iteration 36635: loss = 2.1912186e-08,0.44862556\n",
      "Iteration 36640: loss = 2.1910298e-08,0.44747484\n",
      "Iteration 36645: loss = 2.206181e-08,0.44333723\n",
      "Iteration 36650: loss = 2.2357142e-08,0.43641555\n",
      "Iteration 36655: loss = 2.2809159e-08,0.42660272\n",
      "Iteration 36660: loss = 2.3384308e-08,0.41459405\n",
      "Iteration 36665: loss = 2.405882e-08,0.4010915\n",
      "Iteration 36670: loss = 2.4827392e-08,0.3864077\n",
      "Iteration 36675: loss = 2.5663189e-08,0.3709805\n",
      "Iteration 36680: loss = 2.6582791e-08,0.35491768\n",
      "Iteration 36685: loss = 2.752521e-08,0.33898014\n",
      "Iteration 36690: loss = 2.8507294e-08,0.32315332\n",
      "Iteration 36695: loss = 2.9498503e-08,0.30772597\n",
      "Iteration 36700: loss = 3.0522937e-08,0.29255444\n",
      "Iteration 36705: loss = 3.1546893e-08,0.27795884\n",
      "Iteration 36710: loss = 3.2599203e-08,0.26365933\n",
      "Iteration 36715: loss = 3.365399e-08,0.24993569\n",
      "Iteration 36720: loss = 3.471027e-08,0.23677503\n",
      "Iteration 36725: loss = 3.5770743e-08,0.22407492\n",
      "Iteration 36730: loss = 3.6822993e-08,0.21201405\n",
      "Iteration 36735: loss = 3.788104e-08,0.20042203\n",
      "Iteration 36740: loss = 3.892453e-08,0.1894309\n",
      "Iteration 36745: loss = 3.9994777e-08,0.17869413\n",
      "Iteration 36750: loss = 4.10499e-08,0.16851695\n",
      "Iteration 36755: loss = 4.2090196e-08,0.15884906\n",
      "Iteration 36760: loss = 4.313198e-08,0.14961562\n",
      "Iteration 36765: loss = 4.417251e-08,0.1408424\n",
      "Iteration 36770: loss = 4.518316e-08,0.13257872\n",
      "Iteration 36775: loss = 4.618308e-08,0.12478923\n",
      "Iteration 36780: loss = 4.7164338e-08,0.11740876\n",
      "Iteration 36785: loss = 4.8146717e-08,0.11041325\n",
      "Iteration 36790: loss = 4.9108113e-08,0.10380694\n",
      "Iteration 36795: loss = 5.003763e-08,0.0976453\n",
      "Iteration 36800: loss = 5.095075e-08,0.09181806\n",
      "Iteration 36805: loss = 5.186782e-08,0.086242534\n",
      "Iteration 36810: loss = 5.2771398e-08,0.08099823\n",
      "Iteration 36815: loss = 5.3633656e-08,0.076139286\n",
      "Iteration 36820: loss = 5.4479163e-08,0.07159791\n",
      "Iteration 36825: loss = 5.5316693e-08,0.06725194\n",
      "Iteration 36830: loss = 5.610467e-08,0.06330539\n",
      "Iteration 36835: loss = 5.688705e-08,0.059549954\n",
      "Iteration 36840: loss = 5.7664e-08,0.055975646\n",
      "Iteration 36845: loss = 5.8391723e-08,0.052726988\n",
      "Iteration 36850: loss = 5.9103716e-08,0.049669947\n",
      "Iteration 36855: loss = 5.977625e-08,0.046904065\n",
      "Iteration 36860: loss = 6.044092e-08,0.04423225\n",
      "Iteration 36865: loss = 6.1096934e-08,0.041750945\n",
      "Iteration 36870: loss = 6.1693164e-08,0.039490517\n",
      "Iteration 36875: loss = 6.231931e-08,0.03727524\n",
      "Iteration 36880: loss = 6.288062e-08,0.03529077\n",
      "Iteration 36885: loss = 6.3448425e-08,0.03338546\n",
      "Iteration 36890: loss = 6.400958e-08,0.03156613\n",
      "Iteration 36895: loss = 6.4497165e-08,0.029999379\n",
      "Iteration 36900: loss = 6.503233e-08,0.028414993\n",
      "Iteration 36905: loss = 6.5538174e-08,0.026930835\n",
      "Iteration 36910: loss = 6.5987834e-08,0.025630176\n",
      "Iteration 36915: loss = 6.6473945e-08,0.0243045\n",
      "Iteration 36920: loss = 6.692384e-08,0.023101207\n",
      "Iteration 36925: loss = 6.731887e-08,0.022047086\n",
      "Iteration 36930: loss = 6.772934e-08,0.021012878\n",
      "Iteration 36935: loss = 6.8103866e-08,0.020076431\n",
      "Iteration 36940: loss = 6.8495325e-08,0.01914939\n",
      "Iteration 36945: loss = 6.8868204e-08,0.018287936\n",
      "Iteration 36950: loss = 6.921987e-08,0.017506141\n",
      "Iteration 36955: loss = 6.958434e-08,0.016760206\n",
      "Iteration 36960: loss = 6.98169e-08,0.016559007\n",
      "Iteration 36965: loss = 7.038846e-08,0.018296039\n",
      "Iteration 36970: loss = 6.989648e-08,0.04673963\n",
      "Iteration 36975: loss = 7.220785e-08,0.22457096\n",
      "Iteration 36980: loss = 6.933504e-08,0.035663925\n",
      "Iteration 36985: loss = 6.792624e-08,0.09363623\n",
      "Iteration 36990: loss = 6.749383e-08,0.07017185\n",
      "Iteration 36995: loss = 6.7821496e-08,0.037911594\n",
      "Iteration 37000: loss = 6.854575e-08,0.021080188\n",
      "Iteration 37005: loss = 6.9274556e-08,0.016537923\n",
      "Iteration 37010: loss = 6.983558e-08,0.016929304\n",
      "Iteration 37015: loss = 7.029954e-08,0.017254308\n",
      "Iteration 37020: loss = 7.074376e-08,0.015954291\n",
      "Iteration 37025: loss = 7.12053e-08,0.013814678\n",
      "Iteration 37030: loss = 7.1645985e-08,0.012193646\n",
      "Iteration 37035: loss = 7.201394e-08,0.011484835\n",
      "Iteration 37040: loss = 7.235432e-08,0.011109015\n",
      "Iteration 37045: loss = 7.273446e-08,0.010494186\n",
      "Iteration 37050: loss = 7.307501e-08,0.009845487\n",
      "Iteration 37055: loss = 7.337527e-08,0.0093831085\n",
      "Iteration 37060: loss = 7.3610686e-08,0.009071612\n",
      "Iteration 37065: loss = 7.382596e-08,0.008745429\n",
      "Iteration 37070: loss = 7.401798e-08,0.00846365\n",
      "Iteration 37075: loss = 7.423281e-08,0.008181629\n",
      "Iteration 37080: loss = 7.4406834e-08,0.007939564\n",
      "Iteration 37085: loss = 7.4560326e-08,0.007727317\n",
      "Iteration 37090: loss = 7.473296e-08,0.007506863\n",
      "Iteration 37095: loss = 7.4886344e-08,0.0073051173\n",
      "Iteration 37100: loss = 7.505765e-08,0.0070989295\n",
      "Iteration 37105: loss = 7.52327e-08,0.006887282\n",
      "Iteration 37110: loss = 7.540549e-08,0.0066882824\n",
      "Iteration 37115: loss = 7.557679e-08,0.0064967684\n",
      "Iteration 37120: loss = 7.5712926e-08,0.0063332594\n",
      "Iteration 37125: loss = 7.586093e-08,0.0061750202\n",
      "Iteration 37130: loss = 7.599422e-08,0.0060251392\n",
      "Iteration 37135: loss = 7.613594e-08,0.005874734\n",
      "Iteration 37140: loss = 7.6289595e-08,0.0057158945\n",
      "Iteration 37145: loss = 7.641649e-08,0.0055902377\n",
      "Iteration 37150: loss = 7.650655e-08,0.0054897335\n",
      "Iteration 37155: loss = 7.6615834e-08,0.0053785807\n",
      "Iteration 37160: loss = 7.6744726e-08,0.0052574454\n",
      "Iteration 37165: loss = 7.681829e-08,0.0051623285\n",
      "Iteration 37170: loss = 7.692469e-08,0.005070187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37175: loss = 7.7033455e-08,0.0049687354\n",
      "Iteration 37180: loss = 7.712398e-08,0.0048793955\n",
      "Iteration 37185: loss = 7.7211546e-08,0.0047978265\n",
      "Iteration 37190: loss = 7.732197e-08,0.0047049862\n",
      "Iteration 37195: loss = 7.73881e-08,0.0046884385\n",
      "Iteration 37200: loss = 7.762659e-08,0.0050109862\n",
      "Iteration 37205: loss = 7.723563e-08,0.01131053\n",
      "Iteration 37210: loss = 7.8855614e-08,0.08861148\n",
      "Iteration 37215: loss = 7.448902e-08,0.31151736\n",
      "Iteration 37220: loss = 7.377835e-08,0.10615356\n",
      "Iteration 37225: loss = 7.29953e-08,0.030783184\n",
      "Iteration 37230: loss = 7.267763e-08,0.013860264\n",
      "Iteration 37235: loss = 7.3062466e-08,0.0096640475\n",
      "Iteration 37240: loss = 7.3689e-08,0.008077855\n",
      "Iteration 37245: loss = 7.423389e-08,0.0073369993\n",
      "Iteration 37250: loss = 7.470721e-08,0.0069005336\n",
      "Iteration 37255: loss = 7.5184666e-08,0.00653908\n",
      "Iteration 37260: loss = 7.571358e-08,0.006145837\n",
      "Iteration 37265: loss = 7.624464e-08,0.0057510682\n",
      "Iteration 37270: loss = 7.666544e-08,0.005398269\n",
      "Iteration 37275: loss = 7.697213e-08,0.00508856\n",
      "Iteration 37280: loss = 7.7214445e-08,0.004788118\n",
      "Iteration 37285: loss = 7.7389025e-08,0.0045290277\n",
      "Iteration 37290: loss = 7.751965e-08,0.004324816\n",
      "Iteration 37295: loss = 7.758834e-08,0.0042055673\n",
      "Iteration 37300: loss = 7.7651826e-08,0.0041381777\n",
      "Iteration 37305: loss = 7.771688e-08,0.0040905015\n",
      "Iteration 37310: loss = 7.782379e-08,0.0040191896\n",
      "Iteration 37315: loss = 7.789844e-08,0.0039487635\n",
      "Iteration 37320: loss = 7.800381e-08,0.0038715936\n",
      "Iteration 37325: loss = 7.8094715e-08,0.0038016385\n",
      "Iteration 37330: loss = 7.8181195e-08,0.0037433137\n",
      "Iteration 37335: loss = 7.825087e-08,0.0036892565\n",
      "Iteration 37340: loss = 7.83571e-08,0.0036223996\n",
      "Iteration 37345: loss = 7.842621e-08,0.003571241\n",
      "Iteration 37350: loss = 7.849924e-08,0.0035161912\n",
      "Iteration 37355: loss = 7.862789e-08,0.0034429533\n",
      "Iteration 37360: loss = 7.8713704e-08,0.003391538\n",
      "Iteration 37365: loss = 7.87401e-08,0.0033645825\n",
      "Iteration 37370: loss = 7.878166e-08,0.0033364543\n",
      "Iteration 37375: loss = 7.882957e-08,0.003300536\n",
      "Iteration 37380: loss = 7.891591e-08,0.0032514245\n",
      "Iteration 37385: loss = 7.8984165e-08,0.003209079\n",
      "Iteration 37390: loss = 7.903409e-08,0.003172655\n",
      "Iteration 37395: loss = 7.914157e-08,0.0031180808\n",
      "Iteration 37400: loss = 7.9205456e-08,0.003082991\n",
      "Iteration 37405: loss = 7.9252274e-08,0.0030526982\n",
      "Iteration 37410: loss = 7.9296996e-08,0.0030246547\n",
      "Iteration 37415: loss = 7.9342264e-08,0.0029963779\n",
      "Iteration 37420: loss = 7.938988e-08,0.0029659898\n",
      "Iteration 37425: loss = 7.945237e-08,0.0029352554\n",
      "Iteration 37430: loss = 7.949401e-08,0.0029122946\n",
      "Iteration 37435: loss = 7.953887e-08,0.002886259\n",
      "Iteration 37440: loss = 7.95857e-08,0.0028584236\n",
      "Iteration 37445: loss = 7.965e-08,0.0028280169\n",
      "Iteration 37450: loss = 7.969161e-08,0.0028068367\n",
      "Iteration 37455: loss = 7.97209e-08,0.0027853407\n",
      "Iteration 37460: loss = 7.975975e-08,0.002774624\n",
      "Iteration 37465: loss = 7.983295e-08,0.0027793176\n",
      "Iteration 37470: loss = 7.975877e-08,0.0031045857\n",
      "Iteration 37475: loss = 8.014526e-08,0.005602181\n",
      "Iteration 37480: loss = 7.898536e-08,0.03184363\n",
      "Iteration 37485: loss = 8.190731e-08,0.21140397\n",
      "Iteration 37490: loss = 7.716247e-08,0.028655613\n",
      "Iteration 37495: loss = 7.494261e-08,0.08422946\n",
      "Iteration 37500: loss = 7.435887e-08,0.057762377\n",
      "Iteration 37505: loss = 7.485101e-08,0.02569355\n",
      "Iteration 37510: loss = 7.574558e-08,0.0091492785\n",
      "Iteration 37515: loss = 7.65126e-08,0.0045332867\n",
      "Iteration 37520: loss = 7.714407e-08,0.0050070006\n",
      "Iteration 37525: loss = 7.765484e-08,0.0059855217\n",
      "Iteration 37530: loss = 7.8122156e-08,0.0057034423\n",
      "Iteration 37535: loss = 7.8500655e-08,0.004465255\n",
      "Iteration 37540: loss = 7.876782e-08,0.0033672994\n",
      "Iteration 37545: loss = 7.8939955e-08,0.0029836854\n",
      "Iteration 37550: loss = 7.903443e-08,0.003056546\n",
      "Iteration 37555: loss = 7.914273e-08,0.003063576\n",
      "Iteration 37560: loss = 7.92326e-08,0.0029048973\n",
      "Iteration 37565: loss = 7.930458e-08,0.0027913149\n",
      "Iteration 37570: loss = 7.937076e-08,0.0027808158\n",
      "Iteration 37575: loss = 7.943477e-08,0.002751205\n",
      "Iteration 37580: loss = 7.9500126e-08,0.002697872\n",
      "Iteration 37585: loss = 7.95406e-08,0.0026806975\n",
      "Iteration 37590: loss = 7.9589405e-08,0.0026540004\n",
      "Iteration 37595: loss = 7.965531e-08,0.0026196765\n",
      "Iteration 37600: loss = 7.974226e-08,0.0025863394\n",
      "Iteration 37605: loss = 7.9806625e-08,0.002557668\n",
      "Iteration 37610: loss = 7.983251e-08,0.0025404093\n",
      "Iteration 37615: loss = 7.989666e-08,0.0025148157\n",
      "Iteration 37620: loss = 7.994189e-08,0.002493377\n",
      "Iteration 37625: loss = 7.996735e-08,0.0024774414\n",
      "Iteration 37630: loss = 8.00084e-08,0.0024603067\n",
      "Iteration 37635: loss = 8.005222e-08,0.0024411846\n",
      "Iteration 37640: loss = 8.01194e-08,0.0024147523\n",
      "Iteration 37645: loss = 8.014857e-08,0.0023964804\n",
      "Iteration 37650: loss = 8.021306e-08,0.0023734781\n",
      "Iteration 37655: loss = 8.025713e-08,0.0023551947\n",
      "Iteration 37660: loss = 8.0284615e-08,0.0023392814\n",
      "Iteration 37665: loss = 8.032922e-08,0.0023215911\n",
      "Iteration 37670: loss = 8.037e-08,0.0023066923\n",
      "Iteration 37675: loss = 8.0370086e-08,0.0023005544\n",
      "Iteration 37680: loss = 8.035314e-08,0.0022961437\n",
      "Iteration 37685: loss = 8.0395516e-08,0.002280974\n",
      "Iteration 37690: loss = 8.043921e-08,0.0022660643\n",
      "Iteration 37695: loss = 8.048601e-08,0.0022556786\n",
      "Iteration 37700: loss = 8.046126e-08,0.0023142488\n",
      "Iteration 37705: loss = 8.066305e-08,0.0027567009\n",
      "Iteration 37710: loss = 8.012447e-08,0.008074526\n",
      "Iteration 37715: loss = 8.1911615e-08,0.06665331\n",
      "Iteration 37720: loss = 5.3828526e-08,0.34387034\n",
      "Iteration 37725: loss = 1.4965925e-08,0.6586018\n",
      "Iteration 37730: loss = 7.521911e-09,0.95154107\n",
      "Iteration 37735: loss = 5.4293725e-09,1.0982846\n",
      "Iteration 37740: loss = 5.0069793e-09,1.1305101\n",
      "Iteration 37745: loss = 5.4953073e-09,1.0850822\n",
      "Iteration 37750: loss = 6.7843815e-09,0.98467326\n",
      "Iteration 37755: loss = 9.078433e-09,0.8435316\n",
      "Iteration 37760: loss = 1.2643625e-08,0.6794952\n",
      "Iteration 37765: loss = 1.7722995e-08,0.5129367\n",
      "Iteration 37770: loss = 2.4363109e-08,0.36202407\n",
      "Iteration 37775: loss = 3.2257436e-08,0.23955227\n",
      "Iteration 37780: loss = 4.0612615e-08,0.15094972\n",
      "Iteration 37785: loss = 4.8362477e-08,0.09360552\n",
      "Iteration 37790: loss = 5.461501e-08,0.06004228\n",
      "Iteration 37795: loss = 5.8960165e-08,0.042131267\n",
      "Iteration 37800: loss = 6.1574084e-08,0.033144597\n",
      "Iteration 37805: loss = 6.3011406e-08,0.028585875\n",
      "Iteration 37810: loss = 6.371024e-08,0.026288671\n",
      "Iteration 37815: loss = 6.396803e-08,0.025247304\n",
      "Iteration 37820: loss = 6.3966404e-08,0.024916196\n",
      "Iteration 37825: loss = 6.394e-08,0.02467958\n",
      "Iteration 37830: loss = 6.3921156e-08,0.024419807\n",
      "Iteration 37835: loss = 6.4016525e-08,0.023921136\n",
      "Iteration 37840: loss = 6.423377e-08,0.023137385\n",
      "Iteration 37845: loss = 6.4541496e-08,0.02213429\n",
      "Iteration 37850: loss = 6.501472e-08,0.020842636\n",
      "Iteration 37855: loss = 6.5521995e-08,0.019495076\n",
      "Iteration 37860: loss = 6.601324e-08,0.018238496\n",
      "Iteration 37865: loss = 6.6523064e-08,0.017014755\n",
      "Iteration 37870: loss = 6.7014646e-08,0.015885178\n",
      "Iteration 37875: loss = 6.748784e-08,0.014844788\n",
      "Iteration 37880: loss = 6.7966404e-08,0.013835428\n",
      "Iteration 37885: loss = 6.842149e-08,0.012928603\n",
      "Iteration 37890: loss = 6.885306e-08,0.012113829\n",
      "Iteration 37895: loss = 6.924746e-08,0.011389398\n",
      "Iteration 37900: loss = 6.96337e-08,0.010707852\n",
      "Iteration 37905: loss = 6.9986264e-08,0.0101136\n",
      "Iteration 37910: loss = 7.030684e-08,0.009569904\n",
      "Iteration 37915: loss = 7.064254e-08,0.009043741\n",
      "Iteration 37920: loss = 7.097722e-08,0.008545367\n",
      "Iteration 37925: loss = 7.123388e-08,0.008154569\n",
      "Iteration 37930: loss = 7.151045e-08,0.007758538\n",
      "Iteration 37935: loss = 7.180324e-08,0.0073707863\n",
      "Iteration 37940: loss = 7.203655e-08,0.007063346\n",
      "Iteration 37945: loss = 7.2352364e-08,0.0067140325\n",
      "Iteration 37950: loss = 7.244097e-08,0.006822524\n",
      "Iteration 37955: loss = 7.2999505e-08,0.00837391\n",
      "Iteration 37960: loss = 7.205622e-08,0.030197164\n",
      "Iteration 37965: loss = 7.513511e-08,0.18662228\n",
      "Iteration 37970: loss = 6.960724e-08,0.06349059\n",
      "Iteration 37975: loss = 6.728723e-08,0.10535554\n",
      "Iteration 37980: loss = 6.6772266e-08,0.05990399\n",
      "Iteration 37985: loss = 6.739279e-08,0.025676023\n",
      "Iteration 37990: loss = 6.8342125e-08,0.012615867\n",
      "Iteration 37995: loss = 6.921925e-08,0.010644382\n",
      "Iteration 38000: loss = 7.0006784e-08,0.0114494655\n",
      "Iteration 38005: loss = 7.0827575e-08,0.011165223\n",
      "Iteration 38010: loss = 7.157221e-08,0.009400821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38015: loss = 7.216424e-08,0.0073124915\n",
      "Iteration 38020: loss = 7.2586026e-08,0.005978351\n",
      "Iteration 38025: loss = 7.288431e-08,0.005566607\n",
      "Iteration 38030: loss = 7.3143106e-08,0.00546931\n",
      "Iteration 38035: loss = 7.3380406e-08,0.005221886\n",
      "Iteration 38040: loss = 7.359629e-08,0.004872774\n",
      "Iteration 38045: loss = 7.383054e-08,0.0046161213\n",
      "Iteration 38050: loss = 7.404653e-08,0.00445502\n",
      "Iteration 38055: loss = 7.422117e-08,0.0042921174\n",
      "Iteration 38060: loss = 7.437011e-08,0.0041470015\n",
      "Iteration 38065: loss = 7.449899e-08,0.004043629\n",
      "Iteration 38070: loss = 7.4673586e-08,0.0039042153\n",
      "Iteration 38075: loss = 7.482805e-08,0.0037791338\n",
      "Iteration 38080: loss = 7.497969e-08,0.0036695343\n",
      "Iteration 38085: loss = 7.512771e-08,0.0035655338\n",
      "Iteration 38090: loss = 7.525483e-08,0.0034780202\n",
      "Iteration 38095: loss = 7.5363026e-08,0.0034009537\n",
      "Iteration 38100: loss = 7.5492835e-08,0.0033150169\n",
      "Iteration 38105: loss = 7.560604e-08,0.0032366735\n",
      "Iteration 38110: loss = 7.573539e-08,0.0031571146\n",
      "Iteration 38115: loss = 7.58058e-08,0.0031035922\n",
      "Iteration 38120: loss = 7.5889076e-08,0.003052609\n",
      "Iteration 38125: loss = 7.5976224e-08,0.0029980307\n",
      "Iteration 38130: loss = 7.606382e-08,0.0029443812\n",
      "Iteration 38135: loss = 7.614194e-08,0.0028986398\n",
      "Iteration 38140: loss = 7.619138e-08,0.0028569847\n",
      "Iteration 38145: loss = 7.62769e-08,0.0028094514\n",
      "Iteration 38150: loss = 7.632514e-08,0.0027745853\n",
      "Iteration 38155: loss = 7.6429835e-08,0.002723277\n",
      "Iteration 38160: loss = 7.649589e-08,0.002685315\n",
      "Iteration 38165: loss = 7.6560106e-08,0.0026500765\n",
      "Iteration 38170: loss = 7.66255e-08,0.0026153254\n",
      "Iteration 38175: loss = 7.669484e-08,0.0025803745\n",
      "Iteration 38180: loss = 7.6696644e-08,0.0025924954\n",
      "Iteration 38185: loss = 7.687344e-08,0.0027364022\n",
      "Iteration 38190: loss = 7.651457e-08,0.00515768\n",
      "Iteration 38195: loss = 7.7935226e-08,0.032635976\n",
      "Iteration 38200: loss = 7.305767e-08,0.25061917\n",
      "Iteration 38205: loss = 7.3849314e-08,0.010267963\n",
      "Iteration 38210: loss = 7.2175e-08,0.056236\n",
      "Iteration 38215: loss = 7.1116936e-08,0.055749454\n",
      "Iteration 38220: loss = 7.114016e-08,0.040776026\n",
      "Iteration 38225: loss = 7.156811e-08,0.027470063\n",
      "Iteration 38230: loss = 7.207939e-08,0.01817461\n",
      "Iteration 38235: loss = 7.278391e-08,0.011922807\n",
      "Iteration 38240: loss = 7.364494e-08,0.0077660657\n",
      "Iteration 38245: loss = 7.4399416e-08,0.0052105244\n",
      "Iteration 38250: loss = 7.492197e-08,0.0038173562\n",
      "Iteration 38255: loss = 7.5266755e-08,0.0031738852\n",
      "Iteration 38260: loss = 7.545763e-08,0.0029777535\n",
      "Iteration 38265: loss = 7.556546e-08,0.0029696692\n",
      "Iteration 38270: loss = 7.565026e-08,0.0029757067\n",
      "Iteration 38275: loss = 7.5717104e-08,0.002931362\n",
      "Iteration 38280: loss = 7.584743e-08,0.0028118256\n",
      "Iteration 38285: loss = 7.5981745e-08,0.0026900535\n",
      "Iteration 38290: loss = 7.612254e-08,0.002604687\n",
      "Iteration 38295: loss = 7.625273e-08,0.002549901\n",
      "Iteration 38300: loss = 7.636367e-08,0.0025014991\n",
      "Iteration 38305: loss = 7.649322e-08,0.0024416784\n",
      "Iteration 38310: loss = 7.656274e-08,0.0024017082\n",
      "Iteration 38315: loss = 7.666848e-08,0.0023600017\n",
      "Iteration 38320: loss = 7.674038e-08,0.0023252838\n",
      "Iteration 38325: loss = 7.684594e-08,0.002284948\n",
      "Iteration 38330: loss = 7.691605e-08,0.0022530858\n",
      "Iteration 38335: loss = 7.6983184e-08,0.0022248041\n",
      "Iteration 38340: loss = 7.702713e-08,0.0022036547\n",
      "Iteration 38345: loss = 7.708929e-08,0.0021804248\n",
      "Iteration 38350: loss = 7.7114265e-08,0.002164164\n",
      "Iteration 38355: loss = 7.7160706e-08,0.0021429174\n",
      "Iteration 38360: loss = 7.7221785e-08,0.0021215659\n",
      "Iteration 38365: loss = 7.725095e-08,0.0021033196\n",
      "Iteration 38370: loss = 7.731243e-08,0.002082446\n",
      "Iteration 38375: loss = 7.73552e-08,0.00206526\n",
      "Iteration 38380: loss = 7.737937e-08,0.0020512051\n",
      "Iteration 38385: loss = 7.744499e-08,0.0020288287\n",
      "Iteration 38390: loss = 7.74886e-08,0.002012163\n",
      "Iteration 38395: loss = 7.751424e-08,0.0019977167\n",
      "Iteration 38400: loss = 7.755606e-08,0.0019825161\n",
      "Iteration 38405: loss = 7.760152e-08,0.0019653128\n",
      "Iteration 38410: loss = 7.7642646e-08,0.001951131\n",
      "Iteration 38415: loss = 7.764802e-08,0.0019411201\n",
      "Iteration 38420: loss = 7.766692e-08,0.001931868\n",
      "Iteration 38425: loss = 7.770752e-08,0.001918342\n",
      "Iteration 38430: loss = 7.771389e-08,0.0019081009\n",
      "Iteration 38435: loss = 7.773491e-08,0.0018979736\n",
      "Iteration 38440: loss = 7.7738406e-08,0.001890541\n",
      "Iteration 38445: loss = 7.782388e-08,0.001875873\n",
      "Iteration 38450: loss = 7.776065e-08,0.0019450446\n",
      "Iteration 38455: loss = 7.804796e-08,0.0026052985\n",
      "Iteration 38460: loss = 7.713654e-08,0.012011403\n",
      "Iteration 38465: loss = 6.812492e-08,0.13740392\n",
      "Iteration 38470: loss = 4.792369e-08,0.3029316\n",
      "Iteration 38475: loss = 4.2805496e-08,0.26238945\n",
      "Iteration 38480: loss = 4.29563e-08,0.18615516\n",
      "Iteration 38485: loss = 4.641385e-08,0.12676291\n",
      "Iteration 38490: loss = 5.1834817e-08,0.07908134\n",
      "Iteration 38495: loss = 5.80273e-08,0.044612665\n",
      "Iteration 38500: loss = 6.395913e-08,0.023135746\n",
      "Iteration 38505: loss = 6.8663475e-08,0.011928754\n",
      "Iteration 38510: loss = 7.159323e-08,0.006949892\n",
      "Iteration 38515: loss = 7.302219e-08,0.0048395367\n",
      "Iteration 38520: loss = 7.35747e-08,0.0039442875\n",
      "Iteration 38525: loss = 7.37262e-08,0.0035688179\n",
      "Iteration 38530: loss = 7.380563e-08,0.0034008818\n",
      "Iteration 38535: loss = 7.38041e-08,0.0033650661\n",
      "Iteration 38540: loss = 7.378374e-08,0.0033732953\n",
      "Iteration 38545: loss = 7.391254e-08,0.0032982598\n",
      "Iteration 38550: loss = 7.41313e-08,0.0031555467\n",
      "Iteration 38555: loss = 7.446354e-08,0.0029626347\n",
      "Iteration 38560: loss = 7.4748186e-08,0.002785757\n",
      "Iteration 38565: loss = 7.4970366e-08,0.0026543692\n",
      "Iteration 38570: loss = 7.516453e-08,0.0025563808\n",
      "Iteration 38575: loss = 7.5338356e-08,0.002476058\n",
      "Iteration 38580: loss = 7.549185e-08,0.0024060938\n",
      "Iteration 38585: loss = 7.56001e-08,0.0023546657\n",
      "Iteration 38590: loss = 7.569166e-08,0.0023082073\n",
      "Iteration 38595: loss = 7.579883e-08,0.0022628715\n",
      "Iteration 38600: loss = 7.5884174e-08,0.0022261266\n",
      "Iteration 38605: loss = 7.59516e-08,0.0021934952\n",
      "Iteration 38610: loss = 7.601849e-08,0.0021628863\n",
      "Iteration 38615: loss = 7.6096896e-08,0.0021292898\n",
      "Iteration 38620: loss = 7.616679e-08,0.0020977792\n",
      "Iteration 38625: loss = 7.6227785e-08,0.0020744854\n",
      "Iteration 38630: loss = 7.627327e-08,0.0020523295\n",
      "Iteration 38635: loss = 7.6379756e-08,0.002017634\n",
      "Iteration 38640: loss = 7.645223e-08,0.0019882205\n",
      "Iteration 38645: loss = 7.651291e-08,0.001967348\n",
      "Iteration 38650: loss = 7.651952e-08,0.0019550251\n",
      "Iteration 38655: loss = 7.658302e-08,0.0019332842\n",
      "Iteration 38660: loss = 7.662886e-08,0.0019143237\n",
      "Iteration 38665: loss = 7.669181e-08,0.0018941222\n",
      "Iteration 38670: loss = 7.671515e-08,0.001880844\n",
      "Iteration 38675: loss = 7.675832e-08,0.0018647081\n",
      "Iteration 38680: loss = 7.682268e-08,0.001844866\n",
      "Iteration 38685: loss = 7.684825e-08,0.0018314059\n",
      "Iteration 38690: loss = 7.689004e-08,0.0018167397\n",
      "Iteration 38695: loss = 7.693658e-08,0.0018000442\n",
      "Iteration 38700: loss = 7.695909e-08,0.0017886426\n",
      "Iteration 38705: loss = 7.698364e-08,0.0017765231\n",
      "Iteration 38710: loss = 7.7002e-08,0.0017680691\n",
      "Iteration 38715: loss = 7.704528e-08,0.0017544387\n",
      "Iteration 38720: loss = 7.7047666e-08,0.0017501556\n",
      "Iteration 38725: loss = 7.713627e-08,0.0017503846\n",
      "Iteration 38730: loss = 7.7025874e-08,0.0019308544\n",
      "Iteration 38735: loss = 7.7490576e-08,0.003415651\n",
      "Iteration 38740: loss = 7.602123e-08,0.020481518\n",
      "Iteration 38745: loss = 6.643028e-08,0.18394211\n",
      "Iteration 38750: loss = 5.3698965e-08,0.13343476\n",
      "Iteration 38755: loss = 5.0778496e-08,0.16928658\n",
      "Iteration 38760: loss = 5.2278725e-08,0.112015106\n",
      "Iteration 38765: loss = 5.6129494e-08,0.062221937\n",
      "Iteration 38770: loss = 6.0849594e-08,0.03109878\n",
      "Iteration 38775: loss = 6.553436e-08,0.014284288\n",
      "Iteration 38780: loss = 6.950841e-08,0.006658819\n",
      "Iteration 38785: loss = 7.224898e-08,0.004148626\n",
      "Iteration 38790: loss = 7.360163e-08,0.0037516495\n",
      "Iteration 38795: loss = 7.4134874e-08,0.003727984\n",
      "Iteration 38800: loss = 7.4218825e-08,0.003547974\n",
      "Iteration 38805: loss = 7.4212856e-08,0.0031745997\n",
      "Iteration 38810: loss = 7.4207584e-08,0.002800562\n",
      "Iteration 38815: loss = 7.4207605e-08,0.0025943406\n",
      "Iteration 38820: loss = 7.43108e-08,0.002522796\n",
      "Iteration 38825: loss = 7.4491176e-08,0.0024669834\n",
      "Iteration 38830: loss = 7.476948e-08,0.0023449282\n",
      "Iteration 38835: loss = 7.503173e-08,0.0022036068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38840: loss = 7.527199e-08,0.0020991166\n",
      "Iteration 38845: loss = 7.546405e-08,0.0020391517\n",
      "Iteration 38850: loss = 7.55556e-08,0.002000359\n",
      "Iteration 38855: loss = 7.562527e-08,0.001964836\n",
      "Iteration 38860: loss = 7.5709096e-08,0.0019352775\n",
      "Iteration 38865: loss = 7.5777244e-08,0.0019083152\n",
      "Iteration 38870: loss = 7.586362e-08,0.0018776037\n",
      "Iteration 38875: loss = 7.590773e-08,0.0018586614\n",
      "Iteration 38880: loss = 7.599093e-08,0.0018327885\n",
      "Iteration 38885: loss = 7.606172e-08,0.0018070539\n",
      "Iteration 38890: loss = 7.6119164e-08,0.0017861531\n",
      "Iteration 38895: loss = 7.616307e-08,0.0017692172\n",
      "Iteration 38900: loss = 7.620964e-08,0.0017511481\n",
      "Iteration 38905: loss = 7.627134e-08,0.001733266\n",
      "Iteration 38910: loss = 7.6293816e-08,0.001721388\n",
      "Iteration 38915: loss = 7.6338644e-08,0.0017055199\n",
      "Iteration 38920: loss = 7.63829e-08,0.001690527\n",
      "Iteration 38925: loss = 7.643015e-08,0.001674119\n",
      "Iteration 38930: loss = 7.649396e-08,0.001657333\n",
      "Iteration 38935: loss = 7.6540466e-08,0.0016421434\n",
      "Iteration 38940: loss = 7.6580385e-08,0.0016306071\n",
      "Iteration 38945: loss = 7.660386e-08,0.0016197133\n",
      "Iteration 38950: loss = 7.662622e-08,0.0016096727\n",
      "Iteration 38955: loss = 7.66317e-08,0.0016006496\n",
      "Iteration 38960: loss = 7.665151e-08,0.0015924063\n",
      "Iteration 38965: loss = 7.6697404e-08,0.0015807457\n",
      "Iteration 38970: loss = 7.6690576e-08,0.0015889616\n",
      "Iteration 38975: loss = 7.683044e-08,0.0016489874\n",
      "Iteration 38980: loss = 7.654102e-08,0.002407638\n",
      "Iteration 38985: loss = 7.757827e-08,0.009526531\n",
      "Iteration 38990: loss = 1.3130665e-08,0.7417572\n",
      "Iteration 38995: loss = 2.4539435e-09,1.6208594\n",
      "Iteration 39000: loss = 9.902957e-10,1.6608803\n",
      "Iteration 39005: loss = 6.793244e-10,1.6689833\n",
      "Iteration 39010: loss = 6.428908e-10,1.6749924\n",
      "Iteration 39015: loss = 7.342264e-10,1.6492553\n",
      "Iteration 39020: loss = 9.2675373e-10,1.5930065\n",
      "Iteration 39025: loss = 1.2545452e-09,1.5094576\n",
      "Iteration 39030: loss = 1.8499685e-09,1.3918061\n",
      "Iteration 39035: loss = 2.9144356e-09,1.2384406\n",
      "Iteration 39040: loss = 4.6277076e-09,1.0642748\n",
      "Iteration 39045: loss = 7.077766e-09,0.89093876\n",
      "Iteration 39050: loss = 9.974129e-09,0.743336\n",
      "Iteration 39055: loss = 1.2805398e-08,0.63269323\n",
      "Iteration 39060: loss = 1.5193125e-08,0.55535746\n",
      "Iteration 39065: loss = 1.6959989e-08,0.50424224\n",
      "Iteration 39070: loss = 1.821394e-08,0.4696893\n",
      "Iteration 39075: loss = 1.9322068e-08,0.44046313\n",
      "Iteration 39080: loss = 2.0557167e-08,0.4101047\n",
      "Iteration 39085: loss = 2.2119439e-08,0.3754237\n",
      "Iteration 39090: loss = 2.4118124e-08,0.3354504\n",
      "Iteration 39095: loss = 2.6585184e-08,0.29172868\n",
      "Iteration 39100: loss = 2.9481773e-08,0.24702056\n",
      "Iteration 39105: loss = 3.268635e-08,0.20421977\n",
      "Iteration 39110: loss = 3.610189e-08,0.16534093\n",
      "Iteration 39115: loss = 3.9582613e-08,0.13168032\n",
      "Iteration 39120: loss = 4.2981537e-08,0.10380436\n",
      "Iteration 39125: loss = 4.616237e-08,0.081515744\n",
      "Iteration 39130: loss = 4.9126566e-08,0.063819125\n",
      "Iteration 39135: loss = 5.1841088e-08,0.04999312\n",
      "Iteration 39140: loss = 5.4274526e-08,0.039296567\n",
      "Iteration 39145: loss = 5.641358e-08,0.03116424\n",
      "Iteration 39150: loss = 5.8293235e-08,0.024954332\n",
      "Iteration 39155: loss = 5.99662e-08,0.020140048\n",
      "Iteration 39160: loss = 6.1413196e-08,0.016454406\n",
      "Iteration 39165: loss = 6.269188e-08,0.01359332\n",
      "Iteration 39170: loss = 6.378381e-08,0.011387443\n",
      "Iteration 39175: loss = 6.475171e-08,0.009642157\n",
      "Iteration 39180: loss = 6.559935e-08,0.008275101\n",
      "Iteration 39185: loss = 6.634821e-08,0.0071867835\n",
      "Iteration 39190: loss = 6.698414e-08,0.0063312496\n",
      "Iteration 39195: loss = 6.7522684e-08,0.0056566875\n",
      "Iteration 39200: loss = 6.801442e-08,0.005100219\n",
      "Iteration 39205: loss = 6.843376e-08,0.0046448563\n",
      "Iteration 39210: loss = 6.8805214e-08,0.0042766617\n",
      "Iteration 39215: loss = 6.918504e-08,0.0039236005\n",
      "Iteration 39220: loss = 6.945674e-08,0.00369841\n",
      "Iteration 39225: loss = 6.988997e-08,0.003525041\n",
      "Iteration 39230: loss = 6.965467e-08,0.0054285354\n",
      "Iteration 39235: loss = 7.151143e-08,0.026525913\n",
      "Iteration 39240: loss = 6.548915e-08,0.2273572\n",
      "Iteration 39245: loss = 6.565443e-08,0.030686023\n",
      "Iteration 39250: loss = 6.220948e-08,0.08273223\n",
      "Iteration 39255: loss = 6.051846e-08,0.07271535\n",
      "Iteration 39260: loss = 6.018768e-08,0.053493828\n",
      "Iteration 39265: loss = 6.0936124e-08,0.037376028\n",
      "Iteration 39270: loss = 6.251851e-08,0.025093755\n",
      "Iteration 39275: loss = 6.459177e-08,0.016089289\n",
      "Iteration 39280: loss = 6.63785e-08,0.010383041\n",
      "Iteration 39285: loss = 6.7648266e-08,0.007021637\n",
      "Iteration 39290: loss = 6.8432826e-08,0.005117628\n",
      "Iteration 39295: loss = 6.8973854e-08,0.004015894\n",
      "Iteration 39300: loss = 6.937185e-08,0.003396041\n",
      "Iteration 39305: loss = 6.964634e-08,0.0031012779\n",
      "Iteration 39310: loss = 6.989807e-08,0.0029358119\n",
      "Iteration 39315: loss = 7.0151906e-08,0.0028065443\n",
      "Iteration 39320: loss = 7.044465e-08,0.002655437\n",
      "Iteration 39325: loss = 7.074046e-08,0.002484192\n",
      "Iteration 39330: loss = 7.097632e-08,0.0023379973\n",
      "Iteration 39335: loss = 7.118632e-08,0.0022255238\n",
      "Iteration 39340: loss = 7.1356645e-08,0.0021472867\n",
      "Iteration 39345: loss = 7.148944e-08,0.0020880895\n",
      "Iteration 39350: loss = 7.159481e-08,0.0020384395\n",
      "Iteration 39355: loss = 7.1697116e-08,0.0019922315\n",
      "Iteration 39360: loss = 7.17637e-08,0.0019569541\n",
      "Iteration 39365: loss = 7.184725e-08,0.0019215632\n",
      "Iteration 39370: loss = 7.193341e-08,0.0018849765\n",
      "Iteration 39375: loss = 7.2002116e-08,0.0018522054\n",
      "Iteration 39380: loss = 7.208575e-08,0.0018208477\n",
      "Iteration 39385: loss = 7.2168916e-08,0.0017907162\n",
      "Iteration 39390: loss = 7.2234755e-08,0.0017639198\n",
      "Iteration 39395: loss = 7.2295926e-08,0.0017407911\n",
      "Iteration 39400: loss = 7.235892e-08,0.0017172112\n",
      "Iteration 39405: loss = 7.238727e-08,0.0016981426\n",
      "Iteration 39410: loss = 7.244984e-08,0.001676094\n",
      "Iteration 39415: loss = 7.2475984e-08,0.0016589905\n",
      "Iteration 39420: loss = 7.253703e-08,0.0016391408\n",
      "Iteration 39425: loss = 7.2600535e-08,0.0016186191\n",
      "Iteration 39430: loss = 7.2664484e-08,0.0015984543\n",
      "Iteration 39435: loss = 7.270558e-08,0.0015829032\n",
      "Iteration 39440: loss = 7.276859e-08,0.0015643144\n",
      "Iteration 39445: loss = 7.281566e-08,0.0015469289\n",
      "Iteration 39450: loss = 7.285783e-08,0.0015319906\n",
      "Iteration 39455: loss = 7.288137e-08,0.0015191382\n",
      "Iteration 39460: loss = 7.290165e-08,0.0015077088\n",
      "Iteration 39465: loss = 7.292656e-08,0.0014946169\n",
      "Iteration 39470: loss = 7.2950655e-08,0.0014822333\n",
      "Iteration 39475: loss = 7.3008046e-08,0.0014693887\n",
      "Iteration 39480: loss = 7.299077e-08,0.0014638318\n",
      "Iteration 39485: loss = 7.307934e-08,0.0014550998\n",
      "Iteration 39490: loss = 7.299045e-08,0.001541919\n",
      "Iteration 39495: loss = 7.341432e-08,0.0022375244\n",
      "Iteration 39500: loss = 7.215313e-08,0.010178433\n",
      "Iteration 39505: loss = 9.0198266e-10,1.7139405\n",
      "Iteration 39510: loss = 7.30696e-11,2.1818411\n",
      "Iteration 39515: loss = 3.4681185e-11,2.0371969\n",
      "Iteration 39520: loss = 3.189809e-11,1.9484646\n",
      "Iteration 39525: loss = 3.8485878e-11,1.9246489\n",
      "Iteration 39530: loss = 4.7237544e-11,1.9190623\n",
      "Iteration 39535: loss = 5.513355e-11,1.9081005\n",
      "Iteration 39540: loss = 6.449495e-11,1.8948977\n",
      "Iteration 39545: loss = 7.997417e-11,1.878259\n",
      "Iteration 39550: loss = 9.571449e-11,1.8623016\n",
      "Iteration 39555: loss = 1.09472965e-10,1.8479512\n",
      "Iteration 39560: loss = 1.256429e-10,1.8326036\n",
      "Iteration 39565: loss = 1.4517933e-10,1.8155222\n",
      "Iteration 39570: loss = 1.7317402e-10,1.7947899\n",
      "Iteration 39575: loss = 2.0958203e-10,1.7706378\n",
      "Iteration 39580: loss = 2.569016e-10,1.7428834\n",
      "Iteration 39585: loss = 3.2032513e-10,1.7101599\n",
      "Iteration 39590: loss = 3.990949e-10,1.674244\n",
      "Iteration 39595: loss = 4.9663723e-10,1.6359023\n",
      "Iteration 39600: loss = 6.165563e-10,1.5951595\n",
      "Iteration 39605: loss = 7.583761e-10,1.5533086\n",
      "Iteration 39610: loss = 9.2206304e-10,1.5112927\n",
      "Iteration 39615: loss = 1.1043787e-09,1.4702603\n",
      "Iteration 39620: loss = 1.302761e-09,1.4307735\n",
      "Iteration 39625: loss = 1.5109726e-09,1.3935956\n",
      "Iteration 39630: loss = 1.7159287e-09,1.3598701\n",
      "Iteration 39635: loss = 1.919383e-09,1.3288839\n",
      "Iteration 39640: loss = 2.1201383e-09,1.3000151\n",
      "Iteration 39645: loss = 2.3260227e-09,1.2721992\n",
      "Iteration 39650: loss = 2.538819e-09,1.2446799\n",
      "Iteration 39655: loss = 2.7792078e-09,1.2157769\n",
      "Iteration 39660: loss = 3.0420353e-09,1.1855127\n",
      "Iteration 39665: loss = 3.349853e-09,1.1525004\n",
      "Iteration 39670: loss = 3.7128334e-09,1.116295\n",
      "Iteration 39675: loss = 4.132833e-09,1.0770328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39680: loss = 4.642327e-09,1.0335233\n",
      "Iteration 39685: loss = 5.241217e-09,0.9863335\n",
      "Iteration 39690: loss = 5.959994e-09,0.93495333\n",
      "Iteration 39695: loss = 6.8104065e-09,0.87985986\n",
      "Iteration 39700: loss = 7.816797e-09,0.8210925\n",
      "Iteration 39705: loss = 9.013022e-09,0.7586806\n",
      "Iteration 39710: loss = 1.0441849e-08,0.69275635\n",
      "Iteration 39715: loss = 1.2127848e-08,0.6243755\n",
      "Iteration 39720: loss = 1.4121265e-08,0.553778\n",
      "Iteration 39725: loss = 1.6459586e-08,0.4822745\n",
      "Iteration 39730: loss = 1.9243513e-08,0.40981382\n",
      "Iteration 39735: loss = 2.2371237e-08,0.34112263\n",
      "Iteration 39740: loss = 2.6136766e-08,0.2736231\n",
      "Iteration 39745: loss = 2.962012e-08,0.22955988\n",
      "Iteration 39750: loss = 3.5153043e-08,0.22661935\n",
      "Iteration 39755: loss = 3.48528e-08,0.3017936\n",
      "Iteration 39760: loss = 3.6722923e-08,0.15213817\n",
      "Iteration 39765: loss = 3.954052e-08,0.11707275\n",
      "Iteration 39770: loss = 4.236612e-08,0.11116299\n",
      "Iteration 39775: loss = 4.5147676e-08,0.07971612\n",
      "Iteration 39780: loss = 4.8218165e-08,0.051058415\n",
      "Iteration 39785: loss = 5.153832e-08,0.036423292\n",
      "Iteration 39790: loss = 5.4834544e-08,0.026716895\n",
      "Iteration 39795: loss = 5.7928332e-08,0.016740475\n",
      "Iteration 39800: loss = 6.05002e-08,0.009962905\n",
      "Iteration 39805: loss = 6.248434e-08,0.007218151\n",
      "Iteration 39810: loss = 6.371406e-08,0.0056801676\n",
      "Iteration 39815: loss = 6.4418245e-08,0.004451005\n",
      "Iteration 39820: loss = 6.494393e-08,0.0039612516\n",
      "Iteration 39825: loss = 6.550478e-08,0.003452795\n",
      "Iteration 39830: loss = 6.609378e-08,0.0029151312\n",
      "Iteration 39835: loss = 6.6528145e-08,0.0026401244\n",
      "Iteration 39840: loss = 6.679805e-08,0.0024141439\n",
      "Iteration 39845: loss = 6.706059e-08,0.0022617332\n",
      "Iteration 39850: loss = 6.7312676e-08,0.0021037697\n",
      "Iteration 39855: loss = 6.75444e-08,0.0019825809\n",
      "Iteration 39860: loss = 6.770987e-08,0.0018897958\n",
      "Iteration 39865: loss = 6.7851666e-08,0.0018212851\n",
      "Iteration 39870: loss = 6.802205e-08,0.0017422285\n",
      "Iteration 39875: loss = 6.816937e-08,0.0016797441\n",
      "Iteration 39880: loss = 6.827043e-08,0.0016363316\n",
      "Iteration 39885: loss = 6.84169e-08,0.001582342\n",
      "Iteration 39890: loss = 6.851934e-08,0.0015434759\n",
      "Iteration 39895: loss = 6.860694e-08,0.001508509\n",
      "Iteration 39900: loss = 6.8712275e-08,0.001473749\n",
      "Iteration 39905: loss = 6.8775485e-08,0.0014487306\n",
      "Iteration 39910: loss = 6.8895794e-08,0.0014173815\n",
      "Iteration 39915: loss = 6.8922e-08,0.0014010691\n",
      "Iteration 39920: loss = 6.905039e-08,0.0013795111\n",
      "Iteration 39925: loss = 6.900691e-08,0.0014453055\n",
      "Iteration 39930: loss = 6.941881e-08,0.0019446395\n",
      "Iteration 39935: loss = 6.836191e-08,0.0073031783\n",
      "Iteration 39940: loss = 7.693722e-09,0.9137682\n",
      "Iteration 39945: loss = 1.4610327e-09,1.7172296\n",
      "Iteration 39950: loss = 8.098489e-10,1.6187533\n",
      "Iteration 39955: loss = 7.2177836e-10,1.616958\n",
      "Iteration 39960: loss = 8.2109386e-10,1.5944967\n",
      "Iteration 39965: loss = 1.0649691e-09,1.5251384\n",
      "Iteration 39970: loss = 1.4778087e-09,1.4231607\n",
      "Iteration 39975: loss = 2.1977675e-09,1.2903737\n",
      "Iteration 39980: loss = 3.239734e-09,1.1515577\n",
      "Iteration 39985: loss = 4.483707e-09,1.0301046\n",
      "Iteration 39990: loss = 5.8120517e-09,0.92955136\n",
      "Iteration 39995: loss = 7.0678396e-09,0.85038215\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    PINN_solver_pcgrad.model.load_weights('./checkpoints/pinn_solver_pcgrad')\n",
    "except:\n",
    "    optim = tf.keras.optimizers.Adam(epsilon=1e-30)\n",
    "    PINN_solver_pcgrad.train(N=N, optimizer=optim, method = 'PCG_gradient')\n",
    "    \n",
    "    PINN_solver_pcgrad.model.save_weights('./checkpoints/pinn_solver_pcgrad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_loss_log = np.vstack(PINN_solver.loss_log)\n",
    "pinn_pcgrad_loss_log = np.vstack(PINN_solver_pcgrad.loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "sns.lineplot(x = range(len(pinn_loss_log[:, 0])), y = pinn_loss_log[:, 0],  linewidth = 2, \n",
    "             label = \"$\\mathcal{L}_u$ without PCGrad\", ax = axes[0])\n",
    "sns.lineplot(x = range(len(pinn_pcgrad_loss_log[:, 0])), y = pinn_pcgrad_loss_log[:, 0],  linewidth = 2, \n",
    "             label = \"$\\mathcal{L}_u$ with PCGrad\", ax = axes[0])\n",
    "\n",
    "axes[0].legend()\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "\n",
    "sns.lineplot(x = range(len(pinn_loss_log[:, 1])), y = pinn_loss_log[:, 1],  linewidth = 2, \n",
    "             label = \"$\\mathcal{L}_f$ without PCGrad\", ax = axes[1])\n",
    "\n",
    "sns.lineplot(x = range(len(pinn_pcgrad_loss_log[:, 1])), y = pinn_pcgrad_loss_log[:, 1],  linewidth = 2, \n",
    "             label = \"$\\mathcal{L}_f$ with PCGrad\", ax = axes[1])\n",
    "\n",
    "axes[1].legend()\n",
    "axes[1].set_yscale(\"log\")\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"performance_comparison_large.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution quality comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_performance(df):\n",
    "    \n",
    "    df['system_3.5'] = df['state_1']\n",
    "    df['system_2.0'] = df['state_2']\n",
    "    df['system_1.8'] = df['state_3'] + df['state_7']\n",
    "    df['system_1.5'] = df['state_4'] + df['state_6']\n",
    "    df['system_0'] = df['state_5'] + df['state_8'] + df['state_9'] + df['state_10'] + df['state_11'] + df['state_12']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_column_names = ['state_1', 'state_2', 'state_3', 'state_4', 'state_5', 'state_6', 'state_7',\n",
    "                      'state_8', 'state_9', 'state_10', 'state_11', 'state_12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_solver_solution_df = pd.read_csv('homo_example2.4.csv')\n",
    "matlab_solver_solution_df.columns = ['time'] + state_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matlab_solver_solution_df = get_system_performance(matlab_solver_solution_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_solver_solution_df.drop(labels='time', inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_1</th>\n",
       "      <th>state_2</th>\n",
       "      <th>state_3</th>\n",
       "      <th>state_4</th>\n",
       "      <th>state_5</th>\n",
       "      <th>state_6</th>\n",
       "      <th>state_7</th>\n",
       "      <th>state_8</th>\n",
       "      <th>state_9</th>\n",
       "      <th>state_10</th>\n",
       "      <th>state_11</th>\n",
       "      <th>state_12</th>\n",
       "      <th>system_3.5</th>\n",
       "      <th>system_2.0</th>\n",
       "      <th>system_1.8</th>\n",
       "      <th>system_1.5</th>\n",
       "      <th>system_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98947</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.003901</td>\n",
       "      <td>0.003872</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>4.414700e-08</td>\n",
       "      <td>1.555500e-08</td>\n",
       "      <td>2.232800e-08</td>\n",
       "      <td>6.382400e-11</td>\n",
       "      <td>0.98947</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.97953</td>\n",
       "      <td>0.005252</td>\n",
       "      <td>0.007579</td>\n",
       "      <td>0.007458</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>4.313800e-07</td>\n",
       "      <td>1.523900e-07</td>\n",
       "      <td>2.192600e-07</td>\n",
       "      <td>7.835700e-10</td>\n",
       "      <td>0.97953</td>\n",
       "      <td>0.005252</td>\n",
       "      <td>0.007643</td>\n",
       "      <td>0.007502</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.96988</td>\n",
       "      <td>0.007703</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.010913</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>1.060200e-06</td>\n",
       "      <td>3.744100e-07</td>\n",
       "      <td>5.387400e-07</td>\n",
       "      <td>1.808400e-09</td>\n",
       "      <td>0.96988</td>\n",
       "      <td>0.007703</td>\n",
       "      <td>0.011280</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.96087</td>\n",
       "      <td>0.009961</td>\n",
       "      <td>0.014494</td>\n",
       "      <td>0.014072</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>2.442700e-06</td>\n",
       "      <td>8.729400e-07</td>\n",
       "      <td>1.266300e-06</td>\n",
       "      <td>1.058000e-08</td>\n",
       "      <td>0.96087</td>\n",
       "      <td>0.009961</td>\n",
       "      <td>0.014712</td>\n",
       "      <td>0.014222</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.95218</td>\n",
       "      <td>0.012125</td>\n",
       "      <td>0.017716</td>\n",
       "      <td>0.017086</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>4.300800e-06</td>\n",
       "      <td>1.551000e-06</td>\n",
       "      <td>2.263800e-06</td>\n",
       "      <td>2.732400e-08</td>\n",
       "      <td>0.95218</td>\n",
       "      <td>0.012125</td>\n",
       "      <td>0.018039</td>\n",
       "      <td>0.017307</td>\n",
       "      <td>0.000354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state_1   state_2   state_3   state_4   state_5   state_6   state_7  \\\n",
       "0  0.98947  0.002718  0.003901  0.003872  0.000011  0.000011  0.000016   \n",
       "1  0.97953  0.005252  0.007579  0.007458  0.000045  0.000044  0.000064   \n",
       "2  0.96988  0.007703  0.011154  0.010913  0.000089  0.000087  0.000126   \n",
       "3  0.96087  0.009961  0.014494  0.014072  0.000154  0.000150  0.000218   \n",
       "4  0.95218  0.012125  0.017716  0.017086  0.000229  0.000221  0.000323   \n",
       "\n",
       "    state_8       state_9      state_10      state_11      state_12  \\\n",
       "0  0.000005  4.414700e-08  1.555500e-08  2.232800e-08  6.382400e-11   \n",
       "1  0.000023  4.313800e-07  1.523900e-07  2.192600e-07  7.835700e-10   \n",
       "2  0.000045  1.060200e-06  3.744100e-07  5.387400e-07  1.808400e-09   \n",
       "3  0.000078  2.442700e-06  8.729400e-07  1.266300e-06  1.058000e-08   \n",
       "4  0.000117  4.300800e-06  1.551000e-06  2.263800e-06  2.732400e-08   \n",
       "\n",
       "   system_3.5  system_2.0  system_1.8  system_1.5  system_0  \n",
       "0     0.98947    0.002718    0.003917    0.003883  0.000016  \n",
       "1     0.97953    0.005252    0.007643    0.007502  0.000068  \n",
       "2     0.96988    0.007703    0.011280    0.011000  0.000136  \n",
       "3     0.96087    0.009961    0.014712    0.014222  0.000237  \n",
       "4     0.95218    0.012125    0.018039    0.017307  0.000354  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matlab_solver_solution_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare PINN solution with the solution from Matlab solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = PINN_solver.mu_x, PINN_solver.sigma_x\n",
    "\n",
    "######################################################################################\n",
    "# Test data for validating the model predictions\n",
    "n_star = 500+1\n",
    "x_star = np.linspace(lb, ub, n_star)[1:] #N_star = x_star.shape[0] \n",
    "x_star_normalized = (x_star-mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = PINN_solver.model(x_star_normalized)\n",
    "y_pred_pcgrad = PINN_solver_pcgrad.model(x_star_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred.numpy(), columns = state_column_names)\n",
    "y_pred_pcgrad_df = pd.DataFrame(y_pred_pcgrad.numpy(), columns = state_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = get_system_performance(y_pred_df)\n",
    "y_pred_pcgrad_df = get_system_performance(y_pred_pcgrad_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred_df['system_3.5'], label = r'P$\\left(G=3.5 \\right)$ [No PCGrad]', \n",
    "             linewidth = 4, color = 'green', linestyle='--', ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred_df['system_2.0'], label = r'P$\\left(G=2.0 \\right)$ [No PCGrad]',\n",
    "             linewidth = 4, color = 'blue', linestyle='--', ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred_df['system_1.8'], label = r'P$\\left(G=1.8 \\right)$ [No PCGrad]',\n",
    "             linewidth = 4, color = 'red', linestyle='--', ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred_df['system_1.5'], label = r'P$\\left(G=1.5 \\right)$ [No PCGrad]',\n",
    "             linewidth = 4, color = 'orange', linestyle='--', ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred_df['system_0'], label = r'P$\\left(G=0 \\right)$ [No PCGrad]',\n",
    "             linewidth = 4, color = 'gray', linestyle='--', ax=ax)\n",
    "\n",
    "\n",
    "sns.lineplot(x = x_star.flatten(), y = matlab_solver_solution_df['system_3.5'], label = r'P$\\left(G=3.5 \\right)$ [solver]',\n",
    "             color = 'green', alpha=0.5, dashes=True, linewidth = 4, ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = matlab_solver_solution_df['system_2.0'], label = r'P$\\left(G=2.0 \\right)$ [solver]',\n",
    "             color = 'blue', alpha=0.5, dashes=True, linewidth = 4, ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = matlab_solver_solution_df['system_1.8'], label = r'P$\\left(G=1.8 \\right)$ [solver]',\n",
    "             color = 'red', alpha=0.5, dashes=True, linewidth = 4, ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = matlab_solver_solution_df['system_1.5'], label = r'P$\\left(G=1.5 \\right)$ [solver]',\n",
    "             color = 'orange', alpha=0.5, dashes=True, linewidth = 4, ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = matlab_solver_solution_df['system_0'], label = r'P$\\left(G=0 \\right)$ [solver]',\n",
    "             color = 'gray', alpha=0.5, dashes=True, linewidth = 4, ax=ax)\n",
    "\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred_pcgrad_df['system_3.5'], label = r'P$\\left(G=3.5 \\right)$ [With PCGrad]',\n",
    "             color = 'green', linestyle='dotted', linewidth = 4, ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred_pcgrad_df['system_2.0'], label = r'P$\\left(G=2.0 \\right)$ [With PCGrad]', \n",
    "             color = 'blue', linestyle='dotted', linewidth = 4, ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred_pcgrad_df['system_1.8'], label = r'P$\\left(G=1.8 \\right)$ [With PCGrad]', \n",
    "             color = 'red', linestyle='dotted', linewidth = 4, ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred_pcgrad_df['system_1.5'], label = r'P$\\left(G=1.5 \\right)$ [With PCGrad]',\n",
    "             color = 'orange', linestyle='dotted', linewidth = 4, ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred_pcgrad_df['system_0'], label = r'P$\\left(G=0 \\right)$ [With PCGrad]',\n",
    "             color = 'gray', linestyle='dotted', linewidth = 4, ax=ax)\n",
    "\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Time (years)\", fontdict=dict(weight='bold'), fontsize=24)\n",
    "ax.set_ylabel(\"Probability\", fontdict=dict(weight='bold'), fontsize=24)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.savefig(\"comparison_large.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derive RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_diff = matlab_solver_solution_df - y_pred_df\n",
    "y_diff_pcgrad = matlab_solver_solution_df - y_pred_pcgrad_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_diff['mse'] = y_diff.apply(lambda x: x['system_3.5']**2 + x['system_2.0']**2 + x['system_1.8']**2 + \n",
    "                             x['system_1.5']**2 + x['system_0']**2, axis = 1)\n",
    "\n",
    "y_diff['rmse'] = y_diff['mse'].apply(np.sqrt)\n",
    "\n",
    "y_diff_pcgrad['mse'] = y_diff_pcgrad.apply(lambda x: x['system_3.5']**2 + x['system_2.0']**2 + x['system_1.8']**2 + \n",
    "                             x['system_1.5']**2 + x['system_0']**2, axis = 1)\n",
    "\n",
    "y_diff_pcgrad['rmse'] = y_diff_pcgrad['mse'].apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of PINN without PCGrad is 0.013387323966802\n",
      "RMSE of PINN with PCGrad is 0.0004544708077466215\n"
     ]
    }
   ],
   "source": [
    "print ('RMSE of PINN without PCGrad is', np.mean(y_diff['rmse']))\n",
    "print ('RMSE of PINN with PCGrad is', np.mean(y_diff_pcgrad['rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "027e948c137d6acde9006b592cf7cd1a70a2e52a69a7472c7b58352c83d453a1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
